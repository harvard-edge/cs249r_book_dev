<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/workflow/workflow.html" rel="next">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>🚧 DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">51aaf988</code> • 2025-10-03 14:38 UTC • <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version →</em></a></p>
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-dnn-architectures" id="toc-sec-dnn-architectures" class="nav-link active" data-scroll-target="#sec-dnn-architectures">DNN Architectures</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-dnn-architectures-overview-8d17" id="toc-sec-dnn-architectures-overview-8d17" class="nav-link" data-scroll-target="#sec-dnn-architectures-overview-8d17">Overview</a></li>
  <li><a href="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" id="toc-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="nav-link" data-scroll-target="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f">Multi-Layer Perceptrons: Dense Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-c45a" id="toc-sec-dnn-architectures-pattern-processing-needs-c45a" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-c45a">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-c012" id="toc-sec-dnn-architectures-algorithmic-structure-c012" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-c012">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#architectural-characteristics" id="toc-architectural-characteristics" class="nav-link" data-scroll-target="#architectural-characteristics">Architectural Characteristics</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-fe7e" id="toc-sec-dnn-architectures-computational-mapping-fe7e" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-fe7e">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-7a8f" id="toc-sec-dnn-architectures-system-implications-7a8f" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-7a8f">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-4900" id="toc-sec-dnn-architectures-memory-requirements-4900" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-4900">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-9cb4" id="toc-sec-dnn-architectures-computation-needs-9cb4" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-9cb4">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-fc16" id="toc-sec-dnn-architectures-data-movement-fc16" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-fc16">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" id="toc-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="nav-link" data-scroll-target="#sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c">Convolutional Neural Networks: Spatial Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-4d88" id="toc-sec-dnn-architectures-pattern-processing-needs-4d88" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-4d88">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-2aba" id="toc-sec-dnn-architectures-algorithmic-structure-2aba" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-2aba">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#architectural-characteristics-1" id="toc-architectural-characteristics-1" class="nav-link" data-scroll-target="#architectural-characteristics-1">Architectural Characteristics</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-9108" id="toc-sec-dnn-architectures-computational-mapping-9108" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-9108">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-a9f3" id="toc-sec-dnn-architectures-system-implications-a9f3" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-a9f3">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-080b" id="toc-sec-dnn-architectures-memory-requirements-080b" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-080b">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-bd0e" id="toc-sec-dnn-architectures-computation-needs-bd0e" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-bd0e">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-a44a" id="toc-sec-dnn-architectures-data-movement-a44a" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-a44a">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" id="toc-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="nav-link" data-scroll-target="#sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904">Recurrent Neural Networks: Sequential Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-479a" id="toc-sec-dnn-architectures-pattern-processing-needs-479a" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-479a">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-274d" id="toc-sec-dnn-architectures-algorithmic-structure-274d" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-274d">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#efficiency-characteristics-and-optimization-potential" id="toc-efficiency-characteristics-and-optimization-potential" class="nav-link" data-scroll-target="#efficiency-characteristics-and-optimization-potential">Efficiency Characteristics and Optimization Potential</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-2cb9" id="toc-sec-dnn-architectures-computational-mapping-2cb9" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-2cb9">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-d8b3" id="toc-sec-dnn-architectures-system-implications-d8b3" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-d8b3">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-955b" id="toc-sec-dnn-architectures-memory-requirements-955b" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-955b">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-42fa" id="toc-sec-dnn-architectures-computation-needs-42fa" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-42fa">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-1cce" id="toc-sec-dnn-architectures-data-movement-1cce" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-1cce">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" id="toc-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="nav-link" data-scroll-target="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d">Attention Mechanisms: Dynamic Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-b5e0" id="toc-sec-dnn-architectures-pattern-processing-needs-b5e0" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-b5e0">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-basic-attention-mechanism-9500" id="toc-sec-dnn-architectures-basic-attention-mechanism-9500" class="nav-link" data-scroll-target="#sec-dnn-architectures-basic-attention-mechanism-9500">Basic Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-1af4" id="toc-sec-dnn-architectures-algorithmic-structure-1af4" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-1af4">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-6c7e" id="toc-sec-dnn-architectures-computational-mapping-6c7e" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-6c7e">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-b5aa" id="toc-sec-dnn-architectures-system-implications-b5aa" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-b5aa">System Implications</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-transformers-selfattention-427f" id="toc-sec-dnn-architectures-transformers-selfattention-427f" class="nav-link" data-scroll-target="#sec-dnn-architectures-transformers-selfattention-427f">Transformers: The Attention-First Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-1242" id="toc-sec-dnn-architectures-algorithmic-structure-1242" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-1242">Algorithmic Structure</a></li>
  <li><a href="#efficiency-characteristics-and-optimization-potential-1" id="toc-efficiency-characteristics-and-optimization-potential-1" class="nav-link" data-scroll-target="#efficiency-characteristics-and-optimization-potential-1">Efficiency Characteristics and Optimization Potential</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-9f79" id="toc-sec-dnn-architectures-computational-mapping-9f79" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-9f79">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-295d" id="toc-sec-dnn-architectures-system-implications-295d" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-295d">System Implications</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-architectural-building-blocks-a575" id="toc-sec-dnn-architectures-architectural-building-blocks-a575" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-building-blocks-a575">Architectural Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-perceptron-multilayer-networks-f56e" id="toc-sec-dnn-architectures-perceptron-multilayer-networks-f56e" class="nav-link" data-scroll-target="#sec-dnn-architectures-perceptron-multilayer-networks-f56e">From Perceptron to Multi-Layer Networks</a></li>
  <li><a href="#sec-dnn-architectures-dense-spatial-processing-0fac" id="toc-sec-dnn-architectures-dense-spatial-processing-0fac" class="nav-link" data-scroll-target="#sec-dnn-architectures-dense-spatial-processing-0fac">From Dense to Spatial Processing</a></li>
  <li><a href="#sec-dnn-architectures-evolution-sequence-processing-e1a9" id="toc-sec-dnn-architectures-evolution-sequence-processing-e1a9" class="nav-link" data-scroll-target="#sec-dnn-architectures-evolution-sequence-processing-e1a9">The Evolution of Sequence Processing</a></li>
  <li><a href="#sec-dnn-architectures-modern-architectures-synthesis-innovation-531d" id="toc-sec-dnn-architectures-modern-architectures-synthesis-innovation-531d" class="nav-link" data-scroll-target="#sec-dnn-architectures-modern-architectures-synthesis-innovation-531d">Modern Architectures: Synthesis and Innovation</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-systemlevel-building-blocks-72f6" id="toc-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="nav-link" data-scroll-target="#sec-dnn-architectures-systemlevel-building-blocks-72f6">System-Level Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-core-computational-primitives-bd67" id="toc-sec-dnn-architectures-core-computational-primitives-bd67" class="nav-link" data-scroll-target="#sec-dnn-architectures-core-computational-primitives-bd67">Core Computational Primitives</a>
  <ul class="collapse">
  <li><a href="#computational-building-blocks" id="toc-computational-building-blocks" class="nav-link" data-scroll-target="#computational-building-blocks">Computational Building Blocks</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-memory-access-primitives-4e2e" id="toc-sec-dnn-architectures-memory-access-primitives-4e2e" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-access-primitives-4e2e">Memory Access Primitives</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-primitives-101a" id="toc-sec-dnn-architectures-data-movement-primitives-101a" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-primitives-101a">Data Movement Primitives</a></li>
  <li><a href="#sec-dnn-architectures-system-design-impact-cd41" id="toc-sec-dnn-architectures-system-design-impact-cd41" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-design-impact-cd41">System Design Impact</a>
  <ul class="collapse">
  <li><a href="#energy-consumption-analysis-across-architectures" id="toc-energy-consumption-analysis-across-architectures" class="nav-link" data-scroll-target="#energy-consumption-analysis-across-architectures">Energy Consumption Analysis Across Architectures</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-selection-framework-8b23" id="toc-sec-dnn-architectures-selection-framework-8b23" class="nav-link" data-scroll-target="#sec-dnn-architectures-selection-framework-8b23">Architecture Selection Framework</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-datatoarchitecture-mapping-0b9c" id="toc-sec-dnn-architectures-datatoarchitecture-mapping-0b9c" class="nav-link" data-scroll-target="#sec-dnn-architectures-datatoarchitecture-mapping-0b9c">Data-to-Architecture Mapping</a></li>
  <li><a href="#sec-dnn-architectures-computational-complexity-considerations-93fb" id="toc-sec-dnn-architectures-computational-complexity-considerations-93fb" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-complexity-considerations-93fb">Computational Complexity Considerations</a>
  <ul class="collapse">
  <li><a href="#scalability-and-production-considerations" id="toc-scalability-and-production-considerations" class="nav-link" data-scroll-target="#scalability-and-production-considerations">Scalability and Production Considerations</a></li>
  <li><a href="#hardware-mapping-and-optimization-strategies" id="toc-hardware-mapping-and-optimization-strategies" class="nav-link" data-scroll-target="#hardware-mapping-and-optimization-strategies">Hardware Mapping and Optimization Strategies</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-decision-framework-dbe8" id="toc-sec-dnn-architectures-decision-framework-dbe8" class="nav-link" data-scroll-target="#sec-dnn-architectures-decision-framework-dbe8">Decision Framework</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-fallacies-pitfalls-3e82" id="toc-sec-dnn-architectures-fallacies-pitfalls-3e82" class="nav-link" data-scroll-target="#sec-dnn-architectures-fallacies-pitfalls-3e82">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-dnn-architectures-unified-framework-inductive-biases-a892" id="toc-sec-dnn-architectures-unified-framework-inductive-biases-a892" class="nav-link" data-scroll-target="#sec-dnn-architectures-unified-framework-inductive-biases-a892">Unified Framework: Architectures as Inductive Biases</a></li>
  <li><a href="#sec-dnn-architectures-summary-c495" id="toc-sec-dnn-architectures-summary-c495" class="nav-link" data-scroll-target="#sec-dnn-architectures-summary-c495">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav></header>




<section id="sec-dnn-architectures" class="level1 page-columns page-full">
<h1>DNN Architectures</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity.</em></p>
</div></div><p> <img src="images/png/cover_dl_arch.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>Why do architectural choices in neural networks become critical system design decisions that determine computational feasibility, hardware requirements, and deployment constraints?</em></p>
<p>Neural network architectures represent fundamental engineering decisions that directly determine system performance, resource requirements, and deployment viability. Each architectural choice creates cascading effects throughout the entire system stack: memory bandwidth demands, computational complexity patterns, parallelization opportunities, and hardware acceleration compatibility. Understanding these architectural implications enables engineers to make informed trade-offs between model capability and system constraints, predict computational bottlenecks before they occur, and select appropriate hardware platforms for deployment. These architectural decisions ultimately determine whether a machine learning system can meet its performance requirements within available computational resources, making architectural understanding essential for building scalable, efficient AI systems.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Map fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).</li>
<li>Analyze how architectural patterns shape computational and memory demands.</li>
<li>Evaluate system-level impacts of architectural choices on system attributes.</li>
<li>Compare architectures’ hardware mapping and identify optimization strategies.</li>
<li>Assess trade-offs between complexity and system needs for specific applications.</li>
</ul>
</div>
</div>
</section>
<section id="sec-dnn-architectures-overview-8d17" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-overview-8d17">Overview</h2>
<p>Building on the neural network foundations established in <strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>, this chapter examines how specialized architectures exploit problem structure to achieve computational efficiency. While the fundamental operations—matrix multiplications and nonlinear activations—remain constant, their organization within specialized architectures fundamentally transforms system engineering trade-offs.</p>
<p>Four architectural patterns address the diversity of pattern recognition challenges in deep learning. Each pattern responds to specific limitations of earlier approaches, establishing a systematic progression from general-purpose to increasingly specialized architectures. This evolution reveals both the computational principles underlying each design and their systematic relationships.</p>
<p>The architectural progression begins with Multi-Layer Perceptrons (MLPs), which apply the universal approximation principle through dense connectivity. While theoretically capable, MLPs treat all inputs uniformly, creating computational inefficiencies when data exhibits inherent structure. Convolutional Neural Networks (CNNs) emerged to exploit spatial locality, reducing computational requirements for image-like data through parameter sharing and local connectivity. Recurrent Neural Networks (RNNs) extended the architectural repertoire to sequential data by introducing memory states, enabling temporal pattern processing unattainable with feedforward architectures. Attention mechanisms and Transformers represent the current architectural frontier, replacing fixed structural constraints with dynamic, content-dependent processing. Given their dominance in modern ML systems—from large language models to vision transformers—attention-based architectures receive extensive treatment, examining both fundamental attention concepts and their culmination in the Transformer architecture that has revolutionized sequence modeling across domains.</p>
<p>This architectural evolution demonstrates a fundamental principle: each innovation addresses specific limitations of its predecessors while introducing new computational challenges, establishing a progression that balances representational generality against computational efficiency.</p>
<p>From a systems perspective, each architectural evolution reflects quantifiable trade-offs between computational complexity and pattern recognition capability. The computational demands vary dramatically across architectures: MLPs require maximum computational resources per input element but handle arbitrary relationships. CNNs reduce computation through structural assumptions about spatial locality, achieving 5-10x efficiency gains on typical image tasks. RNNs introduce sequential constraints that allow temporal processing but create dependencies that challenge parallel execution. Transformers eliminate sequential constraints but reintroduce computational complexity through attention mechanisms, typically requiring 2-3x more computation than CNNs for similar performance.</p>
<p>These trade-offs between computational efficiency and representational power drive the systematic architectural evolution we explore throughout this chapter. The detailed quantitative performance analysis, including specific FLOP counts, latency measurements, memory bandwidth utilization, and energy consumption characteristics for each architecture, is covered comprehensively in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong> and <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong> where hardware-specific optimization strategies are systematically addressed.</p>
<p>This chapter examines each architectural pattern through a unified analytical framework that reveals their systematic relationships:</p>
<ol type="1">
<li><strong>Pattern Processing Needs</strong>: What data characteristics and computational challenges drove each architectural innovation</li>
<li><strong>Algorithmic Structure</strong>: How the architecture addresses these needs through specific organizational principles</li>
<li><strong>Computational Mapping</strong>: The resulting demands on memory, computation, and data movement resources</li>
<li><strong>System Implications</strong>: How these computational patterns influence hardware design and system optimization</li>
</ol>
<p>This systems-oriented analysis extends the neural network foundations from <strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>—forward propagation, backpropagation, and gradient descent—by examining how specialized architectures organize these fundamental operations to exploit problem structure. The architectural knowledge developed here provides essential context for understanding hardware acceleration strategies in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong> and optimization techniques in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>. By understanding the evolutionary logic connecting these architectures, engineers can make systematic decisions about architectural selection, resource planning, and system design.</p>
<div id="quiz-question-sec-dnn-architectures-overview-8d17" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key consideration when mapping neural network architectures to computer system resources?</p>
<ol type="a">
<li>Algorithmic complexity</li>
<li>Memory access patterns</li>
<li>User interface design</li>
<li>Data visualization techniques</li>
</ol></li>
<li><p>Explain how dense connectivity patterns in neural networks impact memory bandwidth demands.</p></li>
<li><p>Which neural network architecture is best suited for managing temporal dependencies?</p>
<ol type="a">
<li>Recurrent Neural Networks (RNNs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Multi-layer Perceptrons (MLPs)</li>
<li>Transformers</li>
</ol></li>
<li><p>Discuss the implications of stateful processing on on-chip memory organization in neural networks.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-overview-8d17" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f">Multi-Layer Perceptrons: Dense Pattern Processing</h2>
<p>Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes core computational patterns that appear throughout deep learning models. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="#ref-cybenko1989approximation" role="doc-biblioref">Cybenko 1989</a>; <a href="#ref-hornik1989multilayer" role="doc-biblioref">Hornik, Stinchcombe, and White 1989</a>)</span>, which states that a sufficiently large MLP with non-linear activation functions<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> can approximate any continuous function on a compact domain, given suitable weights and biases.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Universal Approximation Theorem</strong>: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the “AI Winter” of the 1980s and established mathematical foundations for modern deep learning.</p></div><div id="ref-cybenko1989approximation" class="csl-entry" role="listitem">
Cybenko, G. 1989. <span>“Approximation by Superpositions of a Sigmoidal Function.”</span> <em>Mathematics of Control, Signals, and Systems</em> 2 (4): 303–14. <a href="https://doi.org/10.1007/bf02551274">https://doi.org/10.1007/bf02551274</a>.
</div><div id="ref-hornik1989multilayer" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66. <a href="https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/10.1016/0893-6080(89)90020-8</a>.
</div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Activation Functions</strong>: Non-linear mathematical functions like ReLU, sigmoid, and tanh that introduce non-linearity into neural networks. Without them, multiple layers would collapse to a single linear transformation, making ReLU’s simple max(0,x) operation necessary for deep learning’s success since 2012.</p></div></div><p>In practice, the UAT explains why MLPs succeed across diverse tasks while revealing their limitations. The theorem guarantees that <em>some</em> MLP can approximate any function, yet provides no guidance on requisite network size or weight determination. Consequently, MLPs can theoretically solve any pattern recognition problem but may require impractically large networks or extensive computation. This theoretical power drives the selection of MLPs for tabular data, recommendation systems, and problems where input relationships are unknown, while the practical limitations motivated the development of specialized architectures such as CNNs for spatial data and RNNs for sequential data, each optimized for different computational patterns and system requirements.</p>
<p>When applied to the MNIST handwritten digit recognition challenge<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, an MLP demonstrates its computational approach by transforming a <span class="math inline">\(28\times 28\)</span> pixel image into digit classification.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>MNIST Dataset</strong>: Created by Yann LeCun in 1998 from NIST’s database of handwritten digits, MNIST’s 60,000 training images became the “fruit fly” of machine learning research. Despite achieving 99.7% accuracy being considered solved, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.</p></div></div><section id="sec-dnn-architectures-pattern-processing-needs-c45a" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-c45a">Pattern Processing Needs</h3>
<p>Deep learning models frequently encounter problems where any input feature may influence any output, absent inherent constraints on these relationships. Financial market analysis exemplifies this challenge: any economic indicator may affect any market outcome. Similarly, in natural language processing, the meaning of a word may depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.</p>
<p>Dense pattern processing addresses these challenges through several key capabilities. First, it enables unrestricted feature interactions where each output can depend on any combination of inputs. Second, it supports learned feature importance, enabling the system to determine which connections matter rather than relying on prescribed relationships. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.</p>
<p>The MNIST digit recognition task illustrates this uncertainty: while humans might focus on specific parts of digits (loops in ‘6’ or crossings in ‘8’), the pixel combinations critical for classification remain indeterminate. A ‘7’ written with a serif may share pixel patterns with a ‘2’, while variations in handwriting mean discriminative features may appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.</p>
<p>This requirement for unrestricted connectivity leads directly to the mathematical foundation of MLPs.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-c012" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-c012">Algorithmic Structure</h3>
<p>MLPs enable unrestricted feature interactions through a direct algorithmic solution: comprehensive connectivity between all nodes. This connectivity requirement manifests through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers.</p>
<p>This architectural principle translates the dense connectivity pattern into matrix multiplication operations<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, establishing the mathematical foundation that renders MLPs computationally tractable. As illustrated in <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;1</a>, each layer transforms its input through a fundamental operation combining linear transformation with nonlinear activation:</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>GEMM (General Matrix Multiply)</strong>: The fundamental operation underlying neural networks, accounting for 80-95% of computation time in dense neural networks. GEMM performs C = αAB + βC and has been optimized for decades. Modern implementations like cuBLAS achieve 80-95% of theoretical peak performance on GPUs, making GEMM optimization important for ML systems.</p></div></div><p><span class="math display">\[
\mathbf{h}^{(l)} = f\big(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}\big)
\]</span></p>
<p>In this equation, <span class="math inline">\(\mathbf{h}^{(l)}\)</span> represents the layer <span class="math inline">\(l\)</span> output (activation vector), <span class="math inline">\(\mathbf{W}^{(l)}\)</span> denotes the weight matrix for layer <span class="math inline">\(l\)</span>, <span class="math inline">\(\mathbf{b}^{(l)}\)</span> denotes the bias vector, and <span class="math inline">\(f(\cdot)\)</span> denotes the activation function (such as ReLU). The superscript <span class="math inline">\((l)\)</span> indicates the layer number, with bold symbols representing vectors and matrices. This compact notation encapsulates the core operation of neural networks: linear transformation followed by nonlinear activation.</p>
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5aab87381f560ddc7f720233b9e89a654d299fa1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Layered Transformations: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: [@reagen2017deep]."><img src="dnn_architectures_files/mediabag/5aab87381f560ddc7f720233b9e89a654d299fa1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Layered Transformations</strong>: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: <span class="citation" data-cites="reagen2017deep">(<a href="#ref-reagen2017deep" role="doc-biblioref">Reagen et al. 2017</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2017deep" class="csl-entry" role="listitem">
Reagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. <em>Deep Learning for Computer Architects</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01756-8">https://doi.org/10.1007/978-3-031-01756-8</a>.
</div></div></figure>
</div>
<p>The dimensions of these operations reveal the computational scale of dense pattern processing:</p>
<ul>
<li>Input vector: <span class="math inline">\(\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}\)</span> represents all potential input features</li>
<li>Weight matrices: <span class="math inline">\(\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}\)</span> capture all possible input-output relationships</li>
<li>Output vector: <span class="math inline">\(\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}\)</span> produces transformed representations</li>
</ul>
<div id="callout-example*-1.1" class="callout callout-example" title="Concrete Computation Example">
<p></p><details class="callout-example fbx-default closebutton" open=""><summary><strong>Example: </strong>Concrete Computation Example</summary><div>Consider a simplified 4-pixel image processed by a 3-neuron hidden layer:<p></p>
<p><strong>Input</strong>: <span class="math inline">\(\mathbf{h}^{(0)} = [0.8, 0.2, 0.9, 0.1]\)</span> (4 pixel intensities)</p>
<p><strong>Weight matrix</strong>: <span class="math inline">\(\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 &amp; -0.3 &amp; 0.2 &amp; 0.7 \\ 0.1 &amp; 0.8 &amp; -0.4 &amp; 0.3 \\ -0.2 &amp; 0.4 &amp; 0.6 &amp; -0.1 \end{bmatrix}\)</span> (3×4 matrix)</p>
<p><strong>Computation</strong>: <span class="math display">\[\begin{gather*}
\mathbf{z}^{(1)} = \mathbf{W}^{(1)}\mathbf{h}^{(0)} = \begin{bmatrix} 0.5×0.8 + (-0.3)×0.2 + 0.2×0.9 + 0.7×0.1 \\ 0.1×0.8 + 0.8×0.2 + (-0.4)×0.9 + 0.3×0.1 \\ (-0.2)×0.8 + 0.4×0.2 + 0.6×0.9 + (-0.1)×0.1 \end{bmatrix}
\\
= \begin{bmatrix} 0.65 \\ -0.17 \\ 0.47 \end{bmatrix}
\end{gather*}\]</span> <strong>After ReLU</strong>: <span class="math inline">\(\mathbf{h}^{(1)} = [0.65, 0, 0.47]\)</span> (negative values zeroed)</p>
<p>Each hidden neuron combines ALL input pixels with different weights, demonstrating unrestricted feature interaction.</p>
</div></details>
</div>
<p>The MNIST example demonstrates the practical scale of these operations:</p>
<ul>
<li>Each 784-dimensional input (<span class="math inline">\(28\times 28\)</span> pixels) connects to every neuron in the first hidden layer</li>
<li>A hidden layer with 100 neurons requires a <span class="math inline">\(784\times 100\)</span> weight matrix</li>
<li>Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature</li>
</ul>
<p>This algorithmic structure addresses the need for arbitrary feature relationships while creating specific computational patterns that computer systems must accommodate.</p>
<section id="architectural-characteristics" class="level4">
<h4 class="anchored" data-anchor-id="architectural-characteristics">Architectural Characteristics</h4>
<p>This architectural approach exhibits both strengths and limitations. Dense connectivity provides the universal approximation capability established earlier but introduces computational redundancy. While this theoretical power enables MLPs to model any continuous function given sufficient width, this flexibility necessitates numerous parameters to learn relatively simple patterns. The dense connections ensure that every input feature influences every output, yielding maximum expressiveness at the cost of maximum computational expense.</p>
<p>These trade-offs motivate sophisticated optimization techniques that reduce computational demands while preserving model capability. Structured pruning can eliminate 80-90% of connections with minimal accuracy loss, while quantization reduces precision requirements from 32-bit to 8-bit or lower. These model compression strategies, along with hardware-specific optimizations that exploit the regular structure of dense matrix operations, are systematically addressed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong> and <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>, building directly on the architectural foundations established here.</p>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-fe7e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-fe7e">Computational Mapping</h3>
<p>The mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. This mapping progresses from mathematical abstraction to computational reality, as demonstrated in the first implementation shown in <a href="#lst-mlp_layer_matrix" class="quarto-xref">Listing&nbsp;1</a>.</p>
<p>The function mlp_layer_matrix directly mirrors the mathematical equation, employing high-level matrix operations (<code>matmul</code>) to express the computation in a single line while abstracting the underlying complexity. This implementation style characterizes deep learning frameworks, where optimized libraries manage the actual computation.</p>
<div id="lst-mlp_layer_matrix" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: <strong>Dense Layer Implementation</strong>: Neural networks perform weighted sum and activation functions across layers using matrix operations through The code. This emphasizes the core computational pattern in multi-layer perceptrons.
</figcaption>
<div aria-describedby="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_matrix(X, W, b):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input matrix (batch_size × num_inputs)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W: weight matrix (num_inputs × num_outputs)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b: bias vector (num_outputs)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(matmul(X, W) <span class="op">+</span> b)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># One clean line of math</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>In contrast, the second implementation, <code>mlp_layer_compute</code> (shown in <a href="#lst-mlp_layer_compute" class="quarto-xref">Listing&nbsp;2</a>), exposes the actual computational pattern through nested loops. This version reveals what really happens when we compute a layer’s output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.</p>
<div id="lst-mlp_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: <strong>Core Computational Pattern</strong>: Computes each output neuron by accumulating weighted contributions from all inputs across the batch. This implementation exposes the detailed step-by-step process of how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.
</figcaption>
<div aria-describedby="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_compute(X, W, b):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each sample in the batch</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute each output neuron</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> out <span class="kw">in</span> <span class="bu">range</span>(num_outputs):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initialize with bias</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            Z[batch,out] <span class="op">=</span> b[out]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate weighted inputs</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> in_ <span class="kw">in</span> <span class="bu">range</span>(num_inputs):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                Z[batch,out] <span class="op">+=</span> X[batch,in_] <span class="op">*</span> W[in_,out]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(Z)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.</p>
<p>In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use optimizations through libraries like BLAS<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> or cuBLAS, these patterns drive key system design decisions. The hardware architectures that accelerate these matrix operations, including GPU tensor cores and specialized AI accelerators, are covered in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Basic Linear Algebra Subprograms (BLAS)</strong>: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency.</p></div></div></section>
<section id="sec-dnn-architectures-system-implications-7a8f" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-7a8f">System Implications</h3>
<p>When analyzing how computational patterns impact computer systems, we examine three core dimensions: memory requirements, computation needs, and data movement. This framework enables systematic analysis of how algorithmic patterns influence system design decisions across all neural network architectures, revealing both commonalities and distinctive characteristics. These system-level considerations build directly on the foundational concepts of neural network computation patterns, memory systems, and system scaling discussed in <strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>.</p>
<section id="sec-dnn-architectures-memory-requirements-4900" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-4900">Memory Requirements</h4>
<p>For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there’s no inherent locality in these accesses; every output needs every input and its corresponding weights.</p>
<p>These memory access patterns enable optimization through careful data organization and reuse. Modern processors handle these dense access patterns through specialized approaches: CPUs leverage their cache hierarchy for data reuse, while GPUs employ memory architectures designed for high-bandwidth access to large parameter matrices. Frameworks abstract these optimizations through high-performance matrix operations (as detailed in our earlier analysis).</p>
</section>
<section id="sec-dnn-architectures-computation-needs-9cb4" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-9cb4">Computation Needs</h4>
<p>The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this requires 784 multiply-accumulates per output neuron. With 100 neurons in the hidden layer, 78,400 multiply-accumulates are performed for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.</p>
<p>This computational structure enables specific optimization strategies in modern hardware. The dense matrix multiplication pattern can be parallelized across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while software frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.</p>
</section>
<section id="sec-dnn-architectures-data-movement-fc16" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-fc16">Data Movement</h4>
<p>The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating large data transfer demands between memory and compute units.</p>
<p>The predictable data movement patterns enable strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms; CPUs use prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Software frameworks orchestrate these data movements through memory management systems that reduce redundant transfers and increase data reuse.</p>
<div id="quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>What is the primary advantage of using Multi-Layer Perceptrons (MLPs) for dense pattern processing?</p>
<ol type="a">
<li>Low computational cost</li>
<li>Ability to model arbitrary feature interactions</li>
<li>High interpretability of model decisions</li>
<li>Reduced memory requirements</li>
</ol></li>
<li><p>Explain how the Universal Approximation Theorem supports the use of MLPs in deep learning systems.</p></li>
<li><p>In the context of MLPs, the operation that transforms input vectors through matrix multiplication followed by element-wise activation is known as ____.</p></li>
<li><p>How does the dense connectivity pattern of MLPs impact system design in terms of memory and computation?</p></li>
<li><p>In a production system using MLPs for image recognition, what is a likely challenge related to data movement?</p>
<ol type="a">
<li>Insufficient data storage capacity</li>
<li>Excessive data redundancy</li>
<li>Lack of data preprocessing</li>
<li>Limited bandwidth for data transfer</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c">Convolutional Neural Networks: Spatial Pattern Processing</h2>
<p>The computational intensity and parameter requirements of MLPs reveal a fundamental mismatch when applied to structured data. While the Universal Approximation Theorem guarantees that MLPs can learn any function, it provides no efficiency guidance. When processing images, MLPs must learn spatial relationships such as edge detection and texture recognition from scratch for every position in the image, leading to massive parameter counts and computational waste. This inefficiency motivated the development of architectural patterns that exploit inherent data structure.</p>
<p>Convolutional Neural Networks emerged as the solution to this challenge, introducing two key innovations that enhance efficiency for spatially structured data. Parameter sharing allows the same feature detector to be applied across different spatial positions, reducing parameters from millions to thousands while improving generalization. Local connectivity restricts connections to spatially adjacent regions, reflecting the biological insight that nearby pixels are more likely to be related than distant ones.</p>
<p>These architectural innovations represent a fundamental trade-off in deep learning design: sacrificing the theoretical generality of MLPs for practical efficiency gains when data exhibits known structure. While MLPs treat each input element independently, CNNs exploit spatial relationships to achieve computational savings and improved performance on vision tasks.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-4d88" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-4d88">Pattern Processing Needs</h3>
<p>Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel’s relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features: edges form shapes, shapes form objects, and objects form scenes.</p>
<p>This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.</p>
<p>Focusing on image processing to illustrate these principles, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image. A cat is still a cat whether it appears in the top-left or bottom-right corner. This indicates two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>ImageNet Revolution</strong>: AlexNet’s dramatic victory in the 2012 ImageNet challenge <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span> (reducing error from 26% to 16%) sparked the deep learning renaissance. ImageNet’s 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that “big data + big compute + big models” could achieve superhuman performance.</p><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div></div></div><div id="fig-cnn-spatial-processing" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Spatial Feature Extraction: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position."><img src="dnn_architectures_files/mediabag/c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Spatial Feature Extraction</strong>: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.
</figcaption>
</figure>
</div>
<p>This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and <span class="citation" data-cites="lecun1989backpropagation">Y. LeCun et al. (<a href="#ref-lecun1989backpropagation" role="doc-biblioref">1989</a>)</span>. CNNs achieve this through several key innovations: parameter sharing<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, local connectivity, and translation invariance<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Yann LeCun and CNNs</strong>: LeCun’s 1989 LeNet architecture was inspired by Hubel and Wiesel’s discovery of simple and complex cells in cat visual cortex <span class="citation" data-cites="hubel1962receptive">(<a href="#ref-hubel1962receptive" role="doc-biblioref">Hubel and Wiesel 1962</a>)</span>. LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks. As illustrated in <a href="#fig-cnn-spatial-processing" class="quarto-xref">Figure&nbsp;2</a>, CNNs address spatial pattern processing through a different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position, a process known as convolution.</p><div id="ref-hubel1962receptive" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1962. <span>“Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex.”</span> <em>The Journal of Physiology</em> 160 (1): 106–54. <a href="https://doi.org/10.1113/jphysiol.1962.sp006837">https://doi.org/10.1113/jphysiol.1962.sp006837</a>.
</div></div><div id="ref-lecun1989backpropagation" class="csl-entry" role="listitem">
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. <span>“Backpropagation Applied to Handwritten Zip Code Recognition.”</span> <em>Neural Computation</em> 1 (4): 541–51. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>.
</div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Parameter Sharing</strong>: CNNs reuse the same filter weights across spatial positions, reducing parameters substantially. A CNN processing 224×224 images might use 3×3 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a 5,500x reduction enabling practical computer vision.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Translation Invariance</strong>: CNNs detect features regardless of spatial position. A cat’s ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolution’s sliding window design and is important for computer vision, where objects appear at arbitrary locations in images.</p></div></div></section>
<section id="sec-dnn-architectures-algorithmic-structure-2aba" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-2aba">Algorithmic Structure</h3>
<p>The core operation in a CNN can be expressed mathematically as:</p>
<p><span class="math display">\[
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
\]</span></p>
<p>This equation describes how CNNs process spatial data. <span class="math inline">\(\mathbf{H}^{(l)}_{i,j,k}\)</span> is the output at spatial position <span class="math inline">\((i,j)\)</span> in channel <span class="math inline">\(k\)</span> of layer <span class="math inline">\(l\)</span>. The triple sum iterates over the filter dimensions: <span class="math inline">\((di,dj)\)</span> scans the spatial filter size, and <span class="math inline">\(c\)</span> covers input channels. <span class="math inline">\(\mathbf{W}^{(l)}_{di,dj,c,k}\)</span> represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods.</p>
<p>Breaking down the notation further, <span class="math inline">\((i,j)\)</span> corresponds to spatial positions, <span class="math inline">\(k\)</span> indexes output channels, <span class="math inline">\(c\)</span> indexes input channels, and <span class="math inline">\((di,dj)\)</span> spans the local receptive field<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. Unlike the dense matrix multiplication of MLPs, this operation:</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Receptive Field</strong>: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might “see” a 7×7 region even with 3×3 filters, due to stacking. Understanding receptive field size is important for ensuring networks can capture features at the right scale for the task.</p></div></div><ul>
<li>Processes local neighborhoods (typically <span class="math inline">\(3\times 3\)</span> or <span class="math inline">\(5\times 5\)</span>)</li>
<li>Reuses the same weights at each spatial position</li>
<li>Maintains spatial structure in its output</li>
</ul>
<p>To illustrate this process concretely, consider the MNIST digit classification task with <span class="math inline">\(28\times 28\)</span> grayscale images. Each convolutional layer applies a set of filters (e.g., <span class="math inline">\(3\times 3\)</span>) that slide across the image, computing local weighted sums. If we use 32 filters with padding to preserve dimensions, the layer produces a <span class="math inline">\(28\times 28\times 32\)</span> output, where each spatial position contains 32 different feature measurements of its local neighborhood. This contrasts sharply with the Multi-Layer Perceptron (MLP) approach, where the entire image is flattened into a 784-dimensional vector before processing.</p>
<p>This algorithmic structure directly implements the requirements for spatial pattern processing, creating distinct computational patterns that influence system design. Unlike MLPs, convolutional networks preserve spatial locality, allowing for efficient hierarchical feature extraction. These properties drive architectural optimizations in AI accelerators, where operations such as data reuse, tiling, and parallel filter computation are important for performance.</p>
<p>The effectiveness of CNNs can be understood through the lens of group theory<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, which provides a mathematical framework for understanding symmetries in data. Translation invariance emerges because convolution is equivariant with respect to the translation group—if we shift the input image, the output feature maps shift by the same amount. Mathematically, if <span class="math inline">\(T_v\)</span> represents translation by vector <span class="math inline">\(v\)</span>, then a convolutional layer <span class="math inline">\(f\)</span> satisfies: <span class="math inline">\(f(T_v x) = T_v f(x)\)</span>. This equivariance property allows CNNs to learn features that generalize across spatial locations.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Group Theory in Neural Networks</strong>: Mathematical framework describing how CNNs preserve spatial relationships. Translation equivariance means shifting an input image shifts the output feature maps by the same amount—a property enabling CNNs to recognize objects regardless of position, foundational to computer vision success.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Inductive Bias</strong>: Prior assumptions built into model architecture about the structure of data. CNNs assume spatial locality and translation invariance, drastically reducing the space of functions they can learn compared to MLPs. This constraint enables better generalization with fewer parameters—a key principle in machine learning architecture design.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Hypothesis Space</strong>: The set of all possible functions a model can represent given its architecture and parameters. MLPs have a larger hypothesis space than CNNs for images, but CNNs’ constrained space contains better solutions for visual tasks, demonstrating that architectural constraints often improve rather than limit performance. Recent work has extended these principles to other symmetry groups, developing Group-Equivariant CNNs that handle rotations and reflections <span class="citation" data-cites="cohen2016group">(<a href="#ref-cohen2016group" role="doc-biblioref"><strong>cohen2016group?</strong></a>)</span>.</p></div></div><p>The choice of convolution as the fundamental operation reflects deeper principles about inductive bias<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> in neural architecture design. By restricting connectivity to local neighborhoods and sharing parameters across spatial positions, CNNs encode prior knowledge about the structure of visual data: that important features are local and translation-invariant. This architectural constraint reduces the hypothesis space<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> that the network must search, enabling more efficient learning from limited data compared to fully connected networks.</p>
<p>CNNs naturally implement hierarchical representation learning through their layered structure. Early layers detect low-level features like edges and textures with small receptive fields, while deeper layers combine these into increasingly complex patterns with larger receptive fields. This hierarchical organization mirrors the structure of the visual cortex and enables CNNs to build compositional representations: complex objects are represented as compositions of simpler parts. The mathematical foundation for this emerges from the fact that stacking convolutional layers creates a tree-like dependency structure, where each deep neuron depends on an exponentially large set of input pixels, enabling efficient representation of hierarchical patterns.</p>
<section id="architectural-characteristics-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="architectural-characteristics-1">Architectural Characteristics</h4>
<p>Parameter sharing dramatically reduces complexity compared to MLPs by reusing the same filters across spatial locations. This sharing embodies the assumption that useful features (such as edges or textures) can appear anywhere in an image, making the same feature detector valuable across all spatial positions.</p>
<p>The architectural efficiency of CNNs enables further optimization through specialized techniques. Depthwise separable convolutions decompose standard convolutions into depthwise and pointwise operations, reducing computation by 8-9× for typical mobile deployments. Channel pruning eliminates entire feature maps based on importance metrics, achieving 40-50% FLOPs reduction with &lt;1% accuracy loss. These optimization strategies, along with hardware mappings that exploit spatial locality and data reuse in convolution operations, are comprehensively analyzed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong> and <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<p>As illustrated in <a href="#fig-cnn" class="quarto-xref">Figure&nbsp;3</a>, convolution operations involve sliding a small filter over the input image to generate a feature map<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. This process captures local structures while maintaining translation invariance. For an interactive visual exploration of convolutional networks, the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> project provides an insightful demonstration of how these networks are constructed.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Feature Map</strong>: The output of applying a convolutional filter to an input, representing detected features at different spatial locations. A 64-filter layer produces 64 feature maps, each highlighting different patterns like edges, textures, or shapes. Feature maps become more abstract (detecting objects, faces) in deeper layers compared to early layers (detecting edges, colors).</p></div></div><div id="fig-cnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="fc7cbac26cbac9fcfc0d4c00b2bae655b6fafb65.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Convolution Operation: Neural networks process input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position."><img src="dnn_architectures_files/mediabag/fc7cbac26cbac9fcfc0d4c00b2bae655b6fafb65.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Convolution Operation</strong>: Neural networks process input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-9108" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-9108">Computational Mapping</h3>
<p>Convolution operations create computational patterns fundamentally different from MLP dense matrix multiplication. This translation from mathematical operations to implementation details reveals distinct computational characteristics.</p>
<p>The first implementation, <code>conv_layer_spatial</code> (shown in <a href="#lst-conv_layer_spatial" class="quarto-xref">Listing&nbsp;3</a>), uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.</p>
<div id="lst-conv_layer_spatial" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: <strong>Convolution Operation</strong>: Neural networks process input data through hierarchical feature extraction using a simple convolution operation that combines a kernel and bias before applying an activation function.
</figcaption>
<div aria-describedby="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_spatial(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> convolution(<span class="bu">input</span>, kernel) <span class="op">+</span> bias</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The second implementation, conv_layer_compute (see <a href="#lst-conv_layer_compute" class="quarto-xref">Listing&nbsp;4</a>), reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. These nested loops reveal the true nature of convolution’s computational structure.</p>
<div id="lst-conv_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Nested Loops</strong>: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.
</figcaption>
<div aria-describedby="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_compute(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop 1: Process each image in batch</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> image <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 2&amp;3: Move across image spatially</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(height):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(width):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>       <span class="co"># Loop 4: Compute each output feature</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> out_channel <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>         result <span class="op">=</span> bias[out_channel]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Loop 5&amp;6: Move across kernel window</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> ky <span class="kw">in</span> <span class="bu">range</span>(kernel_height):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> kx <span class="kw">in</span> <span class="bu">range</span>(kernel_width):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Loop 7: Process each input feature</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> in_channel <span class="kw">in</span> <span class="bu">range</span>(num_input_channels):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Get input value from correct window position</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>              in_y <span class="op">=</span> y <span class="op">+</span> ky</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>              in_x <span class="op">=</span> x <span class="op">+</span> kx</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>              <span class="co"># Perform multiply-accumulate operation</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>              result <span class="op">+=</span> (</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                <span class="bu">input</span>[image, in_y, in_x, in_channel]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> kernel[ky, kx, in_channel, out_channel]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Store result for this output position</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>         output[image, y, x, out_channel] <span class="op">=</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The seven nested loops reveal different aspects of the computation:</p>
<ul>
<li>Outer loops (1-3) manage position: which image and where in the image</li>
<li>Middle loop (4) handles output features: computing different learned patterns</li>
<li>Inner loops (5-7) perform the actual convolution: sliding the kernel window</li>
</ul>
<p>Examining this process in detail, the outer two loops (<code>for y</code> and <code>for x</code>) traverse each spatial position in the output feature map (for the MNIST example, this traverses all <span class="math inline">\(28\times 28\)</span> positions). At each position, values are computed for each output channel (<code>for k</code> loop), representing different learned features or patterns—the 32 different feature detectors.</p>
<p>The inner three loops implement the actual convolution operation at each position. For each output value, we process a local <span class="math inline">\(3\times 3\)</span> region of the input (the <code>dy</code> and <code>dx</code> loops) across all input channels (<code>for c</code> loop). This creates a sliding window effect, where the same <span class="math inline">\(3\times 3\)</span> filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP’s global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.</p>
<p>For our MNIST example with <span class="math inline">\(3\times 3\)</span> filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. This operation must be repeated for every spatial position <span class="math inline">\((28\times 28)\)</span> and every output channel (32).</p>
<p>While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle. These patterns influence system design, creating both challenges and opportunities for optimization.</p>
</section>
<section id="sec-dnn-architectures-system-implications-a9f3" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-a9f3">System Implications</h3>
<p>Applying our three-dimensional analysis framework to CNNs, the spatial nature of processing creates distinctive patterns in memory requirements, computation needs, and data movement that differ from MLP dense connectivity.</p>
<section id="sec-dnn-architectures-memory-requirements-080b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-080b">Memory Requirements</h4>
<p>For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size <span class="math inline">\(3\times 3\)</span> requires storing only 288 weight parameters <span class="math inline">\((3\times 3\times 32)\)</span>, in contrast to the 78,400 weights needed for our MLP’s fully-connected layer. The system must store feature maps for all spatial positions, creating a different memory demand. A <span class="math inline">\(28\times 28\)</span> input with 32 output channels requires storing 25,088 activation values <span class="math inline">\((28\times 28\times 32)\)</span>.</p>
<p>These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Processors optimize these spatial patterns by caching filter weights for reuse across positions while streaming feature map data. Frameworks implement spatial optimizations through specialized memory layouts that enable filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently. CPUs use their cache hierarchy to keep frequently used filters resident, while GPUs employ specialized memory architectures designed for the spatial access patterns of image processing. The detailed architecture design principles for these specialized processors are covered in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-bd0e" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-bd0e">Computation Needs</h4>
<p>The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with <span class="math inline">\(3\times 3\)</span> filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates <span class="math inline">\((3\times 3\times 32)\)</span>, and this must be repeated for all 784 spatial positions <span class="math inline">\((28\times 28)\)</span>. While each individual computation involves fewer operations than an MLP layer, the total computational load remains large due to spatial repetition.</p>
<p>This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. The model optimization techniques that further reduce these computational demands, including specialized convolution optimizations and sparsity patterns, are detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>SIMD (Single Instruction, Multiple Data)</strong>: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is important for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.</p></div></div></section>
<section id="sec-dnn-architectures-data-movement-a44a" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-a44a">Data Movement</h4>
<p>The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each <span class="math inline">\(3\times 3\)</span> filter weight is reused 784 times (once for each position in the <span class="math inline">\(28\times 28\)</span> feature map). This creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.</p>
<p>The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.</p>
<div id="quiz-question-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>What is a primary advantage of convolutional neural networks (CNNs) over multi-layer perceptrons (MLPs) when processing spatial data?</p>
<ol type="a">
<li>CNNs can process data in parallel more efficiently.</li>
<li>CNNs have a simpler architecture than MLPs.</li>
<li>CNNs are better at processing temporal data.</li>
<li>CNNs require fewer parameters due to weight sharing.</li>
</ol></li>
<li><p>Explain how the spatial pattern processing capability of CNNs contributes to their effectiveness in image recognition tasks.</p></li>
<li><p>Order the following steps in a convolutional neural network’s processing of an image: (1) Apply filters to detect features, (2) Flatten the feature maps, (3) Use pooling to reduce dimensionality, (4) Classify using fully connected layers.</p></li>
<li><p>In a production system, what are the implications of CNNs’ memory requirements on system design?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904">Recurrent Neural Networks: Sequential Pattern Processing</h2>
<p>Convolutional Neural Networks achieved efficiency gains by exploiting spatial locality, yet their architectural assumptions fail when patterns depend on temporal order rather than spatial proximity. While CNNs excel at recognizing "what" is present in data through shared feature detectors, they cannot capture "when" events occur or how they relate across time. This limitation manifests in domains such as natural language processing, where word meaning depends on sentential context, and time-series analysis, where future values depend on historical patterns.</p>
<p>Sequential data presents a fundamental challenge: patterns can span arbitrary temporal distances, rendering fixed-size kernels ineffective. Unlike spatial convolution, where a 3×3 filter captures local relationships, temporal relationships may require connecting events separated by hundreds or thousands of time steps. Traditional feedforward architectures, including CNNs, process each input independently and cannot maintain the temporal context necessary for these long-range dependencies.</p>
<p>Recurrent Neural Networks address this architectural limitation by introducing memory as a fundamental component of the computational model. Rather than processing inputs in isolation, RNNs maintain an internal state that propagates information from previous time steps, enabling the network to condition its current output on historical context. This architecture embodies another trade-off: while CNNs sacrifice theoretical generality for spatial efficiency, RNNs introduce computational dependencies that challenge parallel execution in exchange for temporal processing capabilities.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Coverage Note">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Coverage Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section provides focused coverage of RNNs, emphasizing their core contributions to sequential processing and the architectural principles that influenced modern attention mechanisms. While RNNs introduced critical concepts—memory states, temporal dependencies, and sequential computation—contemporary practice increasingly favors attention-based architectures for sequence modeling. We focus on foundational principles rather than extensive implementation variants, dedicating substantial depth to the attention mechanisms and Transformers (<a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="quarto-xref">Section&nbsp;1.5</a>) that have largely superseded RNNs in production systems while building directly on the insights gained from recurrent architectures.</p>
</div>
</div>
<section id="sec-dnn-architectures-pattern-processing-needs-479a" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-479a">Pattern Processing Needs</h3>
<p>Sequential pattern processing addresses scenarios where current input interpretation depends on preceding information. In natural language processing, word meaning often depends heavily on previous words in the sentence. Context determines interpretation, as evidenced by the varying meanings of words based on surrounding terms. Similarly, in speech recognition, phoneme interpretation depends on surrounding sounds, while financial forecasting requires understanding historical data patterns.</p>
<p>The fundamental challenge in sequential processing lies in maintaining and updating relevant context over time. Human text comprehension does not restart with each word; rather, a running understanding evolves as new information is processed. Similarly, time-series data processing encounters patterns spanning different timescales, from immediate dependencies to long-term trends. This necessitates an architecture capable of both maintaining state over time and updating it based on new inputs.</p>
<p>These requirements translate into specific architectural demands: the system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must accommodate variable-length sequences while maintaining computational efficiency. These requirements culminate in the recurrent neural network (RNN) architecture.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-274d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-274d">Algorithmic Structure</h3>
<p>RNNs address sequential processing through recurrent connections, distinguishing them from MLPs and CNNs. Rather than merely mapping inputs to outputs, RNNs maintain an internal state updated at each time step, creating a memory mechanism that propagates information forward in time. This temporal dependency modeling capability was first explored by <span class="citation" data-cites="elman1990finding">Elman (<a href="#ref-elman1990finding" role="doc-biblioref">1990</a>)</span>, who demonstrated RNN capacity to identify structure in time-dependent data. Basic RNNs suffer from the vanishing gradient problem<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, constraining their ability to learn long-term dependencies.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Vanishing Gradient Problem</strong>: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude &lt; 1, gradients multiply by values &lt; 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependencies—a key limitation solved by LSTMs and attention mechanisms.</p></div></div><p>The core operation in a basic RNN can be expressed mathematically as: <span class="math display">\[
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span> where <span class="math inline">\(\mathbf{h}_t\)</span> denotes the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{x}_t\)</span> denotes the input at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{W}_{hh}\)</span> contains the recurrent weights, and <span class="math inline">\(\mathbf{W}_{xh}\)</span> contains the input weights, as illustrated in the unfolded network structure in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>.</p>
<p>In word sequence processing, each word may be represented as a 100-dimensional vector (<span class="math inline">\(\mathbf{x}_t\)</span>), with a hidden state of 128 dimensions (<span class="math inline">\(\mathbf{h}_t\)</span>). At each time step, the network combines the current input with its previous state to update its sequential understanding, establishing a memory mechanism capable of capturing patterns across time steps.</p>
<p>This recurrent structure fulfills sequential processing requirements through connections that maintain internal state and propagate information forward in time. Rather than processing all inputs independently, RNNs process sequential data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>. This architecture suits tasks including language modeling, speech recognition, and time-series forecasting.</p>
<p>RNNs implement a recursive algorithm where each time step’s function call depends on the result of the previous call. Analogous to recursive functions that maintain state through the call stack, RNNs maintain state through their hidden vectors. The mathematical formula <span class="math inline">\(\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)\)</span> directly parallels recursive function definitions where <code>f(n) = g(f(n-1), input(n))</code>. This correspondence explains RNN capacity to handle variable-length sequences: just as recursive algorithms process lists of arbitrary length by applying the same function recursively, RNNs process sequences of any length by applying the same recurrent computation.</p>
<section id="efficiency-characteristics-and-optimization-potential" class="level4">
<h4 class="anchored" data-anchor-id="efficiency-characteristics-and-optimization-potential">Efficiency Characteristics and Optimization Potential</h4>
<p>Sequential processing creates computational bottlenecks but enables unique efficiency characteristics for memory usage. RNNs achieve constant memory overhead for hidden state storage regardless of sequence length, making them extremely memory-efficient for long sequences. While Transformers require O(n²) memory for sequence length n, RNNs maintain fixed memory usage, enabling processing of sequences thousands of steps long on modest hardware.</p>
<p>Structured pruning of hidden-to-hidden connections can achieve 10x speedup while maintaining sequence modeling capability. The recurrent weight matrix <span class="math inline">\(W_{hh}\)</span> typically dominates parameter count for large hidden states, but magnitude-based pruning reveals that 70-80% of these connections contribute minimally to temporal dependencies. Block-structured pruning maintains computational efficiency while enabling significant model compression.</p>
<p>Sequential operations accumulate quantization errors, requiring careful quantization point placement and gradient scaling for stable low-precision training. Unlike feedforward networks where quantization errors remain localized, RNN errors propagate through time, making INT8 quantization more challenging. Per-timestep quantization schemes and careful handling of hidden state precision are required for maintaining accuracy in quantized RNN deployments.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="00d28078c95f0324945a05b54b6eed89ff2acb28.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Recurrent Neural Network Unfolding: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences."><img src="dnn_architectures_files/mediabag/00d28078c95f0324945a05b54b6eed89ff2acb28.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Recurrent Neural Network Unfolding</strong>: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-2cb9" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-2cb9">Computational Mapping</h3>
<p>RNN sequential processing creates computational patterns fundamentally different from both MLPs and CNNs. This implementation approach demonstrates how temporal dependencies translate into specific computational requirements.</p>
<p>As shown in <a href="#lst-rnn_layer_step" class="quarto-xref">Listing&nbsp;5</a>, the <code>rnn_layer_step</code> function demonstrates how the operation looks using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input <code>x_t</code> and previous hidden state <code>h_prev</code>, along with two weight matrices: <code>W_hh</code> for hidden-to-hidden connections and <code>W_xh</code> for input-to-hidden connections. Through matrix multiplication operations (<code>matmul</code>), it merges the previous state and current input to generate the next hidden state.</p>
<div id="lst-rnn_layer_step" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: <strong>RNN Layer Step</strong>: Neural networks process sequential data through transformations that integrate current inputs and past states.
</figcaption>
<div aria-describedby="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># x_t: input at time t (batch_size × input_dim)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># h_prev: previous hidden state (batch_size × hidden_dim)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># W_hh: recurrent weights (hidden_dim × hidden_dim)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># W_xh: input weights (input_dim × hidden_dim)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  h_t <span class="op">=</span> activation(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    matmul(h_prev, W_hh)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> matmul(x_t, W_xh)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> b</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation (<a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>). Its actual implementation reveals a more detailed computational reality.</p>
<div id="lst-rnn_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: <strong>Recurrent Layer Computation</strong>: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.
</figcaption>
<div aria-describedby="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize next hidden state</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> np.zeros_like(h_prev)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in the batch</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute recurrent contribution</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (h_prev × W_hh)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> h_prev[batch,j] <span class="op">*</span> W_hh[j,i]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 3: Compute input contribution (x_t × W_xh)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(input_dim):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> x_t[batch,j] <span class="op">*</span> W_xh[j,i]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Add bias and apply activation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            h_t[batch,i] <span class="op">=</span> activation(h_t[batch,i] <span class="op">+</span> b[i])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>rnn_layer_compute</code> expose the core computational pattern of RNNs (see <a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>). Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights <code>W_hh</code>. Loop 3 then incorporates new information from the current input through the input weights <code>W_xh</code>. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.</p>
<p>For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one <span class="math inline">\(128\times 128\)</span> for the recurrent connection and one <span class="math inline">\(100\times 128\)</span> for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle.</p>
</section>
<section id="sec-dnn-architectures-system-implications-d8b3" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-d8b3">System Implications</h3>
<p>Following the analytical framework established for MLPs, RNNs exhibit distinctive patterns in memory requirements, computation needs, and data movement that differ significantly from both dense and spatial processing architectures.</p>
<section id="sec-dnn-architectures-memory-requirements-955b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-955b">Memory Requirements</h4>
<p>RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For the example with input dimension 100 and hidden state dimension 128, this requires storing 12,800 weights for input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 weights for recurrent connections <span class="math inline">\((128\times 128)\)</span>. Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. The system must maintain the hidden state, which constitutes a key factor in memory usage and access patterns.</p>
<p>These memory access patterns create a different profile from MLPs and CNNs. Processors optimize sequential patterns by maintaining weight matrices in cache while streaming through temporal elements. Frameworks optimize temporal processing by batching sequences and managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies; CPUs leverage their cache hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures designed for maintaining state across sequential operations. The specialized hardware optimizations for sequential processing, including memory banking and pipeline architectures, are detailed in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-42fa" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-42fa">Computation Needs</h4>
<p>The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 multiply-accumulates for the recurrent connection <span class="math inline">\((128\times 128)\)</span>.</p>
<p>This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step’s hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.</p>
<p>Processors address sequential constraints through specialized approaches. CPUs pipeline operations within time steps while maintaining temporal ordering. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Software frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible, enabling more efficient utilization of parallel processing resources while respecting the sequential constraints inherent in recurrent architectures.</p>
</section>
<section id="sec-dnn-architectures-data-movement-1cce" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-1cce">Data Movement</h4>
<p>The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.</p>
<p>For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.</p>
<p>Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.</p>
<p>While RNNs established foundational concepts for sequential processing, their architectural constraints—sequential dependencies preventing parallelization and fixed hidden states limiting long-range relationship modeling—motivated the development of attention mechanisms. The following section examines attention-based architectures in depth, including their culmination in Transformers, which have largely superseded RNNs in production systems through dynamic, content-dependent processing that eliminates sequential constraints. This extensive treatment reflects attention mechanisms’ dominance in modern ML systems and their fundamental reimagining of sequential pattern processing.</p>
<div id="quiz-question-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Why are Recurrent Neural Networks (RNNs) particularly suited for sequential data processing?</p>
<ol type="a">
<li>They maintain an internal state that can capture temporal dependencies.</li>
<li>They have a fixed-size input and output structure.</li>
<li>They process data in parallel across all time steps.</li>
<li>They are primarily designed for spatial data processing.</li>
</ol></li>
<li><p>Explain how RNNs handle variable-length sequences efficiently compared to MLPs and CNNs.</p></li>
<li><p>The core operation in a basic RNN involves updating the hidden state using the formula: ____.</p></li>
<li><p>In a production system using RNNs for speech recognition, what are the implications of the sequential processing nature of RNNs on system design?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d">Attention Mechanisms: Dynamic Pattern Processing</h2>
<p>Recurrent Neural Networks successfully introduced memory to handle sequential dependencies, but their fixed sequential processing creates fundamental limitations. RNNs process information in temporal order, making it difficult to capture relationships between distant elements and impossible to parallelize computation across sequence positions. More critically, RNNs assume that temporal proximity correlates with importance—that nearby words or time steps are more relevant than distant ones. This assumption breaks down in many real-world scenarios.</p>
<p>Consider the sentence "The cat, which was sitting by the window overlooking the garden, was sleeping." Here, "cat" and "sleeping" are separated by multiple intervening words, yet they form the core subject-predicate relationship. RNN architectures would process all the intervening elements sequentially, potentially losing this crucial connection in their fixed-capacity hidden state. This limitation revealed the need for architectures that could identify and weight relationships based on content rather than position.</p>
<p>Attention mechanisms emerged as the solution to this architectural constraint by introducing dynamic connectivity patterns that adapt based on input content. Rather than processing elements in predetermined order with fixed relationships, attention mechanisms compute the relevance between all pairs of elements and weight their interactions accordingly. This represents a fundamental shift from structural constraints to learned, data-dependent processing patterns.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-b5e0" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-b5e0">Pattern Processing Needs</h3>
<p>Dynamic pattern processing addresses scenarios where relationships between elements are not fixed by architecture but instead emerge from content. Language translation exemplifies this challenge: when translating “the bank by the river,” understanding “bank” requires attending to “river,” but in “the bank approved the loan,” the important relationship is with “approved” and “loan.” Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, an architecture is required that can dynamically determine which relationships matter.</p>
<p>Expanding beyond language, this requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.</p>
<p>Synthesizing these requirements, dynamic processing demands specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. These capabilities naturally lead us to the Transformer architecture, which implements them through attention mechanisms. <a href="#fig-transformer-attention-visualized" class="quarto-xref">Figure&nbsp;5</a> shows the relationships learned for an attention head between subwords in a sentence.</p>
<div id="fig-transformer-attention-visualized" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0ac6a22c6278235896e8ca59ca4a0d5274e79296.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Attention Weights: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language."><img src="dnn_architectures_files/mediabag/0ac6a22c6278235896e8ca59ca4a0d5274e79296.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Attention Weights</strong>: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-basic-attention-mechanism-9500" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-basic-attention-mechanism-9500">Basic Attention Mechanism</h3>
<p>Attention mechanisms represent a fundamental shift from fixed architectural connections to dynamic, content-based interactions between sequence elements. This section explores the mathematical foundations of attention, examining how query-key-value operations enable flexible pattern processing. We analyze the computational requirements, memory access patterns, and system implications that make attention both powerful and computationally demanding.</p>
<section id="sec-dnn-architectures-algorithmic-structure-1af4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-1af4">Algorithmic Structure</h4>
<p>Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. This approach enables processing of relationships that are not fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism lies a fundamental operation that can be expressed mathematically as:</p>
<div class="no-row-height column-margin column-container"></div><p><span class="math display">\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\]</span></p>
<p>This equation shows scaled dot-product attention. <span class="math inline">\(\mathbf{Q}\)</span> (queries) and <span class="math inline">\(\mathbf{K}\)</span> (keys) are matrix-multiplied to compute similarity scores, divided by <span class="math inline">\(\sqrt{d_k}\)</span> (key dimension) for numerical stability, then normalized with softmax<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> to get attention weights. These weights are applied to <span class="math inline">\(\mathbf{V}\)</span> (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Softmax Function</strong>: Converts a vector of real numbers into a probability distribution where all values sum to 1. Defined as <span class="math inline">\(\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}\)</span>, softmax amplifies differences between inputs (larger values get disproportionately higher probabilities) while ensuring valid attention weights for combining information sources.</p></div><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Query-Key-Value Attention</strong>: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieve—a design that enables flexible, content-based information access.</p></div></div><p>In this equation, <span class="math inline">\(\mathbf{Q}\)</span> (queries), <span class="math inline">\(\mathbf{K}\)</span> (keys), and <span class="math inline">\(\mathbf{V}\)</span> (values)<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> represent learned projections of the input. For a sequence of length <span class="math inline">\(N\)</span> with dimension <span class="math inline">\(d\)</span>, this operation creates an <span class="math inline">\(N\times N\)</span> attention matrix, determining how each position should attend to all others.</p>
<p>The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an <span class="math inline">\(N\times N\)</span> attention matrix through query-key interactions. These steps are illustrated in <a href="#fig-attention" class="quarto-xref">Figure&nbsp;6</a>. Finally, it uses these attention weights to combine value vectors, producing the output.</p>
<div id="fig-attention" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Query-Key-Value Interaction: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Query-Key-Value Interaction</strong>: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
<p>The key is that, unlike the fixed weight matrices found in previous architectures, as shown in <a href="#fig-attention-weightcalc" class="quarto-xref">Figure&nbsp;7</a>, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.</p>
<div id="fig-attention-weightcalc" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="35532ca7bb1ac04e612f9919916aa444c2648681.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Dynamic Attention Weights: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/35532ca7bb1ac04e612f9919916aa444c2648681.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Dynamic Attention Weights</strong>: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-6c7e" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-6c7e">Computational Mapping</h4>
<p>Attention mechanisms create computational patterns that differ significantly from previous architectures. The implementation approach shown in <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a> demonstrates how dynamic connectivity translates into specific computational requirements.</p>
<div id="lst-attention_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: <strong>Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_matrix(Q, K, V):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Q, K, V: (batch_size × seq_len × d_model)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> <span class="op">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>             sqrt(d_k)           <span class="co"># Compute attention scores</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)    <span class="co"># Normalize scores</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(weights, V)  <span class="co"># Combine values</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_compute(Q, K, V):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize outputs</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in batch</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute attention for each query position</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Loop 3: Compare with each key position</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute attention score</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                    scores[b,i,j] <span class="op">+=</span> Q[b,i,d] <span class="op">*</span> K[b,j,d]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                scores[b,i,j] <span class="op">/=</span> sqrt(d_k)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax to scores</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            scores[b,i] <span class="op">=</span> softmax(scores[b,i])</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Combine values using attention weights</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                    outputs[b, i, d] <span class="op">+=</span> (</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                       scores[b, i, j]</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                       <span class="op">*</span> V[b, j, d]</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>attention_layer_compute</code> reveal the true nature of attention’s computational pattern (see <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a>). The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating a quadratic computation pattern with respect to sequence length. The fourth loop uses these attention weights to combine values from all positions, producing the final output.</p>
</section>
<section id="sec-dnn-architectures-system-implications-b5aa" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-b5aa">System Implications</h4>
<p>Applying our three-dimensional analysis framework to attention mechanisms reveals patterns in memory requirements, computation needs, and data movement that distinguish them from previous architectures through dynamic connectivity.</p>
<section id="sec-dnn-architectures-memory-requirements-3dc1" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-3dc1">Memory Requirements</h5>
<p>In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length <span class="math inline">\(N\)</span> and dimension d, each attention layer must store an <span class="math inline">\(N\times N\)</span> attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized <span class="math inline">\(d\times d\)</span>), and input and output feature maps of size <span class="math inline">\(N\times d\)</span>. The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-a41e" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-a41e">Computation Needs</h5>
<p>Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs substantial multiply-accumulate operations across multiple computational stages. The query-key interactions alone require <span class="math inline">\(N\times N\times d\)</span> multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.</p>
</section>
<section id="sec-dnn-architectures-data-movement-12b1" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-12b1">Data Movement</h5>
<p>Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.</p>
<p>These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-transformers-selfattention-427f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-transformers-selfattention-427f">Transformers: The Attention-First Architecture</h3>
<p>While attention mechanisms introduced the concept of dynamic pattern processing, they were initially applied as additions to existing architectures, particularly RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from the fundamental limitations of recurrent architectures: sequential processing constraints that prevented efficient parallelization and difficulties with very long sequences. The breakthrough insight was recognizing that attention mechanisms alone could replace both convolutional and recurrent processing entirely.</p>
<p>Transformers, introduced in the landmark "Attention is All You Need" paper<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> by <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>, represent the culmination of architectural evolution by eliminating all structural constraints in favor of pure content-dependent processing. Rather than adding attention to RNNs, Transformers built the entire architecture around attention mechanisms, introducing self-attention as the primary computational pattern. This architectural decision traded the parameter efficiency of CNNs and the sequential coherence of RNNs for maximum flexibility and parallelizability.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>“Attention is All You Need”</strong>: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.</p></div><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> In <em>Advances in Neural Information Processing Systems</em>, 30:5998–6008. <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>.
</div></div><p>This represents the final step in our architectural journey: from MLPs that connected everything to everything, to CNNs that connected locally, to RNNs that connected sequentially, to Transformers that connect dynamically based on learned content relationships. Each evolution sacrificed constraints for capabilities, with Transformers achieving maximum expressivity at the cost of maximum computational requirements.</p>
<section id="sec-dnn-architectures-algorithmic-structure-1242" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-1242">Algorithmic Structure</h4>
<p>The key innovation in Transformers lies in their use of self-attention layers. In a self-attention layer, the queries, keys, and values are all derived from the same input sequence. This allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence “The animal didn’t cross the street because it was too wide,” self-attention allows the model to link “it” with “street,” capturing long-range dependencies that are challenging for traditional sequential models.</p>
<p>The self-attention mechanism can be expressed mathematically in a form similar to the basic attention mechanism: <span class="math display">\[
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{X}\)</span> is the input sequence, and <span class="math inline">\(\mathbf{W_Q}\)</span>, <span class="math inline">\(\mathbf{W_K}\)</span>, and <span class="math inline">\(\mathbf{W_V}\)</span> are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.</p>
<p>Building on this foundation, Transformers employ multi-head attention, which extends the self-attention mechanism by running multiple attention functions in parallel. Each “head” involves a separate set of query/key/value projections that can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.</p>
<p>The mathematical formulation for multi-head attention is: <span class="math display">\[
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
\]</span> where each attention head is computed as: <span class="math display">\[
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
\]</span></p>
<p>A critical component in both self-attention and multi-head attention is the scaling factor <span class="math inline">\(\sqrt{d_k}\)</span>, which serves an important mathematical purpose. This factor prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. For queries and keys of dimension <span class="math inline">\(d_k\)</span>, their dot product has variance <span class="math inline">\(d_k\)</span>, so dividing by <span class="math inline">\(\sqrt{d_k}\)</span> normalizes the variance to 1, maintaining stable gradients and enabling effective learning.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Attention Scaling</strong>: Without the <span class="math inline">\(\sqrt{d_k}\)</span> scaling factor, large dot products would cause the softmax to saturate, producing gradients close to zero and hindering learning. This mathematical insight enables stable optimization of large Transformer models.</p></div></div><p>Beyond the mathematical mechanics, attention mechanisms can be understood conceptually as implementing a form of content-addressable memory system. Like hash tables that retrieve values based on key matching, attention computes similarity between a query and all available keys, then retrieves a weighted combination of corresponding values. The dot product similarity <code>Q·K</code> functions like a hash function that measures how well each key matches the query. The softmax normalization ensures the weights sum to 1, implementing a probabilistic retrieval mechanism. This connection helps explain why attention is so effective for tasks requiring flexible information retrieval—it provides a differentiable approximation to database lookup operations.</p>
<p>From an information-theoretic perspective, attention mechanisms implement optimal information aggregation under uncertainty. The attention weights can be viewed as representing uncertainty about which parts of the input contain relevant information for the current processing step. The softmax operation implements a maximum entropy principle: among all possible ways to distribute attention across input positions, softmax selects the distribution with maximum entropy subject to the constraint that similarity scores determine relative importance <span class="citation" data-cites="cover2006elements">(<a href="#ref-cover2006elements" role="doc-biblioref">Cover and Thomas 2006</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cover2006elements" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2006. <em>Elements of Information Theory</em>. 2nd ed. John Wiley &amp; Sons.
</div></div></section>
<section id="efficiency-characteristics-and-optimization-potential-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="efficiency-characteristics-and-optimization-potential-1">Efficiency Characteristics and Optimization Potential</h4>
<p>Attention mechanisms are highly redundant, with many heads learning similar patterns. Head pruning and low-rank attention factorization can reduce computation by 50-80% with careful implementation. Analysis of large Transformer models reveals that most attention heads can be classified into a few common patterns (positional, syntactic, semantic), suggesting that explicit architectural specialization could replace learned redundancy.</p>
<p>Attention operations are particularly sensitive to quantization due to the softmax operation and the quadratic number of attention scores. Separate quantization schemes for Q, K, V projections and careful handling of softmax operations are required for stable quantization. Post-training INT8 quantization typically achieves 2-3% accuracy loss, while INT4 quantization requires more sophisticated quantization-aware training approaches.</p>
<p>The quadratic scaling with sequence length creates fundamental efficiency limitations. Sparse attention patterns (such as local windows, strided patterns, or learned sparsity) can reduce complexity from O(n²) to O(n log n) or O(n) while maintaining most modeling capability. Linear attention approximations trade some expressive power for linear scaling, enabling processing of much longer sequences on limited hardware.</p>
<p>This information-theoretic interpretation reveals why attention is so effective for selective processing. The mechanism automatically balances two competing objectives: focusing on the most relevant information (minimizing entropy) while maintaining sufficient breadth to avoid missing important details (maximizing entropy). The attention pattern emerges as the optimal trade-off between these objectives, explaining why transformers can effectively handle long sequences and complex dependencies.</p>
<p>Self-attention can be understood as learning dynamic activation patterns across the input sequence. Unlike CNNs which apply fixed filters or RNNs which use fixed recurrence patterns, attention learns which elements should activate together based on their content. This creates a form of adaptive connectivity where the effective network topology changes for each input. Recent research has shown that attention heads in trained models often specialize in detecting specific linguistic or semantic patterns <span class="citation" data-cites="clark2019what">(<a href="#ref-clark2019what" role="doc-biblioref">Clark et al. 2019</a>)</span>, suggesting that the mechanism naturally discovers interpretable structural regularities in data.</p>
<div class="no-row-height column-margin column-container"><div id="ref-clark2019what" class="csl-entry" role="listitem">
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. <span>“What Does BERT Look at? An Analysis of BERT’s Attention.”</span> In <em>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, 276–86. <a href="https://doi.org/10.18653/v1/W19-4828">https://doi.org/10.18653/v1/W19-4828</a>.
</div></div><p>The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see <a href="#fig-transformer" class="quarto-xref">Figure&nbsp;8</a>). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated significant effectiveness across a wide range of tasks, from natural language processing to computer vision, transforming deep learning architectures across domains.</p>
<div id="fig-transformer" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="aa3aa170a3acdaf91ddfd26c9ea74821e1dd5a86.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Attention Head: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need."><img src="dnn_architectures_files/mediabag/aa3aa170a3acdaf91ddfd26c9ea74821e1dd5a86.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Attention Head</strong>: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-9f79" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-9f79">Computational Mapping</h4>
<p>While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers (see <a href="#lst-self_attention_layer" class="quarto-xref">Listing&nbsp;8</a>):</p>
<div id="lst-self_attention_layer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: <strong>Self-Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention_layer(X, W_Q, W_K, W_V, d_k):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input tensor (batch_size × seq_len × d_model)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_Q, W_K, W_V: weight matrices (d_model × d_k)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> matmul(X, W_Q)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> matmul(X, W_K)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> matmul(X, W_V)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(attention_weights, V)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_head_attention(</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    X, W_Q, W_K, W_V, W_O, num_heads, d_k</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        head_output <span class="op">=</span> self_attention_layer(</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            X, W_Q[i], W_K[i], W_V[i], d_k</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        outputs.append(head_output)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    concat_output <span class="op">=</span> torch.cat(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    final_output <span class="op">=</span> matmul(concat_output, W_O)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-system-implications-295d" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-295d">System Implications</h4>
<p>This implementation reveals key computational characteristics that apply to basic attention mechanisms, with Transformer self-attention representing a specific case. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute <code>Q</code>, <code>K</code>, and <code>V</code> simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.</p>
<p>Second, the attention score computation results in a matrix of size <code>(seq_len × seq_len)</code>, leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.</p>
<p>Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model’s representational power.</p>
<p>Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length <span class="math inline">\(N\)</span> and embedding dimension <span class="math inline">\(d\)</span>, the main operations involve matrices of sizes <span class="math inline">\((N\times d)\)</span>, <span class="math inline">\((d\times d)\)</span>, and <span class="math inline">\((N\times N)\)</span>. These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.</p>
<p>Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix <span class="math inline">\((N\times N)\)</span> and the intermediate results for each attention head create substantial memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.</p>
<p>These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.</p>
<div id="quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the primary advantage of attention mechanisms over traditional architectures like CNNs and RNNs?</p>
<ol type="a">
<li>Dynamic relationship processing based on content</li>
<li>Sequential data processing</li>
<li>Fixed spatial pattern processing</li>
<li>Dense connectivity patterns</li>
</ol></li>
<li><p>Explain how attention mechanisms compute relationships between elements in a sequence. What are the key components involved?</p></li>
<li><p>Order the following steps in the attention mechanism process: (1) Apply softmax to scores, (2) Compute query, key, and value projections, (3) Generate attention matrix, (4) Combine values using attention weights.</p></li>
<li><p>In a production system using Transformer models, what is a significant computational challenge posed by attention mechanisms?</p>
<ol type="a">
<li>Linear scaling with sequence length</li>
<li>Limited parallel processing capability</li>
<li>Quadratic complexity with sequence length</li>
<li>Fixed connectivity patterns</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architectural-building-blocks-a575" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-building-blocks-a575">Architectural Building Blocks</h2>
<p>Having examined four major architectural families—MLPs, CNNs, RNNs, and Transformers—each with distinct computational characteristics and system implications, a unifying perspective emerges. Deep learning architectures, while presented as distinct approaches in previous sections, are better understood as compositions of building blocks that evolved over time. Analogous to complex LEGO structures built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research <span class="citation" data-cites="lecun2015deep">(<a href="#ref-lecun2015deep" role="doc-biblioref">Yann LeCun, Bengio, and Hinton 2015</a>)</span>. Each architectural innovation introduced new building blocks while discovering novel applications of existing ones.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun2015deep" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>“Deep Learning.”</span> <em>Nature</em> 521 (7553): 436–44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div><div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, F. 1958. <span>“The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div></div><p>These building blocks and their evolution illuminate modern architectural design. The simple perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="#ref-rosenblatt1958perceptron" role="doc-biblioref">Rosenblatt 1958</a>)</span> evolved into multi-layer networks <span class="citation" data-cites="rumelhart1986learning">(<a href="#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>, which subsequently spawned specialized patterns for spatial and sequential processing. Each advancement preserved useful elements from predecessors while introducing new computational primitives. Contemporary architectures, such as Transformers, represent carefully engineered combinations of these building blocks.</p>
<p>This progression reveals both the evolution of neural networks and the discovery and refinement of core computational patterns that remain relevant. The exploration of different neural network architectures demonstrates deep learning’s significant evolution, with each new architecture introducing distinct computational demands and system-level challenges.</p>
<p><a href="#tbl-dl-evolution" class="quarto-xref">Table&nbsp;1</a> summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table captures the major shifts in deep learning architecture design and corresponding changes in system-level considerations. The progression spans from early dense matrix operations optimized for CPUs, through convolutions leveraging GPU acceleration and sequential operations necessitating sophisticated memory hierarchies, to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.</p>
<div id="tbl-dl-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Deep Learning Evolution</strong>: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.
</figcaption>
<div aria-describedby="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Era</th>
<th style="text-align: left;">Dominant Architecture</th>
<th style="text-align: left;">Key Primitives</th>
<th style="text-align: left;">System Focus</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Early NN</td>
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Dense Matrix Ops</td>
<td style="text-align: left;">CPU optimization</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN Revolution</td>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Convolutions</td>
<td style="text-align: left;">GPU acceleration</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sequence Modeling</td>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Sequential Ops</td>
<td style="text-align: left;">Memory hierarchies</td>
</tr>
<tr class="even">
<td style="text-align: left;">Attention Era</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Attention, Dynamic Compute</td>
<td style="text-align: left;">Flexible accelerators, High-bandwidth memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Examination of these building blocks reveals how primitives evolved and combined to create increasingly powerful neural network architectures.</p>
<section id="sec-dnn-architectures-perceptron-multilayer-networks-f56e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-perceptron-multilayer-networks-f56e">From Perceptron to Multi-Layer Networks</h3>
<p>While we examined MLPs earlier as a mechanism for dense pattern processing, here we focus on how they established building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.</p>
<p>The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a paradigm that transcends specific architecture types.</p>
<p>Most significantly, the development of MLPs established the backpropagation algorithm<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, which to this day remains the cornerstone of neural network optimization. This key contribution has enabled the development of deep architectures and influenced how later architectures would be designed to maintain gradient flow.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Backpropagation Algorithm</strong>: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This “learning by error propagation” algorithm made deep networks practical and remains virtually unchanged in modern systems—a testament to its importance.</p></div></div><p>These building blocks, layered feature transformation, non-linear activation, and gradient-based learning, set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.</p>
</section>
<section id="sec-dnn-architectures-dense-spatial-processing-0fac" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-dense-spatial-processing-0fac">From Dense to Spatial Processing</h3>
<p>The development of CNNs marked an architectural innovation, specifically the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several building blocks that would influence all future architectures.</p>
<p>The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div><div id="fn22"><p><sup>22</sup>&nbsp;<strong>ResNet Revolution</strong>: ResNet (2016) solved the “degradation problem” where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts (<span class="math inline">\(\mathcal{F}(\mathbf{x}) + \mathbf{x}\)</span>) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.</p></div><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div></div><p>Perhaps even more influential was the introduction of skip connections through ResNets<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>. Originally they were designed to help train very deep CNNs, skip connections have become a building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.</p>
<p>CNNs also introduced batch normalization, a technique for stabilizing neural network optimization by normalizing intermediate features <span class="citation" data-cites="ioffe2015batch">(<a href="#ref-ioffe2015batch" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ioffe2015batch" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> <em>International Conference on Machine Learning</em>, 448–56.
</div></div><p>These innovations, such as parameter sharing, skip connections, and normalization, transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.</p>
</section>
<section id="sec-dnn-architectures-evolution-sequence-processing-e1a9" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-evolution-sequence-processing-e1a9">The Evolution of Sequence Processing</h3>
<p>While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the concept of maintaining and updating state, a building block that influenced how networks could process sequential information, <span class="citation" data-cites="elman1990finding">(<a href="#ref-elman1990finding" role="doc-biblioref">Elman 1990</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-elman1990finding" class="csl-entry" role="listitem">
Elman, Jeffrey L. 1990. <span>“Finding Structure in Time.”</span> <em>Cognitive Science</em> 14 (2): 179–211. <a href="https://doi.org/10.1207/s15516709cog1402\_1">https://doi.org/10.1207/s15516709cog1402\_1</a>.
</div><div id="fn23"><p><sup>23</sup>&nbsp;<strong>LSTM Origins</strong>: Sepp Hochreiter and Jürgen Schmidhuber invented LSTMs in 1997 to solve the “vanishing gradient problem” that plagued RNNs. Their gating mechanism was inspired by biological neurons’ ability to selectively retain information—a breakthrough that enabled sequence modeling and facilitated modern language models.</p></div><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Gated Recurrent Unit (GRU)</strong>: Simplified version of LSTM introduced by Cho et al.&nbsp;(2014) with only 2 gates instead of 3, reducing parameters by ~25% while maintaining similar performance. GRUs became popular for their computational efficiency and easier training, proving that architectural simplification can sometimes improve rather than hurt performance.</p></div><div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div><div id="ref-cho2014properties" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>“On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.”</span> In <em>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</em>, 103–11. Association for Computational Linguistics.
</div></div><p>The development of LSTMs<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> and GRUs<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> brought sophisticated gating mechanisms to neural networks <span class="citation" data-cites="hochreiter1997long cho2014properties">(<a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>; <a href="#ref-cho2014properties" role="doc-biblioref">Cho et al. 2014</a>)</span>. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.</p>
<p>Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight, that architectural patterns could adapt to input structure, laid groundwork for more flexible architectures.</p>
<p>Sequence models also popularized the concept of attention through encoder-decoder architectures <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <em>arXiv Preprint arXiv:1409.0473</em>, September. <a href="http://arxiv.org/abs/1409.0473v7">http://arxiv.org/abs/1409.0473v7</a>.
</div></div></section>
<section id="sec-dnn-architectures-modern-architectures-synthesis-innovation-531d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-modern-architectures-synthesis-innovation-531d">Modern Architectures: Synthesis and Innovation</h3>
<p>Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through strategic combination and refinement of existing components. The Transformer architecture exemplifies this approach: at its core, MLP-style feedforward networks process features between attention layers. The attention mechanism itself builds on sequence model concepts while eliminating recurrent connections, instead employing position embeddings inspired by CNN intuitions. The architecture extensively utilizes skip connections (see <a href="#fig-example-skip-connection" class="quarto-xref">Figure&nbsp;9</a>), inherited from ResNets, while layer normalization, evolved from CNN batch normalization, stabilizes optimization <span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>“Layer Normalization.”</span> <em>arXiv Preprint arXiv:1607.06450</em>, July. <a href="http://arxiv.org/abs/1607.06450v1">http://arxiv.org/abs/1607.06450v1</a>.
</div></div><div id="fig-example-skip-connection" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="72dd6cccbe43b09e773930fb403ff04d18ab90f1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Residual Connection: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance."><img src="dnn_architectures_files/mediabag/72dd6cccbe43b09e773930fb403ff04d18ab90f1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Residual Connection</strong>: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.
</figcaption>
</figure>
</div>
<p>This composition of building blocks creates emergent capabilities exceeding the sum of individual components. The self-attention mechanism, while building on previous attention concepts, enables novel forms of dynamic pattern processing. The arrangement of these components—attention followed by feedforward layers, with skip connections and normalization—has proven sufficiently effective to become a template for new architectures.</p>
<p>Recent innovations in vision and language models follow this pattern of recombining building blocks. Vision Transformers<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> adapt the Transformer architecture to images while maintaining its essential components <span class="citation" data-cites="dosovitskiy2021image">(<a href="#ref-dosovitskiy2021image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. These modern architectural innovations demonstrate the principles of efficient scaling covered in <strong><a href="../core/efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 10: Efficient AI</a></strong>, while their practical implementation challenges and optimizations are explored in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Vision Transformers (ViTs)</strong>: Google’s 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as “words.” ViTs split a <span class="math inline">\(224\times 224\)</span> image into <span class="math inline">\(16\times 16\)</span> patches (196 “tokens”), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.</p></div><div id="ref-dosovitskiy2021image" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>International Conference on Learning Representations</em>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div><p>The following comparison of primitive utilization across different neural network architectures illustrates how modern architectures synthesize and innovate upon previous approaches:</p>
<div id="tbl-primitive-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Primitive Utilization</strong>: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns.
</figcaption>
<div aria-describedby="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive Type</th>
<th style="text-align: left;">MLP</th>
<th style="text-align: left;">CNN</th>
<th style="text-align: left;">RNN</th>
<th style="text-align: left;">Transformer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computational</td>
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Convolution (Matrix Mult.)</td>
<td style="text-align: left;">Matrix Mult. + State Update</td>
<td style="text-align: left;">Matrix Mult. + Attention</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Access</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Strided</td>
<td style="text-align: left;">Sequential + Random</td>
<td style="text-align: left;">Random (Attention)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Movement</td>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Broadcast + Gather</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in <a href="#tbl-primitive-comparison" class="quarto-xref">Table&nbsp;2</a>, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.</p>
<p>This synthesis of primitives in Transformers shows how modern architectures innovate by recombining and refining existing building blocks, rather than inventing entirely new computational paradigms. This evolutionary process provides insight into the development of future architectures and helps guide the design of efficient systems to support them.</p>
<div id="quiz-question-sec-dnn-architectures-architectural-building-blocks-a575" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which of the following innovations introduced the concept of parameter sharing in neural networks?</p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol></li>
<li><p>True or False: The backpropagation algorithm is a fundamental building block that remains unchanged in modern neural network training.</p></li>
<li><p>Explain how skip connections, first introduced in ResNets, have influenced modern neural network architectures like Transformers.</p></li>
<li><p>The introduction of ____ in sequence models allowed networks to dynamically focus on relevant information, paving the way for Transformer architectures.</p></li>
<li><p>In a production system using Transformers for natural language processing, what are the system-level challenges associated with their memory access patterns?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-architectural-building-blocks-a575" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-systemlevel-building-blocks-72f6" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-systemlevel-building-blocks-72f6">System-Level Building Blocks</h2>
<p>Examination of different deep learning architectures enables distillation of their system requirements into primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be decomposed further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these operations.</p>
<section id="sec-dnn-architectures-core-computational-primitives-bd67" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-core-computational-primitives-bd67">Core Computational Primitives</h3>
<p>Three operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. These operations are primitive because they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.</p>
<p>Matrix multiplication represents the basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we’re computing weighted combinations, which is the core operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a <span class="math inline">\(784\times 100\)</span> weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications (turning a <span class="math inline">\(3\times 3\)</span> convolution into a matrix operation, as illustrated in <a href="#fig-im2col-diagram" class="quarto-xref">Figure&nbsp;10</a>), and Transformers use it extensively in their attention mechanisms.</p>
<section id="computational-building-blocks" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="computational-building-blocks">Computational Building Blocks</h4>
<p>Modern neural networks operate through three fundamental computational patterns that appear across all architectures. Understanding these patterns provides insight into how different architectures achieve their computational goals and why certain hardware optimizations are effective.</p>
<p>The detailed analysis of sparse computation patterns, including structured and unstructured sparsity, hardware-aware optimization strategies, and algorithm-hardware co-design principles, is covered systematically in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong> and <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<div id="fig-im2col-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Convolution as Matrix Multiplication: Reshaping convolutional layers into matrix multiplications using the im2col technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms."><img src="dnn_architectures_files/mediabag/b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Convolution as Matrix Multiplication</strong>: Reshaping convolutional layers into matrix multiplications using the <code>im2col</code> technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.
</figcaption>
</figure>
</div>
<p>The im2col<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> (image to column) technique, developed by Intel in the 1990s, accomplishes matrix reshaping by unfolding overlapping image patches into columns of a matrix, as illustrated in <a href="#fig-im2col-diagram" class="quarto-xref">Figure&nbsp;10</a>. Each sliding window position in the convolution becomes a column in the transformed matrix, while the filter kernels are arranged as rows. This allows the convolution operation to be expressed as a standard GEMM (General Matrix Multiply) operation. The transformation trades memory consumption—duplicating data where windows overlap—for computational efficiency, enabling CNNs to leverage decades of BLAS optimizations and achieving 5-10x speedups on CPUs. In modern systems, these matrix multiplications map to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel; NVIDIA’s A100 tensor cores can achieve up to 312 TFLOPS (32-bit) through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve">MKL</a>) that exploit these hardware capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>im2col (Image to Column)</strong>: A data layout transformation developed by Intel in the 1990s that converts convolution operations into matrix multiplications by unfolding image patches into columns. This approach trades memory consumption (through data duplication) for computational efficiency, enabling CNNs to leverage decades of GEMM optimizations and achieving 5-10x speedups on CPUs.</p></div><div id="fn27"><p><sup>27</sup>&nbsp;<strong>Systolic Array</strong>: A network of processing elements that rhythmically compute and pass data through neighbors, like a “heartbeat” of computation. Invented by H.T. Kung and Charles Leiserson in 1978, systolic arrays achieve high throughput by overlapping computation with data movement—Google’s TPU systolic arrays perform 65,536 multiply-accumulate operations per clock cycle. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a <span class="math inline">\(3\times 3\)</span> convolution becomes a <span class="math inline">\(9\times N\)</span> matrix multiplication) and carefully managing data layout in memory to maximize spatial locality.</p></div></div><p>Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a <span class="math inline">\(3\times 3\)</span> convolution filter slides across the <span class="math inline">\(28\times 28\)</span> input, requiring <span class="math inline">\(26\times 26\)</span> windows of computation, assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google’s TPU uses a <span class="math inline">\(128\times 128\)</span> systolic array<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory.</p>
<p>Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a fundamental capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys; for a sequence of length 512, 512 different weight patterns must be computed on the fly. Unlike fixed patterns where the computation graph is known in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges: hardware must provide flexible data routing (modern GPUs employ dynamic scheduling) and support variable computation patterns, while software frameworks require efficient mechanisms for handling data-dependent execution paths (PyTorch’s dynamic computation graphs, TensorFlow’s dynamic control flow).</p>
<p>These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (<span class="math inline">\(512\times 512\)</span> operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing <span class="math inline">\(512\times 512\)</span> attention patterns at runtime). The way these primitives interact creates specific demands on system design, ranging from memory hierarchy organization to computation scheduling.</p>
<p>The building blocks we’ve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, recognizing how these operations shape the demands placed on memory systems becomes essential and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.</p>
</section>
</section>
<section id="sec-dnn-architectures-memory-access-primitives-4e2e" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-memory-access-primitives-4e2e">Memory Access Primitives</h3>
<p>The efficiency of deep learning models depends heavily on memory access and management. Memory access often constitutes the primary bottleneck in modern ML systems; even though a matrix multiplication unit may be capable of performing thousands of operations per cycle, it will remain idle if data is not available at the requisite time. For example, accessing data from DRAM typically requires hundreds of cycles, while on-chip computation requires only a few cycles.</p>
<p>Three memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.</p>
<p>Sequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the <span class="math inline">\(784\times 100\)</span> weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems; DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.</p>
<p>Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with <span class="math inline">\(3\times 3\)</span> filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization, where the im2col transformation in deep learning frameworks converts convolution’s strided access into efficient matrix multiplications.</p>
<p>Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.</p>
<p>These different memory access patterns contribute to the overall memory requirements of each architecture. To illustrate this, <a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.</p>
<div id="tbl-arch-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Memory Access Complexity</strong>: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size (<span class="math inline">\(n &gt; h\)</span>).
</figcaption>
<div aria-describedby="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 24%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Input Dependency</th>
<th style="text-align: left;">Parameter Storage</th>
<th style="text-align: left;">Activation Storage</th>
<th style="text-align: left;">Scaling Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times W)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times W)\)</span></td>
<td style="text-align: left;">Predictable</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Constant</td>
<td style="text-align: left;"><span class="math inline">\(O(K \times C)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B\times H_{\text{img}}\)</span> <span class="math inline">\(\times W_{\text{img}})\)</span></td>
<td style="text-align: left;">Efficient</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(h^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times T \times h)\)</span></td>
<td style="text-align: left;">Challenging</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times d)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times N^2)\)</span></td>
<td style="text-align: left;">Problematic</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span>: Input or sequence size</li>
<li><span class="math inline">\(W\)</span>: Layer width</li>
<li><span class="math inline">\(B\)</span>: Batch size</li>
<li><span class="math inline">\(K\)</span>: Kernel size</li>
<li><span class="math inline">\(C\)</span>: Number of channels</li>
<li><span class="math inline">\(H_{\text{img}}\)</span>: Height of input feature map (CNN)</li>
<li><span class="math inline">\(W_{\text{img}}\)</span>: Width of input feature map (CNN)</li>
<li><span class="math inline">\(h\)</span>: Hidden state size (RNN)</li>
<li><span class="math inline">\(T\)</span>: Sequence length</li>
<li><span class="math inline">\(d\)</span>: Model dimensionality</li>
</ul>
<p><a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory access patterns complement the computational scaling behaviors examined later in <a href="#tbl-computational-complexity" class="quarto-xref">Table&nbsp;6</a>, providing a complete picture of each architecture’s resource requirements. These memory complexity considerations are crucial when making system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.</p>
<p>The impact of these patterns becomes clear when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a <span class="math inline">\(3\times 3\)</span> filter), making effective data reuse necessary for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.</p>
<p>Working set size, the amount of data needed simultaneously for computation, varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.</p>
<p>Understanding these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.</p>
</section>
<section id="sec-dnn-architectures-data-movement-primitives-101a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-primitives-101a">Data Movement Primitives</h3>
<p>While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself, as moving data from off-chip memory typically requires 100-1000$ imes$ more energy than performing a floating-point operation.</p>
<p>Four data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. <a href="#fig-collective-comm" class="quarto-xref">Figure&nbsp;11</a> illustrates these patterns and their relationships. Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects, NVIDIA GPUs provide hardware multicast capabilities, achieving up to 600 GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.</p>
<div id="fig-collective-comm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Collective Communication Patterns: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads."><img src="dnn_architectures_files/mediabag/f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Collective Communication Patterns</strong>: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.
</figcaption>
</figure>
</div>
<p>Scatter operations distribute different elements to different destinations. When parallelizing a <span class="math inline">\(512\times 512\)</span> matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging, as memory conflicts and load imbalance, can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIA’s NVLink offering 600 GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.</p>
<p>Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging, random gathering can be <span class="math inline">\(10\times\)</span> slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.</p>
<p>Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from <span class="math inline">\(O(n)\)</span> to <span class="math inline">\(O(\log n)\)</span>), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.</p>
<p>These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:</p>
<ul>
<li>Broadcasting query vectors (<span class="math inline">\(512\times 64\)</span> elements)</li>
<li>Gathering relevant keys and values (<span class="math inline">\(512\times 512\times 64\)</span> elements)</li>
<li>Reducing attention scores (<span class="math inline">\(512\times 512\)</span> elements per sequence)</li>
</ul>
<p>The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.</p>
<div class="no-row-height column-margin column-container"><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Parameter Scaling</strong>: The leap from AlexNet’s 60 million parameters (2012) to GPT-3’s 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training.</p></div></div></section>
<section id="sec-dnn-architectures-system-design-impact-cd41" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-design-impact-cd41">System Design Impact</h3>
<p>The computational, memory access, and data movement primitives we’ve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective ML systems.</p>
<p>One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Tensor Processing Units</strong>: Google’s TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU’s <span class="math inline">\(128\times 128\)</span> systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.</p></div><div id="fn30"><p><sup>30</sup>&nbsp;<strong>High Bandwidth Memory (HBM)</strong>: Stacked DRAM technology providing 1+ TB/s bandwidth compared to 500 GB/s for traditional GDDR6, developed by AMD and Hynix. HBM enables the massive data movement required by modern AI workloads—GPT-3 training requires moving 1.75 TB of parameters through memory during each forward pass.</p></div><div id="fn31"><p><sup>31</sup>&nbsp;<strong>Scratchpad Memory</strong>: Programmer-controlled on-chip memory providing predictable, fast access without cache management overhead. Unlike caches, scratchpads require explicit data movement but enable precise control over memory allocation—critical for neural network accelerators where memory access patterns are known and performance must be deterministic.</p></div></div><p>Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM)<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> to support the diverse working set sizes of different neural network layers.</p>
<p>The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.</p>
<p><a href="#tbl-sys-design-implications" class="quarto-xref">Table&nbsp;4</a> summarizes the system implications of these primitives:</p>
<div id="tbl-sys-design-implications" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Primitive-Hardware Co-Design</strong>: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.
</figcaption>
<div aria-describedby="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive</th>
<th style="text-align: left;">Hardware Impact</th>
<th style="text-align: left;">Software Optimization</th>
<th style="text-align: left;">Key Challenges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Tensor Cores</td>
<td style="text-align: left;">Batching, GEMM libraries</td>
<td style="text-align: left;">Parallelization, precision</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Specialized datapaths</td>
<td style="text-align: left;">Data layout optimization</td>
<td style="text-align: left;">Stride handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dynamic Computation</td>
<td style="text-align: left;">Flexible routing</td>
<td style="text-align: left;">Dynamic graph execution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sequential Access</td>
<td style="text-align: left;">Burst mode DRAM</td>
<td style="text-align: left;">Contiguous allocation</td>
<td style="text-align: left;">Access latency</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Random Access</td>
<td style="text-align: left;">Large caches</td>
<td style="text-align: left;">Memory-aware scheduling</td>
<td style="text-align: left;">Cache misses</td>
</tr>
<tr class="even">
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Specialized interconnects</td>
<td style="text-align: left;">Operation fusion</td>
<td style="text-align: left;">Bandwidth</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gather/Scatter</td>
<td style="text-align: left;">High-bandwidth memory</td>
<td style="text-align: left;">Work distribution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Despite these advancements, several bottlenecks persist in deep learning models. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.</p>
<section id="energy-consumption-analysis-across-architectures" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="energy-consumption-analysis-across-architectures">Energy Consumption Analysis Across Architectures</h4>
<p>Energy consumption patterns vary dramatically across neural network architectures, with implications for both datacenter deployment and edge computing scenarios. Each architectural pattern exhibits distinct energy characteristics that inform deployment decisions and optimization strategies.</p>
<p>Dense matrix operations in MLPs achieve excellent arithmetic intensity<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> (computation per data movement) but consume substantial absolute energy. Each multiply-accumulate operation consumes approximately 4.6pJ, while data movement from DRAM costs 640pJ per 32-bit value. For typical MLP inference, 70-80% of energy goes to data movement rather than computation, making memory bandwidth optimization critical for energy efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Arithmetic Intensity</strong>: The ratio of floating-point operations to memory accesses, measured in FLOPS per byte. High arithmetic intensity (&gt;10 FLOPS/byte) enables efficient hardware utilization, while low intensity (&lt;1 FLOPS/byte) makes workloads memory-bound. Attention mechanisms typically have low arithmetic intensity, explaining their energy inefficiency.</p></div></div><p>Convolutional operations reduce energy consumption through data reuse but exhibit variable efficiency depending on implementation. Im2col-based convolution implementations trade memory for simplicity, often doubling memory requirements and energy consumption. Direct convolution implementations achieve 3-5x better energy efficiency by eliminating redundant data movement, particularly for larger kernel sizes.</p>
<p>Sequential processing in RNNs creates energy efficiency opportunities through temporal data reuse. The constant memory footprint of RNN hidden states enables aggressive caching strategies, reducing DRAM access energy by 80-90% for long sequences. The sequential dependencies limit parallelization opportunities, often resulting in suboptimal hardware utilization and higher energy per operation.</p>
<p>Attention mechanisms in Transformers exhibit the highest energy consumption per operation due to quadratic scaling and complex data movement patterns. Self-attention operations consume 2-3x more energy per FLOP than standard matrix multiplication due to irregular memory access patterns and the need to store attention matrices. This energy cost scales quadratically with sequence length, making long-sequence processing energy-prohibitive without architectural modifications.</p>
<p>System designers must navigate trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.</p>
<p>Balancing these trade-offs requires consideration of the target workloads and deployment scenarios. Understanding the nature of each primitive guides the development of both hardware and software optimizations in ML systems, allowing designers to make informed decisions about system architecture and resource allocation.</p>
<p>The comprehensive analysis of architectural patterns, computational primitives, and system implications throughout this chapter establishes the foundation for addressing a practical challenge: how do engineers systematically choose the right architecture for their specific problem? The diversity of neural network architectures, each optimized for different data patterns and computational constraints, requires a structured approach to architecture selection. This selection process must consider not only algorithmic performance but also deployment constraints covered in <strong><a href="../core/ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong> and operational efficiency requirements detailed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
<div id="quiz-question-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following operations is considered a core computational primitive in deep learning systems?</p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Gradient descent</li>
<li>Batch normalization</li>
<li>Dropout</li>
</ol></li>
<li><p>Explain how sliding window operations are optimized in modern hardware systems.</p></li>
<li><p>In a production system using Transformers, what is a significant challenge posed by dynamic computation?</p>
<ol type="a">
<li>Fixed computation patterns</li>
<li>Predictable memory access</li>
<li>Runtime decision making</li>
<li>Sequential data processing</li>
</ol></li>
<li><p>Discuss the impact of memory access patterns on system performance in deep learning architectures.</p></li>
<li><p>How might you apply the concept of data movement primitives in optimizing a distributed ML system?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-selection-framework-8b23" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-selection-framework-8b23">Architecture Selection Framework</h2>
<p>The exploration of neural network architectures, from dense MLPs to dynamic Transformers, demonstrates how each design embodies specific assumptions about data structure and computational patterns. MLPs assume arbitrary feature relationships, CNNs exploit spatial locality, RNNs capture temporal dependencies, and Transformers model complex relational patterns. For practitioners facing real-world problems, a fundamental question emerges: how to systematically select the appropriate architecture for a specific use case?</p>
<p>The diversity of available architectures can be overwhelming, particularly when each claims superiority for different scenarios. Successful architecture selection depends less on following trends and more on understanding principles: matching data characteristics to architectural strengths, evaluating computational constraints against system capabilities, and balancing accuracy requirements with deployment realities.</p>
<p>This systematic approach to architecture selection draws upon the computational patterns and system implications we have explored throughout this chapter. By understanding how different architectures process information and their corresponding resource requirements, engineers can make informed decisions that align with both problem requirements and practical constraints. The framework integrates principles from efficient AI design <strong><a href="../core/efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 10: Efficient AI</a></strong> with practical deployment considerations as discussed in ML operations <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
<section id="sec-dnn-architectures-datatoarchitecture-mapping-0b9c" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-datatoarchitecture-mapping-0b9c">Data-to-Architecture Mapping</h3>
<p>The first step in systematic architecture selection involves understanding how different data types align with architectural strengths. Each neural network architecture evolved to address specific patterns in data: MLPs handle arbitrary relationships in tabular data, CNNs exploit spatial locality in images, RNNs capture temporal dependencies in sequences, and Transformers model complex relational patterns where any element might influence any other.</p>
<p>This alignment is not coincidental—it reflects fundamental computational trade-offs. Architectures that match data characteristics can leverage natural structure for efficiency, while mismatched architectures must work against their design assumptions, leading to poor performance or excessive resource consumption.</p>
<p><a href="#tbl-architecture-selection" class="quarto-xref">Table&nbsp;5</a> provides a systematic framework for matching data characteristics to appropriate architectures:</p>
<div id="tbl-architecture-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: <strong>Architecture Selection Framework</strong>: Systematic matching of data characteristics to neural network architectures based on computational requirements and pattern types.
</figcaption>
<div aria-describedby="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 33%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Data Type</th>
<th style="text-align: left;">Key Characteristics</th>
<th style="text-align: left;">Example Applications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MLPs</td>
<td style="text-align: left;">Tabular/Structured</td>
<td style="text-align: left;">• No spatial/temporal •&nbsp;Arbitrary&nbsp;relationships •&nbsp;Dense&nbsp;connectivity</td>
<td style="text-align: left;">• Financial modeling •&nbsp;Medical&nbsp;measurements •&nbsp;Recommendation systems</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNNs</td>
<td style="text-align: left;">Spatial/Grid-like</td>
<td style="text-align: left;">• Local patterns •&nbsp;Translation&nbsp;equivariance •&nbsp;Parameter&nbsp;sharing</td>
<td style="text-align: left;">• Image recognition •&nbsp;2D&nbsp;sensor&nbsp;data •&nbsp;Signal&nbsp;processing</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RNNs</td>
<td style="text-align: left;">Sequential/Temporal</td>
<td style="text-align: left;">• Temporal dependencies •&nbsp;Variable&nbsp;length •&nbsp;Memory&nbsp;across time</td>
<td style="text-align: left;">• Time series forecasting •&nbsp;Simple language tasks •&nbsp;Speech recognition</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transformers</td>
<td style="text-align: left;">Complex Relational</td>
<td style="text-align: left;">• Long-range dependencies •&nbsp;Attention&nbsp;mechanisms •&nbsp;Dynamic relationships</td>
<td style="text-align: left;">• Language understanding •&nbsp;Machine translation •&nbsp;Complex reasoning tasks</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While data characteristics guide initial architecture selection, computational constraints often determine final feasibility. Understanding the scaling behavior of each architecture enables realistic resource planning and deployment decisions.</p>
</section>
<section id="sec-dnn-architectures-computational-complexity-considerations-93fb" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-complexity-considerations-93fb">Computational Complexity Considerations</h3>
<p>Architecture selection must account for computational and memory trade-offs that fundamentally determine deployment feasibility. Each architecture exhibits distinct scaling behaviors that create different bottlenecks as problem size increases. Understanding these patterns enables realistic resource planning and prevents costly architectural mismatches during deployment.</p>
<p>The computational profile of each architecture reflects its underlying design philosophy. Dense architectures like MLPs prioritize representational capacity through full connectivity, while structured architectures like CNNs achieve efficiency through parameter sharing and locality assumptions. Sequential architectures like RNNs trade parallelization for memory efficiency, while attention-based architectures like Transformers exchange memory for computational flexibility. For completeness, we examine these same architectures from both computational scaling and memory access perspectives (see <a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a>), as each viewpoint reveals different optimization opportunities and system design considerations.</p>
<p><a href="#tbl-computational-complexity" class="quarto-xref">Table&nbsp;6</a> summarizes the key computational characteristics of each architecture:</p>
<div id="tbl-computational-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: <strong>Computational Complexity Comparison</strong>: Scaling behaviors and resource requirements for major neural network architectures. Variables: <span class="math inline">\(d\)</span> = dimension, <span class="math inline">\(h\)</span> = hidden size, <span class="math inline">\(k\)</span> = kernel size, <span class="math inline">\(c\)</span>&nbsp;=&nbsp;channels, <span class="math inline">\(H,W\)</span> = spatial dimensions, <span class="math inline">\(T\)</span> = time steps, <span class="math inline">\(n\)</span> = sequence length, <span class="math inline">\(b\)</span> = batch size.
</figcaption>
<div aria-describedby="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 20%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Parameters</th>
<th style="text-align: left;">Forward Pass</th>
<th style="text-align: left;">Memory</th>
<th style="text-align: left;">Parallelization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MLPs</td>
<td style="text-align: left;"><span class="math inline">\(O(d_{\text{in}}\times\)</span> <span class="math inline">\(d_{\text{out}})\)</span> per&nbsp;layer</td>
<td style="text-align: left;"><span class="math inline">\(O(d_{\text{in}}\times\)</span> <span class="math inline">\(d_{\text{out}})\)</span> per&nbsp;layer</td>
<td style="text-align: left;"><span class="math inline">\(O(d^2)\)</span> weights <span class="math inline">\(O(d\times b)\)</span> activations</td>
<td style="text-align: left;">Excellent Matrix ops parallel</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNNs</td>
<td style="text-align: left;"><span class="math inline">\(O(k^2\times\)</span> <span class="math inline">\(c_{\text{in}}\times\)</span> <span class="math inline">\(c_{\text{out}})\)</span> per layer</td>
<td style="text-align: left;"><span class="math inline">\(O(H\times W\times\)</span> <span class="math inline">\(k^2\times\)</span> <span class="math inline">\(c_{\text{in}}\times\)</span> <span class="math inline">\(c_{\text{out}})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(H\times W\times c)\)</span> features <span class="math inline">\(O(k^2\times c^2)\)</span> weights</td>
<td style="text-align: left;">Good Spatial independence</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RNNs</td>
<td style="text-align: left;"><span class="math inline">\(O(h^2+h\times d)\)</span> total</td>
<td style="text-align: left;"><span class="math inline">\(O(T\times h^2)\)</span> for <span class="math inline">\(T\)</span> time&nbsp;steps</td>
<td style="text-align: left;"><span class="math inline">\(O(h)\)</span> hidden state (constant)</td>
<td style="text-align: left;">Poor Sequential deps</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transformers</td>
<td style="text-align: left;"><span class="math inline">\(O(d^2)\)</span> projections <span class="math inline">\(O(d^2\times h)\)</span> multi-head</td>
<td style="text-align: left;"><span class="math inline">\(O(n^2\times d+n\)</span> <span class="math inline">\(\times d²)\)</span> per layer</td>
<td style="text-align: left;"><span class="math inline">\(O(n^2)\)</span> attention <span class="math inline">\(O(n\times d)\)</span> sequences</td>
<td style="text-align: left;">Excellent (positions) Limited by memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="scalability-and-production-considerations" class="level4">
<h4 class="anchored" data-anchor-id="scalability-and-production-considerations">Scalability and Production Considerations</h4>
<p>Production deployment introduces constraints beyond algorithmic performance, including latency requirements, memory limitations, energy budgets, and fault tolerance needs. Each architecture exhibits distinct production characteristics that determine real-world feasibility.</p>
<p>MLPs and CNNs scale well across multiple devices through data parallelism, achieving near-linear speedups with proper batch size scaling. RNNs face fundamental parallelization challenges due to sequential dependencies, requiring pipeline parallelism or other specialized techniques. Transformers achieve excellent parallelization across sequence positions but suffer from quadratic memory scaling that limits batch sizes and effective utilization.</p>
<p>MLPs provide predictable latency proportional to layer size, making them suitable for real-time applications with strict SLA requirements. CNNs exhibit variable latency depending on implementation strategy and hardware capabilities, with optimized implementations achieving sub-millisecond inference. RNNs create latency dependencies on sequence length, making them challenging for interactive applications. Transformers provide excellent throughput for batch processing but struggle with single-inference latency due to attention overhead.</p>
<p>Memory requirements vary significantly across architectures in production environments. MLPs require fixed memory proportional to model size, enabling straightforward capacity planning. CNNs need variable memory for feature maps that scales with input resolution, requiring dynamic memory management for variable-size inputs. RNNs maintain constant memory for hidden states but may require unbounded memory for very long sequences. Transformers face quadratic memory growth that creates hard limits on sequence length in production.</p>
<p>Fault tolerance and recovery characteristics differ substantially between architectures. MLPs and CNNs exhibit stateless computation that enables straightforward checkpointing and recovery. RNNs maintain temporal state that complicates distributed training and failure recovery procedures. Transformers combine stateless computation with massive memory requirements, making checkpoint sizes a practical concern for large models.</p>
<p>Hardware mapping efficiency varies considerably across architectural patterns. Modern MLPs achieve 80-90% of peak hardware performance on specialized tensor units. CNNs reach 60-75% efficiency depending on layer configuration and memory hierarchy design. RNNs typically achieve 30-50% of peak performance due to sequential constraints and irregular memory access patterns. Transformers achieve 70-85% efficiency for large batch sizes but drop significantly for small batches due to attention overhead.</p>
</section>
<section id="hardware-mapping-and-optimization-strategies" class="level4">
<h4 class="anchored" data-anchor-id="hardware-mapping-and-optimization-strategies">Hardware Mapping and Optimization Strategies</h4>
<p>Different architectural patterns require distinct optimization strategies for efficient hardware mapping. Understanding these patterns enables systematic performance tuning and hardware selection decisions.</p>
<p>Dense matrix operations in MLPs map naturally to tensor processing units and GPU tensor cores. These operations benefit from several key optimizations: matrix tiling to fit cache hierarchies, mixed-precision computation to double throughput, and operation fusion to reduce memory traffic. Optimal tile sizes depend on cache hierarchy, typically 64x64 for L1 cache and 256x256 for L2, while tensor cores achieve peak efficiency with specific dimension multiples such as 16x16 blocks for Volta architecture.</p>
<p>CNNs benefit from specialized convolution algorithms and data layout optimizations that differ significantly from dense matrix operations. Im2col transformations convert convolutions to matrix multiplication but double memory usage. Winograd algorithms reduce arithmetic complexity by 2.25x for 3x3 convolutions at the cost of numerical stability. Direct convolution with custom kernels achieves optimal memory efficiency but requires architecture-specific tuning.</p>
<p>RNNs require fundamentally different optimization approaches due to their temporal dependencies. Loop unrolling reduces control overhead but increases memory usage. State vectorization enables SIMD operations across multiple sequences. Wavefront parallelization exploits independence across timesteps for bidirectional processing.</p>
<p>Transformers demand specialized attention optimizations due to their quadratic complexity. FlashAttention algorithms reduce memory usage from O(n²) to O(n) through online softmax computation and gradient recomputation. Sparse attention patterns including local, strided, and random approaches maintain modeling capability while reducing complexity. Multi-query attention shares key and value projections across heads, reducing memory bandwidth by 30-50%.</p>
<p>Multi-Layer Perceptrons represent the most straightforward computational pattern, with costs dominated by matrix multiplications. The dense connectivity that enables MLPs to model arbitrary relationships comes at the price of quadratic parameter growth with layer width. Each neuron connects to every neuron in the previous layer, creating substantial parameter counts that grow quadratically with network width. The computation is dominated by matrix-vector products, which are highly optimized on modern hardware. Matrix operations are inherently parallel and map efficiently to GPU architectures, with each output neuron computed independently. The optimization techniques for reducing these parameter counts, including pruning and low-rank approximations specifically targeting dense layers, are covered in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p>
<p>Convolutional Neural Networks achieve computational efficiency through parameter sharing and spatial locality, but their costs scale with both spatial dimensions and channel depth. The convolution operation’s computational intensity depends heavily on kernel size and feature map resolution. Parameter sharing across spatial locations dramatically reduces memory compared to equivalent MLPs, while computational cost grows linearly with image resolution and quadratically with kernel size. Feature map memory dominates usage and can become prohibitive for high-resolution inputs. Spatial independence enables parallel processing across different spatial locations and channels, though memory bandwidth often becomes the limiting factor.</p>
<p>Recurrent Neural Networks optimize for memory efficiency at the cost of parallelization. Their sequential nature creates computational bottlenecks but enables processing of variable-length sequences with constant memory overhead. The hidden-to-hidden connections (<span class="math inline">\(h^2\)</span> term) dominate parameter count for large hidden states. Sequential dependencies prevent parallel processing across time, making RNNs inherently slower than feedforward alternatives. Their constant memory usage for hidden state storage makes RNNs memory-efficient for long sequences, with this efficiency coming at the cost of computational speed.</p>
<p>Transformers achieve maximum flexibility through attention mechanisms but pay a steep price in memory usage. Their quadratic scaling with sequence length creates fundamental limits on the sequences they can process. Parameter count scales with model dimension but remains independent of sequence length. The <span class="math inline">\(n^2\)</span> term from attention computation dominates for long sequences, while the <span class="math inline">\(n \times d^2\)</span> term from feed-forward layers dominates for short sequences. Attention matrices create the primary memory bottleneck, as each attention head must store pairwise similarities between all sequence positions, leading to prohibitive memory usage for long sequences. While parallelization is excellent across sequence positions and attention heads, the quadratic memory requirement often forces smaller batch sizes, limiting effective parallelization.</p>
<p>These complexity patterns create distinct optimal domains for each architecture. MLPs excel when parameter efficiency is not critical, CNNs dominate for moderate-resolution spatial data, RNNs remain viable for very long sequences where memory is constrained, and Transformers excel for complex relational tasks where their computational cost can be justified by superior performance.</p>
</section>
</section>
<section id="sec-dnn-architectures-decision-framework-dbe8" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-decision-framework-dbe8">Decision Framework</h3>
<p>Effective architecture selection requires balancing multiple competing factors: data characteristics, computational resources, performance requirements, and deployment constraints. While data patterns provide initial guidance and complexity analysis establishes feasibility bounds, final architectural choices often involve nuanced trade-offs demanding systematic evaluation.</p>
<div id="fig-dnn-fm-framework" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4ac702c3bcbdc8d4be392737471a9b509237236e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Architecture Selection Decision Framework: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility."><img src="dnn_architectures_files/mediabag/4ac702c3bcbdc8d4be392737471a9b509237236e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Architecture Selection Decision Framework</strong>: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.
</figcaption>
</figure>
</div>
<p><a href="#fig-dnn-fm-framework" class="quarto-xref">Figure&nbsp;12</a> provides a structured approach to architecture selection decisions, ensuring comprehensive consideration of all relevant factors while avoiding common pitfalls such as selection based on novelty or perceived sophistication. The decision flowchart guides systematic architecture selection by first matching data characteristics to architectural strengths, then validating against practical constraints. The process is inherently iterative—resource limitations or performance gaps often necessitate reconsidering earlier choices.</p>
<p>This framework applies systematically through four key steps. First, data analysis: pattern types in data provide the strongest initial signal. Spatial data naturally aligns with CNNs, sequential data with RNNs. Second, progressive constraint validation: each constraint check (memory, computational budget, inference speed) acts as a filter. Failing any constraint necessitates either scaling down the current architecture or considering a fundamentally different approach.</p>
<p>Third, iterative trade-off handling when accuracy targets remain unmet. Additional model capacity may be required, necessitating a return to constraint checking. If deployment hardware cannot support the chosen architecture, reconsidering the entire architectural approach may be necessary. Fourth, anticipate multiple iterations, as real projects typically cycle through this framework several times before achieving optimal balance between data fit, computational feasibility, and deployment requirements.</p>
<p>This systematic approach prevents architecture selection based solely on novelty or perceived sophistication, ensuring alignment of choices with both problem requirements and system capabilities.</p>
<div id="quiz-question-sec-dnn-architectures-selection-framework-8b23" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>According to the architecture selection decision framework, what should be the first step when choosing a neural network architecture?</p>
<ol type="a">
<li>Check memory budget constraints</li>
<li>Analyze the type of data patterns</li>
<li>Evaluate training time requirements</li>
<li>Consider deployment hardware limitations</li>
</ol></li>
<li><p>Explain why the architecture selection process is described as ‘inherently iterative’ and provide an example of when iteration might be necessary.</p></li>
<li><p>Based on the computational complexity comparison, which architecture would be most suitable for processing very long sequences where memory is severely constrained?</p>
<ol type="a">
<li>Transformers due to excellent parallelization</li>
<li>CNNs due to parameter sharing efficiency</li>
<li>RNNs due to constant memory usage O(h)</li>
<li>MLPs due to simple computational patterns</li>
</ol></li>
<li><p>Order the following constraint validation steps in the decision framework: (1) Check inference speed requirements, (2) Validate memory budget, (3) Assess training time constraints, (4) Verify deployment readiness.</p></li>
<li><p>How do the memory access patterns discussed earlier (sequential, strided, random) relate to the architecture selection decision framework?</p></li>
<li><p>In a production system where deployment hardware has limited computational resources, what does the decision framework suggest if accuracy targets are not met?</p>
<ol type="a">
<li>Always increase model capacity regardless of constraints</li>
<li>Compromise on accuracy to meet hardware limitations</li>
<li>Iterate back to reconsider the entire architecture choice</li>
<li>Focus only on software optimizations</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-selection-framework-8b23" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-fallacies-pitfalls-3e82" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-fallacies-pitfalls-3e82">Fallacies and Pitfalls</h2>
<p>Neural network architectures represent specialized computational structures designed for different data types and problem domains, which creates common misconceptions about their selection and deployment. The rich variety of architectural patterns—from dense networks to transformers—often leads engineers to make choices based on novelty or perceived sophistication rather than task-specific requirements and computational constraints.</p>
<p><strong>Fallacy:</strong> <em>More complex architectures always perform better than simpler ones.</em></p>
<p>This misconception prompts teams to immediately adopt transformer-based models or elaborate architectures without understanding their requirements. While sophisticated architectures such as transformers excel at complex tasks requiring long-range dependencies, they consume significantly more computational resources and memory. For numerous problems, particularly those with limited data or clear structural patterns, simpler architectures such as MLPs or CNNs achieve comparable accuracy with substantially less computational overhead. Architecture selection should correspond to problem complexity rather than defaulting to the most advanced option.</p>
<p><strong>Pitfall:</strong> <em>Ignoring the computational implications of architectural choices during model selection.</em></p>
<p>Many practitioners select architectures based solely on accuracy metrics from academic papers without considering computational requirements. A CNN’s spatial locality assumptions might deliver excellent accuracy for image tasks but require specialized memory access patterns. Similarly, RNNs’ sequential dependencies create serialization bottlenecks that limit parallelization opportunities. This oversight leads to deployment failures when models cannot meet latency requirements or exceed memory constraints in production environments.</p>
<p><strong>Fallacy:</strong> <em>Architecture performance is independent of hardware characteristics.</em></p>
<p>This belief assumes that all architectures perform equally well across different hardware platforms. In reality, different architectures exploit different hardware features: CNNs benefit from specialized tensor cores, MLPs leverage high-bandwidth memory, and RNNs require efficient sequential processing capabilities. A model that achieves optimal performance on GPUs might perform poorly on mobile devices or embedded processors. Understanding hardware-architecture alignment is crucial for effective deployment strategies.</p>
<p><strong>Pitfall:</strong> <em>Mixing architectural patterns without understanding their interaction effects.</em></p>
<p>Combining different architectural components (such as adding attention layers to CNNs or using skip connections in RNNs) can create unexpected computational bottlenecks. Each architectural pattern exhibits distinct memory access patterns and computational characteristics. Naive combinations may eliminate the performance benefits of individual components or create memory bandwidth conflicts. Successful hybrid architectures require careful analysis of how different patterns interact at the system level.</p>
<p><strong>Pitfall:</strong> <em>Designing architectures without considering the full hardware-software co-design implications across the deployment pipeline.</em></p>
<p>Many architecture decisions optimize for high-end GPU performance without considering the complete system lifecycle from development through deployment. An architecture designed for large-scale compute clusters may be poorly suited for edge deployment due to memory constraints, lack of specialized compute units, or limited parallelization capabilities. Similarly, architectures optimized for inference latency might sacrifice development efficiency, leading to longer development cycles and higher computational costs. Effective architecture selection requires analyzing the entire system stack including compute infrastructure, model compilation and optimization tools, target deployment hardware, and operational constraints. The choice between CNN depth and width, transformer head configurations, or activation functions has cascading effects on memory bandwidth utilization, cache efficiency, and numerical precision requirements that must be considered holistically rather than in isolation.</p>
</section>
<section id="sec-dnn-architectures-unified-framework-inductive-biases-a892" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-unified-framework-inductive-biases-a892">Unified Framework: Architectures as Inductive Biases</h2>
<p>The architectural diversity explored—from MLPs to Transformers—can be understood through a unified theoretical framework: each architecture embodies specific inductive biases that constrain the hypothesis space and guide learning toward solutions appropriate for different data types and problem structures.</p>
<p>Different architectures can be arranged in a hierarchy of decreasing inductive bias. CNNs exhibit the strongest inductive biases through local connectivity, parameter sharing, and translation equivariance. These constraints dramatically reduce the parameter space while limiting flexibility to spatial data with local structure. RNNs demonstrate moderate inductive bias through sequential processing and shared temporal weights. The hidden state mechanism assumes that past information influences current processing, rendering them appropriate for temporal sequences.</p>
<p>MLPs maintain minimal architectural bias beyond layer-wise processing. Dense connectivity allows modeling arbitrary relationships but requires more data to learn structure that other architectures encode explicitly. Transformers represent adaptive inductive bias through learned attention patterns. The architecture can dynamically adjust its inductive bias based on the data, combining flexibility with the ability to discover relevant structural regularities.</p>
<p>All successful architectures implement forms of hierarchical representation learning, but through different mechanisms. CNNs build spatial hierarchies through progressive receptive field expansion, learning features from edges to objects to scenes. RNNs build temporal hierarchies through hidden state evolution, capturing dependencies from local transitions to long-term patterns. Transformers build content-dependent hierarchies through multi-head attention, allowing different heads to capture different types of relationships simultaneously.</p>
<p>This hierarchical organization reflects a fundamental principle: complex patterns can be efficiently represented through composition of simpler components. The success of deep learning can be understood as the discovery that gradient-based optimization can effectively learn these compositional structures when provided with appropriate architectural inductive biases.</p>
<p>The theoretical insights about representation learning have direct implications for systems engineering. Hierarchical representations require computational patterns that can efficiently compose lower-level features into higher-level abstractions. This drives system design decisions:</p>
<ul>
<li>Memory hierarchies must align with representational hierarchies to minimize data movement costs</li>
<li>Parallelization strategies must respect the dependency structure of hierarchical computation</li>
<li>Hardware accelerators must efficiently support the matrix operations that implement feature composition</li>
<li>Software frameworks must provide abstractions that enable efficient hierarchical computation across diverse architectures</li>
</ul>
<p>Understanding architectures as embodying different inductive biases helps explain both their strengths and their systems requirements, providing a principled foundation for architecture selection and system optimization decisions.</p>
</section>
<section id="sec-dnn-architectures-summary-c495" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-summary-c495">Summary</h2>
<p>Neural network architectures form specialized computational structures tailored to process different types of data and solve distinct classes of problems. Multi-Layer Perceptrons handle tabular data through dense connections, convolutional networks exploit spatial locality in images, and recurrent networks process sequential information. Each architecture embodies specific assumptions about data structure and computational patterns. Modern transformer architectures unify many of these concepts through attention mechanisms that dynamically route information based on relevance rather than fixed connectivity patterns.</p>
<p>Despite their apparent diversity, these architectures share fundamental computational primitives that recur across different designs. Matrix multiplication operations form the computational core, whether in dense layers, convolutions, or attention mechanisms. Memory access patterns vary significantly between architectures, with some requiring sliding window operations for local processing while others demand global information aggregation. Dynamic computation patterns in attention mechanisms create data-dependent execution flows that challenge traditional optimization approaches.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Different architectures embody specific assumptions about data structure: MLPs for tabular data, CNNs for spatial relationships, RNNs for sequences, Transformers for flexible attention</li>
<li>Shared computational primitives including matrix operations, sliding windows, and dynamic routing form the foundation across diverse architectures</li>
<li>Memory access patterns and data movement requirements vary significantly between architectures, directly impacting system performance and optimization strategies</li>
<li>Understanding the mapping between algorithmic intent and system implementation enables effective performance optimization and hardware selection</li>
</ul>
</div>
</div>
<p>The architectural foundations established in this chapter—computational patterns, memory access characteristics, and data movement primitives—directly inform the design of specialized hardware and optimization strategies explored in subsequent chapters. Understanding that CNNs exhibit spatial locality enables the development of systolic arrays optimized for convolution operations (<strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>). Recognizing that Transformers demand quadratic memory scaling motivates attention-specific optimizations such as FlashAttention and sparse attention patterns (<strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>). The progression from architectural understanding to hardware design to algorithmic optimization represents a systematic approach to ML systems engineering.</p>
<p>As architectures become more dynamic and sophisticated, the relationship between algorithmic innovation and systems optimization becomes increasingly critical for achieving practical performance gains in real-world deployments. The operational challenges of deploying and maintaining these sophisticated architectures in production environments are comprehensively addressed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, while the broader implications for sustainable AI development, including energy efficiency considerations stemming from architectural choices, are explored in <strong><a href="../core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 17: Sustainable AI</a></strong>.</p>
<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
<div id="quiz-question-sec-dnn-architectures-summary-c495" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a core computational primitive commonly found in deep learning architectures?</p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Sorting algorithms</li>
<li>String matching</li>
<li>Graph traversal</li>
</ol></li>
<li><p>True or False: The architectural diversity in deep learning systems does not significantly impact system design and computational requirements.</p></li>
<li><p>Explain how the identification of shared computational primitives can aid in the design of deep learning systems.</p></li>
<li><p>In a production system, what are the implications of optimizing fundamental computational patterns for deep learning architectures?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-summary-c495" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-dnn-architectures-overview-8d17" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key consideration when mapping neural network architectures to computer system resources?</strong></p>
<ol type="a">
<li>Algorithmic complexity</li>
<li>Memory access patterns</li>
<li>User interface design</li>
<li>Data visualization techniques</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Memory access patterns. This is correct because memory access patterns determine how data moves through the memory hierarchy, which is crucial for efficient system resource utilization. Other options like algorithmic complexity and user interface design do not directly relate to system resource mapping.</p>
<p><em>Learning Objective</em>: Understand the key considerations for mapping neural network architectures to system resources.</p></li>
<li><p><strong>Explain how dense connectivity patterns in neural networks impact memory bandwidth demands.</strong></p>
<p><em>Answer</em>: Dense connectivity patterns in neural networks require high memory bandwidth because they involve extensive data transfer between neurons, leading to increased demands on the memory hierarchy. For example, fully connected layers in a neural network need to access and process large amounts of data simultaneously, which can strain memory resources. This is important because efficient memory bandwidth utilization is crucial for optimizing neural network performance on hardware.</p>
<p><em>Learning Objective</em>: Analyze the impact of neural network connectivity patterns on memory bandwidth requirements.</p></li>
<li><p><strong>Which neural network architecture is best suited for managing temporal dependencies?</strong></p>
<ol type="a">
<li>Recurrent Neural Networks (RNNs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Multi-layer Perceptrons (MLPs)</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Recurrent Neural Networks (RNNs). This is correct because RNNs are specifically designed to handle temporal dependencies by maintaining state information across time steps. Other architectures like MLPs and CNNs are not inherently designed for temporal sequence processing.</p>
<p><em>Learning Objective</em>: Identify the neural network architecture suited for specific computational challenges.</p></li>
<li><p><strong>Discuss the implications of stateful processing on on-chip memory organization in neural networks.</strong></p>
<p><em>Answer</em>: Stateful processing in neural networks, such as in RNNs, requires careful on-chip memory organization to maintain state information across time steps. This involves allocating memory resources efficiently to store and update state variables, which can impact the design of memory hierarchies and data flow within the chip. For example, stateful processing might necessitate larger on-chip caches or specialized memory units to handle the dynamic data requirements. This is important because it affects the overall efficiency and performance of the neural network when implemented in hardware.</p>
<p><em>Learning Objective</em>: Understand the impact of stateful processing on memory organization in neural networks.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-overview-8d17" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary advantage of using Multi-Layer Perceptrons (MLPs) for dense pattern processing?</strong></p>
<ol type="a">
<li>Low computational cost</li>
<li>Ability to model arbitrary feature interactions</li>
<li>High interpretability of model decisions</li>
<li>Reduced memory requirements</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Ability to model arbitrary feature interactions. MLPs allow each output to depend on any combination of inputs, making them versatile for dense pattern processing. Options A, C, and D are incorrect because MLPs are computationally intensive, not easily interpretable, and have high memory requirements.</p>
<p><em>Learning Objective</em>: Understand the advantages of MLPs in dense pattern processing.</p></li>
<li><p><strong>Explain how the Universal Approximation Theorem supports the use of MLPs in deep learning systems.</strong></p>
<p><em>Answer</em>: The Universal Approximation Theorem states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain. This supports the use of MLPs in deep learning by providing a theoretical foundation that they can learn complex patterns and functions, making them versatile for various applications. This is important because it justifies the use of MLPs in systems where flexibility and adaptability are required.</p>
<p><em>Learning Objective</em>: Understand the theoretical foundation provided by the Universal Approximation Theorem for MLPs.</p></li>
<li><p><strong>In the context of MLPs, the operation that transforms input vectors through matrix multiplication followed by element-wise activation is known as ____. </strong></p>
<p><em>Answer</em>: dense connectivity. This operation involves connecting each neuron to every neuron in adjacent layers, allowing complex feature interactions.</p>
<p><em>Learning Objective</em>: Recall the key operation that enables MLPs to perform dense pattern processing.</p></li>
<li><p><strong>How does the dense connectivity pattern of MLPs impact system design in terms of memory and computation?</strong></p>
<p><em>Answer</em>: Dense connectivity in MLPs requires storing and accessing large weight matrices, leading to high memory demands. Each output neuron requires multiple multiply-accumulate operations, increasing computational needs. Systems must optimize data movement and computation through efficient memory hierarchies and parallel processing. This is crucial for achieving performance efficiency in MLP implementations.</p>
<p><em>Learning Objective</em>: Analyze the system-level implications of dense connectivity patterns in MLPs.</p></li>
<li><p><strong>In a production system using MLPs for image recognition, what is a likely challenge related to data movement?</strong></p>
<ol type="a">
<li>Insufficient data storage capacity</li>
<li>Excessive data redundancy</li>
<li>Lack of data preprocessing</li>
<li>Limited bandwidth for data transfer</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Limited bandwidth for data transfer. MLPs require significant data movement due to their all-to-all connectivity, which can strain system bandwidth. Options B, C, and D are less relevant as the primary challenge is efficiently moving large volumes of data.</p>
<p><em>Learning Objective</em>: Identify challenges related to data movement in MLP-based systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary advantage of convolutional neural networks (CNNs) over multi-layer perceptrons (MLPs) when processing spatial data?</strong></p>
<ol type="a">
<li>CNNs can process data in parallel more efficiently.</li>
<li>CNNs have a simpler architecture than MLPs.</li>
<li>CNNs are better at processing temporal data.</li>
<li>CNNs require fewer parameters due to weight sharing.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. CNNs require fewer parameters due to weight sharing. This is because CNNs use the same filters across different spatial positions, reducing the number of unique weights needed compared to MLPs.</p>
<p><em>Learning Objective</em>: Understand the parameter efficiency of CNNs due to weight sharing in spatial data processing.</p></li>
<li><p><strong>Explain how the spatial pattern processing capability of CNNs contributes to their effectiveness in image recognition tasks.</strong></p>
<p><em>Answer</em>: CNNs are effective in image recognition because they can detect spatial patterns such as edges, textures, and shapes regardless of their position in the image. This is achieved through convolutional layers that apply filters across the image, creating translation invariance and allowing CNNs to recognize objects in various positions.</p>
<p><em>Learning Objective</em>: Understand the role of spatial pattern processing in CNNs for image recognition.</p></li>
<li><p><strong>Order the following steps in a convolutional neural network’s processing of an image: (1) Apply filters to detect features, (2) Flatten the feature maps, (3) Use pooling to reduce dimensionality, (4) Classify using fully connected layers.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Apply filters to detect features, (3) Use pooling to reduce dimensionality, (2) Flatten the feature maps, (4) Classify using fully connected layers. This sequence reflects the typical flow in CNNs from feature detection to classification.</p>
<p><em>Learning Objective</em>: Understand the sequential processing steps in CNNs for image analysis.</p></li>
<li><p><strong>In a production system, what are the implications of CNNs’ memory requirements on system design?</strong></p>
<p><em>Answer</em>: CNNs’ memory requirements impact system design by necessitating efficient weight reuse and feature map management. Systems must optimize memory access patterns, such as caching frequently used weights and streaming feature map data, to handle the spatial locality and high data throughput demands of CNNs.</p>
<p><em>Learning Objective</em>: Analyze the memory implications of CNN architecture on system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-1d8c" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Why are Recurrent Neural Networks (RNNs) particularly suited for sequential data processing?</strong></p>
<ol type="a">
<li>They maintain an internal state that can capture temporal dependencies.</li>
<li>They have a fixed-size input and output structure.</li>
<li>They process data in parallel across all time steps.</li>
<li>They are primarily designed for spatial data processing.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. They maintain an internal state that can capture temporal dependencies. This allows RNNs to model the sequence of data over time, unlike architectures designed for fixed-size inputs or spatial data.</p>
<p><em>Learning Objective</em>: Understand why RNNs are suitable for sequential data due to their ability to capture temporal dependencies.</p></li>
<li><p><strong>Explain how RNNs handle variable-length sequences efficiently compared to MLPs and CNNs.</strong></p>
<p><em>Answer</em>: RNNs handle variable-length sequences by maintaining an internal state that is updated at each time step, allowing them to process sequences of any length. Unlike MLPs and CNNs, which require fixed-size inputs, RNNs’ recurrent connections enable them to adapt to the sequence length dynamically. This is important for tasks like language modeling where input size can vary.</p>
<p><em>Learning Objective</em>: Explain the efficiency of RNNs in handling variable-length sequences compared to other neural network architectures.</p></li>
<li><p><strong>The core operation in a basic RNN involves updating the hidden state using the formula: ____.</strong></p>
<p><em>Answer</em>: <span class="math inline">\(\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)\)</span>. This formula shows how the hidden state is updated using the previous state, current input, and respective weights.</p>
<p><em>Learning Objective</em>: Recall the mathematical formula used in RNNs for updating the hidden state.</p></li>
<li><p><strong>In a production system using RNNs for speech recognition, what are the implications of the sequential processing nature of RNNs on system design?</strong></p>
<p><em>Answer</em>: The sequential processing nature of RNNs implies that each time step depends on the previous one, which can limit parallelization across time steps. This requires careful system design to manage computational and memory resources efficiently, often leveraging batch processing and optimized data movement strategies to maintain throughput. This is important for real-time applications such as speech recognition.</p>
<p><em>Learning Objective</em>: Understand the system design implications of using RNNs in production systems, especially concerning their sequential processing nature.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-3904" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the primary advantage of attention mechanisms over traditional architectures like CNNs and RNNs?</strong></p>
<ol type="a">
<li>Dynamic relationship processing based on content</li>
<li>Sequential data processing</li>
<li>Fixed spatial pattern processing</li>
<li>Dense connectivity patterns</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Dynamic relationship processing based on content. This is correct because attention mechanisms can adapt their processing patterns based on the input data, unlike fixed-pattern architectures like CNNs and RNNs.</p>
<p><em>Learning Objective</em>: Understand the primary advantage of attention mechanisms in handling dynamic relationships.</p></li>
<li><p><strong>Explain how attention mechanisms compute relationships between elements in a sequence. What are the key components involved?</strong></p>
<p><em>Answer</em>: Attention mechanisms compute relationships by using queries, keys, and values derived from the input sequence. They calculate an attention matrix through query-key interactions, which is then used to weight the value vectors, allowing the model to focus on relevant information. This process enables dynamic pattern processing based on content.</p>
<p><em>Learning Objective</em>: Explain the computational process of attention mechanisms and identify their key components.</p></li>
<li><p><strong>Order the following steps in the attention mechanism process: (1) Apply softmax to scores, (2) Compute query, key, and value projections, (3) Generate attention matrix, (4) Combine values using attention weights.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Compute query, key, and value projections, (3) Generate attention matrix, (1) Apply softmax to scores, (4) Combine values using attention weights. This order reflects the sequential steps in processing input sequences through attention mechanisms.</p>
<p><em>Learning Objective</em>: Understand the sequential steps involved in the attention mechanism process.</p></li>
<li><p><strong>In a production system using Transformer models, what is a significant computational challenge posed by attention mechanisms?</strong></p>
<ol type="a">
<li>Linear scaling with sequence length</li>
<li>Limited parallel processing capability</li>
<li>Quadratic complexity with sequence length</li>
<li>Fixed connectivity patterns</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Quadratic complexity with sequence length. This is correct because attention mechanisms require computing an attention matrix that scales quadratically with the sequence length, posing computational challenges for long sequences.</p>
<p><em>Learning Objective</em>: Identify computational challenges associated with attention mechanisms in Transformer models.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-architectural-building-blocks-a575" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following innovations introduced the concept of parameter sharing in neural networks?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Convolutional Neural Networks (CNNs). CNNs introduced parameter sharing by using the same parameters across different parts of the input, making networks more efficient. MLPs and RNNs do not inherently use parameter sharing in the same way.</p>
<p><em>Learning Objective</em>: Understand the role of parameter sharing in CNNs and its impact on neural network efficiency.</p></li>
<li><p><strong>True or False: The backpropagation algorithm is a fundamental building block that remains unchanged in modern neural network training.</strong></p>
<p><em>Answer</em>: True. The backpropagation algorithm, introduced with MLPs, remains a cornerstone of neural network training due to its effectiveness in propagating gradients through deep architectures.</p>
<p><em>Learning Objective</em>: Recognize the lasting impact of the backpropagation algorithm on neural network training.</p></li>
<li><p><strong>Explain how skip connections, first introduced in ResNets, have influenced modern neural network architectures like Transformers.</strong></p>
<p><em>Answer</em>: Skip connections, introduced in ResNets, allow gradients to flow directly through networks, mitigating the vanishing gradient problem. They enable deeper networks by providing direct paths for information flow. In Transformers, skip connections are used extensively to improve optimization and performance, allowing for efficient training of complex models.</p>
<p><em>Learning Objective</em>: Analyze the influence of skip connections on the design and training of modern neural networks.</p></li>
<li><p><strong>The introduction of ____ in sequence models allowed networks to dynamically focus on relevant information, paving the way for Transformer architectures.</strong></p>
<p><em>Answer</em>: attention mechanisms. Attention mechanisms enable networks to focus on important parts of the input dynamically, which is foundational for Transformer architectures.</p>
<p><em>Learning Objective</em>: Understand the significance of attention mechanisms in the evolution of neural network architectures.</p></li>
<li><p><strong>In a production system using Transformers for natural language processing, what are the system-level challenges associated with their memory access patterns?</strong></p>
<p><em>Answer</em>: Transformers require high memory bandwidth due to their attention mechanisms, which involve random memory access patterns. This can lead to challenges in efficiently managing data movement and memory utilization, necessitating advanced hardware solutions like flexible accelerators and high-bandwidth memory to optimize performance.</p>
<p><em>Learning Objective</em>: Evaluate the system-level challenges of deploying Transformers in production environments.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-architectural-building-blocks-a575" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following operations is considered a core computational primitive in deep learning systems?</strong></p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Gradient descent</li>
<li>Batch normalization</li>
<li>Dropout</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Matrix multiplication. This is correct because matrix multiplication is a fundamental operation in neural networks, used extensively in layer computations, convolutions, and attention mechanisms. Gradient descent, batch normalization, and dropout are optimization and regularization techniques, not core computational primitives.</p>
<p><em>Learning Objective</em>: Understand the fundamental computational operations underpinning deep learning architectures.</p></li>
<li><p><strong>Explain how sliding window operations are optimized in modern hardware systems.</strong></p>
<p><em>Answer</em>: Sliding window operations are optimized using specialized memory access patterns and data buffering schemes. For example, Google’s TPU uses a systolic array to systematically flow data through processing elements, maximizing data reuse. Software frameworks transform these operations into efficient matrix multiplications and manage data layout to enhance spatial locality. This optimization is crucial for efficient CNN processing.</p>
<p><em>Learning Objective</em>: Analyze how hardware and software optimizations enhance the efficiency of sliding window operations in deep learning.</p></li>
<li><p><strong>In a production system using Transformers, what is a significant challenge posed by dynamic computation?</strong></p>
<ol type="a">
<li>Fixed computation patterns</li>
<li>Predictable memory access</li>
<li>Runtime decision making</li>
<li>Sequential data processing</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Runtime decision making. This is correct because dynamic computation requires decisions to be made during runtime, which creates specific implementation challenges like flexible routing of data and support for variable computation patterns. Fixed computation patterns and predictable memory access are not characteristics of dynamic computation.</p>
<p><em>Learning Objective</em>: Identify challenges associated with dynamic computation in Transformer models.</p></li>
<li><p><strong>Discuss the impact of memory access patterns on system performance in deep learning architectures.</strong></p>
<p><em>Answer</em>: Memory access patterns like sequential, strided, and random access significantly impact system performance. Sequential access is efficient and aligns well with modern memory systems. Strided access, common in CNNs, is less efficient but supported through caching strategies. Random access, prevalent in Transformers, poses challenges due to cache misses and unpredictable latencies. Optimizing these patterns is crucial for performance.</p>
<p><em>Learning Objective</em>: Evaluate how different memory access patterns affect the performance of deep learning systems.</p></li>
<li><p><strong>How might you apply the concept of data movement primitives in optimizing a distributed ML system?</strong></p>
<p><em>Answer</em>: In a distributed ML system, understanding data movement primitives like broadcast, scatter, gather, and reduction can optimize communication. For instance, using efficient broadcast techniques can reduce data transfer time in parallel processing, while optimized gather operations can minimize latency in attention mechanisms. Implementing these optimizations can enhance system performance and scalability.</p>
<p><em>Learning Objective</em>: Apply knowledge of data movement primitives to optimize distributed machine learning systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-selection-framework-8b23" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>According to the architecture selection decision framework, what should be the first step when choosing a neural network architecture?</strong></p>
<ol type="a">
<li>Check memory budget constraints</li>
<li>Analyze the type of data patterns</li>
<li>Evaluate training time requirements</li>
<li>Consider deployment hardware limitations</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Analyze the type of data patterns. The framework emphasizes that data characteristics provide the strongest initial signal for architecture selection, as spatial data aligns with CNNs, sequential data with RNNs, etc. Other constraints are validated later in the process.</p>
<p><em>Learning Objective</em>: Understand the systematic approach to architecture selection starting with data analysis.</p></li>
<li><p><strong>Explain why the architecture selection process is described as ‘inherently iterative’ and provide an example of when iteration might be necessary.</strong></p>
<p><em>Answer</em>: The process is iterative because resource limitations or performance gaps often require reconsidering earlier choices. For example, if a chosen Transformer architecture meets accuracy targets but exceeds memory budget constraints, you must iterate back to either scale down the model or consider a different architecture like an RNN. This iterative nature ensures optimal balance between requirements and constraints.</p>
<p><em>Learning Objective</em>: Understand the iterative nature of architecture selection and when reconsideration is necessary.</p></li>
<li><p><strong>Based on the computational complexity comparison, which architecture would be most suitable for processing very long sequences where memory is severely constrained?</strong></p>
<ol type="a">
<li>Transformers due to excellent parallelization</li>
<li>CNNs due to parameter sharing efficiency</li>
<li>RNNs due to constant memory usage O(h)</li>
<li>MLPs due to simple computational patterns</li>
</ol>
<p><em>Answer</em>: The correct answer is C. RNNs due to constant memory usage O(h). RNNs maintain constant memory overhead for hidden state storage regardless of sequence length, making them extremely memory-efficient for long sequences. Transformers have O(n²) memory scaling, making them unsuitable for memory-constrained long sequence processing.</p>
<p><em>Learning Objective</em>: Apply computational complexity analysis to make informed architecture selection decisions.</p></li>
<li><p><strong>Order the following constraint validation steps in the decision framework: (1) Check inference speed requirements, (2) Validate memory budget, (3) Assess training time constraints, (4) Verify deployment readiness.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Validate memory budget, (3) Assess training time constraints, (1) Check inference speed requirements, (4) Verify deployment readiness. This sequence reflects the progressive constraint filtering approach where each step acts as a filter before moving to the next consideration.</p>
<p><em>Learning Objective</em>: Understand the systematic progression of constraint validation in architecture selection.</p></li>
<li><p><strong>How do the memory access patterns discussed earlier (sequential, strided, random) relate to the architecture selection decision framework?</strong></p>
<p><em>Answer</em>: Memory access patterns directly influence the constraint validation steps in the decision framework. For example, Transformers’ random access patterns create higher memory bandwidth demands and potential cache misses, affecting both memory budget and inference speed constraints. Understanding these patterns helps predict system performance during the constraint validation phase, ensuring realistic architecture selection decisions.</p>
<p><em>Learning Objective</em>: Connect memory access pattern analysis with practical architecture selection constraints.</p></li>
<li><p><strong>In a production system where deployment hardware has limited computational resources, what does the decision framework suggest if accuracy targets are not met?</strong></p>
<ol type="a">
<li>Always increase model capacity regardless of constraints</li>
<li>Compromise on accuracy to meet hardware limitations</li>
<li>Iterate back to reconsider the entire architecture choice</li>
<li>Focus only on software optimizations</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Iterate back to reconsider the entire architecture choice. The framework emphasizes that if deployment hardware can’t support your choice after trying to increase capacity for accuracy, you may need to reconsider the entire architecture, finding a different approach that better balances accuracy and resource constraints.</p>
<p><em>Learning Objective</em>: Apply the iterative decision framework to resolve conflicts between accuracy targets and deployment constraints.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-selection-framework-8b23" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-summary-c495" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a core computational primitive commonly found in deep learning architectures?</strong></p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Sorting algorithms</li>
<li>String matching</li>
<li>Graph traversal</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Matrix multiplication. This is a fundamental operation in many deep learning architectures, such as MLPs and Transformers, due to its role in connecting layers and processing data. Sorting algorithms, string matching, and graph traversal are not typically core operations in deep learning.</p>
<p><em>Learning Objective</em>: Identify core computational primitives in deep learning systems.</p></li>
<li><p><strong>True or False: The architectural diversity in deep learning systems does not significantly impact system design and computational requirements.</strong></p>
<p><em>Answer</em>: False. The architectural diversity in deep learning systems significantly impacts system design and computational requirements because each architecture has unique patterns and primitives that dictate memory and processing needs.</p>
<p><em>Learning Objective</em>: Understand the impact of architectural diversity on system design.</p></li>
<li><p><strong>Explain how the identification of shared computational primitives can aid in the design of deep learning systems.</strong></p>
<p><em>Answer</em>: Identifying shared computational primitives helps streamline system design by focusing on optimizing these common operations across different architectures. For example, optimizing matrix multiplication can improve performance in both MLPs and Transformers. This is important because it allows for more efficient hardware and software solutions, leading to scalable and powerful systems.</p>
<p><em>Learning Objective</em>: Explain the role of shared computational primitives in system design.</p></li>
<li><p><strong>In a production system, what are the implications of optimizing fundamental computational patterns for deep learning architectures?</strong></p>
<p><em>Answer</em>: Optimizing fundamental computational patterns, such as matrix multiplication and sliding windows, can lead to significant improvements in system efficiency and scalability. For example, enhanced data movement and memory access patterns can reduce latency and increase throughput. This is important because it allows for handling larger models and datasets, ultimately pushing the boundaries of AI capabilities.</p>
<p><em>Learning Objective</em>: Discuss the practical implications of optimizing computational patterns in production systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-summary-c495" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>



</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">DL Primer</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <span class="nav-page-text">AI Workflow</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>