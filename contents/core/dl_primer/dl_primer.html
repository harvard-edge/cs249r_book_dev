<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/dnn_architectures/dnn_architectures.html" rel="next">
<link href="../../../contents/core/ml_systems/ml_systems.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-7d7a324115cf7dbad1614d42bf25b632.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dl_primer/dl_primer.html">DL Primer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>ðŸš§ DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">a9787f74</code> â€¢ 2025-09-14 14:27 UTC â€¢ <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version â†’</em></a></p>
<p>ðŸŽ‰ <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news â†’</a><br></p>
<p>ðŸš€ <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">TinyðŸ”¥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>ðŸ§  <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>ðŸ“¦ <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-dl-primer" id="toc-sec-dl-primer" class="nav-link active" data-scroll-target="#sec-dl-primer">DL Primer</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-dl-primer-overview-9e60" id="toc-sec-dl-primer-overview-9e60" class="nav-link" data-scroll-target="#sec-dl-primer-overview-9e60">Overview</a></li>
  <li><a href="#sec-dl-primer-evolution-deep-learning-fb02" id="toc-sec-dl-primer-evolution-deep-learning-fb02" class="nav-link" data-scroll-target="#sec-dl-primer-evolution-deep-learning-fb02">The Evolution to Deep Learning</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-rulebased-programming-16ec" id="toc-sec-dl-primer-rulebased-programming-16ec" class="nav-link" data-scroll-target="#sec-dl-primer-rulebased-programming-16ec">Rule-Based Programming</a></li>
  <li><a href="#sec-dl-primer-classical-machine-learning-0bc8" id="toc-sec-dl-primer-classical-machine-learning-0bc8" class="nav-link" data-scroll-target="#sec-dl-primer-classical-machine-learning-0bc8">Classical Machine Learning</a></li>
  <li><a href="#sec-dl-primer-neural-networks-representation-learning-0d1a" id="toc-sec-dl-primer-neural-networks-representation-learning-0d1a" class="nav-link" data-scroll-target="#sec-dl-primer-neural-networks-representation-learning-0d1a">Neural Networks and Representation Learning</a></li>
  <li><a href="#sec-dl-primer-neural-system-implications-1ef5" id="toc-sec-dl-primer-neural-system-implications-1ef5" class="nav-link" data-scroll-target="#sec-dl-primer-neural-system-implications-1ef5">Neural System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-computation-patterns-0254" id="toc-sec-dl-primer-computation-patterns-0254" class="nav-link" data-scroll-target="#sec-dl-primer-computation-patterns-0254">Computation Patterns</a></li>
  <li><a href="#sec-dl-primer-memory-systems-5119" id="toc-sec-dl-primer-memory-systems-5119" class="nav-link" data-scroll-target="#sec-dl-primer-memory-systems-5119">Memory Systems</a></li>
  <li><a href="#sec-dl-primer-system-scaling-49fe" id="toc-sec-dl-primer-system-scaling-49fe" class="nav-link" data-scroll-target="#sec-dl-primer-system-scaling-49fe">System Scaling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dl-primer-biological-artificial-neurons-bb4f" id="toc-sec-dl-primer-biological-artificial-neurons-bb4f" class="nav-link" data-scroll-target="#sec-dl-primer-biological-artificial-neurons-bb4f">Biological to Artificial Neurons</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-biological-intelligence-7528" id="toc-sec-dl-primer-biological-intelligence-7528" class="nav-link" data-scroll-target="#sec-dl-primer-biological-intelligence-7528">Biological Intelligence</a></li>
  <li><a href="#sec-dl-primer-transition-artificial-neurons-f44f" id="toc-sec-dl-primer-transition-artificial-neurons-f44f" class="nav-link" data-scroll-target="#sec-dl-primer-transition-artificial-neurons-f44f">Transition to Artificial Neurons</a></li>
  <li><a href="#sec-dl-primer-artificial-intelligence-cfb8" id="toc-sec-dl-primer-artificial-intelligence-cfb8" class="nav-link" data-scroll-target="#sec-dl-primer-artificial-intelligence-cfb8">Artificial Intelligence</a></li>
  <li><a href="#sec-dl-primer-computational-translation-1c53" id="toc-sec-dl-primer-computational-translation-1c53" class="nav-link" data-scroll-target="#sec-dl-primer-computational-translation-1c53">Computational Translation</a></li>
  <li><a href="#sec-dl-primer-system-requirements-fdc2" id="toc-sec-dl-primer-system-requirements-fdc2" class="nav-link" data-scroll-target="#sec-dl-primer-system-requirements-fdc2">System Requirements</a></li>
  <li><a href="#sec-dl-primer-evolution-impact-ae30" id="toc-sec-dl-primer-evolution-impact-ae30" class="nav-link" data-scroll-target="#sec-dl-primer-evolution-impact-ae30">Evolution and Impact</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-neural-network-fundamentals-68cd" id="toc-sec-dl-primer-neural-network-fundamentals-68cd" class="nav-link" data-scroll-target="#sec-dl-primer-neural-network-fundamentals-68cd">Neural Network Fundamentals</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-basic-architecture-0553" id="toc-sec-dl-primer-basic-architecture-0553" class="nav-link" data-scroll-target="#sec-dl-primer-basic-architecture-0553">Basic Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-neurons-activations-622e" id="toc-sec-dl-primer-neurons-activations-622e" class="nav-link" data-scroll-target="#sec-dl-primer-neurons-activations-622e">Neurons and Activations</a></li>
  <li><a href="#sec-dl-primer-layers-connections-fa4f" id="toc-sec-dl-primer-layers-connections-fa4f" class="nav-link" data-scroll-target="#sec-dl-primer-layers-connections-fa4f">Layers and Connections</a></li>
  <li><a href="#sec-dl-primer-data-flow-transformations-5260" id="toc-sec-dl-primer-data-flow-transformations-5260" class="nav-link" data-scroll-target="#sec-dl-primer-data-flow-transformations-5260">Data Flow and Transformations</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-weights-biases-71bc" id="toc-sec-dl-primer-weights-biases-71bc" class="nav-link" data-scroll-target="#sec-dl-primer-weights-biases-71bc">Weights and Biases</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-weight-matrices-709c" id="toc-sec-dl-primer-weight-matrices-709c" class="nav-link" data-scroll-target="#sec-dl-primer-weight-matrices-709c">Weight Matrices</a></li>
  <li><a href="#sec-dl-primer-connection-patterns-a389" id="toc-sec-dl-primer-connection-patterns-a389" class="nav-link" data-scroll-target="#sec-dl-primer-connection-patterns-a389">Connection Patterns</a></li>
  <li><a href="#sec-dl-primer-bias-terms-367a" id="toc-sec-dl-primer-bias-terms-367a" class="nav-link" data-scroll-target="#sec-dl-primer-bias-terms-367a">Bias Terms</a></li>
  <li><a href="#sec-dl-primer-parameter-organization-6d17" id="toc-sec-dl-primer-parameter-organization-6d17" class="nav-link" data-scroll-target="#sec-dl-primer-parameter-organization-6d17">Parameter Organization</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-network-topology-11ff" id="toc-sec-dl-primer-network-topology-11ff" class="nav-link" data-scroll-target="#sec-dl-primer-network-topology-11ff">Network Topology</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-basic-structure-56bb" id="toc-sec-dl-primer-basic-structure-56bb" class="nav-link" data-scroll-target="#sec-dl-primer-basic-structure-56bb">Basic Structure</a></li>
  <li><a href="#sec-dl-primer-design-tradeoffs-adee" id="toc-sec-dl-primer-design-tradeoffs-adee" class="nav-link" data-scroll-target="#sec-dl-primer-design-tradeoffs-adee">Design Trade-offs</a></li>
  <li><a href="#sec-dl-primer-connection-patterns-c46f" id="toc-sec-dl-primer-connection-patterns-c46f" class="nav-link" data-scroll-target="#sec-dl-primer-connection-patterns-c46f">Connection Patterns</a></li>
  <li><a href="#sec-dl-primer-parameter-considerations-10f3" id="toc-sec-dl-primer-parameter-considerations-10f3" class="nav-link" data-scroll-target="#sec-dl-primer-parameter-considerations-10f3">Parameter Considerations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dl-primer-learning-process-38a0" id="toc-sec-dl-primer-learning-process-38a0" class="nav-link" data-scroll-target="#sec-dl-primer-learning-process-38a0">Learning Process</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-training-overview-1171" id="toc-sec-dl-primer-training-overview-1171" class="nav-link" data-scroll-target="#sec-dl-primer-training-overview-1171">Training Overview</a></li>
  <li><a href="#sec-dl-primer-forward-propagation-d412" id="toc-sec-dl-primer-forward-propagation-d412" class="nav-link" data-scroll-target="#sec-dl-primer-forward-propagation-d412">Forward Propagation</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-layer-computation-5636" id="toc-sec-dl-primer-layer-computation-5636" class="nav-link" data-scroll-target="#sec-dl-primer-layer-computation-5636">Layer Computation</a></li>
  <li><a href="#sec-dl-primer-mathematical-representation-6c2a" id="toc-sec-dl-primer-mathematical-representation-6c2a" class="nav-link" data-scroll-target="#sec-dl-primer-mathematical-representation-6c2a">Mathematical Representation</a></li>
  <li><a href="#sec-dl-primer-computational-process-a092" id="toc-sec-dl-primer-computational-process-a092" class="nav-link" data-scroll-target="#sec-dl-primer-computational-process-a092">Computational Process</a></li>
  <li><a href="#sec-dl-primer-practical-considerations-6167" id="toc-sec-dl-primer-practical-considerations-6167" class="nav-link" data-scroll-target="#sec-dl-primer-practical-considerations-6167">Practical Considerations</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-loss-functions-d892" id="toc-sec-dl-primer-loss-functions-d892" class="nav-link" data-scroll-target="#sec-dl-primer-loss-functions-d892">Loss Functions</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-basic-concepts-397d" id="toc-sec-dl-primer-basic-concepts-397d" class="nav-link" data-scroll-target="#sec-dl-primer-basic-concepts-397d">Basic Concepts</a></li>
  <li><a href="#sec-dl-primer-classification-losses-9278" id="toc-sec-dl-primer-classification-losses-9278" class="nav-link" data-scroll-target="#sec-dl-primer-classification-losses-9278">Classification Losses</a></li>
  <li><a href="#sec-dl-primer-loss-computation-b815" id="toc-sec-dl-primer-loss-computation-b815" class="nav-link" data-scroll-target="#sec-dl-primer-loss-computation-b815">Loss Computation</a></li>
  <li><a href="#sec-dl-primer-training-implications-e004" id="toc-sec-dl-primer-training-implications-e004" class="nav-link" data-scroll-target="#sec-dl-primer-training-implications-e004">Training Implications</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-backward-propagation-9a49" id="toc-sec-dl-primer-backward-propagation-9a49" class="nav-link" data-scroll-target="#sec-dl-primer-backward-propagation-9a49">Backward Propagation</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-gradient-flow-66f2" id="toc-sec-dl-primer-gradient-flow-66f2" class="nav-link" data-scroll-target="#sec-dl-primer-gradient-flow-66f2">Gradient Flow</a></li>
  <li><a href="#sec-dl-primer-gradient-computation-b46b" id="toc-sec-dl-primer-gradient-computation-b46b" class="nav-link" data-scroll-target="#sec-dl-primer-gradient-computation-b46b">Gradient Computation</a></li>
  <li><a href="#sec-dl-primer-implementation-aspects-411b" id="toc-sec-dl-primer-implementation-aspects-411b" class="nav-link" data-scroll-target="#sec-dl-primer-implementation-aspects-411b">Implementation Aspects</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-optimization-process-5160" id="toc-sec-dl-primer-optimization-process-5160" class="nav-link" data-scroll-target="#sec-dl-primer-optimization-process-5160">Optimization Process</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-gradient-descent-basics-903a" id="toc-sec-dl-primer-gradient-descent-basics-903a" class="nav-link" data-scroll-target="#sec-dl-primer-gradient-descent-basics-903a">Gradient Descent Basics</a></li>
  <li><a href="#sec-dl-primer-batch-processing-89d7" id="toc-sec-dl-primer-batch-processing-89d7" class="nav-link" data-scroll-target="#sec-dl-primer-batch-processing-89d7">Batch Processing</a></li>
  <li><a href="#sec-dl-primer-training-loop-e2e1" id="toc-sec-dl-primer-training-loop-e2e1" class="nav-link" data-scroll-target="#sec-dl-primer-training-loop-e2e1">Training Loop</a></li>
  <li><a href="#sec-dl-primer-practical-considerations-accc" id="toc-sec-dl-primer-practical-considerations-accc" class="nav-link" data-scroll-target="#sec-dl-primer-practical-considerations-accc">Practical Considerations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dl-primer-prediction-phase-4204" id="toc-sec-dl-primer-prediction-phase-4204" class="nav-link" data-scroll-target="#sec-dl-primer-prediction-phase-4204">Prediction Phase</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-inference-basics-47d7" id="toc-sec-dl-primer-inference-basics-47d7" class="nav-link" data-scroll-target="#sec-dl-primer-inference-basics-47d7">Inference Basics</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-training-vs-inference-098e" id="toc-sec-dl-primer-training-vs-inference-098e" class="nav-link" data-scroll-target="#sec-dl-primer-training-vs-inference-098e">Training vs Inference</a></li>
  <li><a href="#sec-dl-primer-basic-pipeline-8598" id="toc-sec-dl-primer-basic-pipeline-8598" class="nav-link" data-scroll-target="#sec-dl-primer-basic-pipeline-8598">Basic Pipeline</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-preprocessing-992d" id="toc-sec-dl-primer-preprocessing-992d" class="nav-link" data-scroll-target="#sec-dl-primer-preprocessing-992d">Pre-processing</a></li>
  <li><a href="#sec-dl-primer-inference-dd7f" id="toc-sec-dl-primer-inference-dd7f" class="nav-link" data-scroll-target="#sec-dl-primer-inference-dd7f">Inference</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-network-initialization-3d95" id="toc-sec-dl-primer-network-initialization-3d95" class="nav-link" data-scroll-target="#sec-dl-primer-network-initialization-3d95">Network Initialization</a></li>
  <li><a href="#sec-dl-primer-forward-pass-computation-81c3" id="toc-sec-dl-primer-forward-pass-computation-81c3" class="nav-link" data-scroll-target="#sec-dl-primer-forward-pass-computation-81c3">Forward Pass Computation</a></li>
  <li><a href="#sec-dl-primer-resource-requirements-9f9b" id="toc-sec-dl-primer-resource-requirements-9f9b" class="nav-link" data-scroll-target="#sec-dl-primer-resource-requirements-9f9b">Resource Requirements</a></li>
  <li><a href="#sec-dl-primer-optimization-opportunities-d5ec" id="toc-sec-dl-primer-optimization-opportunities-d5ec" class="nav-link" data-scroll-target="#sec-dl-primer-optimization-opportunities-d5ec">Optimization Opportunities</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-postprocessing-93d9" id="toc-sec-dl-primer-postprocessing-93d9" class="nav-link" data-scroll-target="#sec-dl-primer-postprocessing-93d9">Post-processing</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-case-study-usps-postal-service-aa64" id="toc-sec-dl-primer-case-study-usps-postal-service-aa64" class="nav-link" data-scroll-target="#sec-dl-primer-case-study-usps-postal-service-aa64">Case Study: USPS Postal Service</a>
  <ul class="collapse">
  <li><a href="#sec-dl-primer-realworld-problem-5233" id="toc-sec-dl-primer-realworld-problem-5233" class="nav-link" data-scroll-target="#sec-dl-primer-realworld-problem-5233">Real-world Problem</a></li>
  <li><a href="#sec-dl-primer-system-development-02bf" id="toc-sec-dl-primer-system-development-02bf" class="nav-link" data-scroll-target="#sec-dl-primer-system-development-02bf">System Development</a></li>
  <li><a href="#sec-dl-primer-complete-pipeline-2253" id="toc-sec-dl-primer-complete-pipeline-2253" class="nav-link" data-scroll-target="#sec-dl-primer-complete-pipeline-2253">Complete Pipeline</a></li>
  <li><a href="#sec-dl-primer-results-impact-bb54" id="toc-sec-dl-primer-results-impact-bb54" class="nav-link" data-scroll-target="#sec-dl-primer-results-impact-bb54">Results and Impact</a></li>
  <li><a href="#sec-dl-primer-key-takeaways-64ec" id="toc-sec-dl-primer-key-takeaways-64ec" class="nav-link" data-scroll-target="#sec-dl-primer-key-takeaways-64ec">Key Takeaways</a></li>
  </ul></li>
  <li><a href="#sec-dl-primer-summary-19d0" id="toc-sec-dl-primer-summary-19d0" class="nav-link" data-scroll-target="#sec-dl-primer-summary-19d0">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dl_primer/dl_primer.html">DL Primer</a></li></ol></nav></header>




<section id="sec-dl-primer" class="level1 page-columns page-full">
<h1>DL Primer</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALLÂ·E 3 Prompt: A rectangular illustration divided into two halves on a clean white background. The left side features a detailed and colorful depiction of a biological neural network, showing interconnected neurons with glowing synapses and dendrites. The right side displays a sleek and modern artificial neural network, represented by a grid of interconnected nodes and edges resembling a digital circuit. The transition between the two sides is distinct but harmonious, with each half clearly illustrating its respective theme: biological on the left and artificial on the right.</em></p>
</div></div><p> <img src="images/png/cover_nn_primer.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What inspiration from nature drives the development of machine learning systems, and how do biological neural processes inform their design?</em></p>
<p>Neural networks draw their conceptual foundation from biological neural systems, but their computational implementation requires understanding specific mathematical mechanisms. Artificial neurons mimic biological neurons through weighted connections and activation functions, but their learning processes rely on gradient-based optimization rather than biological mechanisms. These computational building blocksâ€”artificial neurons, network structures, and training algorithmsâ€”establish the foundation for all subsequent machine learning engineering decisions. Understanding how biological inspiration translates into practical computation enables engineers to make informed choices about model design, computational requirements, and system trade-offs.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the biological inspiration for artificial neural networks and how this foundation informs their design and function.</p></li>
<li><p>Explore the fundamental structure of neural networks, including neurons, layers, and connections.</p></li>
<li><p>Examine the processes of forward propagation, backward propagation, and optimization as the core mechanisms of learning.</p></li>
<li><p>Understand the complete machine learning pipeline, from pre-processing through neural computation to post-processing.</p></li>
<li><p>Compare and contrast training and inference phases, understanding their distinct computational requirements and optimizations.</p></li>
<li><p>Learn how neural networks process data to extract patterns and make predictions, bridging theoretical concepts with computational implementations.</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-dl-primer-overview-9e60" class="level2">
<h2 class="anchored" data-anchor-id="sec-dl-primer-overview-9e60">Overview</h2>
<p>Neural networks, a foundational concept within machine learning and artificial intelligence, are computational models inspired by the structure and function of biological neural systems. These networks represent a critical intersection of algorithms, mathematical frameworks, and computing infrastructure, making them integral to solving complex problems in AI.</p>
<p>When studying neural networks, it is helpful to place them within the broader hierarchy of AI and machine learning. <a href="#fig-ai-ml-dl" class="quarto-xref">Figure&nbsp;1</a> provides a visual representation of this context. AI, as the overarching field, encompasses all computational methods that aim to mimic human cognitive functions. Within AI, machine learning includes techniques that enable systems to learn patterns from data. Neural networks, a key subset of ML, form the backbone of more advanced learning systems, including deep learning, by modeling complex relationships in data through interconnected computational units.</p>
<div id="fig-ai-ml-dl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ai-ml-dl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ai_dl_progress_nvidia.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: AI Hierarchy: Neural networks form a critical component of deep learning within machine learning and artificial intelligence by modeling complex patterns in large datasets. Machine learning algorithms enable systems to learn from data, making them an essential subset of the broader AI field."><img src="images/png/ai_dl_progress_nvidia.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ai-ml-dl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>AI Hierarchy</strong>: Neural networks form a critical component of deep learning within machine learning and artificial intelligence by modeling complex patterns in large datasets. Machine learning algorithms enable systems to learn from data, making them an essential subset of the broader AI field.
</figcaption>
</figure>
</div>
<p>The emergence of neural networks reflects key shifts in how AI systems process information across three fundamental dimensions:</p>
<ul>
<li><p><strong>Data</strong>: From manually structured and rule based datasets to raw, high dimensional data. Neural networks are particularly adept at learning from complex and unstructured data, making them essential for tasks involving images, speech, and text.</p></li>
<li><p><strong>Algorithms</strong>: From explicitly programmed rules to adaptive systems capable of learning patterns directly from data. Neural networks eliminate the need for manual feature engineering by discovering representations automatically through layers of interconnected units.</p></li>
<li><p><strong>Computation</strong>: From simple, sequential operations to massively parallel computations. The scalability of neural networks has driven demand for advanced hardware, such as GPUs, that can efficiently process large models and datasets.</p></li>
</ul>
<p>These shifts emphasize the importance of understanding neural networks not only as mathematical constructs but also as practical components of real world AI systems. The development and deployment of neural networks require careful consideration of computational efficiency, data processing workflows, and hardware optimization. To build a strong foundation, this chapter focuses on the core principles of neural networks, exploring their structure, functionality, and learning mechanisms. Understanding these basics prepares readers to explore more advanced architectures and their systems level implications in later chapters.</p>
<div id="quiz-question-sec-dl-primer-overview-9e60" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the relationship between AI, machine learning, and neural networks?</p>
<ol type="a">
<li>AI is a subset of machine learning, which includes neural networks.</li>
<li>AI, machine learning, and neural networks are independent fields.</li>
<li>Neural networks are a subset of AI, which includes machine learning.</li>
<li>Machine learning is a subset of AI, and neural networks are a subset of machine learning.</li>
</ol></li>
<li><p>True or False: Neural networks require manual feature engineering to learn patterns from data.</p></li>
<li><p>Explain how neural networks have shifted computational requirements in AI systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-overview-9e60" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-dl-primer-evolution-deep-learning-fb02" class="level2">
<h2 class="anchored" data-anchor-id="sec-dl-primer-evolution-deep-learning-fb02">The Evolution to Deep Learning</h2>
<p>The current era of AI represents a transformative advance in computational problem solving, marking the latest stage in an evolution from rule based programming through classical machine learning to modern neural networks. To understand its significance, we must trace this progression and examine how each approach builds upon and addresses the limitations of its predecessors.</p>
<section id="sec-dl-primer-rulebased-programming-16ec" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-rulebased-programming-16ec">Rule-Based Programming</h3>
<p>Traditional programming requires developers to explicitly define rules that tell computers how to process inputs and produce outputs. Consider a simple game like Breakout, shown in <a href="#fig-breakout" class="quarto-xref">Figure&nbsp;2</a>. The program needs explicit rules for every interaction: when the ball hits a brick, the code must specify that the brick should be removed and the ballâ€™s direction should be reversed. While this approach works well for games with clear physics and limited states, it demonstrates an inherent limitation of rule based systems.</p>
<div id="fig-breakout" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-breakout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="e868a3869708202033534d3affebb34572b591dc.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Rule-Based System: Traditional programming relies on explicitly defined rules to map inputs to outputs, limiting adaptability to complex or uncertain environments as every possible scenario must be anticipated and coded. This approach contrasts with machine learning, where systems learn patterns from data instead of relying on pre-programmed logic."><img src="dl_primer_files/mediabag/e868a3869708202033534d3affebb34572b591dc.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-breakout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Rule-Based System</strong>: Traditional programming relies on explicitly defined rules to map inputs to outputs, limiting adaptability to complex or uncertain environments as every possible scenario must be anticipated and coded. This approach contrasts with machine learning, where systems learn patterns from data instead of relying on pre-programmed logic.
</figcaption>
</figure>
</div>
<p>This rule based paradigm extends to all traditional programming, as illustrated in <a href="#fig-traditional" class="quarto-xref">Figure&nbsp;3</a>. The program takes both rules for processing and input data to produce outputs. Early artificial intelligence research explored whether this approach could scale to solve complex problems by encoding sufficient rules to capture intelligent behavior.</p>
<div id="fig-traditional" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-traditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="2141c896d3885901604d1968bd755f4e5f07efaf.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Rule-Based Programming: Traditional programs operate on data using explicitly defined rules, forming the basis for early AI systems but lacking the adaptability of modern machine learning approaches. This paradigm contrasts with data-driven learning, where the system infers rules from examples rather than relying on pre-programmed logic."><img src="dl_primer_files/mediabag/2141c896d3885901604d1968bd755f4e5f07efaf.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-traditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Rule-Based Programming</strong>: Traditional programs operate on data using explicitly defined rules, forming the basis for early AI systems but lacking the adaptability of modern machine learning approaches. This paradigm contrasts with data-driven learning, where the system infers rules from examples rather than relying on pre-programmed logic.
</figcaption>
</figure>
</div>
<p>However, the limitations of rule based approaches become evident when addressing complex real world tasks. Consider the problem of recognizing human activities, shown in <a href="#fig-activity-rules" class="quarto-xref">Figure&nbsp;4</a>. Initial rules might appear straightforward: classify movement below 4 mph as walking and faster movement as running. Yet real world complexity quickly emerges. The classification must account for variations in speed, transitions between activities, and numerous edge cases. Each new consideration requires additional rules, leading to increasingly complex decision trees.</p>
<div id="fig-activity-rules" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-activity-rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/activities.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Rule-Based Programming: Traditional programs rely on explicitly defined rules to operate on data, forming the basis for early AI systems but lacking adaptability in complex tasks."><img src="images/png/activities.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-activity-rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Rule-Based Programming</strong>: Traditional programs rely on explicitly defined rules to operate on data, forming the basis for early AI systems but lacking adaptability in complex tasks.
</figcaption>
</figure>
</div>
<p>This challenge extends to computer vision tasks. Detecting objects like cats in images would require rules about pointed ears, whiskers, typical body shapes. Such rules would need to account for variations in viewing angle, lighting conditions, partial occlusions, and natural variations among instances. Early computer vision systems attempted this approach through geometric rules but achieved success only in controlled environments with well-defined objects.</p>
<p>This knowledge engineering approach characterized artificial intelligence research in the 1970s and 1980s. Expert systems encoded domain knowledge as explicit rules, showing promise in specific domains with well defined parameters but struggling with tasks humans perform naturally, such as object recognition, speech understanding, or natural language interpretation. These limitations highlighted a fundamental challenge: many aspects of intelligent behavior rely on implicit knowledge that resists explicit rule based representation.</p>
</section>
<section id="sec-dl-primer-classical-machine-learning-0bc8" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-classical-machine-learning-0bc8">Classical Machine Learning</h3>
<p>The limitations of pure rule based systems led researchers to explore approaches that could learn from data. Machine learning offered a promising direction: instead of writing rules for every situation, we could write programs that found patterns in examples. However, the success of these methods still depended heavily on human insight to define what patterns might be important, a process known as feature engineering.</p>
<p>Feature engineering involves transforming raw data into representations that make patterns more apparent to learning algorithms. In computer vision, researchers developed sophisticated methods to extract meaningful patterns from images. The Histogram of Oriented Gradients (HOG) method, shown in <a href="#fig-hog" class="quarto-xref">Figure&nbsp;5</a>, exemplifies this approach. HOG works by first identifying edges in an image, which are places where brightness changes sharply and often indicate object boundaries. It then divides the image into small cells and measures how edges are oriented within each cell, summarizing these orientations in a histogram. This transformation converts raw pixel values into a representation that captures important shape information while being robust to variations in lighting and small changes in position.</p>
<div id="fig-hog" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/hog.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: HOG Method: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes."><img src="images/png/hog.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>HOG Method</strong>: Identifies edges in images to create a histogram of gradients, transforming pixel values into shape descriptors that are invariant to lighting changes.
</figcaption>
</figure>
</div>
<p>Other feature extraction methods like SIFT (Scale-Invariant Feature Transform) and Gabor filters provided different ways to capture patterns in images. SIFT found distinctive points that could be recognized even when an objectâ€™s size or orientation changed. Gabor filters helped identify textures and repeated patterns. Each method encoded different types of human insight about what makes visual patterns recognizable.</p>
<p>These engineered features enabled significant advances in computer vision during the 2000s. Systems could now recognize objects with some robustness to real world variations, leading to applications in face detection, pedestrian detection, and object recognition. However, the approach had fundamental limitations. Experts needed to carefully design feature extractors for each new problem, and the resulting features might miss important patterns that were not anticipated in their design.</p>
</section>
<section id="sec-dl-primer-neural-networks-representation-learning-0d1a" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-neural-networks-representation-learning-0d1a">Neural Networks and Representation Learning</h3>
<p>Neural networks represent a fundamental shift in how we approach problem solving with computers, establishing a new programming paradigm that learns from data rather than following explicit rules. This shift becomes particularly evident when considering tasks like computer vision, specifically identifying objects in images.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of Deep Learning">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Deep Learning</summary><div><strong>Deep Learning</strong> is a <em>subfield</em> of machine learning that utilizes <em>artificial neural networks with multiple layers</em> to <em>automatically learn hierarchical representations</em> from data. This approach enables the extraction of <em>complex patterns</em> from large datasets, facilitating tasks like <em>image recognition, natural language processing, and speech recognition</em> without explicit feature engineering. Deep learningâ€™s effectiveness arises from its ability to <em>learn features directly</em> from raw data, <em>adapt to diverse data structures</em>, and <em>scale with increasing data volume</em>.<p></p>
</div></details>
</div>
<p>Unlike traditional programming approaches that require manual rule specification, deep learning utilizes artificial neural networks with multiple layers to automatically learn hierarchical representations from data. This enables systems to extract complex patterns from large datasets, facilitating tasks like image recognition, natural language processing, and speech recognition without explicit feature engineering. The effectiveness of deep learning comes from its ability to learn features directly from raw data, adapt to diverse data structures, and scale with increasing data volume.</p>
<p>Deep learning fundamentally differs by learning directly from raw data. Traditional programming, as we saw earlier in <a href="#fig-traditional" class="quarto-xref">Figure&nbsp;3</a>, required both rules and data as inputs to produce answers. Machine learning inverts this relationship, as shown in <a href="#fig-deeplearning" class="quarto-xref">Figure&nbsp;6</a>. Instead of writing rules, we provide examples (data) and their correct answers to discover the underlying rules automatically. This shift eliminates the need for humans to specify what patterns are important.</p>
<div id="fig-deeplearning" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deeplearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="6fbbdff9cc8d9f112795580e2cb59b395d80de35.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Data-Driven Rule Discovery: Deep learning systems learn patterns and relationships directly from data, eliminating the need for manually specified rules and enabling automated feature extraction from raw inputs. This contrasts with traditional programming, where both rules and data are required to generate outputs, and machine learning, where rules are inferred from labeled data."><img src="dl_primer_files/mediabag/6fbbdff9cc8d9f112795580e2cb59b395d80de35.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deeplearning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Data-Driven Rule Discovery</strong>: Deep learning systems learn patterns and relationships directly from data, eliminating the need for manually specified rules and enabling automated feature extraction from raw inputs. This contrasts with traditional programming, where both rules and data are required to generate outputs, and machine learning, where rules are inferred from labeled data.
</figcaption>
</figure>
</div>
<p>The system discovers these patterns automatically from examples. When shown millions of images of cats, the system learns to identify increasingly complex visual patterns, from simple edges to more sophisticated combinations that make up cat like features. This mirrors how our own visual system works, building up understanding from basic visual elements to complex objects.</p>
<p>Unlike traditional approaches where performance often plateaus with more data and computation, deep learning systems continue to improve as we provide more resources. More training examples help the system recognize more variations and nuances. More computational power enables the system to discover more subtle patterns. This scalability has led to dramatic improvements in performance. For example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today.</p>
<p>This different approach has profound implications for how we build AI systems. Deep learningâ€™s ability to learn directly from raw data eliminates the need for manual feature engineering, but it comes with new demands. We need sophisticated infrastructure to handle massive datasets, powerful computers to process this data, and specialized hardware to perform the complex mathematical calculations efficiently. The computational requirements of deep learning have driven the development of new types of computer chips optimized for these calculations.</p>
<p>The success of deep learning in computer vision exemplifies how this approach, when given sufficient data and computation, can surpass traditional methods. This pattern has repeated across many domains, from speech recognition to game playing, establishing deep learning as a transformative approach to artificial intelligence.</p>
</section>
<section id="sec-dl-primer-neural-system-implications-1ef5" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-neural-system-implications-1ef5">Neural System Implications</h3>
<p>The progression from traditional programming to deep learning represents not just a shift in how we solve problems, but a fundamental transformation in computing system requirements. This transformation becomes particularly critical when we consider the full spectrum of ML systems, from massive cloud deployments to resource constrained Tiny ML devices.</p>
<p>Traditional programs follow predictable patterns. They execute sequential instructions, access memory in regular patterns, and utilize computing resources in well understood ways. A typical rule based image processing system might scan through pixels methodically, applying fixed operations with modest and predictable computational and memory requirements. These characteristics made traditional programs relatively straightforward to deploy across different computing platforms.</p>
<p>Machine learning with engineered features introduced new complexities. Feature extraction algorithms required more intensive computation and structured data movement. The HOG feature extractor discussed earlier, for instance, requires multiple passes over image data, computing gradients and constructing histograms. While this increased both computational demands and memory complexity, the resource requirements remained relatively predictable and scalable across platforms.</p>
<p>Deep learning, however, fundamentally reshapes system requirements across multiple dimensions. <a href="#tbl-evolution" class="quarto-xref">Table&nbsp;1</a> shows the evolution of system requirements across programming paradigms:</p>
<div id="tbl-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>System Resource Evolution</strong>: Programming paradigms shift system demands from sequential computation to structured parallelism with feature engineering, and finally to massive matrix operations and complex memory hierarchies in deep learning. This table clarifies how deep learning fundamentally alters system requirements compared to traditional programming and machine learning with engineered features, impacting computation and memory access patterns.
</figcaption>
<div aria-describedby="tbl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 24%">
<col style="width: 25%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">System Aspect</th>
<th style="text-align: left;">Traditional Programming</th>
<th style="text-align: left;">ML with Features</th>
<th style="text-align: left;">Deep Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computation</td>
<td style="text-align: left;">Sequential, predictable paths</td>
<td style="text-align: left;">Structured parallel operations</td>
<td style="text-align: left;">Massive matrix parallelism</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Access</td>
<td style="text-align: left;">Small, predictable patterns</td>
<td style="text-align: left;">Medium, batch-oriented</td>
<td style="text-align: left;">Large, complex hierarchical patterns</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Movement</td>
<td style="text-align: left;">Simple input/output flows</td>
<td style="text-align: left;">Structured batch processing</td>
<td style="text-align: left;">Intensive cross-system movement</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hardware Needs</td>
<td style="text-align: left;">CPU-centric</td>
<td style="text-align: left;">CPU with vector units</td>
<td style="text-align: left;">Specialized accelerators</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Resource Scaling</td>
<td style="text-align: left;">Fixed requirements</td>
<td style="text-align: left;">Linear with data size</td>
<td style="text-align: left;">Exponential with complexity</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These differences manifest in several critical ways, with implications across the entire ML systems spectrum.</p>
<section id="sec-dl-primer-computation-patterns-0254" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-computation-patterns-0254">Computation Patterns</h4>
<p>While traditional programs follow sequential logic flows, deep learning requires massive parallel operations on matrices. This shift explains why conventional CPUs, designed for sequential processing, prove inefficient for neural network computations. The need for parallel processing has driven the adoption of specialized hardware architectures, ranging from powerful cloud GPUs to specialized mobile processors to Tiny ML accelerators.</p>
</section>
<section id="sec-dl-primer-memory-systems-5119" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-memory-systems-5119">Memory Systems</h4>
<p>Traditional programs typically maintain small, fixed memory footprints. Deep learning models, however, must manage parameters across complex memory hierarchies. Memory bandwidth often becomes the primary performance bottleneck, creating particular challenges for resource-constrained systems. This drives different optimization strategies across the ML systems spectrum, ranging from memory-rich cloud deployments to heavily optimized Tiny ML implementations.</p>
</section>
<section id="sec-dl-primer-system-scaling-49fe" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-system-scaling-49fe">System Scaling</h4>
<p>Perhaps most importantly, deep learning fundamentally changes how systems scale and the critical importance of efficiency. Traditional programs have relatively fixed resource requirements with predictable performance characteristics. Deep learning systems, however, can consume exponentially more resources as models grow in complexity. This relationship between model capability and resource consumption makes system efficiency a central concern.</p>
<p>The need to bridge algorithmic concepts with hardware realities becomes crucial. While traditional programs map relatively straightforwardly to standard computer architectures, deep learning requires us to think carefully about:</p>
<ul>
<li>How to efficiently map matrix operations to physical hardware</li>
<li>Ways to minimize data movement across memory hierarchies</li>
<li>Methods to balance computational capability with resource constraints</li>
<li>Techniques to optimize both algorithm and system-level efficiency</li>
</ul>
<p>These fundamental shifts explain why deep learning has spurred innovations across the entire computing stack. From specialized hardware accelerators to new memory architectures to sophisticated software frameworks, the demands of deep learning continue to reshape computer system design. Interestingly, many of these challenges, efficiency, scaling, and adaptability, are ones that biological systems have already solved. This brings us to a critical question: what can we learn from natureâ€™s own information processing system and strive to mimic them as artificially intelligent systems.</p>
<div id="quiz-question-sec-dl-primer-evolution-deep-learning-fb02" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a limitation of rule-based programming in AI systems?</p>
<ol type="a">
<li>It requires explicit rules for every scenario, limiting adaptability.</li>
<li>It automatically learns from data without human intervention.</li>
<li>It scales efficiently with complex real-world tasks.</li>
<li>It uses deep neural networks to process inputs.</li>
</ol></li>
<li><p>Explain how deep learning differs from traditional programming in terms of system requirements.</p></li>
<li><p>Order the following AI methodologies from earliest to latest in terms of development: (1) Classical Machine Learning, (2) Rule-Based Programming, (3) Deep Learning.</p></li>
<li><p>What is a key advantage of deep learning over traditional machine learning with engineered features?</p>
<ol type="a">
<li>Deep learning requires less computational power.</li>
<li>Deep learning uses fixed resource requirements.</li>
<li>Deep learning is less adaptable to diverse data structures.</li>
<li>Deep learning eliminates the need for manual feature engineering.</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-evolution-deep-learning-fb02" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dl-primer-biological-artificial-neurons-bb4f" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dl-primer-biological-artificial-neurons-bb4f">Biological to Artificial Neurons</h2>
<p>The quest to create artificial intelligence has been profoundly influenced by our understanding of biological intelligence, particularly the human brain. This isnâ€™t surprising; the brain represents the most sophisticated information processing system we know of. It is capable of learning, adapting, and solving complex problems while maintaining remarkable energy efficiency. The way our brains function has provided fundamental insights that continue to shape how we approach artificial intelligence.</p>
<section id="sec-dl-primer-biological-intelligence-7528" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-biological-intelligence-7528">Biological Intelligence</h3>
<p>When we observe biological intelligence, several key principles emerge. The brain demonstrates an extraordinary ability to learn from experience, constantly modifying its neural connections based on new information and interactions with the environment. This adaptability is fundamental; every experience potentially alters the brainâ€™s structure and refines its responses for future situations. This biological capability directly inspired one of the core principles of machine learning: the ability to learn and improve from data rather than following fixed, pre-programmed rules.</p>
<p>Another striking feature of biological intelligence is its parallel processing capability. The brain processes vast amounts of information simultaneously, with different regions specializing in specific functions while working in concert. This distributed, parallel architecture stands in stark contrast to traditional sequential computing and has significantly influenced modern AI system design. The brainâ€™s ability to efficiently coordinate these parallel processes while maintaining coherent function represents a level of sophistication weâ€™re still working to fully understand and replicate.</p>
<p>The brainâ€™s pattern recognition capabilities are particularly noteworthy. Biological systems excel at identifying patterns in complex, noisy data, whether it is recognizing faces in a crowd, understanding speech in a noisy environment, or identifying objects from partial information. This remarkable ability has inspired numerous AI applications, particularly in computer vision and speech recognition systems. The brain accomplishes these tasks with an efficiency that artificial systems are still striving to match.</p>
<p>Perhaps most remarkably, biological systems achieve all this with incredible energy efficiency. The human brain operates on approximately 20 watts of power<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, about the same as a low-power light bulb, while performing complex cognitive tasks that would require orders of magnitude more power in current artificial systems. This efficiency hasnâ€™t just impressed researchers; it has become a crucial goal in the development of AI hardware and algorithms.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Brain Energy Efficiency</strong>: The human brain contains approximately 86 billion neurons and performs roughly 10^16 operations per second on just 20 wattsâ€”equivalent to running a single LED light bulb. In contrast, training GPT-3 consumed about 1,287 megawatt-hours of electricity. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing.</p></div></div><p>These biological principles have led to two distinct but complementary approaches in artificial intelligence. The first attempts to directly mimic neural structure and function, leading to artificial neural networks and deep learning architectures that structurally resemble biological neural networks. The second takes a more abstract approach, adapting biological principles to work efficiently within the constraints of computer hardware without necessarily copying biological structures exactly. In the following sections, we will explore how these approaches manifest in practice, beginning with the fundamental building block of neural networks: the neuron itself.</p>
</section>
<section id="sec-dl-primer-transition-artificial-neurons-f44f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-transition-artificial-neurons-f44f">Transition to Artificial Neurons</h3>
<p>To understand how biological principles translate into artificial systems, we must first examine the basic unit of biological information processing: the neuron. This cellular building block provides the blueprint for its artificial counterpart and helps us understand how complex neural networks emerge from simple components working in concert.</p>
<p>In biological systems, the neuron (or cell) is the basic functional unit of the nervous system. Understanding its structure is crucial before we draw parallels to artificial systems. <a href="#fig-bio_nn2ai_nn" class="quarto-xref">Figure&nbsp;7</a> illustrates the structure of a biological neuron.</p>
<div id="fig-bio_nn2ai_nn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bio_nn2ai_nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/bio_nn2ai_nn.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Biological Neuron Mapping: Artificial neurons abstract key functions from their biological counterparts, receiving weighted inputs at dendrites, summing them in the cell body, and producing an output via the axon, analogous to activation functions in artificial neural networks. this abstraction forms the foundation for building complex artificial neural networks capable of sophisticated information processing. Source: geeksforgeeks."><img src="images/png/bio_nn2ai_nn.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bio_nn2ai_nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Biological Neuron Mapping</strong>: Artificial neurons abstract key functions from their biological counterparts, receiving weighted inputs at dendrites, summing them in the cell body, and producing an output via the axon, analogous to activation functions in artificial neural networks. this abstraction forms the foundation for building complex artificial neural networks capable of sophisticated information processing. Source: geeksforgeeks.
</figcaption>
</figure>
</div>
<p>A biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cellâ€™s basic life processes. Extending from the soma are branch-like structures called dendrites, which act as receivers for incoming signals from other neurons. The connections between neurons occur at synapses<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, which modulate the strength of the transmitted signals. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Synapses</strong>: From the Greek word â€œsynapteinâ€ meaning â€œto clasp together,â€ synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memoryâ€”a principle directly mimicked by adjustable weights in artificial neural networks.</p></div></div><p>The neuron functions as follows: Dendrites act as receivers, collecting input signals from other neurons. Synapses at these connections modulate the strength of each signal, determining how much influence each input has. The soma integrates these weighted signals and decides whether to trigger an output signal. If triggered, the axon transmits this signal to other neurons.</p>
<p>Each element of a biological neuron has a computational analog in artificial systems, reflecting the principles of learning, adaptability, and efficiency found in nature. To better understand how biological intelligence informs artificial systems, <a href="#tbl-bio_nn2ai_nn" class="quarto-xref">Table&nbsp;2</a> captures the mapping between the components of biological and artificial neurons. This should be viewed alongside <a href="#fig-bio_nn2ai_nn" class="quarto-xref">Figure&nbsp;7</a> for a complete picture. Together, they paint a picture of the biological-to-artificial neuron mapping.</p>
<div id="tbl-bio_nn2ai_nn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bio_nn2ai_nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Neuron Correspondence</strong>: Biological neurons inspire artificial neuron design through analogous componentsâ€”dendrites map to inputs (receiving signals), synapses map to weights (modulating connection strength), the soma to net input, and the axon to outputâ€”establishing a foundation for computational modeling of intelligence. This table clarifies how key functions of biological neurons are abstracted and implemented in artificial neural networks, enabling learning and information processing.
</figcaption>
<div aria-describedby="tbl-bio_nn2ai_nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Biological Neuron</th>
<th style="text-align: left;">Artificial Neuron</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cell</td>
<td style="text-align: left;">Neuron / Node</td>
</tr>
<tr class="even">
<td style="text-align: left;">Dendrites</td>
<td style="text-align: left;">Inputs</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Synapses</td>
<td style="text-align: left;">Weights</td>
</tr>
<tr class="even">
<td style="text-align: left;">Soma</td>
<td style="text-align: left;">Net Input</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Axon</td>
<td style="text-align: left;">Output</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Each component serves a similar function, albeit through vastly different mechanisms. Here, we explain these mappings and their implications for artificial neural networks.</p>
<ol type="1">
<li><p><strong>Cell <span class="math inline">\(\longleftrightarrow\)</span> Neuron/Node</strong>: The artificial neuron or node serves as the fundamental computational unit, mirroring the cellâ€™s role in biological systems.</p></li>
<li><p><strong>Dendrites <span class="math inline">\(\longleftrightarrow\)</span> Inputs</strong>: Dendrites in biological neurons receive incoming signals from other neurons, analogous to how inputs feed into artificial neurons. They act as the signal receivers, like antennas collecting information.</p></li>
<li><p><strong>Synapses <span class="math inline">\(\longleftrightarrow\)</span> Weights</strong>: Synapses modulate the strength of connections between neurons, directly analogous to weights in artificial neurons. These weights are adjustable, enabling learning and optimization over time by controlling how much influence each input has.</p></li>
<li><p><strong>Soma <span class="math inline">\(\longleftrightarrow\)</span> Net Input</strong>: The net input in artificial neurons sums weighted inputs to determine activation, similar to how the soma integrates signals in biological neurons.</p></li>
<li><p><strong>Axon <span class="math inline">\(\longleftrightarrow\)</span> Output</strong>: The output of an artificial neuron passes processed information to subsequent network layers, much like an axon transmits signals to other neurons.</p></li>
</ol>
<p>This mapping illustrates how artificial neural networks simplify and abstract biological processes while preserving their essential computational principles. However, understanding individual neurons is just the beginningâ€”the true power of neural networks emerges from how these basic units work together in larger systems.</p>
</section>
<section id="sec-dl-primer-artificial-intelligence-cfb8" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-artificial-intelligence-cfb8">Artificial Intelligence</h3>
<p>The translation from biological principles to artificial computation requires a deep appreciation of what makes biological neural networks so effective at both the cellular and network levels. The brain processes information through distributed computation across billions of neurons, each operating relatively slowly compared to silicon transistors. A biological neuron fires at approximately 200 Hz, while modern processors operate at gigahertz frequencies. Despite this speed limitation, the brainâ€™s parallel architecture enables sophisticated real-time processing of complex sensory input, decision making, and control of behavior.</p>
<p>This computational efficiency emerges from the brainâ€™s basic organizational principles. Each neuron acts as a simple processing unit, integrating inputs from thousands of other neurons and producing a binary output signal based on whether this integrated input exceeds a threshold. The connection strengths between neurons, mediated by synapses, are continuously modified through experience. This synaptic plasticity forms the basis for learning and adaptation in biological neural networks. These biological principles suggest key computational elements needed in artificial neural systems:</p>
<ul>
<li>Simple processing units that integrate multiple inputs</li>
<li>Adjustable connection strengths between units</li>
<li>Nonlinear activation based on input thresholds</li>
<li>Parallel processing architecture</li>
<li>Learning through modification of connection strengths</li>
</ul>
</section>
<section id="sec-dl-primer-computational-translation-1c53" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-computational-translation-1c53">Computational Translation</h3>
<p>We face the challenge of capturing the essence of neural computation within the rigid framework of digital systems. The implementation of biological principles in artificial neural systems represents a nuanced balance between biological fidelity and computational efficiency. At its core, an artificial neuron captures the essential computational properties of its biological counterpart through mathematical operations that can be efficiently executed on digital hardware.</p>
<p><a href="#tbl-bio2comp" class="quarto-xref">Table&nbsp;3</a> provides a systematic view of how key biological features map to their computational counterparts. Each biological feature has an analog in computational systems, revealing both the possibilities and limitations of digital neural implementation, which we will learn more about later.</p>
<p>The basic computational unit in artificial neural networks, the artificial neuron, simplifies the complex electrochemical processes of biological neurons into three fundamental operations. First, input signals are weighted, mimicking how biological synapses modulate incoming signals with different strengths. Second, these weighted inputs are summed together, analogous to how a biological neuron integrates incoming signals in its cell body. Finally, the summed input passes through an activation function that determines the neuronâ€™s output, similar to how a biological neuron fires based on whether its membrane potential exceeds a threshold.</p>
<div id="tbl-bio2comp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bio2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Biological-Computational Analogies</strong>: Artificial neurons abstract key principles of biological neural systems, mapping neuron firing to activation functions, synaptic strength to weighted connections, and signal integration to summation operationsâ€”establishing a foundation for digital neural implementation. Distributed memory and parallel processing in biological systems find computational counterparts in weight matrices and concurrent computation, respectively, highlighting both the power and limitations of this abstraction.
</figcaption>
<div aria-describedby="tbl-bio2comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Biological Feature</th>
<th style="text-align: left;">Computational Translation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Neuron firing</td>
<td style="text-align: left;">Activation function</td>
</tr>
<tr class="even">
<td style="text-align: left;">Synaptic strength</td>
<td style="text-align: left;">Weighted connections</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Signal integration</td>
<td style="text-align: left;">Summation operation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Distributed memory</td>
<td style="text-align: left;">Weight matrices</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Parallel processing</td>
<td style="text-align: left;">Concurrent computation</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This mathematical abstraction preserves key computational principles while enabling efficient digital implementation. The weighting of inputs allows the network to learn which connections are important, just as biological neural networks strengthen or weaken synaptic connections through experience. The summation operation captures how biological neurons integrate multiple inputs into a single decision. The activation function introduces nonlinearity essential for learning complex patterns, much like the threshold-based firing of biological neurons.</p>
<p>Memory in artificial neural networks takes a markedly different form from biological systems. While biological memories are distributed across synaptic connections and neural patterns, artificial networks store information in discrete weights and parameters. This architectural difference reflects the constraints of current computing hardware, where memory and processing are physically separated rather than integrated as in biological systems. Despite these implementation differences, artificial neural networks achieve similar functional capabilities in pattern recognition and learning.</p>
<p>The brainâ€™s massive parallelism represents a fundamental challenge in artificial implementation. While biological neural networks process information through billions of neurons operating simultaneously, artificial systems approximate this parallelism through specialized hardware like GPUs and tensor processing units. These devices efficiently compute the matrix operations that form the mathematical foundation of artificial neural networks, achieving parallel processing at a different scale and granularity than biological systems.</p>
</section>
<section id="sec-dl-primer-system-requirements-fdc2" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-system-requirements-fdc2">System Requirements</h3>
<p>The computational translation of neural principles creates specific demands on the underlying computing infrastructure. These requirements emerge from the fundamental differences between biological and artificial implementations of neural processing, shaping how we design and build systems capable of supporting artificial neural networks.</p>
<p><a href="#tbl-comp2sys" class="quarto-xref">Table&nbsp;4</a> shows how each computational element drives particular system requirements. From this mapping, we can see how the choices made in computational translation directly influence the hardware and system architecture needed for implementation.</p>
<div id="tbl-comp2sys" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comp2sys-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Computational Demands</strong>: Artificial neural network design directly translates into specific system requirements; for example, efficient activation functions necessitate fast nonlinear operation units and large-scale weight storage demands high-bandwidth memory access. Understanding this mapping guides hardware and system architecture choices for effective implementation of artificial intelligence.
</figcaption>
<div aria-describedby="tbl-comp2sys-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Computational Element</th>
<th style="text-align: left;">System Requirements</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Activation functions</td>
<td style="text-align: left;">Fast nonlinear operation units</td>
</tr>
<tr class="even">
<td style="text-align: left;">Weight operations</td>
<td style="text-align: left;">High-bandwidth memory access</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Parallel computation</td>
<td style="text-align: left;">Specialized parallel processors</td>
</tr>
<tr class="even">
<td style="text-align: left;">Weight storage</td>
<td style="text-align: left;">Large-scale memory systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Learning algorithms</td>
<td style="text-align: left;">Gradient computation hardware</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Storage architecture represents a critical requirement, driven by the fundamental difference in how biological and artificial systems handle memory. In biological systems, memory and processing are intrinsically integratedâ€”synapses both store connection strengths and process signals. Artificial systems, however, must maintain a clear separation between processing units and memory. This creates a need for both high-capacity storage to hold millions or billions of connection weights and high-bandwidth pathways to move this data quickly between storage and processing units. The efficiency of this data movement often becomes a critical bottleneck that biological systems do not face.</p>
<p>The learning process itself imposes distinct requirements on artificial systems. While biological networks modify synaptic strengths through local chemical processes, artificial networks must coordinate weight updates across the entire network. This creates substantial computational and memory demands during trainingâ€”systems must not only store current weights but also maintain space for gradients and intermediate calculations. The requirement to backpropagate error signals, with no real biological analog, further complicates the system architecture.</p>
<p>Energy efficiency emerges as a final critical requirement, highlighting perhaps the starkest contrast between biological and artificial implementations. The human brainâ€™s remarkable energy efficiency, which operates on approximately 20 watts, stands in sharp contrast to the substantial power demands of artificial neural networks. Current systems often require orders of magnitude more energy to implement similar capabilities. This gap drives ongoing research in more efficient hardware architectures and has profound implications for the practical deployment of neural networks, particularly in resource-constrained environments like mobile devices or edge computing systems.</p>
</section>
<section id="sec-dl-primer-evolution-impact-ae30" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-evolution-impact-ae30">Evolution and Impact</h3>
<p>We can now better appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their eraâ€”primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Perceptron</strong>: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be â€œthe embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.â€ While overly optimistic, this breakthrough laid the foundation for all modern neural networks.</p></div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>â€œLearning Representations by Back-Propagating Errors.â€</span> <em>Nature</em> 323 (6088): 533â€“36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Backpropagation</strong>: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the â€œcredit assignment problemâ€â€”how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Interestingly, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed.</p></div></div><p>The development of backpropagation algorithms in the 1980s <span class="citation" data-cites="rumelhart1986learning">(<a href="#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>, which we will learn about later, represented a theoretical breakthrough<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and provided a systematic way to train multi-layer networks. However, the computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks.</p>
<div id="fig-trends" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/trends_65d82031.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Computational Growth: Exponential increases in computational powerâ€”initially at a 1.4Ã— rate from 1952â€“2010, then accelerating to a doubling every 3.4 months from 2012â€“2022â€”enabled the scaling of deep learning models. this trend, coupled with a 10-month doubling cycle for large-scale models after 2015, directly addresses the historical bottleneck of training complex neural networks and fueled the recent advances in the field. Source: EPOCH AI."><img src="images/png/trends_65d82031.png" class="img-fluid figure-img" data-fig-pos="htb"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Computational Growth</strong>: Exponential increases in computational powerâ€”initially at a 1.4Ã— rate from 1952â€“2010, then accelerating to a doubling every 3.4 months from 2012â€“2022â€”enabled the scaling of deep learning models. this trend, coupled with a 10-month doubling cycle for large-scale models after 2015, directly addresses the historical bottleneck of training complex neural networks and fueled the recent advances in the field. Source: EPOCH AI.
</figcaption>
</figure>
</div>
<p>The term â€œdeep learningâ€ gained prominence in the 2010s, coinciding with significant advances in computational power and data accessibility. The field has since experienced exponential growth, as illustrated in <a href="#fig-trends" class="quarto-xref">Figure&nbsp;8</a>. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> initially followed a <span class="math inline">\(1.4\times\)</span> improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>FLOPS</strong>: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 required approximately 3.14 Ã— 10^23 FLOPSâ€”more computation than was available to the entire world before 1960.</p></div></div><p>The evolutionary trends were driven by parallel advances across three fundamental dimensions: data availability, algorithmic innovations, and computing infrastructure. These three factors, namely, data, algorithms, and infrastructure, reinforced each other in a virtuous cycle that continues to drive progress in the field today. As <a href="#fig-virtuous-cycle" class="quarto-xref">Figure&nbsp;9</a> shows, more powerful computing infrastructure enabled processing larger datasets. Larger datasets drove algorithmic innovations. Better algorithms demanded more sophisticated computing systems. This virtuous cycle continues to drive progress in the field today.</p>
<div id="fig-virtuous-cycle" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-virtuous-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="df0674b614a5746ac9435d8d82b3ec5711780d1e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: The virtuous cycle enabled by key breakthroughs in each layer."><img src="dl_primer_files/mediabag/df0674b614a5746ac9435d8d82b3ec5711780d1e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-virtuous-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: The virtuous cycle enabled by key breakthroughs in each layer.
</figcaption>
</figure>
</div>
<p>The data revolution transformed what was possible with neural networks. The rise of the internet and digital devices created unprecedented access to training data. Image sharing platforms provided millions of labeled images. Digital text collections enabled language processing at scale. Sensor networks and IoT devices generated continuous streams of real-world data. This abundance of data provided the raw material needed for neural networks to learn complex patterns effectively.</p>
<p>Algorithmic innovations made it possible to harness this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> allowed models to generalize better to new data. Most importantly, researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Overfitting</strong>: When a model memorizes training examples instead of learning generalizable patternsâ€”like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an â€œexpertâ€ on a practice test who panics when facing slightly different questions on the real exam.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Tensor Processing Unit (TPU)</strong>: Googleâ€™s custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30Ã— faster than contemporary GPUs while using less power. The name reflects their optimization for tensor operationsâ€”multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware.</p></div><div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. <span>â€œIn-Datacenter Performance Analysis of a Tensor Processing Unit.â€</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1â€“12. ISCA â€™17. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div></div><p>Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span class="citation" data-cites="jouppi2017datacenter">(<a href="#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>)</span> pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advancesâ€”frameworks and libraries that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment.</p>
<div id="quiz-question-sec-dl-primer-biological-artificial-neurons-bb4f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which component of a biological neuron is analogous to the weights in an artificial neuron?</p>
<ol type="a">
<li>Dendrites</li>
<li>Synapses</li>
<li>Soma</li>
<li>Axon</li>
</ol></li>
<li><p>Explain how the energy efficiency of the human brain influences the design of AI hardware and algorithms.</p></li>
<li><p>Order the following biological components as they relate to their artificial counterparts: (1) Dendrites, (2) Soma, (3) Axon.</p></li>
<li><p>What is a key system requirement for implementing artificial neural networks inspired by the brainâ€™s parallel processing?</p>
<ol type="a">
<li>High-bandwidth memory access</li>
<li>Large-scale memory systems</li>
<li>Fast nonlinear operation units</li>
<li>Specialized parallel processors</li>
</ol></li>
<li><p>In a production system, what trade-offs might you consider when choosing between mimicking biological structures versus abstracting biological principles for AI design?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-biological-artificial-neurons-bb4f" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dl-primer-neural-network-fundamentals-68cd" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dl-primer-neural-network-fundamentals-68cd">Neural Network Fundamentals</h2>
<p>We can now examine the fundamental building blocks that make machine learning systems work. While the field has grown tremendously in sophistication, all modern neural networks, ranging from simple classifiers to large language models, share a common architectural foundation built upon basic computational units and principles.</p>
<p>This foundation begins with understanding how individual artificial neurons process information, how they are organized into layers, and how these layers are connected to form complete networks. By starting with these fundamental concepts, we can progressively build up to understanding more complex architectures and their applications.</p>
<p>Neural networks have come a long way since their inception in the 1950s, when the perceptron was first introduced. After a period of decline in popularity due to computational and theoretical limitations, the field saw a resurgence in the 2000s, driven by advancements in hardware (e.g., GPUs) and innovations like deep learning. These breakthroughs have made it possible to train networks with millions of parameters, enabling applications once considered impossible.</p>
<section id="sec-dl-primer-basic-architecture-0553" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-basic-architecture-0553">Basic Architecture</h3>
<p>The architecture of a neural network determines how information flows through the system, from input to output. While modern networks can be tremendously complex, they all build upon a few key organizational principles that we will explore in the following sections. Understanding these principles is essential for both implementing neural networks and appreciating how they achieve their remarkable capabilities.</p>
<section id="sec-dl-primer-neurons-activations-622e" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-neurons-activations-622e">Neurons and Activations</h4>
<p>The Perceptron is the basic unit or node that forms the foundation for more complex structures. It functions by taking multiple inputs, each representing a feature of the object under analysis, such as the characteristics of a home for predicting its price or the attributes of a song to forecast its popularity in music streaming services. These inputs are denoted as <span class="math inline">\(x_1, x_2, ..., x_n\)</span>. A perceptron can be configured to perform either regression or classification tasks. For regression, the actual numerical output <span class="math inline">\(\hat{y}\)</span> is used. For classification, the output depends on whether <span class="math inline">\(\hat{y}\)</span> crosses a certain threshold. If <span class="math inline">\(\hat{y}\)</span> exceeds this threshold, the perceptron might output one class (e.g., â€˜yesâ€™), and if it does not, another class (e.g., â€˜noâ€™).</p>
<div id="fig-perceptron" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-perceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0ff5c0804cf53d709bf07787bdcde4abd072ceb8.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Weighted Input Summation: Perceptrons compute a weighted sum of multiple inputs, representing feature values, and pass the result to an activation function to produce an output. each input x_i is multiplied by a corresponding weight w_{ij} before being aggregated, forming the basis for learning complex patterns from data. using this figure."><img src="dl_primer_files/mediabag/0ff5c0804cf53d709bf07787bdcde4abd072ceb8.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-perceptron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Weighted Input Summation</strong>: Perceptrons compute a weighted sum of multiple inputs, representing feature values, and pass the result to an activation function to produce an output. each input <span class="math inline">\(x_i\)</span> is multiplied by a corresponding weight <span class="math inline">\(w_{ij}\)</span> before being aggregated, forming the basis for learning complex patterns from data. using this figure.
</figcaption>
</figure>
</div>
<p><a href="#fig-perceptron" class="quarto-xref">Figure&nbsp;10</a> illustrates the fundamental building blocks of a perceptron, which serves as the foundation for more complex neural networks. A perceptron can be thought of as a miniature decision-maker, utilizing its weights, bias, and activation function to process inputs and generate outputs based on learned parameters. This concept forms the basis for understanding more intricate neural network architectures, such as multilayer perceptrons.</p>
<p>In these advanced structures, layers of perceptrons work in concert, with each layerâ€™s output serving as the input for the subsequent layer. This hierarchical arrangement creates a deep learning model capable of comprehending and modeling complex, abstract patterns within data. By stacking these simple units, neural networks gain the ability to tackle increasingly sophisticated tasks, from image recognition to natural language processing.</p>
<p>Each input <span class="math inline">\(x_i\)</span> has a corresponding weight <span class="math inline">\(w_{ij}\)</span>, and the perceptron simply multiplies each input by its matching weight. This operation is similar to linear regression, where the intermediate output, <span class="math inline">\(z\)</span>, is computed as the sum of the products of inputs and their weights: <span class="math display">\[
z = \sum (x_i \cdot w_{ij})
\]</span></p>
<p>To this intermediate calculation, a bias term <span class="math inline">\(b\)</span> is added, allowing the model to better fit the data by shifting the linear output function up or down. Thus, the intermediate linear combination computed by the perceptron including the bias becomes: <span class="math display">\[
z = \sum (x_i \cdot w_{ij}) + b
\]</span></p>
<p>Common activation functions include:</p>
<ul>
<li><p><strong>ReLU (Rectified Linear Unit)</strong>: Defined as <span class="math inline">\(f(x) = \max(0,x)\)</span>, it introduces sparsity and accelerates convergence in deep networks. Its simplicity and effectiveness have made it the default choice in many modern architectures.</p></li>
<li><p><strong>Sigmoid</strong>: Historically popular, the sigmoid function maps inputs to a range between 0 and 1 but is prone to vanishing gradients in deeper architectures. Itâ€™s particularly useful in binary classification problems where probabilities are needed.</p></li>
<li><p><strong>Tanh</strong>: Similar to sigmoid but maps inputs to a range of <span class="math inline">\(-1\)</span> to 1, centering the data. This centered output often leads to faster convergence in practice compared to sigmoid.</p></li>
</ul>
<div id="fig-nonlinear" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonlinear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="55b563ec34b72c8d629e0c2c558c38331404ac76.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Non-Linear Activation: Neural networks model complex relationships by applying non-linear activation functions to weighted sums of inputs, enabling the representation of non-linear decision boundaries. These functions transform input values, creating the capacity to learn intricate patterns beyond linear combinations via the arrangement of points. Source: Medium, sachin kaushik."><img src="dl_primer_files/mediabag/55b563ec34b72c8d629e0c2c558c38331404ac76.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonlinear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Non-Linear Activation</strong>: Neural networks model complex relationships by applying non-linear activation functions to weighted sums of inputs, enabling the representation of non-linear decision boundaries. These functions transform input values, creating the capacity to learn intricate patterns beyond linear combinations via the arrangement of points. Source: Medium, sachin kaushik.
</figcaption>
</figure>
</div>
<p>These activation functions transform the linear input sum into a non-linear output: <span class="math display">\[
\hat{y} = \sigma(z)
\]</span></p>
<p>Thus, the final output of the perceptron, including the activation function, can be expressed as:</p>
<p><a href="#fig-nonlinear" class="quarto-xref">Figure&nbsp;11</a> shows an example where data exhibit a nonlinear pattern that could not be adequately modeled with a linear approach. The activation function enables the network to learn and represent complex relationships in the data, making it possible to solve sophisticated tasks like image recognition or speech processing.</p>
<p>Thus, the final output of the perceptron, including the activation function, can be expressed as: <span class="math display">\[
z = \sigma\left(\sum (x_i \cdot w_{ij}) + b\right)
\]</span></p>
</section>
<section id="sec-dl-primer-layers-connections-fa4f" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dl-primer-layers-connections-fa4f">Layers and Connections</h4>
<p>While a single perceptron can model simple decisions, the power of neural networks comes from combining multiple neurons into layers. A layer is a collection of neurons that process information in parallel. Each neuron in a layer operates independently on the same input but with its own set of weights and bias, allowing the layer to learn different features or patterns from the same input data.</p>
<p>In a typical neural network, we organize these layers hierarchically:</p>
<ol type="1">
<li><strong>Input Layer</strong>: Receives the raw data features</li>
<li><strong>Hidden Layers</strong>: Process and transform the data through multiple stages</li>
<li><strong>Output Layer</strong>: Produces the final prediction or decision</li>
</ol>
<p><a href="#fig-layers" class="quarto-xref">Figure&nbsp;12</a> illustrates this layered architecture. When data flows through these layers, each successive layer transforms the representation of the data, gradually building more complex and abstract features. This hierarchical processing is what gives deep neural networks their remarkable ability to learn complex patterns.</p>
<div id="fig-layers" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d61e997272cd48b85c2f7e89f6ff4e9f33882d60.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Layered Network Architecture: Deep neural networks transform data through successive layers, enabling the extraction of increasingly complex features and patterns. each layer applies non-linear transformations to the outputs of the previous layer, ultimately mapping raw inputs to desired outputs. Source: brunellon."><img src="dl_primer_files/mediabag/d61e997272cd48b85c2f7e89f6ff4e9f33882d60.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Layered Network Architecture</strong>: Deep neural networks transform data through successive layers, enabling the extraction of increasingly complex features and patterns. each layer applies non-linear transformations to the outputs of the previous layer, ultimately mapping raw inputs to desired outputs. Source: brunellon.
</figcaption>
</figure>
</div>


<div class="no-row-height column-margin column-container"><div class="">
  <div class="margin-video">
    <iframe src="https://www.youtube.com/embed/aircAruvnKk?start=" style="width:100%; aspect-ratio: ; border:0;" allowfullscreen="">
    </iframe>
  </div>
  <p><em>Neural Network - 3Blue1Brown</em></p>
</div></div></section>
<section id="sec-dl-primer-data-flow-transformations-5260" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-data-flow-transformations-5260">Data Flow and Transformations</h4>
<p>As data flows through the network, it is transformed at each layer (l) to extract meaningful patterns. Each layer combines the input data using learned weights and biases, then applies an activation function to introduce non-linearity. This process can be written mathematically as: <span class="math display">\[
\mathbf{z}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l-1)} + \mathbf{b}^{(l)}
\]</span> Where:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{x}^{(l-1)}\)</span> is the input vector from the previous layer</p></li>
<li><p><span class="math inline">\(\mathbf{W}^{(l)}\)</span> is the weight matrix for the current layer</p></li>
<li><p><span class="math inline">\(\mathbf{b}^{(l)}\)</span> is the bias vector</p></li>
<li><p><span class="math inline">\(\mathbf{z}^{(l)}\)</span> is the pre-activation output</p></li>
</ul>
<p>Now that we have covered the basics, letâ€™s look at how these concepts come together in practice. Neural networks excel at tasks like handwritten digit recognition, where they learn to identify patterns in pixel data and classify images into different categories. This practical example introduces some new concepts that we will explore in more depth soon.</p>
</section>
</section>
<section id="sec-dl-primer-weights-biases-71bc" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-weights-biases-71bc">Weights and Biases</h3>
<section id="sec-dl-primer-weight-matrices-709c" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-weight-matrices-709c">Weight Matrices</h4>
<p>Weights in neural networks determine how strongly inputs influence the output of a neuron. While we first discussed weights for a single perceptron, in larger networks, weights are organized into matrices for efficient computation across entire layers. For example, in a layer with <span class="math inline">\(n\)</span> input features and <span class="math inline">\(m\)</span> neurons, the weights form a matrix <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{n \times m}\)</span>. Each column in this matrix represents the weights for a single neuron in the layer. This organization allows the network to process multiple inputs simultaneously, an essential feature for handling real-world data efficiently.</p>
<p>Letâ€™s consider how this extends our previous perceptron equations to handle multiple neurons simultaneously. For a layer of <span class="math inline">\(m\)</span> neurons, instead of computing each neuronâ€™s output separately: <span class="math display">\[
z_j = \sum_{i=1}^n (x_i \cdot w_{ij}) + b_j
\]</span></p>
<p>We can compute all outputs at once using matrix multiplication: <span class="math display">\[
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
\]</span></p>
<p>This matrix organization is more than just mathematical convenience; it reflects how modern neural networks are implemented for efficiency. Each weight <span class="math inline">\(w_{ij}\)</span> represents the strength of the connection between input feature <span class="math inline">\(i\)</span> and neuron <span class="math inline">\(j\)</span> in the layer.</p>
</section>
<section id="sec-dl-primer-connection-patterns-a389" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-connection-patterns-a389">Connection Patterns</h4>
<p>In the simplest and most common case, each neuron in a layer is connected to every neuron in the previous layer, forming what we call a â€œdenseâ€ or â€œfully-connectedâ€ layer. This pattern means that each neuron has the opportunity to learn from all available features from the previous layer.</p>
<p><a href="#fig-connections" class="quarto-xref">Figure&nbsp;13</a> illustrates these dense connections between layers. For a network with layers of sizes <span class="math inline">\((n_1, n_2, n_3)\)</span>, the weight matrices would have dimensions:</p>
<ul>
<li>Between first and second layer: <span class="math inline">\(\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}\)</span></li>
<li>Between second and third layer: <span class="math inline">\(\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}\)</span></li>
</ul>
<div id="fig-connections" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-connections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="dbdfb8f228253cea7198f849d0d6146436d8b878.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Fully-Connected Layers: Multilayer perceptrons (mlps) utilize dense connections between layers, enabling each neuron to integrate information from all neurons in the preceding layer. the weight matrices defining these connectionsâ€”\mathbf{w}^{(1)} \in \mathbb{r}^{n_1 \times n_2} and \mathbf{w}^{(2)} \in \mathbb{r}^{n_2 \times n_3}â€”determine the strength of these integrations and facilitate learning complex patterns from input data. Source: j. mccaffrey."><img src="dl_primer_files/mediabag/dbdfb8f228253cea7198f849d0d6146436d8b878.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-connections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>Fully-Connected Layers</strong>: Multilayer perceptrons (mlps) utilize dense connections between layers, enabling each neuron to integrate information from all neurons in the preceding layer. the weight matrices defining these connectionsâ€”<span class="math inline">\(\mathbf{w}^{(1)} \in \mathbb{r}^{n_1 \times n_2}\)</span> and <span class="math inline">\(\mathbf{w}^{(2)} \in \mathbb{r}^{n_2 \times n_3}\)</span>â€”determine the strength of these integrations and facilitate learning complex patterns from input data. Source: j. mccaffrey.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dl-primer-bias-terms-367a" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-bias-terms-367a">Bias Terms</h4>
<p>Each neuron in a layer also has an associated bias term. While weights determine the relative importance of inputs, biases allow neurons to shift their activation functions. This shifting is crucial for learning, as it gives the network flexibility to fit more complex patterns.</p>
<p>For a layer with <span class="math inline">\(m\)</span> neurons, the bias terms form a vector <span class="math inline">\(\mathbf{b} \in \mathbb{R}^m\)</span>. When we compute the layerâ€™s output, this bias vector is added to the weighted sum of inputs: <span class="math display">\[
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
\]</span></p>
<p>The bias terms effectively allow each neuron to have a different â€œthresholdâ€ for activation, making the network more expressive.</p>
</section>
<section id="sec-dl-primer-parameter-organization-6d17" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-parameter-organization-6d17">Parameter Organization</h4>
<p>The organization of weights and biases across a neural network follows a systematic pattern. For a network with <span class="math inline">\(L\)</span> layers, we maintain:</p>
<ul>
<li><p>A weight matrix <span class="math inline">\(\mathbf{W}^{(l)}\)</span> for each layer <span class="math inline">\(l\)</span></p></li>
<li><p>A bias vector <span class="math inline">\(\mathbf{b}^{(l)}\)</span> for each layer <span class="math inline">\(l\)</span></p></li>
<li><p>Activation functions <span class="math inline">\(f^{(l)}\)</span> for each layer <span class="math inline">\(l\)</span></p></li>
</ul>
<p>This gives us the complete layer computation: <span class="math display">\[
\mathbf{h}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
\]</span> Where <span class="math inline">\(\mathbf{h}^{(l)}\)</span> represents the layerâ€™s output after applying the activation function.</p>
</section>
</section>
<section id="sec-dl-primer-network-topology-11ff" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-network-topology-11ff">Network Topology</h3>
<p>Network topology describes how the basic building blocks weâ€™ve discussed, such as neurons, layers, and connections, come together to form a complete neural network. We can best understand network topology through a concrete example. Consider the task of recognizing handwritten digits, a classic problem in deep learning using the MNIST dataset.</p>
<section id="sec-dl-primer-basic-structure-56bb" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-basic-structure-56bb">Basic Structure</h4>
<p>The fundamental structure of a neural network consists of three main components: input layer, hidden layers, and output layer. As shown in <a href="#fig-mnist-topology-1" class="quarto-xref">Figure&nbsp;14</a><span class="math inline">\(\text{a)}\)</span>, a <span class="math inline">\(28\times 28\)</span> pixel grayscale image of a handwritten digit must be processed through these layers to produce a classification output.</p>
<p>The input layerâ€™s width is directly determined by our data format. As shown in <a href="#fig-mnist-topology-1" class="quarto-xref">Figure&nbsp;14</a><span class="math inline">\(\text{b)}\)</span>, for a <span class="math inline">\(28\times 28\)</span> pixel image, each pixel becomes an input feature, requiring 784 input neurons <span class="math inline">\((28\times 28 = 784)\)</span>. We can think of this either as a 2D grid of pixels or as a flattened vector of 784 values, where each value represents the intensity of one pixel.</p>
<p>The output layerâ€™s structure is determined by our task requirements. For digit classification, we use 10 output neurons, one for each possible digit (0-9). When presented with an image, the network produces a value for each output neuron, where higher values indicate greater confidence that the image represents that particular digit.</p>
<p>Between these fixed input and output layers, we have flexibility in designing the hidden layer topology. The choice of hidden layer structure, including the number of layers to use and their respective widths, represents one of the fundamental design decisions in neural networks. Additional layers increase the networkâ€™s depth, allowing it to learn more abstract features through successive transformations. The width of each layer provides capacity for learning different features at each level of abstraction.</p>
<div id="fig-mnist-topology-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-topology-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d875c93e781359b5804312bce238860a3d56de52.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: \text{a)} A neural network topology for classifying MNIST digits, showing how a 28\times 28 pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification.  \text{b)} Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing."><img src="dl_primer_files/mediabag/d875c93e781359b5804312bce238860a3d56de52.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-topology-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <span class="math inline">\(\text{a)}\)</span> A neural network topology for classifying MNIST digits, showing how a <span class="math inline">\(28\times 28\)</span> pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification. <span class="math inline">\(\text{b)}\)</span> Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing.
</figcaption>
</figure>
</div>
<p>These basic topological choices have significant implications for both the networkâ€™s capabilities and its computational requirements. Each additional layer or neuron increases the number of parameters that must be stored and computed during both training and inference. However, without sufficient depth or width, the network may lack the capacity to learn complex patterns in the data.</p>
</section>
<section id="sec-dl-primer-design-tradeoffs-adee" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-design-tradeoffs-adee">Design Trade-offs</h4>
<p>The design of neural network topology centers on three fundamental decisions: the number of layers (depth), the size of each layer (width), and how these layers connect. Each choice affects both the networkâ€™s learning capability and its computational requirements.</p>
<p>Network depth determines the level of abstraction the network can achieve. Each layer transforms its input into a new representation, and stacking multiple layers allows the network to build increasingly complex features. In our MNIST example, a deeper network might first learn to detect edges, then combine these edges into strokes, and finally assemble strokes into complete digit patterns. However, adding layers isnâ€™t always beneficialâ€”deeper networks increase computational cost substantially, can be harder to train due to vanishing gradients, and may require more sophisticated training techniques.</p>
<p>The width of each layer, which is determined by the number of neurons it contains, controls how much information the network can process in parallel at each stage. Wider layers can learn more features simultaneously but require proportionally more parameters and computation. For instance, if a hidden layer is processing edge features in our digit recognition task, its width determines how many different edge patterns it can detect simultaneously.</p>
<p>A very important consideration in topology design is the total parameter count. For a network with layers of size <span class="math inline">\((n_1, n_2, \ldots, n_L)\)</span>, each pair of adjacent layers <span class="math inline">\(l\)</span> and <span class="math inline">\(l+1\)</span> requires <span class="math inline">\(n_l \times n_{l+1}\)</span> weight parameters, plus <span class="math inline">\(n_{l+1}\)</span> bias parameters. These parameters must be stored in memory and updated during training, making the parameter count a key constraint in practical applications.</p>
<p>When designing networks, we need to balance learning capacity, computational efficiency, and ease of training. While the basic approach connects every neuron to every neuron in the next layer (fully connected), this isnâ€™t always the most effective strategy. Sometimes, using fewer but more strategic connections, as seen in specialized architectures, can achieve better results with less computation. Consider our MNIST exampleâ€”when humans recognize digits, we donâ€™t analyze every pixel independently but look for meaningful patterns like lines and curves. Similarly, we can design our network to focus on local patterns in the image rather than treating each pixel as completely independent.</p>
<p>Another important consideration is how information flows through the network. While the basic flow is from input to output, some network designs include additional paths for information to flow, such as skip connections or residual connections. These alternative paths can make the network easier to train and more effective at learning complex patterns. Think of these as shortcuts that help information flow more directly when needed, similar to how our brain can combine both detailed and general impressions when recognizing objects.</p>
<p>These design decisions have significant practical implications for memory usage for storing network parameters, computational costs during both training and inference, training behavior and convergence, and the networkâ€™s ability to generalize to new examples. The optimal balance of these trade-offs depends heavily on your specific problem, available computational resources, and dataset characteristics. Successful network design requires carefully weighing these factors against practical constraints.</p>
</section>
<section id="sec-dl-primer-connection-patterns-c46f" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-connection-patterns-c46f">Connection Patterns</h4>
<p>Neural networks can be structured with different connection patterns between layers, each offering distinct advantages for learning and computation. Understanding these fundamental patterns provides insight into how networks process information and learn representations from data.</p>
<p>Dense connectivity represents the standard pattern where each neuron connects to every neuron in the subsequent layer. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. This full connectivity enables the network to learn arbitrary relationships between inputs and outputs, but the number of parameters scales quadratically with layer width.</p>
<p>Sparse connectivity patterns introduce purposeful restrictions in how neurons connect between layers. Rather than maintaining all possible connections, neurons connect to only a subset of neurons in the adjacent layer. This approach draws inspiration from biological neural systems, where neurons typically form connections with a limited number of other neurons. In visual processing tasks like our MNIST example, neurons might connect only to inputs representing nearby pixels, reflecting the local nature of visual features.</p>
<p>As networks grow deeper, the path from input to output becomes longer, potentially complicating the learning process. Skip connections address this by adding direct paths between non-adjacent layers. These connections provide alternative routes for information flow, supplementing the standard layer-by-layer progression. In our digit recognition example, skip connections might allow later layers to reference both high-level patterns and the original pixel values directly.</p>
<p>These connection patterns have significant implications for both the theoretical capabilities and practical implementation of neural networks. Dense connections maximize learning flexibility at the cost of computational efficiency. Sparse connections can reduce computational requirements while potentially improving the networkâ€™s ability to learn structured patterns. Skip connections help maintain effective information flow in deeper networks.</p>
</section>
<section id="sec-dl-primer-parameter-considerations-10f3" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-parameter-considerations-10f3">Parameter Considerations</h4>
<p>The arrangement of parameters (weights and biases) in a neural network determines both its learning capacity and computational requirements. While topology defines the networkâ€™s structure, the initialization and organization of parameters plays a crucial role in learning and performance.</p>
<p>Parameter count grows with network width and depth. For our MNIST example, consider a network with a 784-dimensional input layer, two hidden layers of 100 neurons each, and a 10-neuron output layer. The first layer requires 78,400 weights and 100 biases, the second layer 10,000 weights and 100 biases, and the output layer 1,000 weights and 10 biases, totaling 89,610 parameters. Each must be stored in memory and updated during learning.</p>
<p>Parameter initialization is fundamental to network behavior. Setting all parameters to zero would cause neurons in a layer to behave identically, preventing diverse feature learning. Instead, weights are typically initialized randomly, while biases often start at small constant values or even zeros. The scale of these initial values matters significantly, as values that are too large or too small can lead to poor learning dynamics.</p>
<p>The distribution of parameters affects information flow through layers. In digit recognition, if weights are too small, important input details might not propagate to later layers. If too large, the network might amplify noise. Biases help adjust the activation threshold of each neuron, enabling the network to learn optimal decision boundaries.</p>
<p>Different architectures may impose specific constraints on parameter organization. Some share weights across network regions to encode position-invariant pattern recognition. Others might restrict certain weights to zero, implementing sparse connectivity patterns.</p>
<div id="quiz-question-sec-dl-primer-neural-network-fundamentals-68cd" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following activation functions is most commonly used in modern neural networks due to its simplicity and effectiveness?</p>
<ol type="a">
<li>ReLU</li>
<li>Tanh</li>
<li>Sigmoid</li>
<li>Softmax</li>
</ol></li>
<li><p>Explain how the organization of neurons into layers enhances the capability of neural networks to model complex patterns.</p></li>
<li><p>Order the following components of a neural network from input to output: (1) Hidden Layers, (2) Output Layer, (3) Input Layer</p></li>
<li><p>What is the primary advantage of using bias terms in neural networks?</p>
<ol type="a">
<li>To increase the number of parameters</li>
<li>To reduce the need for activation functions</li>
<li>To enhance computational efficiency</li>
<li>To allow neurons to shift activation thresholds</li>
</ol></li>
<li><p>In a production system, what trade-offs would you consider when designing the topology of a neural network for a specific task?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-neural-network-fundamentals-68cd" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dl-primer-learning-process-38a0" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dl-primer-learning-process-38a0">Learning Process</h2>
<p>Neural networks learn to perform tasks through a process of training on examples. This process transforms the network from its initial state, where its weights are randomly initialized, to a trained state where the weights encode meaningful patterns from the training data. Understanding this process is fundamental to both the theoretical foundations and practical implementations of deep learning systems.</p>
<section id="sec-dl-primer-training-overview-1171" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-training-overview-1171">Training Overview</h3>
<p>The core principle of neural network training is supervised learning from labeled examples. Consider our MNIST digit recognition task: we have a dataset of 60,000 training images, each a <span class="math inline">\(28\times 28\)</span> pixel grayscale image paired with its correct digit label. The network must learn the relationship between these images and their corresponding digits through an iterative process of prediction and weight adjustment.</p>
<p>Training operates as a loop, where each iteration involves processing a subset of training examples called a batch. For each batch, the network performs several key operations:</p>
<ul>
<li>Forward computation through the network layers to generate predictions</li>
<li>Evaluation of prediction accuracy using a loss function</li>
<li>Computation of weight adjustments based on prediction errors</li>
<li>Update of network weights to improve future predictions</li>
</ul>
<p>This process can be expressed mathematically. Given an input image <span class="math inline">\(x\)</span> and its true label <span class="math inline">\(y\)</span>, the network computes its prediction: <span class="math display">\[
\hat{y} = f(x; \theta)
\]</span> where <span class="math inline">\(f\)</span> represents the neural network function and <span class="math inline">\(\theta\)</span> represents all trainable parameters (weights and biases, which we discussed earlier). The networkâ€™s error is measured by a loss function <span class="math inline">\(L\)</span>: <span class="math display">\[
\text{loss} = L(\hat{y}, y)
\]</span></p>
<p>This error measurement drives the adjustment of network parameters through a process called â€œbackpropagation,â€ which we will examine in detail later.</p>
<p>In practice, training operates on batches of examples rather than individual inputs. For the MNIST dataset, each training iteration might process, for example, 32, 64, or 128 images simultaneously. This batch processing serves two purposes: it enables efficient use of modern computing hardware through parallel processing, and it provides more stable parameter updates by averaging errors across multiple examples.</p>
<p>The training cycle continues until the network achieves sufficient accuracy or reaches a predetermined number of iterations. Throughout this process, the loss function serves as a guide, with its minimization indicating improved network performance.</p>
</section>
<section id="sec-dl-primer-forward-propagation-d412" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-forward-propagation-d412">Forward Propagation</h3>
<p>Forward propagation, as illustrated in <a href="#fig-forward-propagation" class="quarto-xref">Figure&nbsp;15</a>, is the core computational process in a neural network, where input data flows through the networkâ€™s layers to generate predictions. Understanding this process is essential as it forms the foundation for both network inference and training. Letâ€™s examine how forward propagation works using our MNIST digit recognition example.</p>
<div id="fig-forward-propagation" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forward-propagation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="7cb579a47d65631ddbcd793f25264af43e33ef55.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Forward Propagation Process: Neural networks transform input data into predictions by sequentially applying weighted sums and activation functions across interconnected layers, enabling complex pattern recognition. This layered computation forms the basis for both making inferences and updating model parameters during training."><img src="dl_primer_files/mediabag/7cb579a47d65631ddbcd793f25264af43e33ef55.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forward-propagation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Forward Propagation Process</strong>: Neural networks transform input data into predictions by sequentially applying weighted sums and activation functions across interconnected layers, enabling complex pattern recognition. This layered computation forms the basis for both making inferences and updating model parameters during training.
</figcaption>
</figure>
</div>
<p>When an image of a handwritten digit enters our network, it undergoes a series of transformations through the layers. Each transformation combines the weighted inputs with learned patterns to progressively extract relevant features. In our MNIST example, a <span class="math inline">\(28\times 28\)</span> pixel image is processed through multiple layers to ultimately produce probabilities for each possible digit (0-9).</p>
<p>The process begins with the input layer, where each pixelâ€™s grayscale value becomes an input feature. For MNIST, this means 784 input values <span class="math inline">\((28\times 28 = 784)\)</span>, each normalized between 0 and 1. These values then propagate forward through the hidden layers, where each neuron combines its inputs according to its learned weights and applies a nonlinear activation function.</p>
<section id="sec-dl-primer-layer-computation-5636" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-layer-computation-5636">Layer Computation</h4>
<p>The forward computation through a neural network proceeds systematically, with each layer transforming its inputs into increasingly abstract representations. In our MNIST network, this transformation process occurs in distinct stages.</p>
<p>At each layer, the computation involves two key steps: a linear transformation of inputs followed by a nonlinear activation. The linear transformation combines all inputs to a neuron using learned weights and a bias term. For a single neuron receiving inputs from the previous layer, this computation takes the form: <span class="math display">\[
z = \sum_{i=1}^n w_ix_i + b
\]</span> where <span class="math inline">\(w_i\)</span> represents the weights, <span class="math inline">\(x_i\)</span> the inputs, and <span class="math inline">\(b\)</span> the bias term. For an entire layer of neurons, we can express this more efficiently using matrix operations: <span class="math display">\[
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{W}^{(l)}\)</span> represents the weight matrix for layer <span class="math inline">\(l\)</span>, <span class="math inline">\(\mathbf{A}^{(l-1)}\)</span> contains the activations from the previous layer, and <span class="math inline">\(\mathbf{b}^{(l)}\)</span> is the bias vector.</p>
<p>Following this linear transformation, each layer applies a nonlinear activation function <span class="math inline">\(f\)</span>: <span class="math display">\[
\mathbf{A}^{(l)} = f(\mathbf{Z}^{(l)})
\]</span></p>
<p>This process repeats at each layer, creating a chain of transformations:</p>
<p>Input â†’ Linear Transform â†’ Activation â†’ Linear Transform â†’ Activation â†’ â€¦ â†’ Output</p>
<p>In our MNIST example, the pixel values first undergo a transformation by the first hidden layerâ€™s weights, converting the 784-dimensional input into an intermediate representation. Each subsequent layer further transforms this representation, ultimately producing a 10-dimensional output vector representing the networkâ€™s confidence in each possible digit.</p>
</section>
<section id="sec-dl-primer-mathematical-representation-6c2a" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-mathematical-representation-6c2a">Mathematical Representation</h4>
<p>The complete forward propagation process can be expressed as a composition of functions, each representing a layerâ€™s transformation. Let us formalize this mathematically, building on our MNIST example.</p>
<p>For a network with <span class="math inline">\(L\)</span> layers, we can express the full forward computation as: <span class="math display">\[
\mathbf{A}^{(L)} = f^{(L)}\Big(\mathbf{W}^{(L)}f^{(L-1)}\Big(\mathbf{W}^{(L-1)}\cdots\big(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)})\big)\cdots + \mathbf{b}^{(L-1)}\Big) + \mathbf{b}^{(L)}\Big)
\]</span></p>
<p>While this nested expression captures the complete process, we typically compute it step by step:</p>
<ol type="1">
<li><p>First layer: <span class="math display">\[
\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)}
\]</span> <span class="math display">\[
\mathbf{A}^{(1)} = f^{(1)}(\mathbf{Z}^{(1)})
\]</span></p></li>
<li><p>Hidden layers <span class="math inline">\((l = 2,\ldots, L-1)\)</span>: <span class="math display">\[
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
\]</span> <span class="math display">\[
\mathbf{A}^{(l)} = f^{(l)}(\mathbf{Z}^{(l)})
\]</span></p></li>
<li><p>Output layer: <span class="math display">\[
\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)} + \mathbf{b}^{(L)}
\]</span> <span class="math display">\[
\mathbf{A}^{(L)} = f^{(L)}(\mathbf{Z}^{(L)})
\]</span></p></li>
</ol>
<p>In our MNIST example, if we have a batch of <span class="math inline">\(B\)</span> images, the dimensions of these operations are:</p>
<ul>
<li>Input <span class="math inline">\(\mathbf{X}\)</span>: <span class="math inline">\(B \times 784\)</span></li>
<li>First layer weights <span class="math inline">\(\mathbf{W}^{(1)}\)</span>: <span class="math inline">\(n_1\times 784\)</span></li>
<li>Hidden layer weights <span class="math inline">\(\mathbf{W}^{(l)}\)</span>: <span class="math inline">\(n_l\times n_{l-1}\)</span></li>
<li>Output layer weights <span class="math inline">\(\mathbf{W}^{(L)}\)</span>: <span class="math inline">\(n_{L-1}\times 10\)</span></li>
</ul>
</section>
<section id="sec-dl-primer-computational-process-a092" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-computational-process-a092">Computational Process</h4>
<p>To understand how these mathematical operations translate into actual computation, letâ€™s walk through the forward propagation process for a batch of MNIST images. This process illustrates how data is transformed from raw pixel values to digit predictions.</p>
<p>Consider a batch of 32 images entering our network. Each image starts as a <span class="math inline">\(28\times 28\)</span> grid of pixel values, which we flatten into a 784-dimensional vector. For the entire batch, this gives us an input matrix <span class="math inline">\(\mathbf{X}\)</span> of size <span class="math inline">\(32\times 784\)</span>, where each row represents one image. The values are typically normalized to lie between 0 and 1.</p>
<p>The transformation at each layer proceeds as follows:</p>
<ul>
<li><p><strong>Input Layer Processing</strong>: The network takes our input matrix <span class="math inline">\(\mathbf{X}\)</span> <span class="math inline">\((32\times 784)\)</span> and transforms it using the first layerâ€™s weights. If our first hidden layer has 128 neurons, <span class="math inline">\(\mathbf{W}^{(1)}\)</span> is a <span class="math inline">\(784\times 128\)</span> matrix. The resulting computation <span class="math inline">\(\mathbf{X}\mathbf{W}^{(1)}\)</span> produces a <span class="math inline">\(32\times 128\)</span> matrix.</p></li>
<li><p><strong>Hidden Layer Transformations</strong>: Each element in this matrix then has its corresponding bias added and passes through an activation function. For example, with a ReLU activation, any negative values become zero while positive values remain unchanged. This nonlinear transformation enables the network to learn complex patterns in the data.</p></li>
<li><p><strong>Output Generation</strong>: The final layer transforms its inputs into a <span class="math inline">\(32\times 10\)</span> matrix, where each row contains 10 values corresponding to the networkâ€™s confidence scores for each possible digit. Often, these scores are converted to probabilities using a softmax function: <span class="math display">\[
P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10} e^{z_k}}
\]</span></p></li>
</ul>
<p>For each image in our batch, this gives us a probability distribution over the possible digits. The digit with the highest probability becomes the networkâ€™s prediction.</p>
</section>
<section id="sec-dl-primer-practical-considerations-6167" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-practical-considerations-6167">Practical Considerations</h4>
<p>The implementation of forward propagation requires careful attention to several practical aspects that affect both computational efficiency and memory usage. These considerations become particularly important when processing large batches of data or working with deep networks.</p>
<p>Memory management plays an important role during forward propagation. Each layerâ€™s activations must be stored for potential use in the backward pass during training. For our MNIST example with a batch size of 32, if we have three hidden layers of sizes 128, 256, and 128, the activation storage requirements are:</p>
<ul>
<li>First hidden layer: <span class="math inline">\(32\times 128 = 4,096\)</span> values</li>
<li>Second hidden layer: <span class="math inline">\(32\times 256 = 8,192\)</span> values</li>
<li>Third hidden layer: <span class="math inline">\(32\times 128 = 4,096\)</span> values</li>
<li>Output layer: <span class="math inline">\(32\times 10 = 320\)</span> values</li>
</ul>
<p>This gives us a total of 16,704 values that must be maintained in memory for each batch during training. The memory requirements scale linearly with batch size and can become substantial for larger networks.</p>
<p>Batch processing introduces important trade-offs. Larger batches enable more efficient matrix operations and better hardware utilization but require more memory. For example, doubling the batch size to 64 would double our memory requirements for activations. This relationship between batch size, memory usage, and computational efficiency often guides the choice of batch size in practice.</p>
<p>The organization of computations also affects performance. Matrix operations can be optimized through careful memory layout and the use of specialized libraries. The choice of activation functions impacts not only the networkâ€™s learning capabilities but also its computational efficiency, as some functions (like ReLU) are less expensive to compute than others (like tanh or sigmoid).</p>
<p>These considerations form the foundation for understanding the system requirements of neural networks, which we will explore in more detail in later chapters.</p>
</section>
</section>
<section id="sec-dl-primer-loss-functions-d892" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-loss-functions-d892">Loss Functions</h3>
<p>Neural networks learn by measuring and minimizing their prediction errors. Loss functions provide the Algorithmic Structure for quantifying these errors, serving as the essential feedback mechanism that guides the learning process. Through loss functions, we can convert the abstract goal of â€œmaking good predictionsâ€ into a concrete optimization problem.</p>
<p>To understand the role of loss functions, letâ€™s continue with our MNIST digit recognition example. When the network processes a handwritten digit image, it outputs ten numbers representing its confidence in each possible digit (0-9). The loss function measures how far these predictions deviate from the true answer. For instance, if an image shows a â€œ7â€, we want high confidence for digit â€œ7â€ and low confidence for all other digits. The loss function penalizes the network when its prediction differs from this ideal.</p>
<p>Consider a concrete example: if the network sees an image of â€œ7â€ and outputs confidences:</p>
<pre><code>[0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]</code></pre>
<p>The highest confidence (0.3) is assigned to digit â€œ7â€, but this confidence is quite low, indicating uncertainty in the prediction. A good loss function would produce a high loss value here, signaling that the network needs significant improvement. Conversely, if the network outputs:</p>
<pre><code>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]</code></pre>
<p>The loss function should produce a lower value, as this prediction is much closer to ideal.</p>
<section id="sec-dl-primer-basic-concepts-397d" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-basic-concepts-397d">Basic Concepts</h4>
<p>A loss function measures how far the networkâ€™s predictions are from the correct answers. This difference is expressed as a single number: a lower loss means the predictions are more accurate, while a higher loss indicates the network needs improvement. During training, the loss function guides the network by helping it adjust its weights to make better predictions. For example, in recognizing handwritten digits, the loss will penalize predictions that assign low confidence to the correct digit.</p>
<p>Mathematically, a loss function <span class="math inline">\(L\)</span> takes two inputs: the networkâ€™s predictions <span class="math inline">\(\hat{y}\)</span> and the true values <span class="math inline">\(y\)</span>. For a single training example in our MNIST task: <span class="math display">\[
L(\hat{y}, y) = \text{measure of discrepancy between prediction and truth}
\]</span></p>
<p>When training with batches of data, we typically compute the average loss across all examples in the batch: <span class="math display">\[
L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)
\]</span> where <span class="math inline">\(B\)</span> is the batch size and <span class="math inline">\((\hat{y}_i, y_i)\)</span> represents the prediction and truth for the <span class="math inline">\(i\)</span>-th example.</p>
<p>The choice of loss function depends on the type of task. For our MNIST classification problem, we need a loss function that can:</p>
<ol type="1">
<li>Handle probability distributions over multiple classes</li>
<li>Provide meaningful gradients for learning</li>
<li>Penalize wrong predictions effectively</li>
<li>Scale well with batch processing</li>
</ol>
</section>
<section id="sec-dl-primer-classification-losses-9278" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-classification-losses-9278">Classification Losses</h4>
<p>For classification tasks like MNIST digit recognition, â€œcross-entropyâ€ loss has emerged as the standard choice. This loss function is particularly well-suited for comparing predicted probability distributions with true class labels.</p>
<p>For a single digit image, our network outputs a probability distribution over the ten possible digits. We represent the true label as a one-hot vector where all entries are 0 except for a 1 at the correct digitâ€™s position. For instance, if the true digit is â€œ7â€, the label would be: <span class="math display">\[
y = \big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\big]
\]</span></p>
<p>The cross-entropy loss for this example is: <span class="math display">\[
L(\hat{y}, y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)
\]</span> where <span class="math inline">\(\hat{y}_j\)</span> represents the networkâ€™s predicted probability for digit j. Given our one-hot encoding, this simplifies to: <span class="math display">\[
L(\hat{y}, y) = -\log(\hat{y}_c)
\]</span> where <span class="math inline">\(c\)</span> is the index of the correct class. This means the loss depends only on the predicted probability for the correct digitâ€”the network is penalized based on how confident it is in the right answer.</p>
<p>For example, if our network predicts the following probabilities for an image of â€œ7â€:</p>
<pre><code>Predicted: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1]
True: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</code></pre>
<p>The loss would be <span class="math inline">\(-\log(0.8)\)</span>, which is approximately 0.223. If the network were more confident and predicted 0.9 for the correct digit, the loss would decrease to approximately 0.105.</p>
</section>
<section id="sec-dl-primer-loss-computation-b815" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-loss-computation-b815">Loss Computation</h4>
<p>The practical computation of loss involves considerations for both numerical stability and batch processing. When working with batches of data, we compute the average loss across all examples in the batch.</p>
<p>For a batch of B examples, the cross-entropy loss becomes: <span class="math display">\[
L_{\text{batch}} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})
\]</span></p>
<p>Computing this loss efficiently requires careful consideration of numerical precision. Taking the logarithm of very small probabilities can lead to numerical instability. Consider a case where our network predicts a probability of 0.0001 for the correct class. Computing <span class="math inline">\(\log(0.0001)\)</span> directly might cause underflow or result in imprecise values.</p>
<p>To address this, we typically implement the loss computation with two key modifications:</p>
<ol type="1">
<li><p>Add a small epsilon to prevent taking log of zero: <span class="math display">\[
L = -\log(\hat{y} + \epsilon)
\]</span></p></li>
<li><p>Apply the log-sum-exp trick for numerical stability: <span class="math display">\[
\text{softmax}(z_i) = \frac{\exp\big(z_i - \max(z)\big)}{\sum_j \exp\big(z_j - \max(z)\big)}
\]</span></p></li>
</ol>
<p>For our MNIST example with a batch size of 32, this means:</p>
<ul>
<li>Processing 32 sets of 10 probabilities</li>
<li>Computing 32 individual loss values</li>
<li>Averaging these values to produce the final batch loss</li>
</ul>
</section>
<section id="sec-dl-primer-training-implications-e004" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-training-implications-e004">Training Implications</h4>
<p>Understanding how loss functions influence training helps explain key implementation decisions in deep learning systems.</p>
<p>During each training iteration, the loss value serves multiple purposes:</p>
<ol type="1">
<li>Performance Metric: It quantifies current network accuracy</li>
<li>Optimization Target: Its gradients guide weight updates</li>
<li>Convergence Signal: Its trend indicates training progress</li>
</ol>
<p>For our MNIST classifier, monitoring the loss during training reveals the networkâ€™s learning trajectory. A typical pattern might show:</p>
<ul>
<li>Initial high loss (<span class="math inline">\(\sim 2.3\)</span>, equivalent to random guessing among 10 classes)</li>
<li>Rapid decrease in early training iterations</li>
<li>Gradual improvement as the network fine-tunes its predictions</li>
<li>Eventually stabilizing at a lower loss (<span class="math inline">\(\sim 0.1\)</span>, indicating confident correct predictions)</li>
</ul>
<p>The loss functionâ€™s gradients with respect to the networkâ€™s outputs provide the initial error signal that drives backpropagation. For cross-entropy loss, these gradients have a particularly simple form: the difference between predicted and true probabilities. This mathematical property makes cross-entropy loss especially suitable for classification tasks, as it provides strong gradients even when predictions are very wrong.</p>
<p>The choice of loss function also influences other training decisions:</p>
<ul>
<li>Learning rate selection (larger loss gradients might require smaller learning rates)</li>
<li>Batch size (loss averaging across batches affects gradient stability)</li>
<li>Optimization algorithm behavior</li>
<li>Convergence criteria</li>
</ul>
</section>
</section>
<section id="sec-dl-primer-backward-propagation-9a49" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-backward-propagation-9a49">Backward Propagation</h3>
<p>Backward propagation, often called backpropagation, is the algorithmic cornerstone of neural network training. While forward propagation computes predictions, backward propagation determines how to adjust the networkâ€™s weights to improve these predictions. This process enables neural networks to learn from their mistakes.</p>
<p>To understand backward propagation, letâ€™s continue with our MNIST example. When the network predicts a â€œ3â€ for an image of â€œ7â€, we need a systematic way to adjust weights throughout the network to make this mistake less likely in the future. Backward propagation provides this by calculating how each weight contributed to the error.</p>
<p>The process begins at the networkâ€™s output, where we compare the predicted digit probabilities with the true label. This error then flows backward through the network, with each layerâ€™s weights receiving an update signal based on their contribution to the final prediction. The computation follows the chain rule of calculus, breaking down the complex relationship between weights and final error into manageable steps.</p>
<p>Cost functions play a crucial role in helping neural networks learn by providing a measurable way to evaluate how well the network is performing and guide the optimization process.</p>


<div class="no-row-height column-margin column-container"><div class="">
  <div class="margin-video">
    <iframe src="https://www.youtube.com/embed/IHZwWFHWa-w?start=" style="width:100%; aspect-ratio: ; border:0;" allowfullscreen="">
    </iframe>
  </div>
  <p><em>Gradient descent â€“ Part 1 - 3Blue1Brown</em></p>
</div><div class="">
  <div class="margin-video">
    <iframe src="https://www.youtube.com/embed/Ilg3gGewQ5U?start=" style="width:100%; aspect-ratio: ; border:0;" allowfullscreen="">
    </iframe>
  </div>
  <p><em>Gradient descent â€“ Part 2 - 3Blue1Brown</em></p>
</div></div>

<section id="sec-dl-primer-gradient-flow-66f2" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-gradient-flow-66f2">Gradient Flow</h4>
<p>The flow of gradients through a neural network follows a path opposite to the forward propagation. Starting from the loss at the output layer, gradients propagate backwards, computing how each layer, and ultimately each weight, influenced the final prediction error.</p>
<p>In our MNIST example, consider what happens when the network misclassifies a â€œ7â€ as a â€œ3â€. The loss function generates an initial error signal at the output layer, essentially indicating that the probability for â€œ7â€ should increase while the probability for â€œ3â€ should decrease. This error signal then propagates backward through the network layers.</p>
<p>For a network with L layers, the gradient flow can be expressed mathematically. At each layer l, we compute how the layerâ€™s output affected the final loss: <span class="math display">\[
\frac{\partial L}{\partial \mathbf{A}^{(l)}} = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial \mathbf{A}^{(l)}}
\]</span></p>
<p>This computation cascades backward through the network, with each layerâ€™s gradients depending on the gradients computed in the layer previous to it. The process reveals how each layerâ€™s transformation contributed to the final prediction error. For instance, if certain weights in an early layer strongly influenced a misclassification, they will receive larger gradient values, indicating a need for more substantial adjustment.</p>
<p>However, this process faces important challenges in deep networks. As gradients flow backward through many layers, they can either vanish or explode. When gradients are repeatedly multiplied through many layers, they can become exponentially small, particularly with sigmoid or tanh activation functions. This causes early layers to learn very slowly or not at all, as they receive negligible (vanishing) updates. Conversely, if gradient values are consistently greater than 1, they can grow exponentially, leading to unstable training and destructive weight updates.</p>
</section>
<section id="sec-dl-primer-gradient-computation-b46b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-gradient-computation-b46b">Gradient Computation</h4>
<p>The actual computation of gradients involves calculating several partial derivatives at each layer. For each layer, we need to determine how changes in weights, biases, and activations affect the final loss. These computations follow directly from the chain rule of calculus but must be implemented efficiently for practical neural network training.</p>
<p>At each layer <span class="math inline">\(l\)</span>, we compute three main gradient components:</p>
<ol type="1">
<li><p>Weight Gradients: <span class="math display">\[
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T
\]</span></p></li>
<li><p>Bias Gradients: <span class="math display">\[
\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
\]</span></p></li>
<li><p>Input Gradients (for propagating to previous layer): <span class="math display">\[
\frac{\partial L}{\partial \mathbf{A}^{(l-1)}} = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
\]</span></p></li>
</ol>
<p>In our MNIST example, consider the final layer where the network outputs digit probabilities. If the network predicted <span class="math inline">\([0.1, 0.2, 0.5,\ldots, 0.05]\)</span> for an image of â€œ7â€, the gradient computation would:</p>
<ol type="1">
<li>Start with the error in these probabilities</li>
<li>Compute how weight adjustments would affect this error</li>
<li>Propagate these gradients backward to help adjust earlier layer weights</li>
</ol>
</section>
<section id="sec-dl-primer-implementation-aspects-411b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-implementation-aspects-411b">Implementation Aspects</h4>
<p>The practical implementation of backward propagation requires careful consideration of computational resources and memory management. These implementation details significantly impact training efficiency and scalability.</p>
<p>Memory requirements during backward propagation stem from two main sources. First, we need to store the intermediate activations from the forward pass, as these are required for computing gradients. For our MNIST network with a batch size of 32, each layerâ€™s activations must be maintained:</p>
<ul>
<li>Input layer: <span class="math inline">\(32\times 784\)</span> values</li>
<li>Hidden layers: <span class="math inline">\(32\times h\)</span> values (where <span class="math inline">\(h\)</span> is the layer width)</li>
<li>Output layer: <span class="math inline">\(32\times 10\)</span> values</li>
</ul>
<p>Second, we need storage for the gradients themselves. For each layer, we must maintain gradients of similar dimensions to the weights and biases. Taking our previous example of a network with hidden layers of size 128, 256, and 128, this means storing:</p>
<ul>
<li>First layer gradients: <span class="math inline">\(784\times 128\)</span> values</li>
<li>Second layer gradients: <span class="math inline">\(128\times 256\)</span> values</li>
<li>Third layer gradients: <span class="math inline">\(256\times 128\)</span> values</li>
<li>Output layer gradients: <span class="math inline">\(128\times 10\)</span> values</li>
</ul>
<p>The computational pattern of backward propagation follows a specific sequence:</p>
<ol type="1">
<li>Compute gradients at current layer</li>
<li>Update stored gradients</li>
<li>Propagate error signal to previous layer</li>
<li>Repeat until input layer is reached</li>
</ol>
<p>For batch processing, these computations are performed simultaneously across all examples in the batch, enabling efficient use of matrix operations and parallel processing capabilities.</p>
</section>
</section>
<section id="sec-dl-primer-optimization-process-5160" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dl-primer-optimization-process-5160">Optimization Process</h3>
<section id="sec-dl-primer-gradient-descent-basics-903a" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dl-primer-gradient-descent-basics-903a">Gradient Descent Basics</h4>
<p>The optimization process adjusts the networkâ€™s weights to improve its predictions. Using a method called gradient descent<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, the network calculates how much each weight contributes to the error and updates it to reduce the loss. This process is repeated over many iterations, gradually refining the networkâ€™s ability to make accurate predictions.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Gradient Descent</strong>: Think of gradient descent as finding the bottom of a valley while blindfoldedâ€”you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin â€œgradusâ€ (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later.</p></div></div><p>The fundamental update rule for gradient descent is: <span class="math display">\[
\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta}L
\]</span> where <span class="math inline">\(\theta\)</span> represents any network parameter (weights or biases), <span class="math inline">\(\alpha\)</span> is the learning rate, and <span class="math inline">\(\nabla_{\theta}L\)</span> is the gradient of the loss with respect to that parameter.</p>
<p>For our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses â€œ7â€s with â€œ1â€s, gradient descent will modify the weights to better distinguish between these digits. The learning rate <span class="math inline">\(\alpha\)</span><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> controls how large these adjustments areâ€”too large, and the network might overshoot optimal values; too small, and training will progress very slowly.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Learning Rate</strong>: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a carâ€”too much acceleration and youâ€™ll crash past your destination, too little and youâ€™ll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges.</p></div></div><p>The mathematical foundation of backpropagation involves computing partial derivatives through the chain rule, allowing each weight to understand its specific contribution to the overall error.</p>


<div class="no-row-height column-margin column-container"><div class="">
  <div class="margin-video">
    <iframe src="https://www.youtube.com/embed/tIeHLnjs5U8?start=" style="width:100%; aspect-ratio: ; border:0;" allowfullscreen="">
    </iframe>
  </div>
  <p><em>Backpropagation - 3Blue1Brown</em></p>
</div></div></section>
<section id="sec-dl-primer-batch-processing-89d7" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-batch-processing-89d7">Batch Processing</h4>
<p>Neural networks typically process multiple examples simultaneously during training, an approach known as mini-batch gradient descent. Rather than updating weights after each individual image, we compute the average gradient over a batch of examples before performing the update.</p>
<p>For a batch of size <span class="math inline">\(B\)</span>, the loss gradient becomes: <span class="math display">\[
\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B \nabla_{\theta}L_i
\]</span></p>
<p>In our MNIST training, with a typical batch size of 32, this means:</p>
<ol type="1">
<li>Process 32 images through forward propagation</li>
<li>Compute loss for all 32 predictions</li>
<li>Average the gradients across all 32 examples</li>
<li>Update weights using this averaged gradient</li>
</ol>
</section>
<section id="sec-dl-primer-training-loop-e2e1" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-training-loop-e2e1">Training Loop</h4>
<p>The complete training process combines forward propagation, backward propagation, and weight updates into a systematic training loop. This loop repeats until the network achieves satisfactory performance or reaches a predetermined number of iterations.</p>
<p>A single pass through the entire training dataset is called an epoch. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of 1,875 batch iterations. The training loop structure is:</p>
<ol type="1">
<li>For each epoch:
<ul>
<li>Shuffle training data to prevent learning order-dependent patterns</li>
<li>For each batch:
<ul>
<li>Perform forward propagation</li>
<li>Compute loss</li>
<li>Execute backward propagation</li>
<li>Update weights using gradient descent</li>
</ul></li>
<li>Evaluate network performance</li>
</ul></li>
</ol>
<p>During training, we monitor several key metrics:</p>
<ul>
<li>Training loss: average loss over recent batches</li>
<li>Validation accuracy: performance on held-out test data</li>
<li>Learning progress: how quickly the network improves</li>
</ul>
<p>For our digit recognition task, we might observe the networkâ€™s accuracy improve from 10% (random guessing) to over 95% through multiple epochs of training.</p>
</section>
<section id="sec-dl-primer-practical-considerations-accc" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-practical-considerations-accc">Practical Considerations</h4>
<p>The successful implementation of neural network training requires attention to several key practical aspects that significantly impact learning effectiveness. These considerations bridge the gap between theoretical understanding and practical implementation.</p>
<p>Learning rate selection is perhaps the most critical parameter affecting training. For our MNIST network, the choice of learning rate dramatically influences the training dynamics. A large learning rate of 0.1 might cause unstable training where the loss oscillates or explodes as weight updates overshoot optimal values. Conversely, a very small learning rate of 0.0001 might result in extremely slow convergence, requiring many more epochs to achieve good performance. A moderate learning rate of 0.01 often provides a good balance between training speed and stability, allowing the network to make steady progress while maintaining stable learning.</p>
<p>Convergence monitoring provides crucial feedback during the training process. As training progresses, we typically observe the loss value stabilizing around a particular value, indicating the network is approaching a local optimum. The validation accuracy often plateaus as well, suggesting the network has extracted most of the learnable patterns from the data. The gap between training and validation performance offers insights into whether the network is overfitting or generalizing well to new examples.</p>
<p>Resource requirements become increasingly important as we scale neural network training. The memory footprint must accommodate both model parameters and the intermediate computations needed for backpropagation. Computation scales linearly with batch size, affecting training speed and hardware utilization. Modern training often leverages GPU acceleration, making efficient use of parallel computing capabilities crucial for practical implementation.</p>
<p>Training neural networks also presents several fundamental challenges. Overfitting occurs when the network becomes too specialized to the training data, performing well on seen examples but poorly on new ones. Gradient instability can manifest as either vanishing or exploding gradients, making learning difficult. The interplay between batch size, available memory, and computational resources often requires careful balancing to achieve efficient training while working within hardware constraints.</p>
<div id="quiz-question-sec-dl-primer-learning-process-38a0" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>In the context of neural network training, what is the primary role of the loss function?</p>
<ol type="a">
<li>To initialize network weights</li>
<li>To measure prediction accuracy</li>
<li>To update network architecture</li>
<li>To compute weight adjustments</li>
</ol></li>
<li><p>Explain how batch processing in neural network training enhances computational efficiency.</p></li>
<li><p>The process of adjusting neural network weights based on prediction errors is known as ____. This process uses the gradients of the loss function to update weights and improve predictions.</p></li>
<li><p>Which of the following best describes the impact of using a larger batch size during training?</p>
<ol type="a">
<li>Increases computational efficiency but requires more memory</li>
<li>Decreases computational efficiency and requires less memory</li>
<li>Increases computational efficiency and requires less memory</li>
<li>Decreases computational efficiency but requires more memory</li>
</ol></li>
<li><p>In a production system, how might you decide on an appropriate batch size for training a neural network?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-learning-process-38a0" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dl-primer-prediction-phase-4204" class="level2">
<h2 class="anchored" data-anchor-id="sec-dl-primer-prediction-phase-4204">Prediction Phase</h2>
<p>Neural networks serve two distinct purposes: learning from data during training and making predictions during inference. While weâ€™ve explored how networks learn through forward propagation, backward propagation, and weight updates, the prediction phase operates differently. During inference, networks use their learned parameters to transform inputs into outputs without the need for learning mechanisms. This simpler computational process still requires careful consideration of how data flows through the network and how system resources are utilized. Understanding the prediction phase is crucial as it represents how neural networks are actually deployed to solve real-world problems, from classifying images to generating text predictions.</p>
<section id="sec-dl-primer-inference-basics-47d7" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-inference-basics-47d7">Inference Basics</h3>
<section id="sec-dl-primer-training-vs-inference-098e" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-training-vs-inference-098e">Training vs Inference</h4>
<p>The computation flow fundamentally changes when moving from training to inference. While training requires both forward and backward passes through the network to compute gradients and update weights, inference involves only the forward pass computation. This simpler flow means that each layer needs to perform only one set of operations, transforming inputs to outputs using the learned weights, rather than tracking intermediate values for gradient computation, as illustrated in <a href="#fig-training-vs-inference" class="quarto-xref">Figure&nbsp;16</a>.</p>
<div id="fig-training-vs-inference" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-training-vs-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="e84e561554a677045d65874d0b07e5ebdb3eb141.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;16: Inference vs.&nbsp;Training Flow: During inference, neural networks utilize learned weights for forward pass computation only, simplifying the data flow and reducing computational cost compared to training, which requires both forward and backward passes for weight updates. This streamlined process enables efficient deployment of trained models for real-time predictions."><img src="dl_primer_files/mediabag/e84e561554a677045d65874d0b07e5ebdb3eb141.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-vs-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: <strong>Inference vs.&nbsp;Training Flow</strong>: During inference, neural networks utilize learned weights for forward pass computation only, simplifying the data flow and reducing computational cost compared to training, which requires both forward and backward passes for weight updates. This streamlined process enables efficient deployment of trained models for real-time predictions.
</figcaption>
</figure>
</div>
<p>Parameter freezing is another another major distinction between training and inference phases. During training, weights and biases continuously update to minimize the loss function. In inference, these parameters remain fixed, acting as static transformations learned from the training data. This freezing of parameters not only simplifies computation but also enables optimizations impossible during training, such as weight quantization or pruning.</p>
<p>The structural difference between training loops and inference passes significantly impacts system design. Training operates in an iterative loop, processing multiple batches of data repeatedly across many epochs to refine the networkâ€™s parameters. Inference, in contrast, typically processes each input just once, generating predictions in a single forward pass. This fundamental shift from iterative refinement to single-pass prediction influences how we architect systems for deployment.</p>
<p>Memory and computation requirements differ substantially between training and inference. Training demands considerable memory to store intermediate activations for backpropagation, gradients for weight updates, and optimization states. Inference eliminates these memory-intensive requirements, needing only enough memory to store the model parameters and compute a single forward pass. This reduction in memory footprint, coupled with simpler computation patterns, enables inference to run efficiently on a broader range of devices, from powerful servers to resource-constrained edge devices.</p>
<p>In general, the training phase requires more computational resources and memory for learning, while inference is streamlined for efficient prediction. <a href="#tbl-train-vs-inference" class="quarto-xref">Table&nbsp;5</a> summarizes the key differences between training and inference.</p>
<div id="tbl-train-vs-inference" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-train-vs-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: <strong>Training vs.&nbsp;Inference</strong>: Neural networks transition from a computationally intensive training phaseâ€”requiring both forward and backward passes with updated parametersâ€”to an efficient inference phase using fixed parameters and solely forward passes. This distinction enables deployment on resource-constrained devices by minimizing memory requirements and computational load during prediction.
</figcaption>
<div aria-describedby="tbl-train-vs-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 34%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Training</th>
<th style="text-align: left;">Inference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computation Flow</td>
<td style="text-align: left;">Forward and backward passes, gradient computation</td>
<td style="text-align: left;">Forward pass only, direct input to output</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parameters</td>
<td style="text-align: left;">Continuously updated weights and biases</td>
<td style="text-align: left;">Fixed/frozen weights and biases</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Processing Pattern</td>
<td style="text-align: left;">Iterative loops over multiple epochs</td>
<td style="text-align: left;">Single pass through the network</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Requirements</td>
<td style="text-align: left;">High â€“ stores activations, gradients, optimizer state</td>
<td style="text-align: left;">Lowerâ€“ stores only model | parameters and current input</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Computational Needs</td>
<td style="text-align: left;">Heavy â€“ gradient updates, backpropagation</td>
<td style="text-align: left;">Lighter â€“ matrix multiplication only</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hardware Requirements</td>
<td style="text-align: left;">GPUs/specialized hardware for efficient training</td>
<td style="text-align: left;">Can run on simpler devices, including mobile/edge</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This stark contrast between training and inference phases highlights why system architectures often differ significantly between development and deployment environments. While training requires substantial computational resources and specialized hardware, inference can be optimized for efficiency and deployed across a broader range of devices.</p>
</section>
<section id="sec-dl-primer-basic-pipeline-8598" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-basic-pipeline-8598">Basic Pipeline</h4>
<p>The implementation of neural networks in practical applications requires a complete processing pipeline that extends beyond the network itself. This pipeline, which is illustrated in <a href="#fig-inference-pipeline" class="quarto-xref">Figure&nbsp;17</a> transforms raw inputs into meaningful outputs through a series of distinct stages, each essential for the systemâ€™s operation. Understanding this complete pipeline provides critical insights into the design and deployment of machine learning systems.</p>
<div id="fig-inference-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inference-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c0cfb4a2d13a1b77c336e555ebbfc4976109151d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;17: Inference Pipeline: Machine learning systems transform raw inputs into final outputs through a series of sequential stagesâ€”preprocessing, neural network computation, and post-processingâ€”each critical for accurate prediction and deployment. This pipeline emphasizes the distinction between model architecture and the complete system required for real-world application."><img src="dl_primer_files/mediabag/c0cfb4a2d13a1b77c336e555ebbfc4976109151d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inference-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: <strong>Inference Pipeline</strong>: Machine learning systems transform raw inputs into final outputs through a series of sequential stagesâ€”preprocessing, neural network computation, and post-processingâ€”each critical for accurate prediction and deployment. This pipeline emphasizes the distinction between model architecture and the complete system required for real-world application.
</figcaption>
</figure>
</div>
<p>The key thing to notice from the figure is that machine learning systems operate as hybrid architectures that combine conventional computing operations with neural network computations. The neural network component, focused on learned transformations through matrix operations, represents just one element within a broader computational framework. This framework encompasses both the preparation of input data and the interpretation of network outputs, processes that rely primarily on traditional computing methods.</p>
<p>Consider how data flows through the pipeline in <a href="#fig-inference-pipeline" class="quarto-xref">Figure&nbsp;17</a>:</p>
<ol type="1">
<li>Raw inputs arrive in their original form, which might be images, text, sensor readings, or other data types</li>
<li>Pre-processing transforms these inputs into a format suitable for neural network consumption</li>
<li>The neural network performs its learned transformations</li>
<li>Raw outputs emerge from the network, often in numerical form</li>
<li>Post-processing converts these outputs into meaningful, actionable results</li>
</ol>
<p>This pipeline structure reveals several fundamental characteristics of machine learning systems. The neural network, despite its computational sophistication, functions as a component within a larger system. Performance bottlenecks may arise at any stage of the pipeline, not exclusively within the neural network computation. System optimization must therefore consider the entire pipeline rather than focusing solely on the neural networkâ€™s operation.</p>
<p>The hybrid nature of this architecture has significant implications for system implementation. While neural network computations may benefit from specialized hardware accelerators, pre- and post-processing operations typically execute on conventional processors. This distribution of computation across heterogeneous hardware resources represents a fundamental consideration in system design.</p>
</section>
</section>
<section id="sec-dl-primer-preprocessing-992d" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-preprocessing-992d">Pre-processing</h3>
<p>The pre-processing stage transforms raw inputs into a format suitable for neural network computation. While often overlooked in theoretical discussions, this stage forms a critical bridge between real-world data and neural network operations. Consider our MNIST digit recognition example: before a handwritten digit image can be processed by the neural network we designed earlier, it must undergo several transformations. Raw images of handwritten digits arrive in various formats, sizes, and pixel value ranges. For instance, in <a href="#fig-handwritten" class="quarto-xref">Figure&nbsp;18</a>, we see that the digits are all of different sizes, and even the number 6 is written differently by the same person.</p>
<div id="fig-handwritten" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-handwritten-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/handwritten_digits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;18: Handwritten Digit Variability: Real-world data exhibits substantial variation in style, size, and orientation, necessitating robust pre-processing techniques for reliable machine learning performance. these images exemplify the challenges of digit recognition, where even seemingly simple inputs require normalization and feature extraction before they can be effectively processed by a neural network. Source: o. augereau."><img src="images/png/handwritten_digits.png" class="img-fluid figure-img" style="width:55.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-handwritten-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: <strong>Handwritten Digit Variability</strong>: Real-world data exhibits substantial variation in style, size, and orientation, necessitating robust pre-processing techniques for reliable machine learning performance. these images exemplify the challenges of digit recognition, where even seemingly simple inputs require normalization and feature extraction before they can be effectively processed by a neural network. Source: o. augereau.
</figcaption>
</figure>
</div>
<p>The pre-processing stage standardizes these inputs through conventional computing operations:</p>
<ul>
<li>Image scaling to the required <span class="math inline">\(28\times 28\)</span> pixel dimensions, camera images are usually large(r).</li>
<li>Pixel value normalization from <span class="math inline">\([0,255]\)</span> to <span class="math inline">\([0,1]\)</span>, most cameras generate colored images.</li>
<li>Flattening the 2D image array into a 784-dimensional vector, preparing it for the neural network.</li>
<li>Basic validation to ensure data integrity, making sure the network predicted correctly.</li>
</ul>
<p>What distinguishes pre-processing from neural network computation is its reliance on traditional computing operations rather than learned transformations. While the neural network learns to recognize digits through training, pre-processing operations remain fixed, deterministic transformations. This distinction has important system implications: pre-processing operates on conventional CPUs rather than specialized neural network hardware, and its performance characteristics follow traditional computing patterns.</p>
<p>The effectiveness of pre-processing directly impacts system performance. Poor normalization can lead to reduced accuracy, inconsistent scaling can introduce artifacts, and inefficient implementation can create bottlenecks. Understanding these implications helps in designing robust machine learning systems that perform well in real-world conditions.</p>
</section>
<section id="sec-dl-primer-inference-dd7f" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-inference-dd7f">Inference</h3>
<p>The inference phase represents the operational state of a neural network, where learned parameters are used to transform inputs into predictions. Unlike the training phase we discussed earlier, inference focuses solely on forward computation with fixed parameters.</p>
<section id="sec-dl-primer-network-initialization-3d95" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-network-initialization-3d95">Network Initialization</h4>
<p>Before processing any inputs, the neural network must be properly initialized for inference. This initialization phase involves loading the model parameters learned during training into memory. For our MNIST digit recognition network, this means loading specific weight matrices and bias vectors for each layer. Letâ€™s examine the exact memory requirements for our architecture:</p>
<ul>
<li>Input to first hidden layer:
<ul>
<li>Weight matrix: <span class="math inline">\(784\times 100 = 78,400\)</span> parameters</li>
<li>Bias vector: 100 parameters</li>
</ul></li>
<li>First to second hidden layer:
<ul>
<li>Weight matrix: <span class="math inline">\(100\times 100 = 10,000\)</span> parameters</li>
<li>Bias vector: 100 parameters</li>
</ul></li>
<li>Second hidden layer to output:
<ul>
<li>Weight matrix: <span class="math inline">\(100\times 10 = 1,000\)</span> parameters</li>
<li>Bias vector: 10 parameters</li>
</ul></li>
</ul>
<p>In total, the network requires storage for 89,610 learned parameters (89,400 weights plus 210 biases). Beyond these fixed parameters, memory must also be allocated for intermediate activations during forward computation. For processing a single image, this means allocating space for:</p>
<ul>
<li>First hidden layer activations: 100 values</li>
<li>Second hidden layer activations: 100 values</li>
<li>Output layer activations: 10 values</li>
</ul>
<p>This memory allocation pattern differs significantly from training, where additional memory was needed for gradients, optimizer states, and backpropagation computations.</p>
</section>
<section id="sec-dl-primer-forward-pass-computation-81c3" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-forward-pass-computation-81c3">Forward Pass Computation</h4>
<p>During inference, data propagates through the networkâ€™s layers using the initialized parameters. This forward propagation process, while similar in structure to its training counterpart, operates with different computational constraints and optimizations. The computation follows a deterministic path from input to output, transforming the data at each layer using learned parameters.</p>
<p>For our MNIST digit recognition network, consider the precise computations at each layer. The network processes a pre-processed image represented as a 784-dimensional vector through successive transformations:</p>
<ol type="1">
<li>First Hidden Layer Computation:
<ul>
<li>Input transformation: 784 inputs combine with 78,400 weights through matrix multiplication</li>
<li>Linear computation: <span class="math inline">\(\mathbf{z}^{(1)} = \mathbf{x}\mathbf{W}^{(1)} + \mathbf{b}^{(1)}\)</span></li>
<li>Activation: <span class="math inline">\(\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)})\)</span></li>
<li>Output: 100-dimensional activation vector</li>
</ul></li>
<li>Second Hidden Layer Computation:
<ul>
<li>Input transformation: 100 values combine with 10,000 weights</li>
<li>Linear computation: <span class="math inline">\(\mathbf{z}^{(2)} = \mathbf{a}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}\)</span></li>
<li>Activation: <span class="math inline">\(\mathbf{a}^{(2)} = \text{ReLU}(\mathbf{z}^{(2)})\)</span></li>
<li>Output: 100-dimensional activation vector</li>
</ul></li>
<li>Output Layer Computation:
<ul>
<li>Final transformation: 100 values combine with 1,000 weights</li>
<li>Linear computation: <span class="math inline">\(\mathbf{z}^{(3)} = \mathbf{a}^{(2)}\mathbf{W}^{(3)} + \mathbf{b}^{(3)}\)</span></li>
<li>Activation: <span class="math inline">\(\mathbf{a}^{(3)} = \text{softmax}(\mathbf{z}^{(3)})\)</span></li>
<li>Output: 10 probability values</li>
</ul></li>
</ol>
<p><a href="#tbl-forward-pass" class="quarto-xref">Table&nbsp;6</a> shows how these computations, while mathematically identical to training-time forward propagation, show important operational differences:</p>
<div id="tbl-forward-pass" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: <strong>Forward Pass Optimization</strong>: During inference, neural networks prioritize computational efficiency by retaining only current layer activations and releasing intermediate states, unlike training where complete activation history is maintained for backpropagation. This optimization streamlines output generation by focusing resources on immediate computations rather than gradient preparation.
</figcaption>
<div aria-describedby="tbl-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 36%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Characteristic</th>
<th style="text-align: left;">Training Forward Pass</th>
<th style="text-align: left;">Inference Forward Pass</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Activation Storage</td>
<td style="text-align: left;">Maintains complete activation history for backpropagation</td>
<td style="text-align: left;">Retains only current layer activations</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Pattern</td>
<td style="text-align: left;">Preserves intermediate states throughout forward pass</td>
<td style="text-align: left;">Releases memory after layer computation completes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Computational Flow</td>
<td style="text-align: left;">Structured for gradient computation preparation</td>
<td style="text-align: left;">Optimized for direct output generation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Resource Profile</td>
<td style="text-align: left;">Higher memory requirements for training operations</td>
<td style="text-align: left;">Minimized memory footprint for efficient execution</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This streamlined computation pattern enables efficient inference while maintaining the networkâ€™s learned capabilities. The reduction in memory requirements and simplified computational flow make inference particularly suitable for deployment in resource-constrained environments, such as Mobile ML and Tiny ML.</p>
</section>
<section id="sec-dl-primer-resource-requirements-9f9b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-resource-requirements-9f9b">Resource Requirements</h4>
<p>Neural networks consume computational resources differently during inference compared to training. During inference, resource utilization focuses primarily on efficient forward pass computation and minimal memory overhead. Letâ€™s examine the specific requirements for our MNIST digit recognition network.</p>
<p>Memory requirements during inference can be precisely quantified:</p>
<ol type="1">
<li>Static Memory (Model Parameters):
<ul>
<li>Layer 1: 78,400 weights + 100 biases</li>
<li>Layer 2: 10,000 weights + 100 biases</li>
<li>Layer 3: 1,000 weights + 10 biases</li>
<li>Total: 89,610 parameters (<span class="math inline">\(\approx 358.44\)</span> KB at 32-bit floating point precision)</li>
</ul></li>
<li>Dynamic Memory (Activations):
<ul>
<li>Layer 1 output: 100 values</li>
<li>Layer 2 output: 100 values</li>
<li>Layer 3 output: 10 values</li>
<li>Total: 210 values (<span class="math inline">\(\approx 0.84\)</span> KB at 32-bit floating point precision)</li>
</ul></li>
</ol>
<p>Computational requirements follow a fixed pattern for each input:</p>
<ul>
<li>First layer: 78,400 multiply-adds</li>
<li>Second layer: 10,000 multiply-adds</li>
<li>Output layer: 1,000 multiply-adds</li>
<li>Total: 89,400 multiply-add operations per inference</li>
</ul>
<p>This resource profile stands in stark contrast to training requirements, where additional memory for gradients and computational overhead for backpropagation significantly increase resource demands. The predictable, streamlined nature of inference computations enables various optimization opportunities and efficient hardware utilization.</p>
</section>
<section id="sec-dl-primer-optimization-opportunities-d5ec" class="level4">
<h4 class="anchored" data-anchor-id="sec-dl-primer-optimization-opportunities-d5ec">Optimization Opportunities</h4>
<p>The fixed nature of inference computation presents several opportunities for optimization that are not available during training. Once a neural networkâ€™s parameters are frozen, the predictable pattern of computation allows for systematic improvements in both memory usage and computational efficiency.</p>
<p>Batch size selection represents a fundamental trade-off in inference optimization. During training, large batches were necessary for stable gradient computation, but inference offers more flexibility. Processing single inputs minimizes latency, making it ideal for real-time applications where immediate responses are crucial. However, batch processing can significantly improve throughput by better utilizing parallel computing capabilities, particularly on GPUs. For our MNIST network, consider the memory implications: processing a single image requires storing 210 activation values, while a batch of 32 images requires 6,720 activation values but can process images up to 32 times faster on parallel hardware.</p>
<p>Memory management during inference can be significantly more efficient than during training. Since intermediate values are only needed for forward computation, memory buffers can be carefully managed and reused. The activation values from each layer need only exist until the next layerâ€™s computation is complete. This enables in-place operations where possible, reducing the total memory footprint. Furthermore, the fixed nature of inference allows for precise memory alignment and access patterns optimized for the underlying hardware architecture.</p>
<p>Hardware-specific optimizations become particularly important during inference. On CPUs, computations can be organized to maximize cache utilization and take advantage of SIMD (Single Instruction, Multiple Data) capabilities. GPU deployments benefit from optimized matrix multiplication routines and efficient memory transfer patterns. These optimizations extend beyond pure computational efficiency, as they can significantly impact power consumption and hardware utilization, critical factors in real-world deployments.</p>
<p>The predictable nature of inference also enables more aggressive optimizations like reduced numerical precision. While training typically requires 32-bit floating-point precision to maintain stable gradient computation, inference can often operate with 16-bit or even 8-bit precision while maintaining acceptable accuracy. For our MNIST network, this could reduce the memory footprint from 358.44 KB to 179.22 KB or even 89.61 KB, with corresponding improvements in computational efficiency.</p>
<p>These optimization principles, while illustrated through our simple MNIST feedforward network, represent only the foundation of neural network optimization. More sophisticated architectures introduce additional considerations and opportunities, including specialized designs for spatial data processing, sequential computation, and attention-based computation patterns. These architectural variations and their optimizations will be explored in detail in subsequent chapters, particularly when we discuss deep learning architectures, model optimizations, and efficient AI implementations.</p>
</section>
</section>
<section id="sec-dl-primer-postprocessing-93d9" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-postprocessing-93d9">Post-processing</h3>
<p>The transformation of neural network outputs into actionable predictions requires a return to traditional computing paradigms. Just as pre-processing bridges real-world data to neural computation, post-processing bridges neural outputs back to conventional computing systems. This completes the hybrid computing pipeline we examined earlier, where neural and traditional computing operations work in concert to solve real-world problems.</p>
<p>The complexity of post-processing extends beyond simple mathematical transformations. Real-world systems must handle uncertainty, validate outputs, and integrate with larger computing systems. In our MNIST example, a digit recognition system might require not just the most likely digit, but also confidence measures to determine when human intervention is needed. This introduces additional computational steps: confidence thresholds, secondary prediction checks, and error handling logic, all of which are implemented in traditional computing frameworks.</p>
<p>The computational requirements of post-processing differ significantly from neural network inference. While inference benefits from parallel processing and specialized hardware, post-processing typically runs on conventional CPUs and follows sequential logic patterns. This return to traditional computing brings both advantages and constraints. Operations are more flexible and easier to modify than neural computations, but they may become bottlenecks if not carefully implemented. For instance, computing softmax probabilities for a batch of predictions requires different optimization strategies than the matrix multiplications of neural network layers.</p>
<p>System integration considerations often dominate post-processing design. Output formats must match downstream system requirements, error handling must align with broader system protocols, and performance must meet system-level constraints. In a complete mail sorting system, the post-processing stage must not only identify digits but also format these predictions for the sorting machinery, handle uncertainty cases appropriately, and maintain processing speeds that match physical mail flow rates.</p>
<p>This return to traditional computing paradigms completes the hybrid nature of machine learning systems. Just as pre-processing prepared real-world data for neural computation, post-processing adapts neural outputs for real-world use. Understanding this hybrid nature, the interplay between neural and traditional computing, is essential for designing and implementing effective machine learning systems.</p>
<div id="quiz-question-sec-dl-primer-prediction-phase-4204" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>What is the primary computational difference between the training and inference phases of a neural network?</p>
<ol type="a">
<li>Training involves only forward passes, while inference involves both forward and backward passes.</li>
<li>Inference involves only forward passes, while training involves both forward and backward passes.</li>
<li>Training uses fixed parameters, while inference updates parameters continuously.</li>
<li>Inference requires more memory and computational resources than training.</li>
</ol></li>
<li><p>Explain why inference can be optimized for deployment on resource-constrained devices.</p></li>
<li><p>Order the following stages of the inference pipeline: (1) Pre-processing, (2) Neural Network Computation, (3) Post-processing.</p></li>
<li><p>Which of the following optimizations is typically applied during inference but not during training?</p>
<ol type="a">
<li>Gradient clipping</li>
<li>Batch normalization</li>
<li>Dropout</li>
<li>Weight quantization</li>
</ol></li>
<li><p>In a production system, what trade-offs might you consider when choosing between single input processing and batch processing during inference?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-prediction-phase-4204" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dl-primer-case-study-usps-postal-service-aa64" class="level2">
<h2 class="anchored" data-anchor-id="sec-dl-primer-case-study-usps-postal-service-aa64">Case Study: USPS Postal Service</h2>
<p>The theoretical foundations of neural networks find concrete expression in systems that solve real-world problems at scale. The USPS handwritten digit recognition system exemplifies this translation, demonstrating how the mathematical principles of gradient descent, backpropagation, and activation functions combine to create a production system processing millions of mail pieces daily.</p>
<section id="sec-dl-primer-realworld-problem-5233" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-realworld-problem-5233">Real-world Problem</h3>
<p>The United States Postal Service (USPS) processes over 100 million pieces of mail daily, each requiring accurate routing based on handwritten ZIP codes. In the early 1990s, this task was primarily performed by human operators, making it one of the largest manual data entry operations in the world. The automation of this process through neural networks represents one of the earliest and most successful large-scale deployments of artificial intelligence, embodying many of the principles weâ€™ve explored in this chapter.</p>
<p>Consider the complexity of this task: a ZIP code recognition system must process images of handwritten digits captured under varying conditions, different writing styles, pen types, paper colors, and environmental factors (see <a href="#fig-usps-digit-examples" class="quarto-xref">Figure&nbsp;19</a>). It must make accurate predictions within milliseconds to maintain mail processing speeds. Furthermore, errors in recognition can lead to significant delays and costs from misrouted mail. This real-world constraint meant the system needed not just high accuracy, but also reliable measures of prediction confidence to identify when human intervention was necessary.</p>
<div id="fig-usps-digit-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-usps-digit-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/usps_examples_new.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure&nbsp;19: Handwritten Digit Variability: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for robust feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks."><img src="images/jpg/usps_examples_new.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-usps-digit-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: <strong>Handwritten Digit Variability</strong>: Real-world handwritten digits exhibit significant variations in stroke width, slant, and character formation, posing challenges for automated recognition systems like those used by the USPS. These examples demonstrate the need for robust feature extraction and model generalization to achieve high accuracy in optical character recognition (OCR) tasks.
</figcaption>
</figure>
</div>
<p>This challenging environment presented requirements spanning every aspect of neural network implementation weâ€™ve discussed, from biological inspiration to practical deployment considerations. The success or failure of the system would depend not just on the neural networkâ€™s accuracy, but on the entire pipeline from image capture through to final sorting decisions.</p>
</section>
<section id="sec-dl-primer-system-development-02bf" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-system-development-02bf">System Development</h3>
<p>The development of the USPS digit recognition system required careful consideration at every stage, from data collection to deployment. This process illustrates how theoretical principles of neural networks translate into practical engineering decisions.</p>
<p>Data collection presented the first major challenge. Unlike controlled laboratory environments, postal facilities needed to process mail pieces with tremendous variety. The training dataset had to capture this diversity. Digits written by people of different ages, educational backgrounds, and writing styles formed just part of the challenge. Envelopes came in varying colors and textures, and images were captured under different lighting conditions and orientations. This extensive data collection effort later contributed to the creation of the MNIST database weâ€™ve used in our examples.</p>
<p>The network architecture design required balancing multiple constraints. While deeper networks might achieve higher accuracy, they would also increase processing time and computational requirements. Processing <span class="math inline">\(28\times 28\)</span> pixel images of individual digits needed to complete within strict time constraints while running reliably on available hardware. The network had to maintain consistent accuracy across varying conditions, from well-written digits to hurried scrawls.</p>
<p>Training the network introduced additional complexity. The system needed to achieve high accuracy not just on a test dataset, but on the endless variety of real-world handwriting styles. Careful preprocessing normalized input images to account for variations in size and orientation. Data augmentation techniques increased the variety of training samples. The team validated performance across different demographic groups and tested under actual operating conditions to ensure robust performance.</p>
<p>The engineering team faced a critical decision regarding confidence thresholds. Setting these thresholds too high would route too many pieces to human operators, defeating the purpose of automation. Setting them too low would risk delivery errors. The solution emerged from analyzing the confidence distributions of correct versus incorrect predictions. This analysis established thresholds that optimized the tradeoff between automation rate and error rate, ensuring efficient operation while maintaining acceptable accuracy.</p>
</section>
<section id="sec-dl-primer-complete-pipeline-2253" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-complete-pipeline-2253">Complete Pipeline</h3>
<p>Following a single piece of mail through the USPS recognition system illustrates how the concepts weâ€™ve discussed integrate into a complete solution. The journey from physical mail piece to sorted letter demonstrates the interplay between traditional computing, neural network inference, and physical machinery.</p>
<p>The process begins when an envelope reaches the imaging station. High-speed cameras capture the ZIP code region at rates exceeding several pieces of mail (e.g.&nbsp;10) pieces per second. This image acquisition process must adapt to varying envelope colors, handwriting styles, and environmental conditions. The system must maintain consistent image quality despite the speed of operation, as motion blur and proper illumination present significant engineering challenges.</p>
<p>Pre-processing transforms these raw camera images into a format suitable for neural network analysis. The system must locate the ZIP code region, segment individual digits, and normalize each digit image. This stage employs traditional computer vision techniques: image thresholding adapts to envelope background color, connected component analysis identifies individual digits, and size normalization produces standard <span class="math inline">\(28\times 28\)</span> pixel images. Speed remains critical; these operations must complete within milliseconds to maintain throughput.</p>
<p>The neural network then processes each normalized digit image. The trained network, with its 89,610 parameters (as we detailed earlier), performs forward propagation to generate predictions. Each digit passes through two hidden layers of 100 neurons each, ultimately producing ten output values representing digit probabilities. This inference process, while computationally intensive, benefits from the optimizations we discussed in the previous section.</p>
<p>Post-processing converts these neural network outputs into sorting decisions. The system applies confidence thresholds to each digit prediction. A complete ZIP code requires high confidence in all five digits, a single uncertain digit flags the entire piece for human review. When confidence meets thresholds, the system transmits sorting instructions to mechanical systems that physically direct the mail piece to its appropriate bin.</p>
<p>The entire pipeline operates under strict timing constraints. From image capture to sorting decision, processing must complete before the mail piece reaches its sorting point. The system maintains multiple pieces in various pipeline stages simultaneously, requiring careful synchronization between computing and mechanical systems. This real-time operation illustrates why the optimizations we discussed in inference and post-processing become crucial in practical applications.</p>
</section>
<section id="sec-dl-primer-results-impact-bb54" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-results-impact-bb54">Results and Impact</h3>
<p>The implementation of neural network-based ZIP code recognition transformed USPS mail processing operations. By 2000, several facilities across the country utilized this technology, processing millions of mail pieces daily. This real-world deployment demonstrated both the potential and limitations of neural network systems in mission-critical applications.</p>
<p>Performance metrics revealed interesting patterns that validate many of the principles discussed earlier in this chapter. The system achieved its highest accuracy on clearly written digits, similar to those in the training data. However, performance varied significantly with real-world factors. Lighting conditions affected pre-processing effectiveness. Unusual writing styles occasionally confused the neural network. Environmental vibrations could also impact image quality. These challenges led to continuous refinements in both the physical system and the neural network pipeline.</p>
<p>The economic impact proved substantial. Prior to automation, manual sorting required operators to read and key in ZIP codes at an average rate of one piece per second. The neural network system processed pieces at ten times this rate while reducing labor costs and error rates. However, the system didnâ€™t eliminate human operators entirely; their role shifted to handling uncertain cases and maintaining system performance. This hybrid approach, combining artificial and human intelligence, became a model for other automation projects.</p>
<p>The system also revealed important lessons about deploying neural networks in production environments. Training data quality proved crucial; the network performed best on digit styles well-represented in its training set. Regular retraining helped adapt to evolving handwriting styles. Maintenance required both hardware specialists and machine learning experts, introducing new operational considerations. These insights influenced subsequent deployments of neural networks in other industrial applications.</p>
<p>Perhaps most importantly, this implementation demonstrated how theoretical principles translate into practical constraints. The biological inspiration of neural networks provided the foundation for digit recognition, but successful deployment required careful consideration of system-level factors: processing speed, error handling, maintenance requirements, and integration with existing infrastructure. These lessons continue to inform modern machine learning deployments, where similar challenges of scale, reliability, and integration persist.</p>
</section>
<section id="sec-dl-primer-key-takeaways-64ec" class="level3">
<h3 class="anchored" data-anchor-id="sec-dl-primer-key-takeaways-64ec">Key Takeaways</h3>
<p>The USPS ZIP code recognition system is an excellent example of the journey from biological inspiration to practical neural network deployment that weâ€™ve explored throughout this chapter. It demonstrates how the basic principles of neural computation, from pre-processing through inference to post-processing, come together in solving real-world problems.</p>
<p>The systemâ€™s development shows why understanding both the theoretical foundations and practical considerations is crucial. While the biological visual system processes handwritten digits effortlessly, translating this capability into an artificial system required careful consideration of network architecture, training procedures, and system integration.</p>
<p>The success of this early large-scale neural network deployment helped establish many practices we now consider standard: the importance of comprehensive training data, the need for confidence metrics, the role of pre- and post-processing, and the critical nature of system-level optimization.</p>
<p>As we move forward to explore more complex architectures and applications in subsequent chapters, this case study reminds us that successful deployment requires mastery of both fundamental principles and practical engineering considerations.</p>
<div id="quiz-question-sec-dl-primer-case-study-usps-postal-service-aa64" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>What was a primary challenge in developing the USPS digit recognition system?</p>
<ol type="a">
<li>Implementing a cloud-based storage system</li>
<li>Creating a user-friendly interface for postal workers</li>
<li>Developing a proprietary hardware solution</li>
<li>Ensuring high-speed image capture and processing</li>
</ol></li>
<li><p>True or False: The USPS digit recognition system eliminated the need for human operators entirely.</p></li>
<li><p>Explain how data diversity was addressed in the USPS digit recognition system.</p></li>
<li><p>Order the following stages in the USPS mail processing pipeline: (1) Image Capture, (2) Neural Network Inference, (3) Pre-processing, (4) Post-processing.</p></li>
<li><p>In a production system like the USPS digit recognition system, what trade-offs must be considered when setting confidence thresholds for predictions?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-case-study-usps-postal-service-aa64" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dl-primer-summary-19d0" class="level2">
<h2 class="anchored" data-anchor-id="sec-dl-primer-summary-19d0">Summary</h2>
<p>In this chapter, we explored the foundational concepts of neural networks, bridging the gap between biological inspiration and artificial implementation. We began by examining the remarkable efficiency and adaptability of the human brain, uncovering how its principles influence the design of artificial neurons. From there, we delved into the behavior of a single artificial neuron, breaking down its components and operations. This understanding laid the groundwork for constructing neural networks, where layers of interconnected neurons collaborate to tackle increasingly complex tasks.</p>
<p>The progression from single neurons to network-wide behavior underscored the power of hierarchical learning, where each layer extracts and transforms patterns from raw data into meaningful abstractions. We examined both the learning process and the prediction phase, showing how neural networks first refine their performance through training and then deploy that knowledge through inference. The distinction between these phases revealed important system-level considerations for practical implementations.</p>
<p>Our exploration of the complete processing pipeline, from pre-processing through inference to post-processing, highlighted the hybrid nature of machine learning systems, where traditional computing and neural computation work together. The USPS case study demonstrated how these theoretical principles translate into practical applications, revealing both the power and complexity of deployed neural networks. These real-world considerations, from data collection to system integration, form an essential part of understanding machine learning systems.</p>
<p>In the next chapter, we will expand on these ideas, exploring sophisticated deep learning architectures such as convolutional and recurrent neural networks. These architectures are tailored to process diverse data types, from images and text to time series, enabling breakthroughs across a wide range of applications. By building on the concepts introduced here, we will gain a deeper appreciation for the design, capabilities, and versatility of modern deep learning systems.</p>


<div id="quiz-question-sec-dl-primer-summary-19d0" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the role of layers in a neural network?</p>
<ol type="a">
<li>Layers are used to store data during processing.</li>
<li>Layers transform raw data into meaningful abstractions.</li>
<li>Layers are used to reduce the computational load.</li>
<li>Layers serve as a backup for data in case of failure.</li>
</ol></li>
<li><p>Explain how the distinction between training and inference phases impacts the design of neural network systems.</p></li>
<li><p>The process of transforming raw data into meaningful abstractions in a neural network is known as ____. This process is crucial for hierarchical learning.</p></li>
<li><p>In a production system, what trade-offs might you consider when integrating traditional computing with neural computation?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dl-primer-summary-19d0" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-dl-primer-overview-9e60" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the relationship between AI, machine learning, and neural networks?</strong></p>
<ol type="a">
<li>AI is a subset of machine learning, which includes neural networks.</li>
<li>AI, machine learning, and neural networks are independent fields.</li>
<li>Neural networks are a subset of AI, which includes machine learning.</li>
<li>Machine learning is a subset of AI, and neural networks are a subset of machine learning.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Machine learning is a subset of AI, and neural networks are a subset of machine learning. This hierarchy shows how neural networks fit into the broader context of AI and machine learning.</p>
<p><em>Learning Objective</em>: Understand the hierarchical relationship between AI, machine learning, and neural networks.</p></li>
<li><p><strong>True or False: Neural networks require manual feature engineering to learn patterns from data.</strong></p>
<p><em>Answer</em>: False. Neural networks automatically discover representations through layers of interconnected units, eliminating the need for manual feature engineering.</p>
<p><em>Learning Objective</em>: Recognize the role of neural networks in automating feature learning.</p></li>
<li><p><strong>Explain how neural networks have shifted computational requirements in AI systems.</strong></p>
<p><em>Answer</em>: Neural networks have shifted computational requirements from simple, sequential operations to massively parallel computations. This shift necessitates advanced hardware like GPUs to efficiently process large models and datasets. For example, training a deep learning model on image data requires significant parallel processing power. This is important because it enables the scalability of neural networks to handle complex tasks.</p>
<p><em>Learning Objective</em>: Analyze the impact of neural networks on computational infrastructure in AI.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-overview-9e60" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-evolution-deep-learning-fb02" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a limitation of rule-based programming in AI systems?</strong></p>
<ol type="a">
<li>It requires explicit rules for every scenario, limiting adaptability.</li>
<li>It automatically learns from data without human intervention.</li>
<li>It scales efficiently with complex real-world tasks.</li>
<li>It uses deep neural networks to process inputs.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. It requires explicit rules for every scenario, limiting adaptability. Rule-based systems need predefined rules and struggle with complex tasks that require implicit knowledge.</p>
<p><em>Learning Objective</em>: Understand the limitations of rule-based programming in AI.</p></li>
<li><p><strong>Explain how deep learning differs from traditional programming in terms of system requirements.</strong></p>
<p><em>Answer</em>: Deep learning requires massive parallel operations on matrices and complex memory hierarchies, unlike traditional programming which follows sequential logic flows. This necessitates specialized hardware and efficient resource management to handle large datasets and computational demands. For example, deep learning models benefit from GPUs for parallel processing. This is important because it influences the design and deployment of modern AI systems.</p>
<p><em>Learning Objective</em>: Analyze the system implications of deep learning compared to traditional programming.</p></li>
<li><p><strong>Order the following AI methodologies from earliest to latest in terms of development: (1) Classical Machine Learning, (2) Rule-Based Programming, (3) Deep Learning.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Rule-Based Programming, (1) Classical Machine Learning, (3) Deep Learning. Rule-based programming was the earliest approach, followed by classical machine learning which introduced data-driven pattern discovery, and finally deep learning which automated feature extraction and scaled with data.</p>
<p><em>Learning Objective</em>: Understand the historical progression of AI methodologies.</p></li>
<li><p><strong>What is a key advantage of deep learning over traditional machine learning with engineered features?</strong></p>
<ol type="a">
<li>Deep learning requires less computational power.</li>
<li>Deep learning uses fixed resource requirements.</li>
<li>Deep learning is less adaptable to diverse data structures.</li>
<li>Deep learning eliminates the need for manual feature engineering.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Deep learning eliminates the need for manual feature engineering. It learns hierarchical representations directly from data, which allows it to adapt to diverse data structures and extract complex patterns automatically.</p>
<p><em>Learning Objective</em>: Identify the advantages of deep learning over traditional machine learning.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-evolution-deep-learning-fb02" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-biological-artificial-neurons-bb4f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which component of a biological neuron is analogous to the weights in an artificial neuron?</strong></p>
<ol type="a">
<li>Dendrites</li>
<li>Synapses</li>
<li>Soma</li>
<li>Axon</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Synapses. Synapses modulate the strength of connections between neurons, similar to how weights adjust the influence of inputs in artificial neurons.</p>
<p><em>Learning Objective</em>: Understand the mapping between biological and artificial neuron components.</p></li>
<li><p><strong>Explain how the energy efficiency of the human brain influences the design of AI hardware and algorithms.</strong></p>
<p><em>Answer</em>: The human brain operates on approximately 20 watts, inspiring AI systems to achieve similar efficiency. This influences the development of neuromorphic computing and specialized AI chips to mimic brain-like processing. For example, AI hardware aims to reduce power consumption while maintaining high performance. This is important because energy-efficient systems are crucial for deploying AI in resource-constrained environments.</p>
<p><em>Learning Objective</em>: Analyze the impact of biological energy efficiency on AI system design.</p></li>
<li><p><strong>Order the following biological components as they relate to their artificial counterparts: (1) Dendrites, (2) Soma, (3) Axon.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Dendrites, (2) Soma, (3) Axon. Dendrites correspond to inputs, receiving signals; the soma corresponds to net input, integrating signals; and the axon corresponds to output, transmitting signals.</p>
<p><em>Learning Objective</em>: Understand the sequence of information processing in biological and artificial neurons.</p></li>
<li><p><strong>What is a key system requirement for implementing artificial neural networks inspired by the brainâ€™s parallel processing?</strong></p>
<ol type="a">
<li>High-bandwidth memory access</li>
<li>Large-scale memory systems</li>
<li>Fast nonlinear operation units</li>
<li>Specialized parallel processors</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Specialized parallel processors. These are needed to mimic the brainâ€™s parallel processing capabilities, allowing efficient computation of neural network operations.</p>
<p><em>Learning Objective</em>: Identify system requirements for neural network implementation based on biological principles.</p></li>
<li><p><strong>In a production system, what trade-offs might you consider when choosing between mimicking biological structures versus abstracting biological principles for AI design?</strong></p>
<p><em>Answer</em>: When mimicking biological structures, systems may achieve more natural learning patterns but require complex hardware and higher energy consumption. Abstracting principles allows for simpler, more efficient designs but may lose some adaptability and learning capabilities. For example, neuromorphic chips aim to balance these trade-offs by providing efficient processing while maintaining some biological fidelity. This is important because it affects the scalability and application of AI systems in various environments.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in AI design influenced by biological systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-biological-artificial-neurons-bb4f" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-neural-network-fundamentals-68cd" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following activation functions is most commonly used in modern neural networks due to its simplicity and effectiveness?</strong></p>
<ol type="a">
<li>ReLU</li>
<li>Tanh</li>
<li>Sigmoid</li>
<li>Softmax</li>
</ol>
<p><em>Answer</em>: The correct answer is A. ReLU. This is correct because ReLU introduces sparsity and accelerates convergence, making it the default choice in many architectures. Sigmoid and Tanh are prone to vanishing gradients, while Softmax is typically used in output layers for classification.</p>
<p><em>Learning Objective</em>: Understand the role and advantages of different activation functions in neural networks.</p></li>
<li><p><strong>Explain how the organization of neurons into layers enhances the capability of neural networks to model complex patterns.</strong></p>
<p><em>Answer</em>: Organizing neurons into layers allows each layer to learn different features or patterns. The input layer receives raw data, hidden layers transform data through multiple stages, and the output layer produces the final decision. This hierarchical processing enables deep networks to build complex, abstract features, enhancing their capability to model intricate patterns. For example, in image recognition, initial layers might detect edges, while deeper layers recognize objects.</p>
<p><em>Learning Objective</em>: Understand the hierarchical structure of neural networks and its impact on learning capabilities.</p></li>
<li><p><strong>Order the following components of a neural network from input to output: (1) Hidden Layers, (2) Output Layer, (3) Input Layer</strong></p>
<p><em>Answer</em>: The correct order is: (3) Input Layer, (1) Hidden Layers, (2) Output Layer. The input layer receives raw data, hidden layers process and transform the data, and the output layer produces the final prediction or decision. This sequence is crucial for the flow of information and learning in neural networks.</p>
<p><em>Learning Objective</em>: Understand the flow of data through different layers in a neural network.</p></li>
<li><p><strong>What is the primary advantage of using bias terms in neural networks?</strong></p>
<ol type="a">
<li>To increase the number of parameters</li>
<li>To reduce the need for activation functions</li>
<li>To enhance computational efficiency</li>
<li>To allow neurons to shift activation thresholds</li>
</ol>
<p><em>Answer</em>: The correct answer is D. To allow neurons to shift activation thresholds. Bias terms enable neurons to adjust their activation thresholds, providing flexibility to fit complex patterns. This is important for learning diverse features and improving model expressiveness.</p>
<p><em>Learning Objective</em>: Understand the role of bias terms in neural networks and their impact on learning.</p></li>
<li><p><strong>In a production system, what trade-offs would you consider when designing the topology of a neural network for a specific task?</strong></p>
<p><em>Answer</em>: When designing a neural network topology, consider trade-offs between learning capacity and computational efficiency. More layers (depth) allow for higher abstraction but increase computational cost and training complexity. Wider layers (width) can process more features but require more parameters and memory. The choice of connection patterns (dense vs.&nbsp;sparse) impacts computational efficiency and learning flexibility. For example, in image recognition, using convolutional layers can reduce parameters and focus on local patterns, balancing performance and efficiency.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs involved in designing neural network topologies for specific tasks.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-neural-network-fundamentals-68cd" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-learning-process-38a0" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>In the context of neural network training, what is the primary role of the loss function?</strong></p>
<ol type="a">
<li>To initialize network weights</li>
<li>To measure prediction accuracy</li>
<li>To update network architecture</li>
<li>To compute weight adjustments</li>
</ol>
<p><em>Answer</em>: The correct answer is B. To measure prediction accuracy. The loss function quantifies the difference between predicted and true values, guiding the training process by indicating how much the networkâ€™s predictions deviate from the actual outcomes.</p>
<p><em>Learning Objective</em>: Understand the role of loss functions in neural network training.</p></li>
<li><p><strong>Explain how batch processing in neural network training enhances computational efficiency.</strong></p>
<p><em>Answer</em>: Batch processing enhances computational efficiency by enabling parallel processing of multiple examples, which optimizes hardware utilization and stabilizes parameter updates through error averaging. For example, processing a batch of 64 images simultaneously allows for efficient matrix operations, reducing computational overhead compared to individual processing. This is important because it balances memory usage with processing speed, crucial for large-scale training.</p>
<p><em>Learning Objective</em>: Analyze the advantages of batch processing in neural network training.</p></li>
<li><p><strong>The process of adjusting neural network weights based on prediction errors is known as ____. This process uses the gradients of the loss function to update weights and improve predictions.</strong></p>
<p><em>Answer</em>: backpropagation. This process uses the gradients of the loss function to update weights and improve predictions.</p>
<p><em>Learning Objective</em>: Recall the term for the weight adjustment process in neural networks.</p></li>
<li><p><strong>Which of the following best describes the impact of using a larger batch size during training?</strong></p>
<ol type="a">
<li>Increases computational efficiency but requires more memory</li>
<li>Decreases computational efficiency and requires less memory</li>
<li>Increases computational efficiency and requires less memory</li>
<li>Decreases computational efficiency but requires more memory</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Increases computational efficiency but requires more memory. Larger batch sizes allow for more efficient matrix operations but also increase memory requirements due to the need to store more intermediate activations and gradients.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs of batch size in neural network training.</p></li>
<li><p><strong>In a production system, how might you decide on an appropriate batch size for training a neural network?</strong></p>
<p><em>Answer</em>: Choosing an appropriate batch size involves balancing computational efficiency, memory availability, and hardware capabilities. Larger batch sizes improve efficiency but require more memory, which might not be feasible on all hardware. For example, on a GPU with limited memory, a smaller batch size might be necessary to avoid memory overflow. This decision is important because it affects both training speed and model performance.</p>
<p><em>Learning Objective</em>: Apply knowledge of batch processing to real-world system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-learning-process-38a0" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-prediction-phase-4204" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary computational difference between the training and inference phases of a neural network?</strong></p>
<ol type="a">
<li>Training involves only forward passes, while inference involves both forward and backward passes.</li>
<li>Inference involves only forward passes, while training involves both forward and backward passes.</li>
<li>Training uses fixed parameters, while inference updates parameters continuously.</li>
<li>Inference requires more memory and computational resources than training.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Inference involves only forward passes, while training involves both forward and backward passes. Training requires backpropagation to update weights, which is not needed during inference.</p>
<p><em>Learning Objective</em>: Understand the computational flow differences between training and inference phases.</p></li>
<li><p><strong>Explain why inference can be optimized for deployment on resource-constrained devices.</strong></p>
<p><em>Answer</em>: Inference can be optimized for resource-constrained devices because it requires only the forward pass, reducing memory and computational needs. Fixed parameters allow for optimizations like quantization, and memory can be managed efficiently by releasing intermediate states after use. This enables deployment on devices like mobile phones and edge devices.</p>
<p><em>Learning Objective</em>: Analyze how inference optimizations enable deployment on various hardware.</p></li>
<li><p><strong>Order the following stages of the inference pipeline: (1) Pre-processing, (2) Neural Network Computation, (3) Post-processing.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Pre-processing, (2) Neural Network Computation, (3) Post-processing. Pre-processing prepares raw inputs for the neural network, which then performs learned transformations. Post-processing converts raw outputs into actionable results.</p>
<p><em>Learning Objective</em>: Understand the sequence of stages in the inference pipeline.</p></li>
<li><p><strong>Which of the following optimizations is typically applied during inference but not during training?</strong></p>
<ol type="a">
<li>Gradient clipping</li>
<li>Batch normalization</li>
<li>Dropout</li>
<li>Weight quantization</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Weight quantization. During inference, parameters are fixed, allowing for optimizations like quantization that reduce memory usage and improve computational efficiency.</p>
<p><em>Learning Objective</em>: Identify optimizations specific to the inference phase.</p></li>
<li><p><strong>In a production system, what trade-offs might you consider when choosing between single input processing and batch processing during inference?</strong></p>
<p><em>Answer</em>: Single input processing minimizes latency, crucial for real-time applications, while batch processing improves throughput by utilizing parallel computing capabilities. The trade-off involves balancing response time with system efficiency. For example, a real-time application like speech recognition may prioritize single input processing, whereas an image classification task may benefit from batch processing to maximize hardware utilization.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in inference processing strategies for different applications.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-prediction-phase-4204" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-case-study-usps-postal-service-aa64" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>What was a primary challenge in developing the USPS digit recognition system?</strong></p>
<ol type="a">
<li>Implementing a cloud-based storage system</li>
<li>Creating a user-friendly interface for postal workers</li>
<li>Developing a proprietary hardware solution</li>
<li>Ensuring high-speed image capture and processing</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Ensuring high-speed image capture and processing. This is correct because the system needed to process images of handwritten digits quickly to maintain mail processing speeds. Options B, C, and D were not primary challenges discussed in the context of the USPS system.</p>
<p><em>Learning Objective</em>: Understand the technical challenges faced in deploying neural networks for real-world applications.</p></li>
<li><p><strong>True or False: The USPS digit recognition system eliminated the need for human operators entirely.</strong></p>
<p><em>Answer</em>: False. This is false because the system still required human operators to handle uncertain cases and maintain system performance, illustrating a hybrid approach combining artificial and human intelligence.</p>
<p><em>Learning Objective</em>: Recognize the role of human oversight in automated systems.</p></li>
<li><p><strong>Explain how data diversity was addressed in the USPS digit recognition system.</strong></p>
<p><em>Answer</em>: The USPS system addressed data diversity by collecting a wide variety of training samples, including digits from different ages, educational backgrounds, and writing styles, as well as varying envelope colors and textures. This comprehensive dataset helped the system generalize better to real-world conditions. For example, the inclusion of diverse handwriting styles ensured the model could handle different variations in digit formation. This is important because it enhances the modelâ€™s robustness and accuracy in practical applications.</p>
<p><em>Learning Objective</em>: Analyze how data diversity impacts the training and performance of neural networks in real-world scenarios.</p></li>
<li><p><strong>Order the following stages in the USPS mail processing pipeline: (1) Image Capture, (2) Neural Network Inference, (3) Pre-processing, (4) Post-processing.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Image Capture, (3) Pre-processing, (2) Neural Network Inference, (4) Post-processing. This order reflects the sequence from capturing the image of the mail piece, preparing it for analysis, running it through the neural network, and finally making sorting decisions based on the networkâ€™s output.</p>
<p><em>Learning Objective</em>: Understand the sequential stages of a neural network-based processing pipeline in a real-world system.</p></li>
<li><p><strong>In a production system like the USPS digit recognition system, what trade-offs must be considered when setting confidence thresholds for predictions?</strong></p>
<p><em>Answer</em>: Setting confidence thresholds involves balancing the trade-off between automation rate and error rate. High thresholds may result in too many pieces being flagged for human review, reducing automation benefits. Low thresholds could increase misrouting errors. The USPS system analyzed confidence distributions to optimize these thresholds, ensuring efficient operation while maintaining acceptable accuracy levels. This is important because it directly impacts the systemâ€™s operational efficiency and reliability.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs involved in setting operational parameters in automated systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-case-study-usps-postal-service-aa64" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dl-primer-summary-19d0" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the role of layers in a neural network?</strong></p>
<ol type="a">
<li>Layers are used to store data during processing.</li>
<li>Layers transform raw data into meaningful abstractions.</li>
<li>Layers are used to reduce the computational load.</li>
<li>Layers serve as a backup for data in case of failure.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Layers transform raw data into meaningful abstractions. This is correct because each layer in a neural network extracts and processes patterns, contributing to hierarchical learning. Options A, C, and D do not accurately describe the primary function of layers in neural networks.</p>
<p><em>Learning Objective</em>: Understand the hierarchical learning process facilitated by layers in neural networks.</p></li>
<li><p><strong>Explain how the distinction between training and inference phases impacts the design of neural network systems.</strong></p>
<p><em>Answer</em>: The distinction between training and inference phases impacts system design by requiring different computational resources and optimizations. Training involves adjusting weights and requires more computational power, while inference focuses on deploying learned models efficiently. This distinction is important because it influences hardware choices and system architecture, ensuring that systems are optimized for specific tasks.</p>
<p><em>Learning Objective</em>: Analyze how the different phases of neural network operation influence system design and resource allocation.</p></li>
<li><p><strong>The process of transforming raw data into meaningful abstractions in a neural network is known as ____. This process is crucial for hierarchical learning.</strong></p>
<p><em>Answer</em>: feature extraction. This process is crucial for hierarchical learning as it allows each layer to build on the patterns identified by previous layers.</p>
<p><em>Learning Objective</em>: Recall the process by which neural networks transform data into higher-level abstractions.</p></li>
<li><p><strong>In a production system, what trade-offs might you consider when integrating traditional computing with neural computation?</strong></p>
<p><em>Answer</em>: When integrating traditional computing with neural computation, trade-offs include balancing computational efficiency with flexibility, managing data flow between systems, and ensuring compatibility between different processing units. For example, a system might need to optimize for speed in neural computation while maintaining accuracy in traditional processing. This is important because achieving the right balance can enhance overall system performance and reliability.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs involved in integrating different computational paradigms within a machine learning system.</p></li>
</ol>
<p><a href="#quiz-question-sec-dl-primer-summary-19d0" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ml_systems/ml_systems.html" class="pagination-link" aria-label="ML Systems">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">ML Systems</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="pagination-link" aria-label="DNN Architectures">
        <span class="nav-page-text">DNN Architectures</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>