<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="next">
<link href="../../../contents/core/introduction/introduction.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
</style>
<style>
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
</style>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ml_systems/ml_systems.html">ML Systems</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>ðŸš§ DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">e2acb2a4</code> â€¢ 2025-10-02 19:58 UTC â€¢ <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version â†’</em></a></p>
<p>ðŸŽ‰ <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news â†’</a><br></p>
<p>ðŸš€ <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">TinyðŸ”¥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>ðŸ§  <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>ðŸ“¦ <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-ml-systems" id="toc-sec-ml-systems" class="nav-link active" data-scroll-target="#sec-ml-systems">ML Systems</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ml-systems-overview-db10" id="toc-sec-ml-systems-overview-db10" class="nav-link" data-scroll-target="#sec-ml-systems-overview-db10">Overview</a>
  <ul class="collapse">
  <li><a href="#fundamental-constraints" id="toc-fundamental-constraints" class="nav-link" data-scroll-target="#fundamental-constraints">Fundamental Constraints</a></li>
  <li><a href="#deployment-paradigm-evolution" id="toc-deployment-paradigm-evolution" class="nav-link" data-scroll-target="#deployment-paradigm-evolution">Deployment Paradigm Evolution</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-cloudbased-machine-learning-7606" id="toc-sec-ml-systems-cloudbased-machine-learning-7606" class="nav-link" data-scroll-target="#sec-ml-systems-cloudbased-machine-learning-7606">Cloud-Based Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-b564" id="toc-sec-ml-systems-characteristics-b564" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-b564">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-e12c" id="toc-sec-ml-systems-benefits-e12c" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-e12c">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-e73b" id="toc-sec-ml-systems-challenges-e73b" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-e73b">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-348c" id="toc-sec-ml-systems-use-cases-348c" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-348c">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-edge-machine-learning-06ec" id="toc-sec-ml-systems-edge-machine-learning-06ec" class="nav-link" data-scroll-target="#sec-ml-systems-edge-machine-learning-06ec">Edge Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-09e1" id="toc-sec-ml-systems-characteristics-09e1" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-09e1">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-4fb7" id="toc-sec-ml-systems-benefits-4fb7" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-4fb7">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-2714" id="toc-sec-ml-systems-challenges-2714" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-2714">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-05eb" id="toc-sec-ml-systems-use-cases-05eb" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-05eb">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-mobile-machine-learning-f5b5" id="toc-sec-ml-systems-mobile-machine-learning-f5b5" class="nav-link" data-scroll-target="#sec-ml-systems-mobile-machine-learning-f5b5">Mobile Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-9792" id="toc-sec-ml-systems-characteristics-9792" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-9792">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-99f9" id="toc-sec-ml-systems-benefits-99f9" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-99f9">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-aa62" id="toc-sec-ml-systems-challenges-aa62" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-aa62">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-c808" id="toc-sec-ml-systems-use-cases-c808" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-c808">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-tiny-machine-learning-9d4a" id="toc-sec-ml-systems-tiny-machine-learning-9d4a" class="nav-link" data-scroll-target="#sec-ml-systems-tiny-machine-learning-9d4a">Tiny Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-d52d" id="toc-sec-ml-systems-characteristics-d52d" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-d52d">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-020f" id="toc-sec-ml-systems-benefits-020f" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-020f">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-297b" id="toc-sec-ml-systems-challenges-297b" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-297b">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-3c3f" id="toc-sec-ml-systems-use-cases-3c3f" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-3c3f">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-hybrid-machine-learning-1bbf" id="toc-sec-ml-systems-hybrid-machine-learning-1bbf" class="nav-link" data-scroll-target="#sec-ml-systems-hybrid-machine-learning-1bbf">Hybrid Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-design-patterns-ade8" id="toc-sec-ml-systems-design-patterns-ade8" class="nav-link" data-scroll-target="#sec-ml-systems-design-patterns-ade8">Design Patterns</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-trainserve-split-0d17" id="toc-sec-ml-systems-trainserve-split-0d17" class="nav-link" data-scroll-target="#sec-ml-systems-trainserve-split-0d17">Train-Serve Split</a></li>
  <li><a href="#sec-ml-systems-hierarchical-processing-6114" id="toc-sec-ml-systems-hierarchical-processing-6114" class="nav-link" data-scroll-target="#sec-ml-systems-hierarchical-processing-6114">Hierarchical Processing</a></li>
  <li><a href="#sec-ml-systems-progressive-deployment-2570" id="toc-sec-ml-systems-progressive-deployment-2570" class="nav-link" data-scroll-target="#sec-ml-systems-progressive-deployment-2570">Progressive Deployment</a></li>
  <li><a href="#sec-ml-systems-federated-learning-bf9c" id="toc-sec-ml-systems-federated-learning-bf9c" class="nav-link" data-scroll-target="#sec-ml-systems-federated-learning-bf9c">Federated Learning</a></li>
  <li><a href="#sec-ml-systems-collaborative-learning-7f59" id="toc-sec-ml-systems-collaborative-learning-7f59" class="nav-link" data-scroll-target="#sec-ml-systems-collaborative-learning-7f59">Collaborative Learning</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-realworld-integration-0815" id="toc-sec-ml-systems-realworld-integration-0815" class="nav-link" data-scroll-target="#sec-ml-systems-realworld-integration-0815">Real-World Integration</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-shared-principles-34fe" id="toc-sec-ml-systems-shared-principles-34fe" class="nav-link" data-scroll-target="#sec-ml-systems-shared-principles-34fe">Shared Principles</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-implementation-layer-9002" id="toc-sec-ml-systems-implementation-layer-9002" class="nav-link" data-scroll-target="#sec-ml-systems-implementation-layer-9002">Implementation Layer</a></li>
  <li><a href="#sec-ml-systems-system-principles-layer-db81" id="toc-sec-ml-systems-system-principles-layer-db81" class="nav-link" data-scroll-target="#sec-ml-systems-system-principles-layer-db81">System Principles Layer</a></li>
  <li><a href="#sec-ml-systems-system-considerations-layer-660c" id="toc-sec-ml-systems-system-considerations-layer-660c" class="nav-link" data-scroll-target="#sec-ml-systems-system-considerations-layer-660c">System Considerations Layer</a></li>
  <li><a href="#sec-ml-systems-principles-practice-907d" id="toc-sec-ml-systems-principles-practice-907d" class="nav-link" data-scroll-target="#sec-ml-systems-principles-practice-907d">Principles to Practice</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-system-comparison-8b05" id="toc-sec-ml-systems-system-comparison-8b05" class="nav-link" data-scroll-target="#sec-ml-systems-system-comparison-8b05">System Comparison</a></li>
  <li><a href="#sec-ml-systems-deployment-decision-framework-824f" id="toc-sec-ml-systems-deployment-decision-framework-824f" class="nav-link" data-scroll-target="#sec-ml-systems-deployment-decision-framework-824f">Deployment Decision Framework</a></li>
  <li><a href="#sec-ml-systems-metrics-framework" id="toc-sec-ml-systems-metrics-framework" class="nav-link" data-scroll-target="#sec-ml-systems-metrics-framework">Reference Metrics Framework</a>
  <ul class="collapse">
  <li><a href="#memory-hierarchy-performance" id="toc-memory-hierarchy-performance" class="nav-link" data-scroll-target="#memory-hierarchy-performance">Memory Hierarchy Performance</a></li>
  <li><a href="#energy-efficiency-hierarchy" id="toc-energy-efficiency-hierarchy" class="nav-link" data-scroll-target="#energy-efficiency-hierarchy">Energy Efficiency Hierarchy</a></li>
  <li><a href="#latency-classes" id="toc-latency-classes" class="nav-link" data-scroll-target="#latency-classes">Latency Classes</a></li>
  <li><a href="#communication-constraints" id="toc-communication-constraints" class="nav-link" data-scroll-target="#communication-constraints">Communication Constraints</a></li>
  </ul></li>
  <li><a href="#fallacies-and-pitfalls" id="toc-fallacies-and-pitfalls" class="nav-link" data-scroll-target="#fallacies-and-pitfalls">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-ml-systems-summary-473b" id="toc-sec-ml-systems-summary-473b" class="nav-link" data-scroll-target="#sec-ml-systems-summary-473b">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ml_systems/ml_systems.html">ML Systems</a></li></ol></nav></header>




<section id="sec-ml-systems" class="level1 page-columns page-full">
<h1>ML Systems</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALLÂ·E 3 Prompt: Illustration in a rectangular format depicting the merger of embedded systems with Embedded AI. The left half of the image portrays traditional embedded systems, including microcontrollers and processors, detailed and precise. The right half showcases the world of artificial intelligence, with abstract representations of machine learning models, neurons, and data flow. The two halves are distinctly separated, emphasizing the individual significance of embedded tech and AI, but they come together in harmony at the center.</em></p>
</div></div><p> <img src="images/png/cover_ml_systems.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How do the diverse environments where machine learning operates shape the nature of these systems, and what drives their widespread deployment across computing platforms?</em></p>
<p>Machine learning algorithms must adapt to different computational environments, each imposing distinct constraints and opportunities. Cloud deployments use massive computational resources but face network latency concerns, while mobile devices offer user proximity but operate under severe power limitations. Embedded systems minimize latency through local processing but constrain model complexity, and tiny devices enable widespread sensing while restricting memory usage. These deployment contexts directly determine system architecture, algorithmic choices, and performance trade-offs. Understanding these environment-specific requirements establishes the foundation for engineering decisions in machine learning systems.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Classify ML deployment paradigms based on computational resources, power constraints, and latency requirements</p></li>
<li><p>Compare architectural trade-offs between centralized cloud processing and distributed edge computing</p></li>
<li><p>Analyze resource constraints and their impact on model selection and deployment decisions</p></li>
<li><p>Evaluate real-world applications to determine the most appropriate ML deployment paradigm</p></li>
<li><p>Design system architectures that balance performance, efficiency, and practicality across deployment contexts</p></li>
<li><p>Assess emerging trends in ML systems and predict their influence on future deployment strategies</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-ml-systems-overview-db10" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-overview-db10">Overview</h2>
<p>Building on the framework of data, algorithms, and infrastructure working as an integrated system from <strong><a href="../core/introduction/introduction.html#sec-introduction">Chapter 1: Introduction</a></strong>, this chapter examines how deployment environments<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> shape ML system architecture. The three components manifest differently across the deployment spectrum: cloud systems prioritize algorithmic sophistication with abundant infrastructure, while embedded systems<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> must optimize algorithms for minimal infrastructure, and edge systems balance both concerns while managing distributed data challenges.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Deployment Environments</strong>: The physical and logical contexts where ML systems operate, from hyperscale data centers consuming megawatts to coin-cell powered sensors running for years. Each environment imposes distinct constraints that determine what models can run and how they must be optimized.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Embedded Systems</strong>: Purpose-built computer systems integrated into larger devices, typically with real-time constraints and limited resources. Unlike general-purpose computers, embedded systems optimize for specific tasks. Automotive ECUs manage engine timing within microseconds, while smart thermostats operate for years on batteries.</p></div></div><p>Understanding how deployment environments shape ML systems requires establishing a systematic framework for categorizing these environments. The deployment spectrum represents more than different scales of computation, reflecting core trade-offs between computational resources, latency requirements, privacy constraints, and operational costs that drive all subsequent design decisions.</p>
<p>Modern machine learning systems span this deployment spectrum through four primary paradigms, each addressing specific combinations of these trade-offs. Cloud ML uses massive centralized computing resources in data centers<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> when computational power outweighs latency concerns. Edge ML brings computation closer to data sources when low latency<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal devices<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> when user proximity and offline operation become priorities. Tiny ML enables widespread intelligence on severely constrained devices when power efficiency and cost matter more than computational complexity.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Data Centers</strong>: Modern hyperscale data centers can house hundreds of thousands of servers and consume 20-50 megawatts of power, equivalent to a small city. Googleâ€™s data centers alone process over 189,000 searches per second globally as of 2025.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Latency vs Throughput</strong>: Latency measures the time delay between input and output (critical for real-time applications), while throughput measures predictions processed per unit time (important for batch processing).</p></div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Mobile Power Constraints</strong>: Modern smartphones contain 5000-6000mAh batteries (~18-22Wh) but ML inference can consume 1-5W, reducing battery life significantly. Appleâ€™s Neural Engine and Googleâ€™s Tensor chips were specifically designed to perform AI tasks at &lt;1W power consumption.</p></div></div><p>These paradigms represent systematic responses to four critical decision factors that determine deployment strategy: <strong>privacy requirements</strong> (can data leave local environment?), <strong>latency constraints</strong> (how quickly must the system respond?), <strong>computational resources</strong> (what processing power is available?), and <strong>operational costs</strong> (what are the budget and energy constraints?). The interplay between these factors creates natural boundaries that define when each paradigm excels.</p>
<p>Understanding the specific implications of these decision factors determines the deployment approach for each application. These paradigms function as complementary solutions that excel in different combinations of requirements rather than competing technologies. This systematic understanding forms the foundation for examining how each paradigm addresses specific trade-offs and when hybrid approaches become necessary.</p>
<p><a href="#fig-cloud-edge-TinyML-comparison" class="quarto-xref">Figure&nbsp;1</a> provides a visual overview of how computational resources, latency requirements, and deployment constraints create the deployment spectrum. The following sections examine each paradigm systematically: Cloud ML (<a href="#sec-ml-systems-cloudbased-machine-learning-7606" class="quarto-xref">Section&nbsp;1.2</a>), Edge ML (<a href="#sec-ml-systems-edge-machine-learning-06ec" class="quarto-xref">Section&nbsp;1.3</a>), Mobile ML (<a href="#sec-ml-systems-mobile-machine-learning-f5b5" class="quarto-xref">Section&nbsp;1.4</a>), and Tiny ML (<a href="#sec-ml-systems-tiny-machine-learning-9d4a" class="quarto-xref">Section&nbsp;1.5</a>). The figure shows how these four paradigms occupy distinct positions along multiple dimensions, reflecting the trade-offs that drive deployment decisions.</p>
<div id="fig-cloud-edge-TinyML-comparison" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cloud-edge-TinyML-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="7fd54a49357db81bbdcf6a24cd73c93a78028044.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Distributed Intelligence Spectrum: Machine learning system design involves trade-offs between computational resources, latency, and connectivity, resulting in a spectrum of deployment options ranging from centralized cloud infrastructure to resource-constrained edge and TinyML devices. This figure maps these options, highlighting how each approach balances processing location with device capability and network dependence. Source: [@abiresearch2024tinyml]."><img src="ml_systems_files/mediabag/7fd54a49357db81bbdcf6a24cd73c93a78028044.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloud-edge-TinyML-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Distributed Intelligence Spectrum</strong>: Machine learning system design involves trade-offs between computational resources, latency, and connectivity, resulting in a spectrum of deployment options ranging from centralized cloud infrastructure to resource-constrained edge and TinyML devices. This figure maps these options, highlighting how each approach balances processing location with device capability and network dependence. Source: <span class="citation" data-cites="abiresearch2024tinyml">(<a href="#ref-abiresearch2024tinyml" role="doc-biblioref">Research 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>To understand the differences between these ML deployment options, <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a> provides examples of hardware platforms for each category. These examples show the range of computational resources, power requirements, and cost considerations<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> across the ML systems spectrum. These concrete examples demonstrate the practical implications of each approach.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>ML Hardware Cost Spectrum</strong>: The cost range spans 6 orders of magnitude, from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Power Usage Effectiveness (PUE)</strong>: Data center efficiency metric measuring total facility power divided by IT equipment power. A PUE of 1.0 represents perfect efficiency (impossible in practice), while 1.1-1.3 indicates highly efficient facilities using advanced cooling and power management. Googleâ€™s data centers achieve PUE of 1.12 compared to industry average of 1.8.</p></div></div><div id="tbl-representative-systems" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-representative-systems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Hardware Spectrum</strong>: Machine learning system design necessitates trade-offs between computational resources, power consumption, and cost, as exemplified by the diverse hardware platforms suitable for cloud, edge, mobile, and TinyML deployments. This table quantifies those trade-offs, revealing how device capabilities, from high-end GPUs in cloud servers to low-power microcontrollers in embedded systems, shape the types of models and tasks each platform can effectively support. The quantitative thresholds provide specific decision criteria to help practitioners determine the most appropriate deployment paradigm for their applications. Source: <span class="citation" data-cites="abiresearch2024tinyml">(<a href="#ref-abiresearch2024tinyml" role="doc-biblioref">Research 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-abiresearch2024tinyml" class="csl-entry" role="listitem">
Research, ABI. 2024. <span>â€œTinyML Market Trends and Device Analysis.â€</span> Market Research Report. ABI Research. <a href="https://www.abiresearch.com/market-research/product/1050167/">https://www.abiresearch.com/market-research/product/1050167/</a>.
</div></div><div aria-describedby="tbl-representative-systems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 10%">
<col style="width: 17%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 14%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Example Device</th>
<th style="text-align: left;">Processor</th>
<th style="text-align: left;">Memory</th>
<th style="text-align: left;">Storage</th>
<th style="text-align: left;">Power</th>
<th style="text-align: left;">Price Range</th>
<th style="text-align: left;">Example Models/Tasks</th>
<th style="text-align: left;">Quantitative Thresholds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cloud ML</td>
<td style="text-align: left;">NVIDIA DGX A100</td>
<td style="text-align: left;">8x NVIDIA A100 GPUs (40GB or 80GB per GPU)</td>
<td style="text-align: left;">1 TB System RAM</td>
<td style="text-align: left;">15 TB NVMe SSD</td>
<td style="text-align: left;">6.5 kW</td>
<td style="text-align: left;">$200 K+</td>
<td style="text-align: left;">Large language models,</td>
<td style="text-align: left;"><blockquote class="blockquote">
<p>1000 TFLOPS compute, real-time video processing | &gt;100GB/s memory bandwidth, PUE 1.1-1.3, 100-500ms latency</p>
</blockquote></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Google TPU v4 Pod</td>
<td style="text-align: left;">4096 TPU v4 chips</td>
<td style="text-align: left;">128 TB+</td>
<td style="text-align: left;">Networked storage</td>
<td style="text-align: left;">~1-2 MW</td>
<td style="text-align: left;">Pay-per-use</td>
<td style="text-align: left;">Training foundation models, large-scale ML research</td>
<td style="text-align: left;"><blockquote class="blockquote">
<p>1000 TFLOPS compute, 100GB/s memory bandwidth, PUE 1.1-1.3, 100-500ms latency</p>
</blockquote></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Edge ML</td>
<td style="text-align: left;">NVIDIA Jetson AGX Orin</td>
<td style="text-align: left;">12-core ArmÂ® CortexÂ®-A78AE, NVIDIA Ampere GPU</td>
<td style="text-align: left;">32 GB LPDDR5</td>
<td style="text-align: left;">64GB eMMC</td>
<td style="text-align: left;">15-60 W</td>
<td style="text-align: left;">$899</td>
<td style="text-align: left;">Computer vision, robotics, autonomous systems</td>
<td style="text-align: left;">1-100 TOPS compute, &lt;10W sustained power, &lt;100ms latency requirements</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Intel NUC 12 Pro</td>
<td style="text-align: left;">Intel Core i7-1260P, Intel Iris Xe</td>
<td style="text-align: left;">32 GB DDR4</td>
<td style="text-align: left;">1 TB SSD</td>
<td style="text-align: left;">28 W</td>
<td style="text-align: left;">$750</td>
<td style="text-align: left;">Edge AI servers, industrial automation</td>
<td style="text-align: left;">1-100 TOPS compute, &lt;10W sustained power, &lt;100ms latency requirements</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mobile ML</td>
<td style="text-align: left;">iPhone 15 Pro</td>
<td style="text-align: left;">A17 Pro (6-core CPU, 6-core GPU)</td>
<td style="text-align: left;">8 GB RAM</td>
<td style="text-align: left;">128 GB-1 TB</td>
<td style="text-align: left;">3-5 W</td>
<td style="text-align: left;">$999+</td>
<td style="text-align: left;">Face ID, computational photography, voice recognition</td>
<td style="text-align: left;">1-10 TOPS compute, &lt;2W sustained power, &lt;50ms UI response</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tiny ML</td>
<td style="text-align: left;">Arduino Nano 33 BLE Sense</td>
<td style="text-align: left;">Arm Cortex-M4 @ 64 MHz</td>
<td style="text-align: left;">256 KB RAM</td>
<td style="text-align: left;">1 MB Flash</td>
<td style="text-align: left;">0.02-0.04 W</td>
<td style="text-align: left;">$35</td>
<td style="text-align: left;">Gesture recognition, voice detection</td>
<td style="text-align: left;">&lt;1 TOPS compute, &lt;1mW power, microsecond response times</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">ESP32-CAM</td>
<td style="text-align: left;">Dual-core @ 240MHz</td>
<td style="text-align: left;">520 KB RAM</td>
<td style="text-align: left;">4 MB Flash</td>
<td style="text-align: left;">0.05-0.25 W</td>
<td style="text-align: left;">$10</td>
<td style="text-align: left;">Image classification, motion detection</td>
<td style="text-align: left;">&lt;1 TOPS compute, &lt;1mW power, microsecond response times</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These quantitative thresholds reflect essential relationships between computational requirements, energy consumption, and deployment feasibility. These scaling relationships determine when distributed cloud deployment becomes advantageous versus edge or mobile alternatives. Later chapters covering AI efficiency (<strong><a href="../core/efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 10: Efficient AI</a></strong>), training (<strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>), and optimization (<strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) explore these relationships in detail. Systematic performance measurement and benchmarking methodologies for evaluating these trade-offs across deployment contexts are comprehensively covered in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
<section id="fundamental-constraints" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="fundamental-constraints">Fundamental Constraints</h3>
<p>The diverse deployment paradigms in ML systems exist not by design preference, but out of necessity driven by immutable physical and hardware constraints. These core limitations create hard boundaries that determine what is computationally feasible across different deployment contexts.</p>
<p>The most critical bottleneck in modern computing stems from memory bandwidth scaling differently than computational capacity. While compute power can scale linearly by adding more processing units, memory bandwidth scales approximately as the square root of chip area due to physical routing constraints. This creates an increasingly severe bottleneck where processors become starved for data. In practice, this manifests as ML models spending more time waiting for memory transfers than performing calculations, particularly problematic for large models<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> that require more data than can be efficiently transferred. This constraint drives the need for specialized architectures like TPUs with high-bandwidth memory and forces deployment decisions based on memory hierarchy optimization.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Memory Bottleneck</strong>: When the rate of data transfer from memory to processor becomes the limiting factor in computation. Large models require so many parameters that memory bandwidth, rather than computational capacity, determines performance.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Dennard Scaling</strong>: Named after Robert Dennard (IBM, 1974), the observation that as transistors became smaller, they could operate at higher frequencies while consuming the same power density. This scaling enabled Mooreâ€™s Law until 2005, when physics limitations forced the industry toward multi-core architectures and specialized processors like GPUs and TPUs.</p></div></div><p>Compounding these memory challenges, Dennard scaling<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> breakdown changed computing constraints around 2005, when transistor shrinking stopped reducing power density. Power dissipation per unit area now remains constant or increases with each technology generation, creating hard limits on computational density. For mobile devices, this translates to thermal throttling that reduces performance when sustained computation generates excessive heat. Data centers face similar constraints at scale, requiring extensive cooling infrastructure that can consume 30-40% of total power budget. These power density limits directly drive the need for specialized low-power architectures in mobile and embedded contexts, and explain why edge deployment becomes necessary when power budgets are constrained.</p>
<p>Beyond power considerations, physical limits impose minimum latencies that no engineering optimization can overcome. The speed of light creates an inherent 80ms round-trip time between California and Virginia, while internet routing, DNS resolution, and processing overhead typically add another 20-420ms. This 100-500ms total latency makes real-time applications impossible with pure cloud deployment. Network bandwidth also faces physical constraints: fiber optic cables have theoretical limits, and wireless communication is bounded by spectrum availability and signal propagation physics. These communication constraints create hard boundaries that force local processing for latency-sensitive applications and drive edge deployment decisions.</p>
<p>Finally, heat dissipation becomes the limiting factor as computational density increases. Mobile devices must throttle performance to prevent component damage and maintain user comfort, while data centers require massive cooling systems that limit placement options and increase operational costs. Thermal constraints create cascading effects: higher temperatures reduce semiconductor reliability, increase error rates, and accelerate component aging. These thermal realities force trade-offs between computational performance and sustainable operation, driving specialized cooling solutions in cloud environments and ultra-low-power designs in embedded systems.</p>
<p>These constraints create deployment paradigm boundaries that represent physical necessities rather than engineering preferences. Cloud deployment leverages abundant power and cooling to maximize computational capability, while mobile deployment optimizes for power density limits, and TinyML operates within extreme thermal and power constraints. Understanding these core constraints is essential for selecting appropriate deployment paradigms and setting realistic performance expectations.</p>
</section>
<section id="deployment-paradigm-evolution" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="deployment-paradigm-evolution">Deployment Paradigm Evolution</h3>
<p>Deployment paradigms evolved to work within the physical constraints that bound ML system design.</p>
<p>Deployment paradigms succeed through hardware capabilities and algorithmic optimization that transforms resource constraints into engineering opportunities. Modern ML systems achieve practical deployment across the entire spectrum through systematic optimization techniques including model compression, precision reduction, and hardware-aware design. These optimization foundations, detailed in <strong><a href="../core/efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 10: Efficient AI</a></strong> and <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>, determine whether a model can successfully deploy within specific resource constraints while maintaining acceptable accuracy.</p>
<p>ML systems evolved from centralized to distributed paradigms, enabled by algorithmic efficiency techniques that make resource-constrained deployment practical.</p>
<p>Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML uses extensive computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness is not critical. Popular platforms like AWS SageMaker<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> trained on petabytes of data, though network delays introduce latencies of 100-500ms for online inference (16ms minimum for coast-to-coast light speed plus 50-100ms processing overhead), making sub-10ms real-time applications physically impossible with cloud processing<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>AWS SageMaker</strong>: Amazonâ€™s ML platform launched in 2017, processing over 1 million model training jobs annually by 2023. Provides end-to-end ML workflow management, from data preparation to model deployment, with integrated Jupyter notebooks and automatic model tuning capabilities.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Billion-Parameter Models</strong>: GPT-3 contains 175 billion model components requiring 350GB of memory just for storage <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. GPT-4 is estimated at 1.8 trillion components. For comparison, the human brain has approximately 86 billion neurons with 100 trillion synaptic connections, suggesting AI models are approaching biological complexity.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Cloud Inference Latency</strong>: Network latency includes propagation delay (speed of light limits), routing delays, and processing time. Round-trip from California to Virginia takes minimum 80ms just for light travel. Adding internet routing, DNS lookup, and server processing typically results in 100-500ms total latency.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Edge Latency Advantage</strong>: Edge processing eliminates network round-trips, achieving &lt;10ms response times for local inference. Industrial robots require &lt;1ms control loops, autonomous vehicles need &lt;10ms emergency responses. Both requirements are impossible with cloud processing but achievable with edge deployment.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;<strong>NVIDIA Jetson Ecosystem</strong>: Family of embedded computing boards designed for AI at the edge, from the Jetson Nano Developer Kit ~$99-149 (5W) to the $1,999 AGX Orin (60W). Used in over 1 million deployed robots, drones, and autonomous vehicles worldwide since launch in 2014.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;<strong>IoT Ecosystems</strong>: Interconnected networks of smart devices, sensors, and gateways. A modern smart city might contain 1 million+ IoT devices per square kilometer, generating 2.5 quintillion bytes of data daily, making edge processing essential for real-time responses.</p></div></div><p>Growing demand for real time, low latency processing drove the emergence of Edge ML. Edge computing brings inference capabilities closer to data sources through deployment on industrial gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge systems excel at applications requiring quick responses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> and Googleâ€™s Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> by enabling real time decision making and reducing bandwidth usage through local data processing.</p>
<p>Building on edge computing concepts, Mobile ML uses the computational capabilities of smartphones and tablets. Mobile systems enable personalized, responsive applications while reducing reliance on constant network connectivity. Mobile ML balances the power of edge computing with the ubiquity of personal devices, utilizing onboard sensors such as cameras, GPS, and accelerometers for ML applications. Frameworks like TensorFlow Lite and Core ML enable developers to deploy optimized models on mobile devices, achieving inference times under 30 ms for common tasks. Mobile ML enhances privacy by keeping personal data locally and operates offline, though it must balance model performance with device resource constraints<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, typically 4 to 8 GB RAM and 100 to 200 GB storage.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Mobile Storage Evolution</strong>: iPhone storage grew from 4GB (2007) to 1TB (2023), a 250x increase in 16 years. However, ML models grew even faster: ResNet-50 (25MB, 2015) to modern language models (&gt;1GB compressed), creating ongoing storage pressure despite hardware improvements.</p></div><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Memory Scale Comparison</strong>: TinyML devices operate with 256KB-2MB memory versus smartphones with 12-24GB (48,000-96,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through specialized optimization techniques that dramatically reduce model size and computational requirements while maintaining accuracy (detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>).</p></div><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Ultra-Long Battery Life</strong>: TinyML enables 10+ year deployments on single batteries through duty cycling. Devices sleep 99.9% of the time, wake periodically for inference, then return to sleep. Average power consumption drops to 10-100 microwatts, making decade-long operation feasible on coin-cell batteries.</p></div></div><p>The latest development in this progression, Tiny ML enables ML models to run on extremely resource constrained microcontrollers and small embedded systems. Tiny ML performs local inference without relying on connectivity to cloud, edge, or mobile device processing power. Tiny systems excel in applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>, consuming only milliwatts of power to enable battery life of months or years<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. Tiny ML requires significant model optimization and precision reduction techniques to fit within severe constraints.</p>
<p>Each paradigm addresses different use cases through distinct strengths:</p>
<ul>
<li>Cloud ML remains essential for tasks requiring massive computational power or large scale data analysis.</li>
<li>Edge ML is ideal for applications requiring low latency responses or local data processing in industrial or enterprise environments.</li>
<li>Mobile ML is suited for personalized, responsive applications on smartphones and tablets.</li>
<li>Tiny ML enables AI capabilities in small, power efficient devices, expanding the reach of ML to new domains.</li>
</ul>
<p>This progression reflects a broader trend in computing toward more distributed, localized, and specialized processing. Evolution toward distributed systems stems from requirements for faster response times, improved privacy, reduced bandwidth usage, and operation in environments with limited connectivity while accommodating the specific capabilities and constraints of different device types.</p>
<div id="fig-vMLsizes" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-vMLsizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1b6a65f1ce1f88fffebdd36030c53ddacd39e03d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Device Memory Constraints: AI model deployment spans a wide range of devices with drastically different memory capacities, from cloud servers with 16 GB to microcontroller-based systems with only 320 kb. This progression necessitates specialized optimization techniques and efficient architectures to enable on-device intelligence with limited resources. Source: [@lin2023tiny]."><img src="ml_systems_files/mediabag/1b6a65f1ce1f88fffebdd36030c53ddacd39e03d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vMLsizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Device Memory Constraints</strong>: AI model deployment spans a wide range of devices with drastically different memory capacities, from cloud servers with 16 GB to microcontroller-based systems with only 320 kb. This progression necessitates specialized optimization techniques and efficient architectures to enable on-device intelligence with limited resources. Source: <span class="citation" data-cites="lin2023tiny">(<a href="#ref-lin2023tiny" role="doc-biblioref">Lin et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lin2023tiny" class="csl-entry" role="listitem">
Lin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023. <span>â€œTiny Machine Learning: Progress and Futures [Feature].â€</span> <em>IEEE Circuits and Systems Magazine</em> 23 (3): 8â€“34. <a href="https://doi.org/10.1109/mcas.2023.3302182">https://doi.org/10.1109/mcas.2023.3302182</a>.
</div></div></figure>
</div>
<p><a href="#fig-vMLsizes" class="quarto-xref">Figure&nbsp;2</a> shows the differences between Cloud ML, Edge ML, Mobile ML, and Tiny ML in terms of hardware, latency, connectivity, power requirements, and model complexity. As systems move from Cloud to Edge to Tiny ML, available resources decrease dramatically, presenting significant challenges for deploying machine learning models. This resource disparity becomes particularly apparent when deploying ML models on microcontrollers, the primary hardware platform for Tiny ML. These devices have severely constrained memory and storage capacities, which are often insufficient for conventional complex ML models.</p>
<div id="quiz-question-sec-ml-systems-overview-db10" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary advantage of deploying machine learning models on edge devices?</p>
<ol type="a">
<li>Reduced latency and improved privacy</li>
<li>Maximum computational power</li>
<li>Unlimited storage capacity</li>
<li>No resource constraints</li>
</ol></li>
<li><p>Explain the trade-offs involved in choosing between cloud ML and Tiny ML for a real-time image classification task.</p></li>
<li><p>What is a key challenge when deploying machine learning models on mobile devices?</p>
<ol type="a">
<li>Lack of internet connectivity</li>
<li>Balancing model performance with battery life</li>
<li>Excessive computational power</li>
<li>Unlimited memory availability</li>
</ol></li>
<li><p>In a production system, how might you decide between using Edge ML and Mobile ML for a smart home application?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-overview-db10" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-cloudbased-machine-learning-7606" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-cloudbased-machine-learning-7606">Cloud-Based Machine Learning</h2>
<p>Cloud ML maximizes computational resources while accepting latency constraints. Cloud ML provides the optimal choice when computational power matters more than response time, making it ideal for complex training tasks and inference that can tolerate network delays.</p>
<p>Cloud Machine Learning leverages the scalability and power of centralized cloud infrastructures<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> to handle computationally intensive tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers utilize distributed architectures and specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. This section focuses on the deployment characteristics that make cloud ML systems effective for large-scale applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Cloud Infrastructure Evolution</strong>: Cloud computing for ML emerged from Amazonâ€™s decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending exceeded $250 billion annually.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>NLP Computational Demands</strong>: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training, equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days <span class="citation" data-cites="strubell2019energy">(<a href="#ref-strubell2019energy" role="doc-biblioref">Strubell, Ganesh, and McCallum 2019</a>)</span>. This computational scale drove the need for massive cloud infrastructure.</p><div id="ref-strubell2019energy" class="csl-entry" role="listitem">
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. <span>â€œEnergy and Policy Considerations for Deep Learning in NLP.â€</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 3645â€“50. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/p19-1355">https://doi.org/10.18653/v1/p19-1355</a>.
</div></div></div><div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of Cloud ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Cloud ML</summary><div><strong>Cloud Machine Learning (Cloud ML)</strong> refers to the deployment of machine learning models on <em>centralized computing infrastructures</em>, such as data centers. These systems operate in the <em>kilowatt to megawatt</em> power range and utilize <em>specialized computing systems</em> to handle <em>large scale datasets</em> and train <em>complex models</em>. Cloud ML offers <em>scalability</em> and <em>computational capacity</em>, making it well-suited for tasks requiring extensive resources and collaboration. However, it depends on <em>consistent connectivity</em> and may introduce <em>latency</em> for real-time applications.<p></p>
</div></details>
</div>
<p><a href="#fig-cloud-ml" class="quarto-xref">Figure&nbsp;3</a> provides an overview of Cloud MLâ€™s capabilities, which we will discuss in greater detail throughout this section.</p>
<div id="fig-cloud-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cloud-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4fce3d88761f81181ec3328739a10b05a000b663.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Cloud ML Capabilities: Cloud machine learning systems address challenges related to scale, complexity, and resource management through centralized computing infrastructure and specialized hardware. This figure outlines key considerations for deploying models in the cloud, including the need for robust infrastructure and efficient resource allocation to handle large datasets and complex computations."><img src="ml_systems_files/mediabag/4fce3d88761f81181ec3328739a10b05a000b663.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloud-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Cloud ML Capabilities</strong>: Cloud machine learning systems address challenges related to scale, complexity, and resource management through centralized computing infrastructure and specialized hardware. This figure outlines key considerations for deploying models in the cloud, including the need for robust infrastructure and efficient resource allocation to handle large datasets and complex computations.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-b564" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-b564">Characteristics</h3>
<p>Cloud MLâ€™s defining characteristic is its centralized infrastructure that operates at unprecedented scale. <a href="#fig-cloudml-example" class="quarto-xref">Figure&nbsp;4</a> illustrates this concept with an example from Googleâ€™s Cloud TPU<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> data center. A single cloud deployment can provide 100+ TFLOPS of compute power compared to 1-10 TFLOPS available on mobile devices, representing a 10-100x computational advantage. Cloud service providers offer virtual platforms consisting of high-capacity servers (64-256 cores, 512GB-4TB RAM), expansive storage solutions (petabyte-scale distributed file systems), and robust networking architectures (10-100 Gbps interconnects) housed in globally distributed data centers<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>. These centralized facilities operate at kilowatt to megawatt power scales, enabling computational workloads impossible on resource-constrained devices. However, this centralization introduces critical trade-offs: network round-trip latency of 50-200ms eliminates real-time applications, while operational costs scale linearly with usage ($0.001-0.01 per inference request).</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Tensor Processing Unit (TPU)</strong>: Googleâ€™s custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of compute power, more than most supercomputers.</p></div><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Hyperscale Data Centers</strong>: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoftâ€™s data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes.</p></div></div><div id="fig-cloudml-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cloudml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/cloud_ml_tpu.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Cloud Data Center Scale: Large-scale machine learning systems require centralized infrastructure with massive computational resources and storage capacity. Googleâ€™s cloud TPU data center provides this need, housing specialized AI accelerator hardware to efficiently manage the demands of training and deploying complex models. Source: [@google2024gemini]."><img src="images/jpg/cloud_ml_tpu.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloudml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Cloud Data Center Scale</strong>: Large-scale machine learning systems require centralized infrastructure with massive computational resources and storage capacity. Googleâ€™s cloud TPU data center provides this need, housing specialized AI accelerator hardware to efficiently manage the demands of training and deploying complex models. Source: <span class="citation" data-cites="google2024gemini">(<a href="#ref-google2024gemini" role="doc-biblioref">DeepMind 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-google2024gemini" class="csl-entry" role="listitem">
DeepMind, Google. 2024. <span>â€œGemini: A Family of Highly Capable Multimodal Models.â€</span> <a href="https://blog.google/technology/ai/google-gemini-ai/">https://blog.google/technology/ai/google-gemini-ai/</a>.
</div></div></figure>
</div>
<p>Cloud ML excels in processing massive data volumes through parallelized architectures that exceed the capabilities of individual devices. The centralized infrastructure is designed to handle complex computations and model training (covered in <strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>) tasks that require significant computational power. Cloud infrastructure enables training on datasets requiring hundreds of terabytes of storage and petaflops of computation, resources impossible to provide on edge or mobile devices. Through distributed training across hundreds of GPUs, cloud systems can complete training tasks in hours that would require months on single devices, leading to improved learning capabilities and predictive performance. The detailed memory bandwidth analysis and optimization techniques that enable this performance are covered in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong> and <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p>
<p>Cloud ML also offers exceptional flexibility in deployment and accessibility. Once trained and validated, machine learning models deploy through cloud APIs<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> and services, becoming accessible to users worldwide. Cloud deployment enables integration of ML capabilities into applications across mobile, web, and IoT platforms, regardless of end user computational resources.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>ML APIs</strong>: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Googleâ€™s Vision API launched in 2016, processing over 1 billion images monthly within two years, enabling developers to add AI capabilities without ML expertise.</p></div></div><p>Cloud ML promotes collaboration and resource sharing among teams and organizations. The centralized nature of the cloud infrastructure enables multiple data scientists and engineers to access and work on the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, accelerates the development cycle from experimentation to production, and optimizes resource utilization across teams.</p>
<p>Through pay-as-you-go pricing models<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> offered by cloud service providers, Cloud ML allows organizations to avoid the upfront capital expenditure associated with building and maintaining dedicated ML infrastructure. The ability to scale resources up during intensive training periods and down during lower demand ensures cost effectiveness and financial flexibility in managing machine learning projects.</p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Pay-as-You-Go Pricing</strong>: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware.</p></div></div><p>Cloud ML has transformed machine learning approaches by providing organizations access to advanced AI capabilities without requiring specialized hardware expertise or significant infrastructure investments. This paradigm enables scalable and efficient deployment across organizations of all sizes.</p>
</section>
<section id="sec-ml-systems-benefits-e12c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-e12c">Benefits</h3>
<p>Cloud ML offers several significant benefits that make it a powerful choice for machine learning projects:</p>
<p>Cloud ML provides substantial computational resources through infrastructure designed to handle complex algorithms and process large datasets efficiently. This approach particularly benefits machine learning models requiring significant computational power, such as complex neural networks or models trained on massive datasets. Organizations can overcome local hardware limitations and scale their machine learning projects to meet demanding requirements.</p>
<p>Cloud ML provides dynamic scalability, enabling organizations to adapt easily to changing computational needs. As data volume grows or model complexity increases, cloud infrastructure seamlessly scales up or down to accommodate these changes. Dynamic scaling ensures consistent performance and enables organizations to handle varying workloads without extensive hardware investments. Cloud ML allocates resources on demand, providing cost effective and efficient machine learning project management.</p>
<p>Cloud ML platforms provide access to a wide range of advanced tools and algorithms designed for machine learning. These tools often include prebuilt models, AutoML<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> capabilities, and specialized APIs that simplify the development and deployment of machine learning solutions. Developers can use these resources to accelerate the building, training, and optimization of models. By utilizing advancements in machine learning algorithms and techniques, organizations can implement solutions without needing to develop them from scratch.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>AutoML (Automated Machine Learning)</strong>: Automated systems that democratize ML by handling model selection, hyperparameter tuning, and feature engineering. Google AutoML Vision achieved 93.9% accuracy on ImageNet with minimal human intervention, compared to months of expert work for similar results.</p></div></div><p>Cloud ML creates a collaborative environment that enables teams to work together efficiently. The centralized nature of the cloud infrastructure allows multiple data scientists and engineers to access and contribute to the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, promotes cross-functional collaboration, and accelerates the development and iteration of machine learning models. Teams can easily share code, datasets, and results through version control and project management tools integrated with cloud platforms.</p>
<p>Cloud ML offers a cost effective solution compared to building and maintaining on premises machine learning infrastructure. Cloud service providers offer flexible pricing models, such as pay per use or subscription based plans, allowing organizations to pay only for consumed resources. This approach eliminates upfront capital investments in specialized hardware like GPUs and TPUs, reducing the overall implementation cost. The ability to automatically scale resources during periods of low utilization ensures organizations pay only for actual usage.</p>
<p>Cloud MLâ€™s benefits include immense computational power, dynamic scalability, advanced tools and algorithms, collaborative environments, and cost effectiveness. These capabilities enable organizations to accelerate machine learning initiatives, drive innovation, and gain competitive advantage in data driven environments.</p>
</section>
<section id="sec-ml-systems-challenges-e73b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-e73b">Challenges</h3>
<p>These substantial benefits come with corresponding trade-offs. Cloud ML also presents certain challenges that organizations need to consider:</p>
<p>Latency represents a critical physical constraint in Cloud ML (as detailed in <a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>), forcing architectural decisions where autonomous vehicles requiring sub-10ms emergency responses must use local processing despite 10x higher hardware costs. Beyond technical constraints, cloud latency introduces operational challenges including unpredictable response times that complicate performance monitoring, cascading failures when network issues affect multiple services simultaneously, and difficulty debugging performance issues across distributed infrastructure.</p>
<p>Data privacy and security represent critical challenges when centralizing processing and storage in the cloud. Sensitive data transmitted to remote data centers becomes potentially vulnerable to cyber-attacks and unauthorized access. Cloud environments often attract hackers seeking to exploit vulnerabilities in valuable information repositories. Organizations must implement robust security measures including encryption, strict access controls, and continuous monitoring. Additionally, handling sensitive data in cloud environments complicates compliance with regulations like GDPR<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> or HIPAA<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>GDPR (General Data Protection Regulation)</strong>: European privacy law effective 2018, imposing fines up to â‚¬20 million or 4% of global revenue for violations. Forces ML systems to implement â€œright to be forgottenâ€ and data processing transparency, technically challenging for neural networks.</p></div><div id="fn27"><p><sup>27</sup>&nbsp;<strong>HIPAA (Health Insurance Portability and Accountability Act)</strong>: US healthcare privacy law requiring strict data security measures. ML systems handling medical data must implement encryption, access controls, and audit trails, adding 30-50% to development costs but enabling $150B+ healthcare AI market.</p></div></div><p>Cost management reveals the economic realities of cloud ML that affect deployment decisions and operational complexity. While cloud infrastructure eliminates upfront capital costs, operational expenses scale unpredictably with usage. A production system serving 1 million daily inferences at $0.001 each costs $365,000 annually, while equivalent edge hardware costs $100,000 one-time. The break-even occurs around 100,000-1,000,000 requests, directly determining deployment strategy. Cloud cost structures create operational challenges including unpredictable monthly bills that complicate budgeting, usage spikes that can exceed allocated budgets by orders of magnitude, and the need for sophisticated monitoring systems to track costs across multiple models and services. Organizations must implement cost governance frameworks including automated resource scaling, inference caching strategies, and model optimization pipelines to maintain economic viability.</p>
<p>Network dependency presents another significant challenge for Cloud ML implementations. The requirement for stable and reliable internet connectivity means that any disruptions in network availability directly impact system performance. This dependency becomes particularly problematic in environments with limited, unreliable, or expensive network access. Building resilient ML systems requires robust network infrastructure complemented by appropriate failover mechanisms or offline processing capabilities. Comprehensive approaches to system resilience, failure mode analysis, and robust ML system design are detailed in <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
<p>Vendor lock in often emerges as organizations adopt specific tools, APIs, and services from their chosen cloud provider. This dependency can complicate future transitions between providers or platform migrations. Organizations may encounter challenges with portability, interoperability, and cost implications when considering changes to their cloud ML infrastructure. Strategic planning should include careful evaluation of vendor offerings, consideration of long term goals, and preparation for potential migration scenarios to mitigate lock in risks.</p>
<p>Addressing these challenges requires thorough planning, thoughtful architectural design, and thorough risk mitigation strategies. Organizations must balance Cloud ML benefits against potential challenges based on their specific requirements, data sensitivity concerns, and business objectives. Proactive approaches to these challenges enable organizations to use Cloud ML effectively while maintaining data privacy, security, cost effectiveness, and system reliability.</p>
</section>
<section id="sec-ml-systems-use-cases-348c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-348c">Use Cases</h3>
<p>Despite these challenges, Cloud ML has found widespread adoption across various domains, demonstrating how organizations successfully navigate these trade-offs. Cloud ML has transformed how businesses operate and users interact with technology:</p>
<p>Cloud ML plays a crucial role in powering virtual assistants like Siri and Alexa. These systems use the extensive computational capabilities of the cloud to process and analyze voice inputs in real-time. Using natural language processing algorithms, virtual assistants can understand user queries, extract relevant information, and generate intelligent and personalized responses. The cloudâ€™s scalability and processing power enable these assistants to handle a vast number of user interactions simultaneously, providing a responsive user experience.</p>
<p>Cloud ML forms the backbone of advanced recommendation systems used by platforms like Netflix and Amazon. These systems use the cloudâ€™s ability to process and analyze massive datasets to uncover patterns, preferences, and user behavior. Using collaborative filtering<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> and other machine learning techniques, recommendation systems can offer personalized content or product suggestions tailored to each userâ€™s interests. The cloudâ€™s scalability allows these systems to continuously update and refine their recommendations based on the ever-growing amount of user data, enhancing user engagement and satisfaction.</p>
<div class="no-row-height column-margin column-container"><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Collaborative Filtering</strong>: Recommendation technique analyzing user behavior patterns to predict preferences. Netflixâ€™s algorithm processes 100+ billion data points daily, with collaborative filtering contributing to 80% of watched content and saving $1 billion annually in customer retention.</p></div></div><p>In the financial industry, Cloud ML has revolutionized fraud detection systems. Using cloud computational power, these systems can analyze vast amounts of transactional data in real-time to identify potential fraudulent activities. Machine learning algorithms trained on historical fraud patterns can detect anomalies and suspicious behavior, enabling financial institutions to take proactive measures to prevent fraud and minimize financial losses. The cloudâ€™s ability to process and store large volumes of data makes it an ideal platform for implementing scalable fraud detection systems.</p>
<p>Cloud ML is deeply integrated into our online experiences, shaping the way we interact with digital platforms. From personalized ads on social media feeds to predictive text features in email services, Cloud ML powers smart algorithms that enhance user engagement and convenience. It enables e-commerce sites to recommend products based on a userâ€™s browsing and purchase history, fine-tunes search engines to deliver accurate and relevant results, and automates the tagging and categorization of photos on platforms like Facebook. Through cloud computational resources, these systems can continuously learn and adapt to user preferences, providing a more intuitive and personalized user experience.</p>
<p>Cloud ML plays a role in bolstering user security by powering anomaly detection systems. These systems continuously monitor user activities and system logs to identify unusual patterns or suspicious behavior. By analyzing vast amounts of data in real-time, Cloud ML algorithms can detect potential cyber threats, such as unauthorized access attempts, malware infections, or data breaches. The cloudâ€™s scalability and processing power enable these systems to handle the increasing complexity and volume of security data, providing a proactive approach to protecting users and systems from potential threats.</p>
<div id="quiz-question-sec-ml-systems-cloudbased-machine-learning-7606" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary benefit of using Cloud ML for machine learning projects?</p>
<ol type="a">
<li>Reduced latency for real-time applications</li>
<li>Elimination of data privacy concerns</li>
<li>Complete independence from internet connectivity</li>
<li>Dynamic scalability and resource management</li>
</ol></li>
<li><p>True or False: Cloud ML eliminates the need for data privacy and security measures.</p></li>
<li><p>What are some challenges organizations face when implementing Cloud ML, and how might these be mitigated?</p></li>
<li><p>Order the following steps in deploying a machine learning model using Cloud ML: (1) Train model, (2) Deploy model, (3) Collect data, (4) Validate model.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-cloudbased-machine-learning-7606" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-edge-machine-learning-06ec" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-edge-machine-learning-06ec">Edge Machine Learning</h2>
<p>Cloud MLâ€™s centralized processing introduces latency and privacy constraints that prove limiting for many applications. Edge Machine Learning addresses these limitations by moving computation closer to data sources, trading unlimited computational resources for reduced latency and enhanced data privacy.</p>
<p>Edge ML shifts computation away from centralized servers to process data at the networkâ€™s edge, becoming essential for time-sensitive applications such as autonomous systems, industrial IoT<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>, and smart infrastructure. This paradigm excels when applications cannot tolerate cloud round-trip delays or when data privacy regulations prevent cloud processing. Edge devices, such as gateways and IoT hubs<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>, enable these systems to function efficiently while maintaining acceptable performance with intermediate resource constraints.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Industrial IoT</strong>: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025.</p></div><div id="fn30"><p><sup>30</sup>&nbsp;<strong>IoT Hubs</strong>: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making.</p></div></div><div id="callout-definition*-1.2" class="callout callout-definition" title="Definition of Edge ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Edge ML</summary><div><strong>Edge Machine Learning (Edge ML)</strong> describes the deployment of machine learning models at or near the <em>edge of the network</em>. These systems operate in the <em>tens to hundreds of watts</em> range and rely on <em>localized hardware</em> optimized for <em>real-time processing</em>. Edge ML minimizes <em>latency</em> and enhances <em>privacy</em> by processing data locally, but its primary limitation lies in <em>restricted computational resources</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Edge ML through four key dimensions. <a href="#fig-edge-ml" class="quarto-xref">Figure&nbsp;5</a> provides an overview of this section.</p>
<div id="fig-edge-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edge-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d6102b6914945d2473fe48f7db0c020fb8a79aca.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Edge ML Dimensions: This figure outlines key considerations for edge machine learning, contrasting challenges with benefits and providing representative examples and characteristics. Understanding these dimensions enables designing and deploying effective AI solutions on resource-constrained devices."><img src="ml_systems_files/mediabag/d6102b6914945d2473fe48f7db0c020fb8a79aca.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edge-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Edge ML Dimensions</strong>: This figure outlines key considerations for edge machine learning, contrasting challenges with benefits and providing representative examples and characteristics. Understanding these dimensions enables designing and deploying effective AI solutions on resource-constrained devices.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-09e1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-09e1">Characteristics</h3>
<p>Edge ML achieves these advantages over cloud processing through its core characteristics. Edge ML processes data in a decentralized fashion, as illustrated in <a href="#fig-edgeml-example" class="quarto-xref">Figure&nbsp;6</a>. Instead of sending data to remote servers, devices like smartphones, tablets, and Internet of Things (IoT) devices<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> process data locally. The figure shows various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on collected data without depending on central server resources.</p>
<div class="no-row-height column-margin column-container"><div id="fn31"><p><sup>31</sup>&nbsp;<strong>IoT Device Growth</strong>: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management.</p></div></div><div id="fig-edgeml-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edgeml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/edge_ml_iot.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Edge Device Deployment: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse."><img src="images/jpg/edge_ml_iot.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edgeml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Edge Device Deployment</strong>: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.
</figcaption>
</figure>
</div>
<p>Edge ML operates under intermediate resource constraints that require careful architectural optimization. Typical edge devices provide 1-8GB RAM and 10-100 TOPS compute power at 10-100W power consumption, representing a middle ground between cloud unlimited resources and mobile battery constraints. Memory bandwidth at 25-100 GB/s enables models requiring 100MB-1GB parameters but blocks larger models that dominate cloud applications. These constraints drive specific algorithmic choices: edge inference systems typically use INT8 quantization to reduce memory footprint by 4x and achieve 2-4x speedup compared to FP32 cloud models. Local processing eliminates network round-trip latency of 50-200ms, enabling real-time applications with 1-50ms response requirements. The bandwidth savings become significant at scale: processing 1000 camera feeds locally at 1Mbps each avoids 1Gbps uplink costs and reduces cloud processing expenses by $10,000-100,000 annually.</p>
</section>
<section id="sec-ml-systems-benefits-4fb7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-4fb7">Benefits</h3>
<p>Edge MLâ€™s quantifiable advantages stem from eliminating network dependencies and optimizing for local resource constraints. Latency reduction from 50-200ms (cloud) to 1-50ms (edge) represents a 4-40x improvement that enables entirely new application categories. This improvement becomes essential in safety-critical scenarios: autonomous vehicles requiring &lt;10ms emergency braking decisions, industrial robotics needing &lt;1ms precision control, and augmented reality demanding &lt;20ms motion-to-photon latency for comfort. Energy efficiency improves through localized computation: a smartphone processing 1000 images locally at 0.1J per inference consumes 100J total, while uploading to cloud requires 1-10J per image for wireless transmission alone, creating 10-100x energy penalty that eliminates cloud processing for battery-powered applications<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Latency-Critical Applications</strong>: Autonomous vehicles require &lt;10ms response times for emergency braking decisions. Industrial robotics needs &lt;1ms for precision control. Cloud round-trip latency typically ranges from 50-200ms, making edge processing essential for safety-critical applications.</p></div></div><p>Edge ML provides improved data privacy through local processing, minimizing data breach risks inherent in centralized storage. Sensitive information remains secure without network transmission risks (see also Mobile ML privacy benefits in <a href="#sec-ml-systems-mobile-machine-learning-f5b5" class="quarto-xref">Section&nbsp;1.4</a>).</p>
<p>Operating closer to the data source means less data must be sent over networks, reducing bandwidth usage. This results in cost savings and efficiency gains, especially in environments where bandwidth is limited or costly.</p>
</section>
<section id="sec-ml-systems-challenges-2714" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-2714">Challenges</h3>
<p>These compelling benefits come with corresponding trade-offs. Edge ML faces several challenges. The primary concern is limited computational resources compared to cloud based solutions. Endpoint devices<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> typically have significantly less processing power and storage capacity than cloud servers, limiting the complexity of deployable machine learning models.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Endpoint Device Constraints</strong>: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques.</p></div><div id="fn34"><p><sup>34</sup>&nbsp;<strong>Edge Network Coordination</strong>: For n edge devices, the number of potential communication paths is n(n-1)/2. A network of 1,000 devices has 499,500 possible connections to manage. Software-defined networking and edge orchestration platforms like Kubernetes K3s help manage this complexity.</p></div></div><p>Managing a network of edge nodes introduces complexity, particularly regarding coordination, updates, and maintenance. Ensuring all nodes operate efficiently and remain current with the latest algorithms and security protocols presents logistical challenges. This distributed management problem scales exponentially: coordinating 1,000 edge devices requires managing 499,500 potential communication paths<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>.</p>
<p>While Edge ML offers enhanced data privacy, edge nodes can be more vulnerable to physical and cyber attacks. Developing security protocols that protect data at each node without compromising system efficiency remains a significant deployment challenge.</p>
</section>
<section id="sec-ml-systems-use-cases-05eb" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-05eb">Use Cases</h3>
<p>Despite these challenges, Edge ML has found successful deployment across many applications, from autonomous vehicles and smart homes to industrial Internet of Things (IoT). These examples highlight scenarios where real-time data processing, reduced latency, and enhanced privacy are critical to the operation and success of these technologies. They demonstrate how Edge ML drives advancements in various sectors, enabling more intelligent, responsive, and adaptive systems.</p>
<p>Autonomous vehicles stand as a prime example of Edge MLâ€™s potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. Localized machine learning models assist in quickly analyzing data from various sensors to make immediate driving decisions, ensuring safety and smooth operation.</p>
<p>Edge ML plays a crucial role in efficiently managing various systems in smart homes and buildings, from lighting and heating to security. By processing data locally, these systems can operate more responsively and harmoniously with the occupantsâ€™ habits and preferences, creating a more comfortable living environment.</p>
<p>The Industrial IoT<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> uses Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>, optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors.</p>
<div class="no-row-height column-margin column-container"><div id="fn35"><p><sup>35</sup>&nbsp;<strong>Industry 4.0</strong>: Fourth industrial revolution integrating cyber-physical systems, IoT, and cloud computing into manufacturing. Expected to increase productivity by 20-30% and reduce costs by 15-25% globally, with Germany leading adoption (83% of manufacturers) followed by US (54%).</p></div><div id="fn36"><p><sup>36</sup>&nbsp;<strong>Predictive Maintenance</strong>: ML-driven maintenance scheduling based on equipment condition rather than fixed intervals. Reduces unplanned downtime by 35-45% and maintenance costs by 20-25%. GE saves $1.5 billion annually using predictive analytics across its industrial equipment.</p></div></div><p>The applicability of Edge ML is vast and not limited to these examples. Various other sectors, including healthcare, agriculture, and urban planning, are exploring and integrating Edge ML to develop innovative solutions responsive to real-world needs and challenges, enabling a new generation of smart, interconnected systems.</p>
<div id="quiz-question-sec-ml-systems-edge-machine-learning-06ec" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>What is a primary benefit of using Edge Machine Learning over Cloud ML?</p>
<ol type="a">
<li>Increased computational resources</li>
<li>Reduced latency</li>
<li>Centralized data processing</li>
<li>Unlimited storage capacity</li>
</ol></li>
<li><p>True or False: Edge ML enhances data privacy by processing data locally rather than sending it to centralized servers.</p></li>
<li><p>What challenges might arise when deploying machine learning models on edge devices, and how can they be addressed?</p></li>
<li><p>Edge Machine Learning is crucial for applications requiring real-time decision making, such as ____. This is important because it allows for immediate processing and response.</p></li>
<li><p>Order the following benefits of Edge ML in terms of their impact on system performance: (1) Reduced latency, (2) Enhanced data privacy, (3) Lower bandwidth usage.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-edge-machine-learning-06ec" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-mobile-machine-learning-f5b5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-mobile-machine-learning-f5b5">Mobile Machine Learning</h2>
<p>Edge ML addresses latency and privacy concerns but still requires dedicated edge infrastructure and ongoing connectivity. Mobile Machine Learning extends intelligence directly to personal devices, prioritizing user proximity, offline capability, and privacy while operating under strict power and thermal constraints.</p>
<p>Mobile ML integrates machine learning directly into portable devices like smartphones and tablets, providing users with real-time, personalized capabilities. This paradigm excels when user privacy, offline operation, and immediate responsiveness matter more than computational sophistication. Mobile ML supports applications such as voice recognition<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>, computational photography<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>, and health monitoring while maintaining data privacy through on-device computation. These battery-powered devices must balance performance with power efficiency and thermal management, making them ideal for frequent, short-duration AI tasks.</p>
<div class="no-row-height column-margin column-container"><div id="fn37"><p><sup>37</sup>&nbsp;<strong>Voice Recognition Evolution</strong>: Appleâ€™s Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to &lt;50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines.</p></div><div id="fn38"><p><sup>38</sup>&nbsp;<strong>Computational Photography</strong>: Combines multiple exposures and ML algorithms to enhance image quality. Googleâ€™s Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time.</p></div></div><div id="callout-definition*-1.3" class="callout callout-definition" title="Definition of Mobile ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Mobile ML</summary><div><strong>Mobile Machine Learning (Mobile ML)</strong> enables machine learning models to run directly on <em>portable, battery-powered devices</em> like smartphones and tablets. Operating within the <em>single-digit to tens of watts</em> range, Mobile ML leverages <em>on-device computation</em> to provide <em>personalized and responsive applications</em>. This paradigm preserves <em>privacy</em> and ensures <em>offline functionality</em>, though it must balance <em>performance</em> with <em>battery and storage limitations</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Mobile ML across four key dimensions, revealing how this paradigm balances capability with constraints. <a href="#fig-mobile-ml" class="quarto-xref">Figure&nbsp;7</a> provides an overview of Mobile MLâ€™s capabilities discussed throughout this section.</p>
<div id="fig-mobile-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mobile-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="edadf11d2e25a4f078f21c852982acbd85f96dff.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Mobile ML Capabilities: Mobile machine learning systems balance performance with resource constraints through on-device processing, specialized hardware acceleration, and optimized frameworks. This figure outlines key considerations for deploying ML models on mobile devices, including the trade-offs between computational efficiency, battery life, and model performance."><img src="ml_systems_files/mediabag/edadf11d2e25a4f078f21c852982acbd85f96dff.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mobile-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Mobile ML Capabilities</strong>: Mobile machine learning systems balance performance with resource constraints through on-device processing, specialized hardware acceleration, and optimized frameworks. This figure outlines key considerations for deploying ML models on mobile devices, including the trade-offs between computational efficiency, battery life, and model performance.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-9792" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-9792">Characteristics</h3>
<p>Mobile ML operates within severe resource constraints that drive specific architectural optimizations. Modern flagship devices provide 1-10 TOPS of AI compute through specialized Neural Processing Units while consuming &lt;1W power, achieving 10-100x better energy efficiency than general-purpose processors. System on Chip architectures<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> integrate computation and memory to minimize data movement costs, the primary energy bottleneck in mobile systems. Typical mobile memory bandwidth of 25-50 GB/s limits deployable models to 10-100MB parameter sets, requiring aggressive quantization from FP32 (4 bytes/parameter) to INT8 (1 byte/parameter) for practical deployment. Battery constraints impose critical limits: at 5000mAh capacity (18Wh), continuous 1W ML processing reduces device lifetime from 24 hours to 18 hours, making energy optimization essential for user experience<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn39"><p><sup>39</sup>&nbsp;<strong>Mobile System-on-Chip</strong>: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Appleâ€™s A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor.</p></div><div id="fn40"><p><sup>40</sup>&nbsp;<strong>Neural Processing Unit (NPU)</strong>: Specialized processors optimized for neural network operations. Appleâ€™s Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcommâ€™s Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming &lt;1W.</p></div><div id="fn41"><p><sup>41</sup>&nbsp;<strong>TensorFlow Lite</strong>: Googleâ€™s mobile ML framework launched in 2017, designed to run models &lt;100MB with &lt;100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide.</p></div><div id="fn42"><p><sup>42</sup>&nbsp;<strong>Core ML</strong>: Appleâ€™s framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models.</p></div><div id="fn43"><p><sup>43</sup>&nbsp;<strong>Model Optimization</strong>: Techniques that reduce model size and computational requirements while maintaining accuracy. These optimizations enable deployment on resource-constrained devices (detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>).</p></div></div><p>Mobile ML is supported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> for Android devices and Core ML<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimization techniques<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> to ensure smooth performance within mobile resource constraints.</p>
</section>
<section id="sec-ml-systems-benefits-99f9" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-99f9">Benefits</h3>
<p>Mobile ML enables real time processing of data directly on mobile devices, eliminating the need for constant server communication. This results in faster response times for applications requiring immediate feedback, such as real time translation<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a>, face detection<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a>, or gesture recognition.</p>
<div class="no-row-height column-margin column-container"><div id="fn44"><p><sup>44</sup>&nbsp;<strong>Real-Time Translation</strong>: Google Translate app can translate conversations in 40+ languages offline using on-device neural networks. The offline models are 35-45MB each versus 2GB+ for cloud versions, achieving 90% of cloud accuracy while enabling instant translation without internet.</p></div><div id="fn45"><p><sup>45</sup>&nbsp;<strong>Mobile Face Detection</strong>: Appleâ€™s Face ID uses a 30,000-dot projector and neural networks to create 3D face maps in &lt;2 seconds. The system processes biometric data entirely on-device using the Secure Enclave, making it practically impossible to extract face data even with physical device access.</p></div></div><p>By processing data locally on the device, Mobile ML maintains user privacy with similar benefits to Edge ML (<a href="#sec-ml-systems-edge-machine-learning-06ec" class="quarto-xref">Section&nbsp;1.3</a>), while providing additional personalization through direct access to user behavior patterns and device sensors.</p>
<p>Mobile ML applications can function without constant internet connectivity, making them reliable in areas with poor network coverage or when users are offline. This ensures consistent performance and user experience regardless of network conditions.</p>
</section>
<section id="sec-ml-systems-challenges-aa62" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-aa62">Challenges</h3>
<p>These advantages come with significant trade-offs. Mobile devices face quantified resource constraints that directly limit deployable model complexity and create specific optimization requirements. Memory constraints are particularly severe: flagship phones with 12-24GB RAM allocate only 100MB-1GB to ML applications, versus unlimited cloud allocation. This forces model architectures under 100-500MB total size including weights and activations. Processing power limitations compound memory constraints: mobile NPUs delivering 1-10 TOPS compare unfavorably to cloud GPUs providing 100-1000 TOPS, requiring 10-100x model compression to achieve comparable inference speeds. Storage limitations demand aggressive model optimization: mobile apps typically budget 10-100MB for ML models versus multi-gigabyte cloud deployments<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a>. These constraints cascade through the entire development process, requiring specialized training techniques, quantization-aware optimization, and architectural modifications that preserve accuracy while meeting device limitations.</p>
<div class="no-row-height column-margin column-container"><div id="fn46"><p><sup>46</sup>&nbsp;<strong>Mobile Device Constraints</strong>: Flagship phones typically have 12-24GB RAM and 512GB-2TB storage, versus cloud servers with 256-2048GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W.</p></div></div><p>ML operations impact battery life significantly (as quantified in characteristics above), requiring careful optimization of the model complexity-performance-power triangle.</p>
<p>Mobile devices have limited storage space, necessitating careful consideration of model size. This often requires model compression and quantization techniques, which can affect model accuracy and performance.</p>
</section>
<section id="sec-ml-systems-use-cases-c808" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-c808">Use Cases</h3>
<p>Despite these constraints, Mobile ML has achieved remarkable success across diverse applications. Careful optimization enables sophisticated capabilities within mobile device limitations.</p>
<p>Mobile ML has revolutionized how we use cameras on mobile devices, enabling sophisticated computer vision applications that process visual data in real-time. Modern smartphone cameras now incorporate ML models that can detect faces, analyze scenes, and apply complex filters instantaneously. These models work directly on the camera feed to enable features like portrait mode photography<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a>, where ML algorithms separate foreground subjects from backgrounds. Document scanning applications use ML to detect paper edges, correct perspective, and enhance text readability, while augmented reality applications use ML-powered object detection to accurately place virtual objects in the real world.</p>
<div class="no-row-height column-margin column-container"><div id="fn47"><p><sup>47</sup>&nbsp;<strong>Portrait Mode Photography</strong>: Uses dual cameras or LiDAR to create depth maps, then applies ML-based segmentation to separate subjects from backgrounds. iPhoneâ€™s Portrait mode processes multiple exposures in real-time, achieving DSLR-quality depth-of-field effects that would require expensive lenses and professional editing.</p></div></div><p>Natural language processing on mobile devices has transformed how we interact with our phones and communicate with others. Speech recognition models run directly on device, enabling voice assistants to respond quickly to commands even without internet connectivity. Real-time translation applications can now translate conversations and text without sending data to the cloud, preserving privacy and working reliably regardless of network conditions. Mobile keyboards have become increasingly intelligent, using ML to predict not just the next word but entire phrases based on the userâ€™s writing style and context, while maintaining all learning and personalization locally on the device.</p>
<p>Mobile ML has enabled smartphones and tablets to become sophisticated health monitoring devices. Through clever use of existing sensors combined with ML models, mobile devices can now track physical activity, analyze sleep patterns, and monitor vital signs. For example, cameras can measure heart rate by detecting subtle color changes in the userâ€™s skin, while accelerometers and ML models work together to recognize specific exercises and analyze workout form. These applications process sensitive health data directly on the device, ensuring privacy while providing users with real-time feedback and personalized health insights.</p>
<p>Perhaps the most pervasive but least visible application of Mobile ML lies in how it personalizes and enhances the overall user experience. ML models continuously analyze how users interact with their devices to optimize everything from battery usage to interface layouts. These models learn individual usage patterns to predict which apps users are likely to open next, preload content they might want to see, and adjust system settings like screen brightness and audio levels based on environmental conditions and user preferences. This creates a deeply personalized experience that adapts to each userâ€™s needs while maintaining privacy by keeping all learning and adaptation on the device itself.</p>
<p>Mobile ML bridges the gap between cloud solutions and edge computing, providing efficient, privacy-conscious, and user-friendly machine learning capabilities on personal mobile devices. Continuous advancement in mobile hardware capabilities and optimization techniques expands the possibilities for Mobile ML applications.</p>
<div id="quiz-question-sec-ml-systems-mobile-machine-learning-f5b5" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary benefit of Mobile Machine Learning?</p>
<ol type="a">
<li>Unlimited computational resources</li>
<li>Reduced data privacy concerns</li>
<li>Increased dependency on cloud connectivity</li>
<li>Simplified model deployment</li>
</ol></li>
<li><p>True or False: Mobile ML can operate effectively without internet connectivity.</p></li>
<li><p>Discuss the trade-offs involved in optimizing machine learning models for mobile devices.</p></li>
<li><p>____ is a technique used in Mobile ML to reduce model size and speed up inference while maintaining accuracy.</p></li>
<li><p>In a production system, how might Mobile ML enhance user experience in real-time applications?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-mobile-machine-learning-f5b5" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-tiny-machine-learning-9d4a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-tiny-machine-learning-9d4a">Tiny Machine Learning</h2>
<p>Mobile ML delivers personal AI but still requires sophisticated hardware with significant power and memory resources. Tiny Machine Learning pushes the deployment spectrum to its extreme, bringing intelligence to the smallest, most resource-constrained devices where traditional computing approaches become impossible. This paradigm prioritizes ultra-low power consumption and minimal cost over computational sophistication.</p>
<p>Tiny ML brings intelligence to the smallest devices, from microcontrollers<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a> to embedded sensors, enabling real-time computation in severely resource-constrained environments. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and extreme energy efficiency. Tiny ML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>, often running for months or years on limited power sources such as coin-cell batteries<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a>. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical.</p>
<div class="no-row-height column-margin column-container"><div id="fn48"><p><sup>48</sup>&nbsp;<strong>Microcontrollers</strong>: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM, still thousands of times less than a smartphone.</p></div><div id="fn49"><p><sup>49</sup>&nbsp;<strong>Energy Efficiency in TinyML</strong>: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume &lt;1ÂµW in sleep mode and 100-300ÂµW/MHz when active. Efficient ML inference can run for years on a single coin-cell battery.</p></div><div id="fn50"><p><sup>50</sup>&nbsp;<strong>Coin-Cell Batteries</strong>: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling â€œdeploy-and-forgetâ€ IoT applications.</p></div></div><div id="callout-definition*-1.4" class="callout callout-definition" title="Definition of Tiny ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Tiny ML</summary><div><strong>Tiny Machine Learning (Tiny ML)</strong> refers to the execution of machine learning models on <em>ultra-constrained devices</em>, such as microcontrollers and sensors. These devices operate in the <em>milliwatt to sub-watt</em> power range, prioritizing <em>energy efficiency</em> and <em>compactness</em>. Tiny ML enables <em>localized decision making</em> in resource constrained environments, excelling in applications where <em>extended operation on limited power sources</em> is required. However, it is limited by <em>severely restricted computational resources</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Tiny ML through four critical dimensions that define its unique position in the ML deployment spectrum. <a href="#fig-tiny-ml" class="quarto-xref">Figure&nbsp;8</a> encapsulates the key aspects of Tiny ML discussed in this section.</p>
<div id="fig-tiny-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tiny-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="8ffc2616fc4390e0fec58576b8f19dc1081feeea.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: TinyML System Characteristics: Constrained devices necessitate a focus on efficiency, driving trade-offs between model complexity, accuracy, and energy consumption, while enabling localized intelligence and real-time responsiveness in embedded applications. This figure outlines key aspects of TinyML, including the challenges of resource limitations, example applications, and the benefits of on-device machine learning."><img src="ml_systems_files/mediabag/8ffc2616fc4390e0fec58576b8f19dc1081feeea.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tiny-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>TinyML System Characteristics</strong>: Constrained devices necessitate a focus on efficiency, driving trade-offs between model complexity, accuracy, and energy consumption, while enabling localized intelligence and real-time responsiveness in embedded applications. This figure outlines key aspects of TinyML, including the challenges of resource limitations, example applications, and the benefits of on-device machine learning.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-d52d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-d52d">Characteristics</h3>
<p>Tiny ML focuses on on device machine learning, similar to Mobile ML. Machine learning models are deployed and trained on the device<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a>, eliminating the need for external servers or cloud infrastructures. This enables intelligent decision making where data is generated, making real time insights and actions possible, even in settings where connectivity is limited or unavailable.</p>
<div class="no-row-height column-margin column-container"><div id="fn51"><p><sup>51</sup>&nbsp;<strong>On-Device Training Constraints</strong>: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation where multiple devices collaboratively train a shared model.</p></div><div id="fn52"><p><sup>52</sup>&nbsp;<strong>TinyML Device Scale</strong>: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Googleâ€™s Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously â€œdumbâ€ objects like smart dust sensors.</p></div></div><p>Tiny ML excels in low power and resource constrained settings. These environments require highly optimized solutions that function within available resources. <a href="#fig-TinyML-example" class="quarto-xref">Figure&nbsp;9</a> shows an example Tiny ML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>. Tiny ML meets efficiency requirements through specialized algorithms and models designed to deliver acceptable performance while consuming minimal energy, ensuring extended operational periods, even in battery powered devices like those shown.</p>
<div id="fig-TinyML-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-TinyML-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/tiny_ml.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: TinyML System Scale: These device kits exemplify the extreme miniaturization achievable with TinyML, enabling deployment of machine learning on resource-constrained devices with limited power and memory. such compact systems broaden the applicability of ML to previously inaccessible edge applications, including wearable sensors and embedded IoT devices. Source: [@warden2018speech]"><img src="images/png/tiny_ml.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-TinyML-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>TinyML System Scale</strong>: These device kits exemplify the extreme miniaturization achievable with TinyML, enabling deployment of machine learning on resource-constrained devices with limited power and memory. such compact systems broaden the applicability of ML to previously inaccessible edge applications, including wearable sensors and embedded IoT devices. Source: <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>â€œSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.â€</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="https://doi.org/10.48550/arXiv.1804.03209">https://doi.org/10.48550/arXiv.1804.03209</a>.
</div></div></figure>
</div>
</section>
<section id="sec-ml-systems-benefits-020f" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-020f">Benefits</h3>
<p>Tiny MLâ€™s primary benefit is ultra low latency. Since computation occurs directly on the device, the time required to send data to external servers and receive responses is eliminated. This proves crucial in applications requiring immediate decision making, enabling quick responses to changing conditions.</p>
<p>Tiny ML inherently enhances data security. Because data processing and analysis happen on the device, the risk of data interception during transmission is virtually eliminated. This localized approach to data management ensures that sensitive information stays on the device, strengthening user data security.</p>
<p>Tiny ML operates within an energy efficient framework that enables the extended operational periods described in the characteristics above, making it a sustainable option for long-term deployments.</p>
</section>
<section id="sec-ml-systems-challenges-297b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-297b">Challenges</h3>
<p>Achieving these remarkable benefits requires accepting substantial constraints. Tiny ML faces significant challenges. The primary limitation is constrained computational capabilities. Operating within such limits requires simplified models, which can affect solution accuracy and sophistication.</p>
<p>Tiny ML introduces complex development cycles. Crafting lightweight and effective models demands deep understanding of machine learning principles and embedded systems expertise. This complexity requires collaborative development approaches where multi domain expertise is essential for success.</p>
<p>A central challenge in Tiny ML is model optimization and compression<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a>. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints.</p>
<div class="no-row-height column-margin column-container"><div id="fn53"><p><sup>53</sup>&nbsp;<strong>TinyML Model Optimization</strong>: Specialized techniques that dramatically reduce model size and computational requirements. A typical smartphone model of 50MB might optimize to 250KB for microcontroller deployment while retaining 95% accuracy (techniques detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>).</p></div></div></section>
<section id="sec-ml-systems-use-cases-3c3f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-3c3f">Use Cases</h3>
<p>Despite these formidable challenges, Tiny ML has found successful applications across diverse domains, demonstrating that careful optimization enables sophisticated intelligence on even the most constrained devices.</p>
<p>In wearables, Tiny ML opens the door to smarter, more responsive gadgets. From fitness trackers<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> offering real-time workout feedback to smart glasses<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a> processing visual data on the fly, Tiny ML transforms how we engage with wearable tech, delivering personalized experiences directly from the device.</p>
<div class="no-row-height column-margin column-container"><div id="fn54"><p><sup>54</sup>&nbsp;<strong>TinyML in Fitness Trackers</strong>: Modern fitness trackers use TinyML for activity recognition, sleep analysis, and health monitoring. Apple Watch can detect falls using accelerometer data and on-device ML, automatically calling emergency services. The algorithm analyzes motion patterns in real-time using &lt;1mW power.</p></div><div id="fn55"><p><sup>55</sup>&nbsp;<strong>Smart Glasses with TinyML</strong>: Google Glass Enterprise uses TinyML for real-time object recognition and barcode scanning. The glasses process visual data locally using specialized vision chips consuming &lt;500mW, enabling 8+ hour operation while providing instant augmented reality overlays.</p></div></div><p>In industrial settings, Tiny ML plays a significant role in predictive maintenance. By deploying Tiny ML algorithms on sensors that monitor equipment health, companies can preemptively identify potential issues, reducing downtime and preventing costly breakdowns. On-site data analysis ensures quick responses, potentially stopping minor issues from becoming major problems.</p>
<p>Tiny ML can be employed to create anomaly detection models that identify unusual data patterns. For instance, a smart factory could use Tiny ML to monitor industrial processes and spot anomalies, helping prevent accidents and improve product quality. Similarly, a security company could use Tiny ML to monitor network traffic for unusual patterns, aiding in detecting and preventing cyber-attacks. Tiny ML could monitor patient data for anomalies in healthcare, aiding early disease detection and better patient treatment.</p>
<p>In environmental monitoring, Tiny ML enables real-time data analysis from various field-deployed sensors. These could range from city air quality monitoring to wildlife tracking in protected areas. Through Tiny ML, data can be processed locally, allowing for quick responses to changing conditions and providing a nuanced understanding of environmental patterns, crucial for informed decision making.</p>
<p>Tiny ML represents a significant development in machine learning evolution, bringing intelligence directly to the edge across various fields. This approach enables devices to become connected, intelligent, and capable of making real-time decisions and responses.</p>
<div id="quiz-question-sec-ml-systems-tiny-machine-learning-9d4a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary challenge when implementing Tiny ML on microcontrollers?</p>
<ol type="a">
<li>Complex development cycle</li>
<li>High computational power availability</li>
<li>Unlimited memory resources</li>
<li>High energy consumption</li>
</ol></li>
<li><p>True or False: Tiny ML devices typically require constant connectivity to external servers for data processing.</p></li>
<li><p>Explain how Tiny ML enhances data security in IoT applications.</p></li>
<li><p>Order the following benefits of Tiny ML in terms of their impact on system performance: (1) Ultra-low latency, (2) High data security, (3) Energy efficiency.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-tiny-machine-learning-9d4a" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-hybrid-machine-learning-1bbf" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-hybrid-machine-learning-1bbf">Hybrid Machine Learning</h2>
<p>The increasingly complex demands of modern applications often require a blend of machine learning approaches. Hybrid Machine Learning (Hybrid ML) combines the computational power of the cloud, the efficiency of edge and mobile devices, and the compact capabilities of Tiny ML. This approach enables architects to create systems that balance performance, privacy, and resource efficiency, addressing real-world challenges with innovative, distributed solutions.</p>
<div id="callout-definition*-1.5" class="callout callout-definition" title="Definition of Hybrid ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Hybrid ML</summary><div><strong>Hybrid Machine Learning (Hybrid ML)</strong> refers to the integration of multiple ML paradigms, such as Cloud, Edge, Mobile, and Tiny ML, to form a unified, distributed system. These systems leverage the <em>complementary strengths</em> of each paradigm while addressing their <em>individual limitations</em>. Hybrid ML supports <em>scalability, adaptability,</em> and <em>privacy-preserving capabilities,</em> enabling sophisticated ML applications for diverse scenarios. By combining centralized and decentralized computing, Hybrid ML facilitates efficient resource utilization while meeting the demands of complex real-world requirements.<p></p>
</div></details>
</div>
<section id="sec-ml-systems-design-patterns-ade8" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-design-patterns-ade8">Design Patterns</h3>
<p>Design patterns in Hybrid ML represent reusable solutions to common challenges faced when integrating multiple ML paradigms (cloud, edge, mobile, and tiny). These patterns guide system architects in combining the strengths of different approaches, including the computational power of the cloud and the efficiency of edge devices, while mitigating their individual limitations. By following these patterns, architects can address key trade-offs in performance, latency, privacy, and resource efficiency.</p>
<p>Hybrid ML design patterns serve as blueprints, enabling the creation of scalable, efficient, and adaptive systems tailored to diverse real-world applications. Each pattern reflects a specific strategy for organizing and deploying ML workloads across different tiers of a distributed system, ensuring optimal use of available resources while meeting application-specific requirements.</p>
<p>The following sections examine five essential patterns that address common integration challenges in hybrid ML systems.</p>
<section id="sec-ml-systems-trainserve-split-0d17" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-trainserve-split-0d17">Train-Serve Split</h4>
<p>One of the most common hybrid patterns is the train-serve split, where model training occurs in the cloud but inference happens on edge, mobile, or tiny devices. This pattern takes advantage of the cloudâ€™s vast computational resources for the training phase while benefiting from the low latency and privacy advantages of on-device inference<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a>. For example, smart home devices often use models trained on large datasets in the cloud but run inference locally to ensure quick response times and protect user privacy. In practice, this might involve training models on powerful systems like the NVIDIA DGX A100, utilizing its 8 A100 GPUs and terabyte-scale memory, before deploying optimized versions to edge devices like the NVIDIA Jetson AGX Orin for efficient inference. Similarly, mobile vision models for computational photography are typically trained on powerful cloud infrastructure but deployed to run efficiently on phone hardware.</p>
<div class="no-row-height column-margin column-container"><div id="fn56"><p><sup>56</sup>&nbsp;<strong>Train-Serve Split Economics</strong>: Training large models can cost $1-10M (GPT-3: $4.6M) but inference costs &lt;$0.01 per query when deployed efficiently <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. This 1,000,000x cost difference drives the pattern of expensive cloud training with cheap edge inference.</p><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>â€œLanguage Models Are Few-Shot Learners.â€</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877â€“1901. <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</a>.
</div></div></div></section>
<section id="sec-ml-systems-hierarchical-processing-6114" class="level4">
<h4 class="anchored" data-anchor-id="sec-ml-systems-hierarchical-processing-6114">Hierarchical Processing</h4>
<p>Hierarchical processing creates a multi-tier system where data and intelligence flow between different levels of the ML stack. In industrial IoT applications, tiny sensors might perform basic anomaly detection, edge devices aggregate and analyze data from multiple sensors, and cloud systems handle complex analytics and model updates. For instance, we might see ESP32-CAM devices performing basic image classification at the sensor level with their minimal 520 KB RAM, feeding data up to Jetson AGX Orin devices for more sophisticated computer vision tasks, and ultimately connecting to cloud infrastructure for complex analytics and model updates.</p>
<p>This hierarchy allows each tier to handle tasks appropriate to its capabilities. Tiny ML devices handle immediate, simple decisions; edge devices manage local coordination; and cloud systems tackle complex analytics and learning tasks. Smart city installations often use this pattern, with street-level sensors feeding data to neighborhood-level edge processors, which in turn connect to city-wide cloud analytics.</p>
</section>
<section id="sec-ml-systems-progressive-deployment-2570" class="level4">
<h4 class="anchored" data-anchor-id="sec-ml-systems-progressive-deployment-2570">Progressive Deployment</h4>
<p>Progressive deployment strategies create tiered intelligence architectures that balance computational capability with resource constraints through systematic model compression across deployment tiers. These strategies adapt models for different computational tiers, creating a cascade of increasingly lightweight versions. A model might start as a large, complex version in the cloud, then be progressively compressed and optimized for edge servers, mobile devices, and finally tiny sensors. The optimization techniques explored in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong> enable this progressive deployment strategy.</p>
<p>Consider Amazon Alexaâ€™s production implementation: wake-word detection uses &lt;1KB models on TinyML devices consuming &lt;1mW, edge processing handles simple commands with 1-10MB models at 1-10W power consumption, while complex natural language understanding requires GB+ models in cloud infrastructure. Voice assistant systems exemplify this pattern, where full natural language processing runs in the cloud, while simplified wake-word detection runs on-device. This tiered approach reduces cloud inference costs by 95% while maintaining user experience, demonstrating how production systems achieve both technical performance and economic viability by balancing capability and resource constraints across the ML stack.</p>
<p>While this approach offers significant technical and economic benefits, it also introduces operational complexity including model versioning across tiers, ensuring consistency between model generations, managing failure cascades when network connectivity is lost, and coordinating updates across millions of deployed devices. Production teams must maintain specialized expertise for TinyML optimization, edge orchestration, and cloud scaling, creating significant organizational challenges that affect deployment decisions beyond pure technical requirements.</p>
</section>
<section id="sec-ml-systems-federated-learning-bf9c" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-federated-learning-bf9c">Federated Learning</h4>
<p>Federated learning<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a> represents a sophisticated hybrid approach that addresses the production challenge of learning from distributed data while maintaining privacy compliance. Googleâ€™s production federated learning system processes 6 billion mobile keyboards, training improved models while keeping all typed text local, achieving both privacy compliance and model improvement at scale. Each federated learning round involves 100-10,000 devices contributing model updates, requiring sophisticated orchestration to manage device availability, network conditions, and computational heterogeneity. Production federated systems face unique operational challenges including device dropout rates of 50-90% during training rounds, network bandwidth constraints that limit update frequency, and the need for differential privacy mechanisms to prevent information leakage. The aggregation servers must handle intermittent connectivity, varying computational capabilities across device types, and ensure robust convergence despite non-IID data distributions. These systems require specialized monitoring infrastructure to track training progress across distributed populations, debug convergence issues without accessing raw data, and manage the complex interplay between local learning rates and global aggregation strategies, creating operational complexity that significantly exceeds traditional centralized training deployments.</p>
<div class="no-row-height column-margin column-container"><div id="fn57"><p><sup>57</sup>&nbsp;<strong>Federated Learning Architecture</strong>: Coordinates learning across millions of devices without centralizing data <span class="citation" data-cites="mcmahan2017federated">(<a href="#ref-mcmahan2017federated" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Googleâ€™s federated learning processes 6 billion mobile keyboards, training improved models while keeping all typed text local. Each round involves 100-10,000 devices contributing model updates.</p><div id="ref-mcmahan2017federated" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgÃ¼era y Arcas. 2017. <span>â€œCommunication-Efficient Learning of Deep Networks from Decentralized Data.â€</span> In <em>Artificial Intelligence and Statistics</em>, 1273â€“82. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div></div></section>
<section id="sec-ml-systems-collaborative-learning-7f59" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-collaborative-learning-7f59">Collaborative Learning</h4>
<p>Collaborative learning enables peer-to-peer learning between devices at the same tier, often complementing hierarchical structures.<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a> Autonomous vehicle fleets, for example, might share learning about road conditions or traffic patterns directly between vehicles while also communicating with cloud infrastructure. This horizontal collaboration allows systems to share time-sensitive information and learn from each otherâ€™s experiences without always routing through central servers.</p>
<div class="no-row-height column-margin column-container"><div id="fn58"><p><sup>58</sup>&nbsp;<strong>Tiered Voice Processing</strong>: Amazon Alexa uses a 3-tier system: tiny wake-word detection on-device (&lt;1KB model), edge processing for simple commands (1-10MB models), and cloud processing for complex queries (GB+ models). This reduces cloud costs by 95% while maintaining functionality.</p></div></div></section>
</section>
<section id="sec-ml-systems-realworld-integration-0815" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-realworld-integration-0815">Real-World Integration</h3>
<p>While these design patterns provide valuable templates for hybrid system architecture, real-world implementations require integrating multiple patterns into cohesive solutions. Design patterns establish a foundation for organizing and optimizing ML workloads across distributed systems. However, the practical application of these patterns often requires combining multiple paradigms into integrated workflows. Thus, in practice, ML systems rarely operate in isolation. Instead, they form interconnected networks where each paradigm, including Cloud, Edge, Mobile, and Tiny ML, plays a specific role while communicating with other parts of the system. These interconnected networks follow integration patterns that assign specific roles to Cloud, Edge, Mobile, and Tiny ML systems based on their unique strengths and limitations. Recall that cloud systems excel at training and analytics but require significant infrastructure. Edge systems provide local processing power and reduced latency. Mobile devices offer personal computing capabilities and user interaction. Tiny ML enables intelligence in the smallest devices and sensors.</p>
<p><a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a> illustrates these key interactions through specific connection types: â€œDeployâ€ paths show how models flow from cloud training to various devices, â€œDataâ€ and â€œResultsâ€ show information flow from sensors through processing stages, â€œAnalyzeâ€ shows how processed information reaches cloud analytics, and â€œSyncâ€ demonstrates device coordination. Notice how data generally flows upward from sensors through processing layers to cloud analytics, while model deployments flow downward from cloud training to various inference points. The interactions arenâ€™t strictly hierarchical. Mobile devices might communicate directly with both cloud services and tiny sensors, while edge systems can assist mobile devices with complex processing tasks.</p>
<div id="fig-hybrid" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hybrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="047b9fba8d702fcb8a171c8518556b1e6617c672.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Hybrid System Interactions: Data flows upward from sensors through processing layers to cloud analytics for insights, while trained models deploy downward from the cloud to enable inference at the edge, mobile, and Tiny ML devices. These connection types (deploy, data/results, analyze, and sync) establish a distributed architecture where each paradigm contributes unique capabilities to the overall machine learning system."><img src="ml_systems_files/mediabag/047b9fba8d702fcb8a171c8518556b1e6617c672.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hybrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Hybrid System Interactions</strong>: Data flows upward from sensors through processing layers to cloud analytics for insights, while trained models deploy downward from the cloud to enable inference at the edge, mobile, and Tiny ML devices. These connection types (deploy, data/results, analyze, and sync) establish a distributed architecture where each paradigm contributes unique capabilities to the overall machine learning system.
</figcaption>
</figure>
</div>
<p>To understand how these labeled interactions manifest in real applications, several common scenarios using <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a> illustrate the connections:</p>
<ul>
<li><p><strong>Model Deployment Scenario</strong>: A company develops a computer vision model for defect detection. Following the â€œDeployâ€ paths shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a>, the cloud-trained model is distributed to edge servers in factories, quality control tablets on the production floor, and tiny cameras embedded in the production line. This showcases how a single ML solution can be distributed across different computational tiers for optimal performance.</p></li>
<li><p><strong>Data Flow and Analysis Scenario</strong>: In a smart agriculture system, soil sensors (Tiny ML) collect moisture and nutrient data, following the â€œDataâ€ path to Tiny ML inference. The â€œResultsâ€ flow to edge processors in local stations, which process this information and use the â€œAnalyzeâ€ path to send insights to the cloud for farm-wide analytics, while also sharing results with farmersâ€™ mobile apps. This demonstrates the hierarchical flow shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a> from sensors through processing to cloud analytics.</p></li>
<li><p><strong>Edge-Mobile Assistance Scenario</strong>: When a mobile app needs to perform complex image processing that exceeds the phoneâ€™s capabilities, it utilizes the â€œAssistâ€ connection shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a>. The edge system helps process the heavier computational tasks, sending back results to enhance the mobile appâ€™s performance. This shows how different ML tiers can cooperate to handle demanding tasks.</p></li>
<li><p><strong>Tiny ML-Mobile Integration Scenario</strong>: A fitness tracker uses Tiny ML to continuously monitor activity patterns and vital signs. Using the â€œSyncâ€ pathway shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a>, it synchronizes this processed data with the userâ€™s smartphone, which combines it with other health data before sending consolidated updates via the â€œAnalyzeâ€ path to the cloud for long-term health analysis. This illustrates the common pattern of tiny devices using mobile devices as gateways to larger networks.</p></li>
<li><p><strong>Multi-Layer Processing Scenario</strong>: In a smart retail environment, tiny sensors monitor inventory levels, using â€œDataâ€ and â€œResultsâ€ paths to send inference results to both edge systems for immediate stock management and mobile devices for staff notifications. Following the â€œAnalyzeâ€ path, the edge systems process this data alongside other store metrics, while the cloud analyzes trends across all store locations. This demonstrates how the interactions shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a> enable ML tiers to work together in a complete solution.</p></li>
</ul>
<p>These real-world patterns demonstrate how different ML paradigms naturally complement each other in practice. While each approach has its own strengths, their true power emerges when they work together as an integrated system. By understanding these patterns, system architects can better design solutions that effectively leverage the capabilities of each ML tier while managing their respective constraints.</p>
<div id="quiz-question-sec-ml-systems-hybrid-machine-learning-1bbf" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>What is the primary advantage of using a Hybrid Machine Learning approach?</p>
<ol type="a">
<li>It reduces the need for data privacy measures.</li>
<li>It eliminates the need for cloud-based training.</li>
<li>It combines the strengths of different ML paradigms while addressing their limitations.</li>
<li>It focuses solely on edge device processing.</li>
</ol></li>
<li><p>Explain how the train-serve split pattern in Hybrid ML benefits real-time applications.</p></li>
<li><p>Order the following ML paradigms based on their typical role in a Hybrid ML system from data collection to complex analytics: (1) Cloud ML, (2) Edge ML, (3) Tiny ML.</p></li>
<li><p>True or False: Federated learning in Hybrid ML allows for model training on edge devices while maintaining data privacy.</p></li>
<li><p>In a production system, how might you apply the hierarchical processing pattern to optimize resource utilization?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-hybrid-machine-learning-1bbf" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-shared-principles-34fe" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-shared-principles-34fe">Shared Principles</h2>
<p>Having examined each deployment paradigm individually, we can identify the underlying principles that unite all ML systems. Understanding these shared foundations explains why hybrid approaches work effectively and how techniques transfer between paradigms, providing the foundation for systematic comparison and decision making.</p>
<p>The design and integration patterns illustrate how ML paradigms interact to address real-world challenges. While each paradigm is tailored to specific roles, their interactions reveal recurring principles that guide effective system design. These shared principles provide a unifying framework for understanding both individual ML paradigms and their hybrid combinations. These principles reveal a deeper system design perspective, showing how different ML implementations optimized for distinct contexts converge around core concepts. This convergence forms the foundation for systematically understanding ML systems, despite their diversity and breadth.</p>
<p><a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> illustrates this convergence, highlighting the relationships that underpin practical system design and implementation. Understanding these principles is valuable for working with individual ML systems and for developing hybrid solutions that leverage their strengths, mitigate their limitations, and create cohesive, efficient ML workflows.</p>
<div id="fig-ml-systems-convergence" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-systems-convergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5fedc48a7e99a893f124b30012db0e32753613d4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Convergence of ML Systems: Diverse machine learning deployments (cloud, edge, mobile, and tiny) share foundational principles in data pipelines, resource management, and system architecture, enabling hybrid solutions and systematic design approaches. Understanding these shared principles allows practitioners to adapt techniques across different paradigms and build cohesive, efficient ML workflows despite varying constraints and optimization goals."><img src="ml_systems_files/mediabag/5fedc48a7e99a893f124b30012db0e32753613d4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-systems-convergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Convergence of ML Systems</strong>: Diverse machine learning deployments (cloud, edge, mobile, and tiny) share foundational principles in data pipelines, resource management, and system architecture, enabling hybrid solutions and systematic design approaches. Understanding these shared principles allows practitioners to adapt techniques across different paradigms and build cohesive, efficient ML workflows despite varying constraints and optimization goals.
</figcaption>
</figure>
</div>
<p>This layered analysis reveals why seemingly different ML paradigms share core principles, enabling hybrid approaches and technique transfer between paradigms. At the top, we see the diverse implementations explored throughout this chapter. Cloud ML operates in data centers, focusing on training at scale with vast computational resources. Edge ML emphasizes local processing with inference capabilities closer to data sources. Mobile ML leverages personal devices for user-centric applications. Tiny ML brings intelligence to highly constrained embedded systems and sensors.</p>
<p>Despite their distinct characteristics, the arrows in the figure show how all these implementations connect to the same core system principles. This reflects an important reality in ML systems: even though they may operate at dramatically different scales, from cloud systems processing petabytes to tiny devices handling kilobytes, they all must solve similar fundamental challenges:</p>
<ul>
<li>Managing data pipelines from collection through processing to deployment as detailed in <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong></li>
<li>Balancing resource utilization across compute, memory, energy, and network</li>
<li>Implementing system architectures that effectively integrate models, hardware, and software</li>
</ul>
<p>Core principles lead to shared system considerations around optimization, operations, and trustworthiness. This progression explains why techniques developed for one scale of ML system often transfer effectively to others. The underlying problems (efficiently processing data, managing resources, and ensuring reliable operation) remain consistent even as specific solutions vary based on scale and context.</p>
<p>This convergence becomes particularly valuable as systems move toward hybrid ML architectures. When different ML implementations share fundamental principles, combining them effectively becomes more intuitive. This explains why, for example, a cloud-trained model can be effectively deployed to edge devices, or why mobile and Tiny ML systems can complement each other in IoT applications. This cross-deployment flexibility relies on model optimization techniques (<strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) and careful management of hybrid system operations (<strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>).</p>
<section id="sec-ml-systems-implementation-layer-9002" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-implementation-layer-9002">Implementation Layer</h3>
<p>The top layer of <a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> represents the diverse landscape of ML systems weâ€™ve explored throughout this chapter. Each implementation addresses specific needs and operational contexts, yet all contribute to the broader ecosystem of ML deployment options.</p>
<p>Cloud ML, centered in data centers, provides the foundation for large scale training and complex model serving. With access to vast computational resources like the NVIDIA DGX A100 systems we saw in <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a>, cloud implementations excel at handling massive datasets and training sophisticated models. This makes them particularly suited for tasks requiring extensive computational power, such as training foundation models or processing large-scale analytics. The specialized hardware architectures that enable this computational power are explored as detailed in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<p>Edge ML shifts the focus to local processing, prioritizing inference capabilities closer to data sources. Using devices like the NVIDIA Jetson AGX Orin, edge implementations balance computational power with reduced latency and improved privacy. This approach proves especially valuable in scenarios requiring quick decisions based on local data, such as industrial automation or real-time video analytics.</p>
<p>Mobile ML leverages the capabilities of personal devices, particularly smartphones and tablets. With specialized hardware like Appleâ€™s A17 Pro chip, mobile implementations enable sophisticated ML capabilities while maintaining user privacy and providing offline functionality. This paradigm has revolutionized applications from computational photography to on-device speech recognition while ensuring user data remains on-device for enhanced privacy.</p>
<p>Tiny ML represents the frontier of embedded ML, bringing intelligence to highly constrained devices. Operating on microcontrollers like the Arduino Nano 33 BLE Sense, tiny implementations must carefully balance functionality with severe resource constraints. Despite these limitations, Tiny ML enables ML capabilities in scenarios where power efficiency and size constraints are paramount.</p>
</section>
<section id="sec-ml-systems-system-principles-layer-db81" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-system-principles-layer-db81">System Principles Layer</h3>
<p>The middle layer reveals the fundamental principles that unite all ML systems, regardless of their implementation scale. These core principles remain consistent even as their specific manifestations vary dramatically across different deployments.</p>
<p>Data Pipeline principles govern how systems handle information flow, from initial collection through processing to final deployment. In cloud systems, this might mean processing petabytes of data through distributed pipelines. For tiny systems, it could involve carefully managing sensor data streams within limited memory. Despite these scale differences, all systems must address the same fundamental challenges of data ingestion, transformation, and utilization.</p>
<p>Resource Management emerges as a universal challenge across all implementations. Whether managing thousands of GPUs in a data center or optimizing battery life on a microcontroller, all systems must balance competing demands for computation, memory, energy, and network resources. The quantities involved may differ by orders of magnitude, but the core principles of resource allocation and optimization remain remarkably consistent.</p>
<p>System Architecture principles guide how ML systems integrate models, hardware, and software components. Cloud architectures might focus on distributed computing and scalability, while tiny systems emphasize efficient memory mapping and interrupt handling. Yet all must solve fundamental problems of component integration, data flow optimization, and processing coordination.</p>
</section>
<section id="sec-ml-systems-system-considerations-layer-660c" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-system-considerations-layer-660c">System Considerations Layer</h3>
<p>The bottom layer of <a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> illustrates how fundamental principles manifest in practical system-wide considerations. These considerations span all ML implementations, though their specific challenges and solutions vary based on scale and context.</p>
<p><strong>Optimization and Efficiency</strong> shape how ML systems balance performance with resource utilization. In cloud environments, this often means optimizing model training across GPU clusters while managing energy consumption in data centers. Edge systems focus on reducing model size and accelerating inference without compromising accuracy. Mobile implementations must balance model performance with battery life and thermal constraints. Tiny ML pushes optimization to its limits, requiring extensive model compression and quantization to fit within severely constrained environments. Despite these different emphases, all implementations grapple with the core challenge of maximizing performance within their available resources.</p>
<p><strong>Operational Aspects</strong> affect how ML systems are deployed, monitored, and maintained in production environments. Cloud systems must handle continuous deployment across distributed infrastructure while monitoring model performance at scale. Edge implementations need robust update mechanisms and health monitoring across potentially thousands of devices. Mobile systems require seamless app updates and performance monitoring without disrupting user experience. Tiny ML faces unique challenges in deploying updates to embedded devices while ensuring continuous operation. Across all scales, the fundamental problems of deployment, monitoring, and maintenance remain consistent, even as solutions vary.</p>
<p><strong>Trustworthy AI</strong> considerations ensure ML systems operate reliably, securely, and with appropriate privacy protections. Cloud implementations must secure massive amounts of data while ensuring model predictions remain reliable at scale. Edge systems need to protect local data processing while maintaining model accuracy in diverse environments. Mobile ML must preserve user privacy while delivering consistent performance. Tiny ML systems, despite their size, must still ensure secure operation and reliable inference. These trustworthiness considerations cut across all implementations, reflecting the critical importance of building ML systems that users can depend on.</p>
<p>The progression through these layers, from diverse implementations through core principles to shared considerations, reveals why ML systems can be studied as a unified field despite their apparent differences. While specific solutions may vary dramatically based on scale and context, the fundamental challenges remain remarkably consistent. This understanding becomes particularly valuable as we move toward increasingly sophisticated hybrid systems that combine multiple implementation approaches.</p>
<p>This convergence of fundamental principles across ML implementations helps explain why hybrid approaches work so effectively in practice. As we saw in our discussion of hybrid ML, different implementations naturally complement each other precisely because they share these core foundations. Whether weâ€™re looking at train-serve splits that leverage cloud resources for training and edge devices for inference, or hierarchical processing that combines Tiny ML sensors with edge aggregation and cloud analytics, the shared principles enable seamless integration across scales.</p>
</section>
<section id="sec-ml-systems-principles-practice-907d" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-principles-practice-907d">Principles to Practice</h3>
<p>Convergence of principles explains why techniques and insights transfer well between different scales of ML systems. Understanding data pipelines in cloud environments informs data flow structure in embedded systems. Resource management strategies developed for mobile devices inspire new approaches to cloud optimization. System architecture patterns effective at one scale often adapt well to others.</p>
<p>Understanding these fundamental principles and shared considerations provides a foundation for comparing different ML implementations effectively. While each approach has distinct characteristics and optimal use cases, they all build upon the same core elements. This foundation of shared principles makes systematic comparison possible, revealing the precise trade-offs that should guide architectural decisions.</p>
<div id="quiz-question-sec-ml-systems-shared-principles-34fe" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the shared principles that unify different ML paradigms?</p>
<ol type="a">
<li>They provide a framework for understanding and integrating diverse ML implementations.</li>
<li>They focus solely on optimizing computational resources.</li>
<li>They are specific to cloud-based ML systems.</li>
<li>They are applicable only to resource-constrained environments.</li>
</ol></li>
<li><p>How do shared principles in ML systems facilitate the development of hybrid solutions?</p></li>
<li><p>True or False: The core principles of ML systems, such as resource management and system architecture, vary significantly between cloud and tiny ML implementations.</p></li>
<li><p>What is a key benefit of understanding the convergence of ML system principles?</p>
<ol type="a">
<li>It allows for the exclusive use of cloud resources.</li>
<li>It limits the application of ML to specific environments.</li>
<li>It enables the development of more efficient and cohesive ML workflows.</li>
<li>It simplifies the design of ML systems by focusing only on hardware constraints.</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-shared-principles-34fe" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-system-comparison-8b05" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-system-comparison-8b05">System Comparison</h2>
<p>Building from this understanding of shared principles, systematic comparison across deployment paradigms reveals the precise trade-offs that should drive deployment decisions and highlights scenarios where each paradigm excels, providing practitioners with analytical frameworks for making informed architectural choices.</p>
<p>The relationship between computational resources and deployment location forms one of the most fundamental comparisons across ML systems. As we move from cloud deployments to tiny devices, we observe a dramatic reduction in available computing power, storage, and energy consumption. Cloud ML systems, with their data center infrastructure, can leverage virtually unlimited resources, processing data at the scale of petabytes and training models with billions of parameters. Edge ML systems, while more constrained, still offer significant computational capability through specialized hardware like edge GPUs and neural processing units. Mobile ML represents a middle ground, balancing computational power with energy efficiency on devices like smartphones and tablets. At the far end of the spectrum, TinyML operates under severe resource constraints, often limited to kilobytes of memory and milliwatts of power consumption.</p>
<div id="tbl-big_vs_tiny" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-big_vs_tiny-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Deployment Locations</strong>: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. This table categorizes these deployments by their processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation.
</figcaption>
<div aria-describedby="tbl-big_vs_tiny-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Cloud ML</th>
<th style="text-align: left;">Edge ML</th>
<th style="text-align: left;">Mobile ML</th>
<th style="text-align: left;">Tiny ML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Performance</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Processing Location</td>
<td style="text-align: left;">Centralized cloud servers (Data Centers)</td>
<td style="text-align: left;">Local edge devices (gateways, servers)</td>
<td style="text-align: left;">Smartphones and tablets</td>
<td style="text-align: left;">Ultra-low-power microcontrollers and embedded systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Latency</td>
<td style="text-align: left;">High (100 ms-1000 ms+)</td>
<td style="text-align: left;">Moderate (10-100 ms)</td>
<td style="text-align: left;">Low-Moderate (5-50 ms)</td>
<td style="text-align: left;">Very Low (1-10 ms)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Compute Power</td>
<td style="text-align: left;">Very High (Multiple GPUs/TPUs)</td>
<td style="text-align: left;">High (Edge GPUs)</td>
<td style="text-align: left;">Moderate (Mobile NPUs/GPUs)</td>
<td style="text-align: left;">Very Low (MCU/tiny processors)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Storage Capacity</td>
<td style="text-align: left;">Unlimited (petabytes+)</td>
<td style="text-align: left;">Large (terabytes)</td>
<td style="text-align: left;">Moderate (gigabytes)</td>
<td style="text-align: left;">Very Limited (kilobytes-megabytes)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Energy Consumption</td>
<td style="text-align: left;">Very High (kW-MW range)</td>
<td style="text-align: left;">High (100 s W)</td>
<td style="text-align: left;">Moderate (1-10 W)</td>
<td style="text-align: left;">Very Low (mW range)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scalability</td>
<td style="text-align: left;">Excellent (virtually unlimited)</td>
<td style="text-align: left;">Good (limited by edge hardware)</td>
<td style="text-align: left;">Moderate (per-device scaling)</td>
<td style="text-align: left;">Limited (fixed hardware)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Operational</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Privacy</td>
<td style="text-align: left;">Basic-Moderate (Data leaves device)</td>
<td style="text-align: left;">High (Data stays in local network)</td>
<td style="text-align: left;">High (Data stays on phone)</td>
<td style="text-align: left;">Very High (Data never leaves sensor)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Connectivity Required</td>
<td style="text-align: left;">Constant high-bandwidth</td>
<td style="text-align: left;">Intermittent</td>
<td style="text-align: left;">Optional</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Offline Capability</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">Excellent</td>
<td style="text-align: left;">Complete</td>
</tr>
<tr class="even">
<td style="text-align: left;">Real-time Processing</td>
<td style="text-align: left;">Dependent on network</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">Very Good</td>
<td style="text-align: left;">Excellent</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Deployment</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Cost</td>
<td style="text-align: left;">High ($1000s+/month)</td>
<td style="text-align: left;">Moderate ($100s-1000s)</td>
<td style="text-align: left;">Low ($0-10s)</td>
<td style="text-align: left;">Very Low ($1-10s)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hardware Requirements</td>
<td style="text-align: left;">Cloud infrastructure</td>
<td style="text-align: left;">Edge servers/gateways</td>
<td style="text-align: left;">Modern smartphones</td>
<td style="text-align: left;">MCUs/embedded systems</td>
</tr>
<tr class="even">
<td style="text-align: left;">Development Complexity</td>
<td style="text-align: left;">High (cloud expertise needed)</td>
<td style="text-align: left;">Moderate-High (edge+networking)</td>
<td style="text-align: left;">Moderate (mobile SDKs)</td>
<td style="text-align: left;">High (embedded expertise)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deployment Speed</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Slow</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The operational characteristics of these systems reveal another important dimension of comparison. <a href="#tbl-big_vs_tiny" class="quarto-xref">Table&nbsp;2</a> organizes these characteristics into logical groupings, highlighting performance, operational considerations, costs, and development aspects. For instance, latency shows a clear gradient: cloud systems typically incur delays of 100-1000 ms due to network communication, while edge systems reduce this to 10-100 ms by processing data locally. Mobile ML achieves even lower latencies of 5-50 ms for many tasks, and TinyML systems can respond in 1-10 ms for simple inferences. Similarly, privacy and data handling improve progressively as computation shifts closer to the data source, with TinyML offering the strongest guarantees by keeping data entirely local to the device.</p>
<p>The table provides a high-level view of how these paradigms differ across key dimensions, making it easier to understand the trade-offs and select the most appropriate approach for specific deployment needs.</p>
<p>To complement the details presented in <a href="#tbl-big_vs_tiny" class="quarto-xref">Table&nbsp;2</a>, radar plots are presented below. These visualizations highlight two critical dimensions: performance characteristics and operational characteristics. The performance characteristics plot in <a href="#fig-op_char" class="quarto-xref">Figure&nbsp;12</a> a) focuses on latency, compute power, energy consumption, and scalability. As discussed earlier, Cloud ML demands exceptional compute power and demonstrates good scalability, making it ideal for large scale tasks requiring extensive resources. Tiny ML, in contrast, excels in latency and energy efficiency due to its lightweight and localized processing, suitable for low-power, real-time scenarios. Edge ML and Mobile ML strike a balance, offering moderate scalability and efficiency for a variety of applications.</p>
<div id="fig-op_char" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-op_char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bc71b7cb86d1a14fb63efcb143e44ef94bdea315.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: ML System Trade-Offs: Radar plots quantify performance and operational characteristics across cloud, edge, mobile, and Tiny ML paradigms, revealing inherent trade-offs between compute power, latency, energy consumption, and scalability. These visualizations enable informed selection of the most suitable deployment approach based on application-specific constraints and priorities."><img src="ml_systems_files/mediabag/bc71b7cb86d1a14fb63efcb143e44ef94bdea315.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-op_char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>ML System Trade-Offs</strong>: Radar plots quantify performance and operational characteristics across cloud, edge, mobile, and Tiny ML paradigms, revealing inherent trade-offs between compute power, latency, energy consumption, and scalability. These visualizations enable informed selection of the most suitable deployment approach based on application-specific constraints and priorities.
</figcaption>
</figure>
</div>
<p>The operational characteristics plot in <a href="#fig-op_char" class="quarto-xref">Figure&nbsp;12</a> b) emphasizes data privacy, connectivity independence, offline capability, and real-time processing. Tiny ML emerges as a highly independent and private paradigm, excelling in offline functionality and real-time responsiveness. In contrast, Cloud ML relies on centralized infrastructure and constant connectivity, which can be a limitation in scenarios demanding autonomy or low latency decision making.</p>
<p>Development complexity and deployment considerations also vary significantly across these paradigms. Cloud ML benefits from mature development tools and frameworks but requires expertise in cloud infrastructure. Edge ML demands knowledge of both ML and networking protocols, while Mobile ML developers must understand mobile-specific optimizations and platform constraints. TinyML development, though targeting simpler devices, often requires specialized knowledge of embedded systems and careful optimization to work within severe resource constraints.</p>
<p>Cost structures differ markedly as well. Cloud ML typically involves ongoing operational costs for computation and storage, often running into thousands of dollars monthly for large scale deployments. Edge ML requires significant upfront investment in edge devices but may reduce ongoing costs. Mobile ML leverages existing consumer devices, minimizing additional hardware costs, while TinyML solutions can be deployed for just a few dollars per device, though development costs may be higher.</p>
<p>Each paradigm has distinct advantages and limitations. Cloud ML excels at complex, data-intensive tasks but requires constant connectivity. Edge ML balances computational power with local processing. Mobile ML provides personalized intelligence on ubiquitous devices. TinyML enables ML in previously inaccessible contexts but demands careful optimization. Understanding these trade-offs proves crucial for selecting appropriate deployment strategies for specific applications and constraints.</p>
<div id="quiz-question-sec-ml-systems-system-comparison-8b05" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following ML deployment options is most suitable for applications requiring ultra-low latency and minimal energy consumption?</p>
<ol type="a">
<li>Tiny ML</li>
<li>Edge ML</li>
<li>Mobile ML</li>
<li>Cloud ML</li>
</ol></li>
<li><p>Explain the trade-offs involved in choosing Edge ML over Cloud ML for a real-time video processing application.</p></li>
<li><p>In a scenario where data privacy is a top priority, the most suitable ML deployment option is ____, as it ensures data never leaves the device.</p></li>
<li><p>Order the following ML deployment options by their typical latency from highest to lowest: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-system-comparison-8b05" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-ml-systems-deployment-decision-framework-824f" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-deployment-decision-framework-824f">Deployment Decision Framework</h2>
<p>The diverse paradigms of machine learning systems, including Cloud ML, Edge ML, Mobile ML, and Tiny ML, each have distinct characteristics, trade-offs, and use cases. Selecting an optimal deployment strategy requires careful consideration of multiple factors.</p>
<div id="fig-mlsys-playbook-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="!t" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-playbook-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="216fb8731d81432c618b0b4e7b8e1c1ac3ec6537.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Deployment Decision Logic: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application."><img src="ml_systems_files/mediabag/216fb8731d81432c618b0b4e7b8e1c1ac3ec6537.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-playbook-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>Deployment Decision Logic</strong>: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application.
</figcaption>
</figure>
</div>
<p><a href="#fig-mlsys-playbook-flowchart" class="quarto-xref">Figure&nbsp;13</a> presents a structured framework that distills the chapterâ€™s key insights into a systematic approach for determining the most suitable deployment paradigm based on specific requirements and constraints.</p>
<p>The framework is organized into seven fundamental layers of consideration that encompass both technical and organizational factors:</p>
<p><strong>Technical Constraints:</strong> - <strong>Privacy</strong>: Determines whether processing can occur in the cloud or must remain local to safeguard sensitive data, including regulatory compliance requirements (GDPR, HIPAA, SOX). - <strong>Latency</strong>: Evaluates the required decision making speed, particularly for real time or near real time processing needs, with quantified thresholds (&lt;10ms for safety-critical, &lt;100ms for interactive, &gt;1s for batch). - <strong>Reliability</strong>: Assesses network stability and its impact on deployment feasibility, including failover requirements and offline operation needs. - <strong>Compute Needs</strong>: Identifies whether high-performance infrastructure is required or if lightweight processing suffices, including peak versus sustained performance requirements.</p>
<p><strong>Operational and Organizational Factors:</strong> - <strong>Team Expertise</strong>: Evaluates available technical skills across cloud infrastructure, edge networking, mobile development, and embedded systems, as deployment complexity varies dramatically across paradigms. - <strong>Monitoring and Maintenance Capabilities</strong>: Assesses organizational capacity for ongoing system management: cloud systems require DevOps expertise and cost management, edge systems need distributed device management, while TinyML deployments demand specialized debugging capabilities for resource-constrained environments. - <strong>Cost and Energy Efficiency</strong>: Balances resource availability with financial and energy constraints, including capital expenditure versus operational expenditure trade-offs, budget predictability requirements, and long-term operational cost projections. Comprehensive energy efficiency strategies across the entire ML lifecycle are detailed as discussed in <strong><a href="../core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 17: Sustainable AI</a></strong>.</p>
<p>As designers progress through these layers, each decision point narrows the viable options, ultimately guiding them toward one of the four deployment paradigms through both technical and organizational filtering. These decisions are fundamentally constrained by scaling laws that determine resource requirements, energy consumption, and performance characteristics across different deployment contexts (<strong>?@sec-efficient-ai-ai-scaling-laws-a043</strong>). Organizational constraints often prove as decisive as technical ones: a startup with cloud expertise may choose cloud deployment despite slightly higher latency, while an enterprise with strict regulatory requirements and existing edge infrastructure may prefer edge deployment despite higher complexity. This systematic approach proves valuable across various scenariosâ€”privacy-sensitive healthcare applications prioritize local processing over cloud solutions, high-performance recommendation engines typically favor cloud infrastructure when teams have appropriate DevOps capabilities, and manufacturing systems often choose edge deployment when operational teams can manage distributed infrastructure complexity. Real-world deployment decisions require balancing technical optimization with organizational constraints, making successful paradigm selection a systems engineering challenge that extends beyond pure technical requirements.</p>
<p>This framework provides a practical roadmap for navigating deployment decisions. Following this structured approach, system designers can evaluate trade-offs and align their deployment choices with technical, financial, and operational priorities while addressing the unique challenges of each application. The operational aspects of managing these deployments in production are covered as explored in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, while the benchmarking techniques for evaluating deployment performance are detailed as discussed in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
<div id="quiz-question-sec-ml-systems-deployment-decision-framework-824f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which of the following factors is NOT one of the fundamental layers considered in the deployment decision framework?</p>
<ol type="a">
<li>Scalability</li>
<li>Latency</li>
<li>Privacy</li>
<li>Cost and Energy Efficiency</li>
</ol></li>
<li><p>Explain how the deployment decision framework can guide the choice between Cloud ML and Edge ML for a privacy-sensitive application.</p></li>
<li><p>True or False: The deployment decision framework suggests that applications with strict cost constraints should prioritize Cloud ML over Tiny ML.</p></li>
<li><p>Order the following deployment decision layers from first to last as they appear in the decision-making process: (1) Compute Needs, (2) Privacy, (3) Cost and Energy Efficiency, (4) Latency.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-deployment-decision-framework-824f" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-ml-systems-metrics-framework" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-metrics-framework">Reference Metrics Framework</h2>
<p>Understanding the quantitative characteristics of computing infrastructure is essential for making informed deployment decisions across different ML system architectures. This reference framework provides standard benchmarks that ML systems engineers use throughout the development and deployment process to evaluate feasibility, estimate performance, and optimize resource allocation.</p>
<p>These metrics serve as foundational knowledge for reasoning about system trade-offs. When designing an edge ML system, knowing that typical SSD bandwidth ranges from 1-7 GB/s helps determine whether data loading will become a bottleneck. Understanding that real-time applications require sub-33ms latency helps establish whether local processing is necessary or if cloud-based inference remains viable. The framework organizes these critical specifications across four interconnected dimensions that collectively define system capabilities and constraints.</p>
<section id="memory-hierarchy-performance" class="level3">
<h3 class="anchored" data-anchor-id="memory-hierarchy-performance">Memory Hierarchy Performance</h3>
<p>The memory hierarchy forms the foundation of system performance, with each level offering distinct trade-offs between capacity, bandwidth, and access latency. Understanding these characteristics is crucial for optimizing data flow and minimizing bottlenecks in ML workloads.</p>
<p><strong>RAM</strong>: 50-200 GB/s typical bandwidth provides the primary working space for ML systems. Modern DRAM delivers high-bandwidth access for active model parameters and intermediate computations, making it ideal for inference workloads and training on large models. This bandwidth range represents the sweet spot for most ML applications, offering sufficient throughput for real-time processing while maintaining reasonable cost and power characteristics.</p>
<p><strong>SSD</strong>: 1-7 GB/s sequential read/write serves as the bridge between persistent storage and active memory. Solid-state storage offers fast access for model checkpoints, datasets, and application data, with NVMe drives reaching the upper end of this range. This performance tier makes SSDs particularly valuable for edge deployments where local model storage is required but RAM capacity is limited.</p>
<p><strong>Network Storage</strong>: 1-10 GB/s depending on infrastructure extends storage capabilities across distributed systems. These systems vary widely, from basic network-attached storage suitable for development environments to high-performance parallel file systems in HPC environments that can saturate multiple GPU memory controllers simultaneously.</p>
<p><strong>Cache (L1/L2)</strong>: 1000+ GB/s but limited capacity represents the performance apex of the memory hierarchy. CPU caches provide extremely high bandwidth for frequently accessed data, but their small size (KB to MB) limits utility to hot data paths and critical inner loops where maximum performance is essential.</p>
</section>
<section id="energy-efficiency-hierarchy" class="level3">
<h3 class="anchored" data-anchor-id="energy-efficiency-hierarchy">Energy Efficiency Hierarchy</h3>
<p>While memory hierarchy defines data access patterns, energy efficiency fundamentally determines deployment feasibility across different computing platforms. The dramatic efficiency differences between processor types directly influence architectural choices and deployment strategies.</p>
<p><strong>CPU</strong>: ~100 pJ/MAC (multiply-accumulate) establishes the baseline for general-purpose computation. While consuming significant energy per operation, CPUs offer unmatched flexibility for control logic, preprocessing tasks, and small-scale inference where energy efficiency takes precedence over raw throughput.</p>
<p><strong>GPU</strong>: ~10 pJ/MAC delivers a 10x efficiency improvement over CPUs for parallel workloads. Graphics processors excel at training and high-throughput inference scenarios where their parallel architecture can be fully utilized, making them the workhorse of most ML training infrastructure.</p>
<p><strong>TPU</strong>: ~1 pJ/MAC represents purpose-built efficiency for tensor operations. These specialized units achieve another 10x improvement over GPUs for specific ML operations, particularly those optimized for Googleâ€™s TensorFlow ecosystem, demonstrating the benefits of domain-specific optimization.</p>
<p><strong>Specialized Accelerators</strong>: ~0.1 pJ/MAC push efficiency to theoretical limits through extreme specialization. Domain-specific architectures including neural processing units and neuromorphic chips can achieve 100x better efficiency than CPUs for targeted ML workloads, enabling deployment scenarios previously considered impossible due to power constraints.</p>
</section>
<section id="latency-classes" class="level3">
<h3 class="anchored" data-anchor-id="latency-classes">Latency Classes</h3>
<p>The energy efficiency hierarchy directly influences achievable latency, as higher efficiency often enables local processing that eliminates network delays. These latency classes define the temporal constraints that determine deployment architecture and processing location.</p>
<p><strong>Real-time</strong>: &lt;33ms (30 FPS video processing) represents the most demanding temporal requirements. Applications requiring immediate response, such as autonomous vehicle control, augmented reality rendering, and interactive media processing, must achieve these latencies through local processing since network delays alone often exceed these thresholds.</p>
<p><strong>Interactive</strong>: &lt;500ms (voice interfaces, UI response) accommodates human-perceivable response times where slight delays feel natural. Conversational AI, search queries, and user interface interactions fall into this category, allowing for hybrid architectures that balance local and remote processing based on complexity and context.</p>
<p><strong>Batch Processing</strong>: &gt;1s (acceptable for offline tasks) relaxes temporal constraints in favor of throughput optimization. Non-interactive workloads including training, data preprocessing, and analytics prioritize computational efficiency over individual request latency, enabling sophisticated optimization techniques and resource sharing strategies.</p>
</section>
<section id="communication-constraints" class="level3">
<h3 class="anchored" data-anchor-id="communication-constraints">Communication Constraints</h3>
<p>Finally, communication bandwidth defines the interconnections that enable distributed ML systems and determine data movement capabilities between processing locations. These constraints often prove decisive in deployment decisions, particularly as model sizes and data volumes continue growing.</p>
<p><strong>Datacenter</strong>: 10-100 Gbps interconnects provide the foundation for large-scale ML infrastructure. High-speed networking within cloud environments enables distributed training, model serving across multiple nodes, and fast data movement that can support even the largest contemporary models and training workflows.</p>
<p><strong>Edge</strong>: 1-10 Gbps WAN connections bridge the gap between centralized infrastructure and distributed deployment. These wide area networks connecting edge sites to central infrastructure support model updates and aggregated data upload while maintaining reasonable deployment costs across geographically distributed systems.</p>
<p><strong>Mobile</strong>: 10-100 Mbps cellular connectivity serves the vast majority of end-user devices with highly variable performance characteristics. Smartphone and tablet connections vary significantly based on network generation (4G/5G) and signal quality, requiring robust architectures that gracefully handle bandwidth fluctuations and intermittent connectivity.</p>
<p><strong>IoT</strong>: 1-100 Mbps WiFi/cellular represents the most constrained communication tier where every byte of data transfer must be carefully considered. Internet of Things devices typically use lower-power wireless protocols with variable bandwidth depending on deployment environment and power constraints, often requiring sophisticated data compression and local processing strategies.</p>
<p>Understanding these quantitative boundaries helps ML systems engineers make principled decisions about where to place computation, how to structure data flows, and what performance expectations to set for different deployment contexts. These metrics serve as reference points throughout the subsequent chapters on training, inference, and optimization techniques. The systematic measurement methodologies and evaluation frameworks for quantifying these system characteristics across diverse deployment scenarios are explored in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
</section>
</section>
<section id="fallacies-and-pitfalls" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="fallacies-and-pitfalls">Fallacies and Pitfalls</h2>
<p>The diversity of ML deployment paradigms, from cloud to edge to mobile to tiny, creates a complex decision space where engineers must navigate trade-offs between computational power, latency, privacy, and resource constraints. This complexity leads to several persistent misconceptions about deployment choices and their implications for system design.</p>
<p><strong>Fallacy:</strong> <em>Cloud ML is always superior to edge or embedded deployment because of unlimited computational resources.</em></p>
<p>While cloud infrastructure offers vast computational power and storage, this doesnâ€™t automatically make it the optimal choice for all ML applications. Cloud deployment introduces fundamental trade-offs including network latency (often 50-200ms round trip), privacy concerns when transmitting sensitive data, ongoing operational costs that scale with usage, and complete dependence on network connectivity. Edge and embedded deployments excel in scenarios requiring real-time response (autonomous vehicles need sub-10ms decision making), strict data privacy (medical devices processing patient data), predictable costs (one-time hardware investment versus recurring cloud fees), or operation in disconnected environments (industrial equipment in remote locations). The optimal deployment paradigm depends on specific application requirements rather than raw computational capability.</p>
<p><strong>Pitfall:</strong> <em>Choosing a deployment paradigm based solely on model accuracy metrics without considering system-level constraints.</em></p>
<p>Teams often select deployment strategies by comparing model accuracy in isolation, overlooking critical system requirements that determine real-world viability. A cloud-deployed model achieving 99% accuracy becomes useless for autonomous emergency braking if network latency exceeds reaction time requirements. Similarly, a sophisticated edge model that drains a mobile deviceâ€™s battery in minutes fails despite superior accuracy. Successful deployment requires evaluating multiple dimensions simultaneously: latency requirements, power budgets, network reliability, data privacy regulations, and total cost of ownership. Establish these constraints before model development to avoid expensive architectural pivots late in the project.</p>
<p><strong>Pitfall:</strong> <em>Attempting to deploy desktop-trained models directly to edge or mobile devices without architecture modifications.</em></p>
<p>Models developed on powerful workstations often fail dramatically when deployed to resource-constrained devices. A ResNet-50 model requiring 4GB memory for inference (including activations and batch processing) and 4 billion FLOPs per inference cannot run on a device with 512MB of RAM and a 1 GFLOP/s processor. Beyond simple resource violations, desktop-optimized models may use operations unsupported by mobile hardware (specialized mathematical operations), assume floating-point precision unavailable on embedded systems, or require batch processing incompatible with single-sample inference. Successful deployment demands architecture-aware design from the beginning, including specialized architectural techniques for mobile devices <span class="citation" data-cites="howard2017mobilenets">(<a href="#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span>, integer-only operations for microcontrollers, and optimization strategies that maintain accuracy while reducing computation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>â€œMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,â€</span> April. <a href="https://doi.org/10.48550/arXiv.1704.04861">https://doi.org/10.48550/arXiv.1704.04861</a>.
</div></div></section>
<section id="sec-ml-systems-summary-473b" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-summary-473b">Summary</h2>
<p>This chapter explored the diverse landscape of machine learning systems, revealing how deployment context directly shapes every aspect of system design. From cloud environments with vast computational resources to tiny devices operating under extreme constraints, each paradigm presents unique opportunities and challenges that directly influence architectural decisions, algorithmic choices, and performance trade-offs. The spectrum from cloud to edge to mobile to tiny ML represents more than just different scales of computation; it reflects a fundamental evolution in how we distribute intelligence across computing infrastructure.</p>
<p>The progression from centralized cloud systems to distributed edge and mobile deployments demonstrates how resource constraints drive innovation rather than simply limiting capabilities. Cloud ML leverages centralized power for complex processing but must navigate latency and privacy concerns. Edge ML brings computation closer to data sources, reducing latency while introducing intermediate resource constraints. Mobile ML extends these capabilities to personal devices, balancing user experience with battery life and thermal management. Tiny ML pushes the boundaries of whatâ€™s possible with minimal resources, enabling ubiquitous sensing and intelligence in previously impossible deployment contexts. This evolution showcases how thoughtful system design can transform limitations into opportunities for specialized optimization.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Deployment context drives architectural decisions more than algorithmic preferences</li>
<li>Resource constraints create opportunities for innovation, not just limitations</li>
<li>Hybrid approaches are emerging as the future of ML system design</li>
<li>Privacy and latency considerations increasingly favor distributed intelligence</li>
</ul>
</div>
</div>
<p>These paradigms reflect an ongoing shift toward systems that are finely tuned to specific operational requirements, moving beyond one-size-fits-all approaches toward context-aware system design. As these deployment models mature, hybrid architectures emerge that blend their strengths: cloud-based training paired with edge inference, federated learning across mobile devices, and hierarchical processing that optimizes across the entire spectrum. This evolution demonstrates how deployment contexts will continue driving innovation in system architecture, training methodologies, and optimization techniques, creating more sophisticated and context-aware ML systems.</p>


<div id="quiz-question-sec-ml-systems-summary-473b" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.10</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the progression of machine learning deployment paradigms?</p>
<ol type="a">
<li>From edge devices to cloud-based solutions</li>
<li>Centralized systems to increasingly distributed deployments</li>
<li>From mobile devices to tiny devices</li>
<li>From resource-constrained devices to centralized data centers</li>
</ol></li>
<li><p>Explain how hybrid machine learning approaches blend the strengths of different paradigms.</p></li>
<li><p>True or False: Tiny ML is primarily focused on leveraging large-scale computational resources for model training.</p></li>
<li><p>In a scenario where real-time responsiveness and privacy are critical, the most suitable ML deployment option is ____.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-summary-473b" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ml-systems-overview-db10" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary advantage of deploying machine learning models on edge devices?</strong></p>
<ol type="a">
<li>Reduced latency and improved privacy</li>
<li>Maximum computational power</li>
<li>Unlimited storage capacity</li>
<li>No resource constraints</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Reduced latency and improved privacy. Edge devices process data locally, which reduces latency and keeps data close to the source, enhancing privacy. Options A, C, and D are incorrect as they describe characteristics of cloud or idealized scenarios.</p>
<p><em>Learning Objective</em>: Understand the advantages of edge ML deployment.</p></li>
<li><p><strong>Explain the trade-offs involved in choosing between cloud ML and Tiny ML for a real-time image classification task.</strong></p>
<p><em>Answer</em>: Cloud ML offers powerful computational resources and storage, suitable for complex models, but may introduce latency due to network dependency. Tiny ML, with minimal resources, allows real-time processing with low latency but requires significant model optimization. The choice depends on the applicationâ€™s latency tolerance and resource availability.</p>
<p><em>Learning Objective</em>: Analyze trade-offs between different ML deployment options.</p></li>
<li><p><strong>What is a key challenge when deploying machine learning models on mobile devices?</strong></p>
<ol type="a">
<li>Lack of internet connectivity</li>
<li>Balancing model performance with battery life</li>
<li>Excessive computational power</li>
<li>Unlimited memory availability</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Balancing model performance with battery life. Mobile devices must manage power consumption to maintain battery life while running ML models. Options A, C, and D are incorrect as they do not accurately reflect the challenges of mobile ML deployment.</p>
<p><em>Learning Objective</em>: Identify challenges in mobile ML deployment.</p></li>
<li><p><strong>In a production system, how might you decide between using Edge ML and Mobile ML for a smart home application?</strong></p>
<p><em>Answer</em>: The decision would depend on factors such as the need for real-time processing, privacy concerns, and device capabilities. Edge ML is suitable for low-latency, local data processing, while Mobile ML offers portability and personal data handling. The choice should align with the applicationâ€™s specific requirements and constraints.</p>
<p><em>Learning Objective</em>: Apply deployment concepts to real-world ML system scenarios.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-overview-db10" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-cloudbased-machine-learning-7606" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary benefit of using Cloud ML for machine learning projects?</strong></p>
<ol type="a">
<li>Reduced latency for real-time applications</li>
<li>Elimination of data privacy concerns</li>
<li>Complete independence from internet connectivity</li>
<li>Dynamic scalability and resource management</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Dynamic scalability and resource management. Cloud ML provides dynamic scalability, allowing organizations to scale resources up or down based on computational needs. Options A, C, and D are incorrect because Cloud ML can introduce latency, relies on internet connectivity, and poses data privacy challenges.</p>
<p><em>Learning Objective</em>: Understand the key benefits of Cloud ML in terms of scalability and resource management.</p></li>
<li><p><strong>True or False: Cloud ML eliminates the need for data privacy and security measures.</strong></p>
<p><em>Answer</em>: False. Cloud ML requires robust data privacy and security measures because data is stored and processed in centralized data centers, which can be vulnerable to cyber-attacks.</p>
<p><em>Learning Objective</em>: Recognize the importance of data privacy and security in Cloud ML environments.</p></li>
<li><p><strong>What are some challenges organizations face when implementing Cloud ML, and how might these be mitigated?</strong></p>
<p><em>Answer</em>: Challenges include latency issues, data privacy concerns, cost management, network dependency, and vendor lock-in. Mitigation strategies involve optimizing system design for latency, implementing robust security measures, monitoring and optimizing resource usage, ensuring reliable network infrastructure, and planning for potential vendor transitions.</p>
<p><em>Learning Objective</em>: Identify and describe the challenges of Cloud ML implementation and possible mitigation strategies.</p></li>
<li><p><strong>Order the following steps in deploying a machine learning model using Cloud ML: (1) Train model, (2) Deploy model, (3) Collect data, (4) Validate model.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Collect data, (1) Train model, (4) Validate model, (2) Deploy model. Data collection is the first step, followed by training the model. Validation ensures the modelâ€™s accuracy before deployment.</p>
<p><em>Learning Objective</em>: Understand the typical workflow for deploying a machine learning model using Cloud ML.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-cloudbased-machine-learning-7606" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-edge-machine-learning-06ec" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary benefit of using Edge Machine Learning over Cloud ML?</strong></p>
<ol type="a">
<li>Increased computational resources</li>
<li>Reduced latency</li>
<li>Centralized data processing</li>
<li>Unlimited storage capacity</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Reduced latency. Edge ML processes data locally, which minimizes the time data takes to travel to and from remote servers, thus reducing latency. Options A, C, and D are incorrect because Edge ML typically has limited resources and focuses on decentralized processing.</p>
<p><em>Learning Objective</em>: Understand the key advantages of Edge ML compared to Cloud ML.</p></li>
<li><p><strong>True or False: Edge ML enhances data privacy by processing data locally rather than sending it to centralized servers.</strong></p>
<p><em>Answer</em>: True. This is true because processing data locally on edge devices reduces the need to transmit sensitive information over networks, thus minimizing the risk of data breaches.</p>
<p><em>Learning Objective</em>: Recognize the privacy benefits of Edge ML.</p></li>
<li><p><strong>What challenges might arise when deploying machine learning models on edge devices, and how can they be addressed?</strong></p>
<p><em>Answer</em>: Challenges include limited computational resources and increased complexity in managing edge nodes. Addressing these may involve using model compression techniques and implementing robust management protocols for updates and security. For example, quantization can reduce model size, and automated update systems can ensure nodes remain current.</p>
<p><em>Learning Objective</em>: Identify and propose solutions for challenges in Edge ML deployment.</p></li>
<li><p><strong>Edge Machine Learning is crucial for applications requiring real-time decision making, such as ____. This is important because it allows for immediate processing and response.</strong></p>
<p><em>Answer</em>: autonomous vehicles. This is important because it allows for immediate processing and response, which is critical for safety and operational efficiency.</p>
<p><em>Learning Objective</em>: Recall specific applications where Edge ML is essential.</p></li>
<li><p><strong>Order the following benefits of Edge ML in terms of their impact on system performance: (1) Reduced latency, (2) Enhanced data privacy, (3) Lower bandwidth usage.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Reduced latency, (3) Lower bandwidth usage, (2) Enhanced data privacy. Reduced latency has the most immediate impact on performance, followed by lower bandwidth usage which affects cost and efficiency, and finally enhanced data privacy, which, while crucial, is more of a security and compliance benefit.</p>
<p><em>Learning Objective</em>: Understand and prioritize the benefits of Edge ML for system performance.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-edge-machine-learning-06ec" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-mobile-machine-learning-f5b5" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary benefit of Mobile Machine Learning?</strong></p>
<ol type="a">
<li>Unlimited computational resources</li>
<li>Reduced data privacy concerns</li>
<li>Increased dependency on cloud connectivity</li>
<li>Simplified model deployment</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Reduced data privacy concerns. Mobile ML processes data locally, minimizing the risk of data breaches by keeping sensitive information on the device. Other options are incorrect as Mobile ML operates within resource constraints and aims to reduce cloud dependency.</p>
<p><em>Learning Objective</em>: Understand the benefits of Mobile ML in terms of privacy and offline functionality.</p></li>
<li><p><strong>True or False: Mobile ML can operate effectively without internet connectivity.</strong></p>
<p><em>Answer</em>: True. Mobile ML is designed to function offline, ensuring applications remain responsive and reliable even without network access.</p>
<p><em>Learning Objective</em>: Recognize the offline capabilities of Mobile ML systems.</p></li>
<li><p><strong>Discuss the trade-offs involved in optimizing machine learning models for mobile devices.</strong></p>
<p><em>Answer</em>: Optimizing ML models for mobile devices involves balancing model complexity and performance with constraints like battery life, storage, and computational power. For example, model quantization reduces size and power consumption but may affect accuracy. This is important because it ensures efficient on-device processing without compromising user experience.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs in optimizing ML models for mobile deployment.</p></li>
<li><p><strong>____ is a technique used in Mobile ML to reduce model size and speed up inference while maintaining accuracy.</strong></p>
<p><em>Answer</em>: Quantization. This technique reduces model precision, typically from 32-bit to 8-bit integers, significantly decreasing model size and improving inference speed.</p>
<p><em>Learning Objective</em>: Recall the model optimization techniques used in Mobile ML.</p></li>
<li><p><strong>In a production system, how might Mobile ML enhance user experience in real-time applications?</strong></p>
<p><em>Answer</em>: Mobile ML enhances user experience by providing real-time processing capabilities directly on the device. For example, in computational photography, it allows for immediate image enhancements and effects. This is important because it ensures fast, responsive applications that adapt to user needs without relying on cloud processing.</p>
<p><em>Learning Objective</em>: Apply Mobile ML concepts to real-time application scenarios.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-mobile-machine-learning-f5b5" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-tiny-machine-learning-9d4a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary challenge when implementing Tiny ML on microcontrollers?</strong></p>
<ol type="a">
<li>Complex development cycle</li>
<li>High computational power availability</li>
<li>Unlimited memory resources</li>
<li>High energy consumption</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Complex development cycle. This is correct because developing Tiny ML models requires specialized knowledge in both machine learning and embedded systems due to resource constraints. Options A, C, and D are incorrect as they do not reflect the challenges of Tiny ML.</p>
<p><em>Learning Objective</em>: Understand the challenges associated with deploying Tiny ML in resource-constrained environments.</p></li>
<li><p><strong>True or False: Tiny ML devices typically require constant connectivity to external servers for data processing.</strong></p>
<p><em>Answer</em>: False. This is false because Tiny ML devices are designed to process data locally on the device, eliminating the need for constant connectivity to external servers.</p>
<p><em>Learning Objective</em>: Recognize the independence of Tiny ML devices from constant server connectivity.</p></li>
<li><p><strong>Explain how Tiny ML enhances data security in IoT applications.</strong></p>
<p><em>Answer</em>: Tiny ML enhances data security by processing data locally on the device, which reduces the risk of data interception during transmission. For example, in a smart home system, data related to user behavior is processed directly on the device, minimizing exposure to external threats. This is important because it ensures sensitive information remains secure.</p>
<p><em>Learning Objective</em>: Analyze the benefits of local data processing in enhancing security for Tiny ML applications.</p></li>
<li><p><strong>Order the following benefits of Tiny ML in terms of their impact on system performance: (1) Ultra-low latency, (2) High data security, (3) Energy efficiency.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Ultra-low latency, (3) Energy efficiency, (2) High data security. Ultra-low latency directly impacts real-time responsiveness, energy efficiency ensures long-term operation, and high data security protects sensitive information.</p>
<p><em>Learning Objective</em>: Understand the prioritized benefits of Tiny ML in terms of system performance.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-tiny-machine-learning-9d4a" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-hybrid-machine-learning-1bbf" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary advantage of using a Hybrid Machine Learning approach?</strong></p>
<ol type="a">
<li>It reduces the need for data privacy measures.</li>
<li>It eliminates the need for cloud-based training.</li>
<li>It combines the strengths of different ML paradigms while addressing their limitations.</li>
<li>It focuses solely on edge device processing.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. It combines the strengths of different ML paradigms while addressing their limitations. Hybrid ML leverages the computational power of the cloud, efficiency of edge devices, and capabilities of Tiny ML to create a balanced system. Options A, C, and D are incorrect as they do not capture the essence of Hybrid ML.</p>
<p><em>Learning Objective</em>: Understand the fundamental benefit of integrating multiple ML paradigms in Hybrid ML.</p></li>
<li><p><strong>Explain how the train-serve split pattern in Hybrid ML benefits real-time applications.</strong></p>
<p><em>Answer</em>: The train-serve split pattern benefits real-time applications by leveraging the cloud for model training, which requires significant computational resources, and deploying the trained model to edge or mobile devices for inference. This approach ensures low latency and privacy by processing data locally, which is crucial for applications like smart home devices where quick response times are essential. This is important because it balances the need for powerful training infrastructure with the practical requirements of real-time operation.</p>
<p><em>Learning Objective</em>: Analyze the advantages of the train-serve split pattern in Hybrid ML for real-time applications.</p></li>
<li><p><strong>Order the following ML paradigms based on their typical role in a Hybrid ML system from data collection to complex analytics: (1) Cloud ML, (2) Edge ML, (3) Tiny ML.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Tiny ML, (2) Edge ML, (1) Cloud ML. Tiny ML devices typically handle immediate data collection and basic processing, Edge ML aggregates and analyzes data from multiple sources, and Cloud ML manages complex analytics and model updates. This order reflects the hierarchical processing pattern where each tier handles tasks suited to its capabilities.</p>
<p><em>Learning Objective</em>: Understand the hierarchical processing pattern in Hybrid ML systems.</p></li>
<li><p><strong>True or False: Federated learning in Hybrid ML allows for model training on edge devices while maintaining data privacy.</strong></p>
<p><em>Answer</em>: True. Federated learning enables model training across many edge or mobile devices by sharing model updates rather than raw data with cloud servers, thus preserving data privacy. This is important for applications where privacy is critical but collective learning is beneficial.</p>
<p><em>Learning Objective</em>: Understand the role of federated learning in enhancing privacy in Hybrid ML systems.</p></li>
<li><p><strong>In a production system, how might you apply the hierarchical processing pattern to optimize resource utilization?</strong></p>
<p><em>Answer</em>: In a production system, the hierarchical processing pattern can be applied by assigning tasks based on the capabilities of each ML tier. For example, Tiny ML devices can handle basic anomaly detection, Edge ML can perform local data aggregation and analysis, and Cloud ML can manage complex analytics and model updates. This ensures that each tier utilizes its resources efficiently, reducing latency and improving overall system performance. This is important because it allows for scalable and adaptive ML solutions tailored to specific application needs.</p>
<p><em>Learning Objective</em>: Apply the hierarchical processing pattern to optimize resource utilization in Hybrid ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-hybrid-machine-learning-1bbf" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-shared-principles-34fe" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the shared principles that unify different ML paradigms?</strong></p>
<ol type="a">
<li>They provide a framework for understanding and integrating diverse ML implementations.</li>
<li>They focus solely on optimizing computational resources.</li>
<li>They are specific to cloud-based ML systems.</li>
<li>They are applicable only to resource-constrained environments.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. They provide a framework for understanding and integrating diverse ML implementations. This is correct because the shared principles help unify various ML paradigms, offering a cohesive framework for system design across different contexts. Options A, C, and D are incorrect as they limit the scope of these principles to specific aspects or environments.</p>
<p><em>Learning Objective</em>: Understand the role of shared principles in unifying different ML paradigms.</p></li>
<li><p><strong>How do shared principles in ML systems facilitate the development of hybrid solutions?</strong></p>
<p><em>Answer</em>: Shared principles in ML systems, such as data pipeline management and resource optimization, provide a common foundation that allows different ML implementations to be integrated effectively. For example, a cloud-trained model can be deployed on edge devices because both systems adhere to these core principles. This is important because it enables seamless integration and efficient workflows across diverse ML environments.</p>
<p><em>Learning Objective</em>: Explain the role of shared principles in enabling hybrid ML solutions.</p></li>
<li><p><strong>True or False: The core principles of ML systems, such as resource management and system architecture, vary significantly between cloud and tiny ML implementations.</strong></p>
<p><em>Answer</em>: False. This is false because, despite differences in scale and context, the core principles like resource management and system architecture remain consistent across ML implementations. The specific solutions might vary, but the underlying challenges and principles are shared.</p>
<p><em>Learning Objective</em>: Recognize the consistency of core principles across different ML implementations.</p></li>
<li><p><strong>What is a key benefit of understanding the convergence of ML system principles?</strong></p>
<ol type="a">
<li>It allows for the exclusive use of cloud resources.</li>
<li>It limits the application of ML to specific environments.</li>
<li>It enables the development of more efficient and cohesive ML workflows.</li>
<li>It simplifies the design of ML systems by focusing only on hardware constraints.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. It enables the development of more efficient and cohesive ML workflows. This is correct because understanding the convergence of principles allows for the integration of diverse ML paradigms, leading to more efficient and cohesive system designs. Options A, C, and D are incorrect as they either limit the scope of ML applications or oversimplify the design process.</p>
<p><em>Learning Objective</em>: Identify the benefits of understanding the convergence of ML system principles.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-shared-principles-34fe" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-system-comparison-8b05" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following ML deployment options is most suitable for applications requiring ultra-low latency and minimal energy consumption?</strong></p>
<ol type="a">
<li>Tiny ML</li>
<li>Edge ML</li>
<li>Mobile ML</li>
<li>Cloud ML</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Tiny ML. This is correct because Tiny ML systems are designed for ultra-low latency and minimal energy consumption, making them ideal for real-time, low-power applications. Cloud ML, Edge ML, and Mobile ML have higher energy and latency requirements.</p>
<p><em>Learning Objective</em>: Understand the suitability of different ML deployment options based on latency and energy requirements.</p></li>
<li><p><strong>Explain the trade-offs involved in choosing Edge ML over Cloud ML for a real-time video processing application.</strong></p>
<p><em>Answer</em>: Edge ML offers lower latency and improved data privacy by processing data locally, making it suitable for real-time applications. However, it may have limited computational resources compared to Cloud ML, which can handle larger data volumes and more complex models. This trade-off is important for applications where immediate processing and data privacy are critical.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between Edge ML and Cloud ML in real-time applications.</p></li>
<li><p><strong>In a scenario where data privacy is a top priority, the most suitable ML deployment option is ____, as it ensures data never leaves the device.</strong></p>
<p><em>Answer</em>: Tiny ML. Tiny ML ensures data never leaves the device, providing the highest level of data privacy.</p>
<p><em>Learning Objective</em>: Identify the ML deployment option that maximizes data privacy.</p></li>
<li><p><strong>Order the following ML deployment options by their typical latency from highest to lowest: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML. Cloud ML has the highest latency due to network communication, followed by Edge ML, which processes data locally but still has moderate latency. Mobile ML has low to moderate latency, while Tiny ML offers the lowest latency due to localized processing.</p>
<p><em>Learning Objective</em>: Understand the latency characteristics of different ML deployment options.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-system-comparison-8b05" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-deployment-decision-framework-824f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following factors is NOT one of the fundamental layers considered in the deployment decision framework?</strong></p>
<ol type="a">
<li>Scalability</li>
<li>Latency</li>
<li>Privacy</li>
<li>Cost and Energy Efficiency</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Scalability. This is correct because the framework focuses on Privacy, Latency, Compute Needs, and Cost and Energy Efficiency, not Scalability.</p>
<p><em>Learning Objective</em>: Identify the key factors considered in the deployment decision framework.</p></li>
<li><p><strong>Explain how the deployment decision framework can guide the choice between Cloud ML and Edge ML for a privacy-sensitive application.</strong></p>
<p><em>Answer</em>: The framework suggests prioritizing local processing for privacy-sensitive applications, leading to a preference for Edge ML over Cloud ML. For example, healthcare applications often require data to remain on local devices to protect patient privacy. This is important because it ensures compliance with data protection regulations while maintaining system functionality.</p>
<p><em>Learning Objective</em>: Apply the deployment decision framework to a specific scenario, considering privacy requirements.</p></li>
<li><p><strong>True or False: The deployment decision framework suggests that applications with strict cost constraints should prioritize Cloud ML over Tiny ML.</strong></p>
<p><em>Answer</em>: False. This is false because the framework indicates that applications with strict cost constraints should consider low-cost options like Tiny ML, which are more resource-efficient and budget-friendly.</p>
<p><em>Learning Objective</em>: Understand how cost constraints influence deployment decisions within the framework.</p></li>
<li><p><strong>Order the following deployment decision layers from first to last as they appear in the decision-making process: (1) Compute Needs, (2) Privacy, (3) Cost and Energy Efficiency, (4) Latency.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Privacy, (4) Latency, (1) Compute Needs, (3) Cost and Energy Efficiency. This order reflects the sequence in which each layer is considered to systematically narrow down deployment options based on specific requirements.</p>
<p><em>Learning Objective</em>: Sequence the layers of the decision framework to understand their role in narrowing deployment options.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-deployment-decision-framework-824f" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-summary-473b" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.10</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the progression of machine learning deployment paradigms?</strong></p>
<ol type="a">
<li>From edge devices to cloud-based solutions</li>
<li>Centralized systems to increasingly distributed deployments</li>
<li>From mobile devices to tiny devices</li>
<li>From resource-constrained devices to centralized data centers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Centralized systems to increasingly distributed deployments. This progression reflects the shift from cloud-based systems to more distributed solutions like Edge ML, Mobile ML, and Tiny ML, which are tailored to specific contexts.</p>
<p><em>Learning Objective</em>: Understand the evolution of ML deployment paradigms from centralized to distributed systems.</p></li>
<li><p><strong>Explain how hybrid machine learning approaches blend the strengths of different paradigms.</strong></p>
<p><em>Answer</em>: Hybrid machine learning approaches combine the computational power of cloud-based systems with the low-latency, privacy-preserving features of edge and mobile systems. For example, cloud-based training can be paired with edge inference to optimize resource use and enhance real-time responsiveness. This is important because it allows for flexible and efficient ML solutions tailored to specific application needs.</p>
<p><em>Learning Objective</em>: Analyze the benefits of hybrid ML approaches in leveraging multiple paradigms.</p></li>
<li><p><strong>True or False: Tiny ML is primarily focused on leveraging large-scale computational resources for model training.</strong></p>
<p><em>Answer</em>: False. Tiny ML focuses on deploying machine learning models on resource-constrained devices, not on leveraging large-scale computational resources. It aims to enable ML applications in environments with limited power and computational capabilities.</p>
<p><em>Learning Objective</em>: Correct misconceptions about the focus of Tiny ML.</p></li>
<li><p><strong>In a scenario where real-time responsiveness and privacy are critical, the most suitable ML deployment option is ____. </strong></p>
<p><em>Answer</em>: Edge ML. Edge ML processes data locally, reducing latency and enhancing privacy, making it suitable for real-time applications.</p>
<p><em>Learning Objective</em>: Identify the most appropriate ML paradigm for specific application requirements.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-summary-473b" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/introduction/introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <span class="nav-page-text">DL Primer</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>