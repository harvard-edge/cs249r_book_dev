<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="next">
<link href="../../../contents/core/introduction/introduction.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
</style>
<style>
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
</style>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ml_systems/ml_systems.html">ML Systems</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>🚧 DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">84a26d82</code> • 2025-10-04 23:17 UTC • <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version →</em></a></p>
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-ml-systems" id="toc-sec-ml-systems" class="nav-link active" data-scroll-target="#sec-ml-systems">ML Systems</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ml-systems-overview-db10" id="toc-sec-ml-systems-overview-db10" class="nav-link" data-scroll-target="#sec-ml-systems-overview-db10">Overview</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-unifying-framework" id="toc-sec-ml-systems-unifying-framework" class="nav-link" data-scroll-target="#sec-ml-systems-unifying-framework">Unifying Framework</a></li>
  <li><a href="#sec-ml-systems-why-paradigms-exist" id="toc-sec-ml-systems-why-paradigms-exist" class="nav-link" data-scroll-target="#sec-ml-systems-why-paradigms-exist">Why Different Paradigms Exist</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-cloudbased-machine-learning-7606" id="toc-sec-ml-systems-cloudbased-machine-learning-7606" class="nav-link" data-scroll-target="#sec-ml-systems-cloudbased-machine-learning-7606">Cloud-Based Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-b564" id="toc-sec-ml-systems-characteristics-b564" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-b564">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-e12c" id="toc-sec-ml-systems-benefits-e12c" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-e12c">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-e73b" id="toc-sec-ml-systems-challenges-e73b" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-e73b">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-348c" id="toc-sec-ml-systems-use-cases-348c" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-348c">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-edge-machine-learning-06ec" id="toc-sec-ml-systems-edge-machine-learning-06ec" class="nav-link" data-scroll-target="#sec-ml-systems-edge-machine-learning-06ec">Edge Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-09e1" id="toc-sec-ml-systems-characteristics-09e1" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-09e1">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-4fb7" id="toc-sec-ml-systems-benefits-4fb7" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-4fb7">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-2714" id="toc-sec-ml-systems-challenges-2714" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-2714">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-05eb" id="toc-sec-ml-systems-use-cases-05eb" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-05eb">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-mobile-machine-learning-f5b5" id="toc-sec-ml-systems-mobile-machine-learning-f5b5" class="nav-link" data-scroll-target="#sec-ml-systems-mobile-machine-learning-f5b5">Mobile Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-9792" id="toc-sec-ml-systems-characteristics-9792" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-9792">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-99f9" id="toc-sec-ml-systems-benefits-99f9" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-99f9">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-aa62" id="toc-sec-ml-systems-challenges-aa62" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-aa62">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-c808" id="toc-sec-ml-systems-use-cases-c808" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-c808">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-tiny-machine-learning-9d4a" id="toc-sec-ml-systems-tiny-machine-learning-9d4a" class="nav-link" data-scroll-target="#sec-ml-systems-tiny-machine-learning-9d4a">Tiny Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-characteristics-d52d" id="toc-sec-ml-systems-characteristics-d52d" class="nav-link" data-scroll-target="#sec-ml-systems-characteristics-d52d">Characteristics</a></li>
  <li><a href="#sec-ml-systems-benefits-020f" id="toc-sec-ml-systems-benefits-020f" class="nav-link" data-scroll-target="#sec-ml-systems-benefits-020f">Benefits</a></li>
  <li><a href="#sec-ml-systems-challenges-297b" id="toc-sec-ml-systems-challenges-297b" class="nav-link" data-scroll-target="#sec-ml-systems-challenges-297b">Challenges</a></li>
  <li><a href="#sec-ml-systems-use-cases-3c3f" id="toc-sec-ml-systems-use-cases-3c3f" class="nav-link" data-scroll-target="#sec-ml-systems-use-cases-3c3f">Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-hybrid-machine-learning-1bbf" id="toc-sec-ml-systems-hybrid-machine-learning-1bbf" class="nav-link" data-scroll-target="#sec-ml-systems-hybrid-machine-learning-1bbf">Hybrid Machine Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-design-patterns-ade8" id="toc-sec-ml-systems-design-patterns-ade8" class="nav-link" data-scroll-target="#sec-ml-systems-design-patterns-ade8">Design Patterns</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-trainserve-split-0d17" id="toc-sec-ml-systems-trainserve-split-0d17" class="nav-link" data-scroll-target="#sec-ml-systems-trainserve-split-0d17">Train-Serve Split</a></li>
  <li><a href="#sec-ml-systems-hierarchical-processing-6114" id="toc-sec-ml-systems-hierarchical-processing-6114" class="nav-link" data-scroll-target="#sec-ml-systems-hierarchical-processing-6114">Hierarchical Processing</a></li>
  <li><a href="#sec-ml-systems-progressive-deployment-2570" id="toc-sec-ml-systems-progressive-deployment-2570" class="nav-link" data-scroll-target="#sec-ml-systems-progressive-deployment-2570">Progressive Deployment</a></li>
  <li><a href="#sec-ml-systems-federated-learning-bf9c" id="toc-sec-ml-systems-federated-learning-bf9c" class="nav-link" data-scroll-target="#sec-ml-systems-federated-learning-bf9c">Federated Learning</a></li>
  <li><a href="#sec-ml-systems-collaborative-learning-7f59" id="toc-sec-ml-systems-collaborative-learning-7f59" class="nav-link" data-scroll-target="#sec-ml-systems-collaborative-learning-7f59">Collaborative Learning</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-realworld-integration-0815" id="toc-sec-ml-systems-realworld-integration-0815" class="nav-link" data-scroll-target="#sec-ml-systems-realworld-integration-0815">Real-World Integration</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-shared-principles-34fe" id="toc-sec-ml-systems-shared-principles-34fe" class="nav-link" data-scroll-target="#sec-ml-systems-shared-principles-34fe">Shared Principles</a>
  <ul class="collapse">
  <li><a href="#sec-ml-systems-implementation-layer-9002" id="toc-sec-ml-systems-implementation-layer-9002" class="nav-link" data-scroll-target="#sec-ml-systems-implementation-layer-9002">Implementation Layer</a></li>
  <li><a href="#sec-ml-systems-system-principles-layer-db81" id="toc-sec-ml-systems-system-principles-layer-db81" class="nav-link" data-scroll-target="#sec-ml-systems-system-principles-layer-db81">System Principles Layer</a></li>
  <li><a href="#sec-ml-systems-system-considerations-layer-660c" id="toc-sec-ml-systems-system-considerations-layer-660c" class="nav-link" data-scroll-target="#sec-ml-systems-system-considerations-layer-660c">System Considerations Layer</a></li>
  <li><a href="#sec-ml-systems-principles-practice-907d" id="toc-sec-ml-systems-principles-practice-907d" class="nav-link" data-scroll-target="#sec-ml-systems-principles-practice-907d">Principles to Practice</a></li>
  </ul></li>
  <li><a href="#sec-ml-systems-system-comparison-8b05" id="toc-sec-ml-systems-system-comparison-8b05" class="nav-link" data-scroll-target="#sec-ml-systems-system-comparison-8b05">System Comparison</a></li>
  <li><a href="#sec-ml-systems-deployment-decision-framework-824f" id="toc-sec-ml-systems-deployment-decision-framework-824f" class="nav-link" data-scroll-target="#sec-ml-systems-deployment-decision-framework-824f">Deployment Decision Framework</a></li>
  <li><a href="#fallacies-and-pitfalls" id="toc-fallacies-and-pitfalls" class="nav-link" data-scroll-target="#fallacies-and-pitfalls">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-ml-systems-summary-473b" id="toc-sec-ml-systems-summary-473b" class="nav-link" data-scroll-target="#sec-ml-systems-summary-473b">Summary</a>
  <ul class="collapse">
  <li><a href="#looking-ahead" id="toc-looking-ahead" class="nav-link" data-scroll-target="#looking-ahead">Looking Ahead</a></li>
  </ul></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ml_systems/ml_systems.html">ML Systems</a></li></ol></nav></header>




<section id="sec-ml-systems" class="level1 page-columns page-full">
<h1>ML Systems</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Illustration in a rectangular format depicting the merger of embedded systems with Embedded AI. The left half of the image portrays traditional embedded systems, including microcontrollers and processors, detailed and precise. The right half showcases the world of artificial intelligence, with abstract representations of machine learning models, neurons, and data flow. The two halves are distinctly separated, emphasizing the individual significance of embedded tech and AI, but they come together in harmony at the center.</em></p>
</div></div><p> <img src="images/png/cover_ml_systems.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How do the environments where machine learning operates shape the nature of these systems, and what drives their widespread deployment across computing platforms?</em></p>
<p>Machine learning systems must adapt to radically different computational environments, each imposing distinct constraints and opportunities. Cloud deployments leverage massive computational resources but face network latency, while mobile devices offer user proximity but operate under severe power limitations. Embedded systems minimize latency through local processing but constrain model complexity, and tiny devices enable widespread sensing while restricting memory to kilobytes. These deployment contexts fundamentally determine system architecture, algorithmic choices, and performance trade-offs. Understanding environment-specific requirements establishes the foundation for engineering decisions in machine learning systems. This knowledge enables engineers to select appropriate deployment paradigms and design architectures that balance performance, efficiency, and practicality across the full spectrum of computing platforms.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Classify ML deployment paradigms based on computational resources, power constraints, and latency requirements</p></li>
<li><p>Compare architectural trade-offs between centralized cloud processing and distributed edge computing</p></li>
<li><p>Analyze resource constraints and their impact on model selection and deployment decisions</p></li>
<li><p>Evaluate real-world applications to determine the most appropriate ML deployment paradigm</p></li>
<li><p>Design system architectures that balance performance, efficiency, and practicality across deployment contexts</p></li>
<li><p>Assess emerging trends in ML systems and predict their influence on future deployment strategies</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-ml-systems-overview-db10" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-overview-db10">Overview</h2>
<p>Building on our exploration of AI’s transformative potential, this chapter addresses a fundamental question: What transforms machine learning from mathematical algorithms into engineering systems that operate reliably at scale? The answer lies in understanding that machine learning systems extend far beyond the algorithms themselves to encompass the complete infrastructure, deployment contexts, and operational requirements that enable AI to function in the real world.</p>
<p>Chapter <strong><a href="../core/introduction/introduction.html#sec-introduction">Chapter 1: Introduction</a></strong> established that machine learning systems integrate three fundamental components—data, algorithms, and infrastructure—working as an integrated system. But these components manifest dramatically differently depending on where and how the system operates. A recommendation algorithm trained on identical data behaves fundamentally differently when deployed in a cloud data center versus a smartphone versus an embedded sensor. The deployment context doesn’t merely constrain implementation details; it fundamentally reshapes the entire system architecture and determines what becomes possible.</p>
<p>This systems perspective distinguishes machine learning engineering from pure algorithmic research. While algorithms focus on mathematical optimization and theoretical performance bounds, machine learning systems must address practical constraints: power consumption, memory limitations, network connectivity, real-time requirements, privacy regulations, and operational costs. These constraints drive critical engineering trade-offs that determine whether an AI application succeeds or fails in production.</p>
<p>The deployment spectrum spans from massive cloud data centers consuming megawatts of power to coin-cell powered sensors operating for years without maintenance. Each point along this spectrum represents distinct engineering challenges and opportunities. Cloud systems prioritize algorithmic sophistication with virtually unlimited computational resources but face network latency constraints. Mobile systems eliminate latency and preserve privacy but must operate within severe memory and battery limitations. Embedded systems face even more extreme constraints—kilobytes of memory, milliwatts of power—yet gain years of autonomous operation in remote locations.</p>
<p>Understanding these deployment paradigms requires examining how they address four critical decision factors: privacy requirements (can data leave the local environment?), latency constraints (how quickly must the system respond?), computational resources (what processing power is available?), and operational costs (what are the budget and energy constraints?). The interplay between these factors creates natural boundaries that define when each paradigm excels.</p>
<p>Modern machine learning systems increasingly combine multiple paradigms into hybrid architectures. A voice assistant might use embedded ML for wake-word detection, mobile ML for local speech recognition, edge ML for contextual processing, and cloud ML for complex natural language understanding—all within a single user interaction. This hybrid integration represents the practical reality of modern ML systems, where pure single-paradigm deployments often prove too limiting for real-world requirements.</p>
<p>We examine four primary deployment paradigms that span the computational spectrum: Cloud ML leverages massive centralized resources when computational power outweighs latency concerns; Edge ML brings computation closer to data sources when low latency and privacy matter more than unlimited resources; Mobile ML extends capabilities to personal devices when user proximity and offline operation become priorities; and Tiny ML enables widespread intelligence on severely constrained devices when power efficiency and cost matter more than computational complexity.</p>
<p>Through systematic analysis of these paradigms, we’ll develop the engineering intuition necessary to design ML systems that balance algorithmic sophistication with practical constraints. This foundation proves essential for understanding how theoretical machine learning concepts translate into production systems that reliably serve millions of users. By mastering these deployment paradigms and their trade-offs, you’ll gain the systems thinking required to architect ML solutions that succeed in production environments.</p>
<p><a href="#fig-cloud-edge-TinyML-comparison" class="quarto-xref">Figure&nbsp;1</a> visualizes how computational resources, latency requirements, and deployment constraints create the deployment spectrum. The following sections examine each paradigm systematically: Cloud ML (<a href="#sec-ml-systems-cloudbased-machine-learning-7606" class="quarto-xref">Section&nbsp;1.2</a>), Edge ML (<a href="#sec-ml-systems-edge-machine-learning-06ec" class="quarto-xref">Section&nbsp;1.3</a>), Mobile ML (<a href="#sec-ml-systems-mobile-machine-learning-f5b5" class="quarto-xref">Section&nbsp;1.4</a>), and Tiny ML (<a href="#sec-ml-systems-tiny-machine-learning-9d4a" class="quarto-xref">Section&nbsp;1.5</a>), followed by their integration into Hybrid ML systems (<a href="#sec-ml-systems-hybrid-machine-learning-1bbf" class="quarto-xref">Section&nbsp;1.6</a>). Each paradigm occupies a distinct position along multiple dimensions, reflecting the trade-offs that drive deployment decisions.</p>
<section id="sec-ml-systems-unifying-framework" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-unifying-framework">Unifying Framework</h3>
<p>Despite their apparent differences, all ML deployment paradigms share fundamental principles that enable systematic understanding and effective hybrid combinations. These paradigms represent different optimizations of the same core challenges: managing data pipelines, balancing resource constraints, and implementing reliable system architectures. Whether processing petabytes in cloud data centers or kilobytes on microcontrollers, all systems must address data flow, resource allocation, and component integration.</p>
<p>This convergence explains why techniques transfer effectively between paradigms and why hybrid approaches work so well in practice. Cloud-trained models deploy successfully to edge devices, mobile optimizations inform cloud efficiency strategies, and tiny device insights drive new approaches across all scales. The detailed analysis of these shared principles appears in <a href="#sec-ml-systems-shared-principles-34fe" class="quarto-xref">Section&nbsp;1.7</a>, providing the foundation for understanding both individual paradigms and their integration patterns.</p>
<div id="fig-cloud-edge-TinyML-comparison" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cloud-edge-TinyML-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="7fd54a49357db81bbdcf6a24cd73c93a78028044.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Distributed Intelligence Spectrum: Machine learning system design involves trade-offs between computational resources, latency, and connectivity, resulting in a spectrum of deployment options ranging from centralized cloud infrastructure to resource-constrained edge and TinyML devices. This figure maps these options, highlighting how each approach balances processing location with device capability and network dependence. Source: [@abiresearch2024tinyml]."><img src="ml_systems_files/mediabag/7fd54a49357db81bbdcf6a24cd73c93a78028044.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloud-edge-TinyML-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Distributed Intelligence Spectrum</strong>: Machine learning system design involves trade-offs between computational resources, latency, and connectivity, resulting in a spectrum of deployment options ranging from centralized cloud infrastructure to resource-constrained edge and TinyML devices. This figure maps these options, highlighting how each approach balances processing location with device capability and network dependence. Source: <span class="citation" data-cites="abiresearch2024tinyml">(<a href="#ref-abiresearch2024tinyml" role="doc-biblioref">Research 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
</section>
<section id="sec-ml-systems-why-paradigms-exist" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-why-paradigms-exist">Why Different Paradigms Exist</h3>
<p>The deployment spectrum illustrated in <a href="#fig-cloud-edge-TinyML-comparison" class="quarto-xref">Figure&nbsp;1</a> exists not by design preference, but out of necessity driven by immutable physical and hardware constraints. Understanding these fundamental limitations reveals why ML systems cannot adopt a one-size-fits-all approach and must instead span the full deployment spectrum from cloud to tiny devices.</p>
<p><strong><a href="../core/introduction/introduction.html#sec-introduction">Chapter 1: Introduction</a></strong> established that ML systems integrate data, algorithms, and infrastructure as a unified system. These deployment paradigms represent different manifestations of this integration, where each paradigm optimizes the data-algorithm-infrastructure triad differently based on physical constraints. Cloud ML prioritizes algorithmic complexity through abundant infrastructure, while Mobile ML emphasizes data locality with constrained infrastructure, and Tiny ML maximizes algorithmic efficiency under extreme infrastructure limitations.</p>
<p><strong>Memory Bandwidth Bottlenecks.</strong> The most critical bottleneck in modern computing stems from memory bandwidth scaling differently than computational capacity. While compute power can scale linearly by adding more processing units, memory bandwidth scales approximately as the square root of chip area due to physical routing constraints. This creates a progressively worsening bottleneck where processors become starved for data. In practice, this manifests as ML models spending more time waiting for memory transfers than performing calculations, particularly problematic for large models<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> that require more data than can be efficiently transferred.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Memory Bottleneck</strong>: When the rate of data transfer from memory to processor becomes the limiting factor in computation. Large models require so many parameters that memory bandwidth, rather than computational capacity, determines performance.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Dennard Scaling</strong>: Named after Robert Dennard (IBM, 1974), the observation that as transistors became smaller, they could operate at higher frequencies while consuming the same power density. This scaling enabled Moore’s Law until 2005, when physics limitations forced the industry toward multi-core architectures and specialized processors like GPUs and TPUs.</p></div></div><p><strong>Power Density Limits.</strong> Compounding these memory challenges, Dennard scaling<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> breakdown changed computing constraints around 2005, when transistor shrinking stopped reducing power density. Power dissipation per unit area now remains constant or increases with each technology generation, creating hard limits on computational density. For mobile devices, this translates to thermal throttling that reduces performance when sustained computation generates excessive heat. Data centers face similar constraints at scale, requiring extensive cooling infrastructure that can consume 30-40% of total power budget. These power density limits directly drive the need for specialized low-power architectures in mobile and embedded contexts, and explain why edge deployment becomes necessary when power budgets are constrained.</p>
<p><strong>Communication and Latency Constraints.</strong> Beyond power considerations, physical limits impose minimum latencies that no engineering optimization can overcome. The speed of light creates an inherent 80ms round-trip time between California and Virginia, while internet routing, DNS resolution, and processing overhead typically add another 20-420ms. This 100-500ms total latency makes real-time applications impossible with pure cloud deployment. Network bandwidth also faces physical constraints: fiber optic cables have theoretical limits, and wireless communication is bounded by spectrum availability and signal propagation physics. These communication constraints create hard boundaries that force local processing for latency-sensitive applications and drive edge deployment decisions.</p>
<p><strong>Thermal Management Realities.</strong> Finally, heat dissipation becomes the limiting factor as computational density increases. Mobile devices must throttle performance to prevent component damage and maintain user comfort, while data centers require massive cooling systems that limit placement options and increase operational costs. Thermal constraints create cascading effects: higher temperatures reduce semiconductor reliability, increase error rates, and accelerate component aging. These thermal realities force trade-offs between computational performance and sustainable operation, driving specialized cooling solutions in cloud environments and ultra-low-power designs in embedded systems.</p>
<p>These fundamental constraints drove the evolution of the four distinct deployment paradigms outlined in our overview (<a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>). Understanding these core constraints proves essential for selecting appropriate deployment paradigms and setting realistic performance expectations.</p>
<p>To understand the differences between these ML deployment options, <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a> provides examples of hardware platforms for each category. These examples show the range of computational resources, power requirements, and cost considerations<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> across the ML systems spectrum. These concrete examples demonstrate the practical implications of each approach.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>ML Hardware Cost Spectrum</strong>: The cost range spans 6 orders of magnitude, from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Power Usage Effectiveness (PUE)</strong>: Data center efficiency metric measuring total facility power divided by IT equipment power. A PUE of 1.0 represents perfect efficiency (impossible in practice), while 1.1-1.3 indicates highly efficient facilities using advanced cooling and power management. Google’s data centers achieve PUE of 1.12 compared to industry average of 1.8.</p></div></div><p>These quantitative thresholds reflect essential relationships between computational requirements, energy consumption, and deployment feasibility. These scaling relationships determine when distributed cloud deployment becomes advantageous versus edge or mobile alternatives. Understanding these quantitative trade-offs enables informed deployment decisions across the spectrum of ML systems.</p>
<p><a href="#fig-vMLsizes" class="quarto-xref">Figure&nbsp;2</a> shows the differences between Cloud ML, Edge ML, Mobile ML, and Tiny ML in terms of hardware, latency, connectivity, power requirements, and model complexity. As systems move from Cloud to Edge to Tiny ML, available resources decrease dramatically, presenting significant challenges for deploying machine learning models. This resource disparity becomes particularly apparent when deploying ML models on microcontrollers, the primary hardware platform for Tiny ML. These devices have severely constrained memory and storage capacities, which are often insufficient for conventional complex ML models.</p>
<div id="tbl-representative-systems" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-representative-systems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Hardware Spectrum</strong>: Machine learning system design necessitates trade-offs between computational resources, power consumption, and cost, as exemplified by the diverse hardware platforms suitable for cloud, edge, mobile, and TinyML deployments. This table quantifies those trade-offs, revealing how device capabilities, from high-end GPUs in cloud servers to low-power microcontrollers in embedded systems, shape the types of models and tasks each platform can effectively support. The quantitative thresholds provide specific decision criteria to help practitioners determine the most appropriate deployment paradigm for their applications. Source: <span class="citation" data-cites="abiresearch2024tinyml">(<a href="#ref-abiresearch2024tinyml" role="doc-biblioref">Research 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-abiresearch2024tinyml" class="csl-entry" role="listitem">
Research, ABI. 2024. <span>“TinyML Market Trends and Device Analysis.”</span> Market Research Report. ABI Research. <a href="https://www.abiresearch.com/market-research/product/1050167/">https://www.abiresearch.com/market-research/product/1050167/</a>.
</div></div><div aria-describedby="tbl-representative-systems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 10%">
<col style="width: 17%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 14%">
<col style="width: 18%">
<col style="width: 0%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Example Device</th>
<th style="text-align: left;">Processor</th>
<th style="text-align: left;">Memory</th>
<th style="text-align: left;">Storage</th>
<th style="text-align: left;">Power</th>
<th style="text-align: left;">Price Range</th>
<th style="text-align: left;">Example Models/Tasks</th>
<th style="text-align: left;">Quantitative Thresholds</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cloud ML</td>
<td style="text-align: left;">NVIDIA DGX A100</td>
<td style="text-align: left;">8x NVIDIA A100 GPUs (40GB or 80GB per GPU)</td>
<td style="text-align: left;">1 TB System RAM</td>
<td style="text-align: left;">15 TB NVMe SSD</td>
<td style="text-align: left;">6.5 kW</td>
<td style="text-align: left;">$200 K+</td>
<td style="text-align: left;">Large language models,</td>
<td style="text-align: left;">&gt;1000 TFLOPS compute, real-time video processing, &gt;100GB/s memory bandwidth, PUE 1.1-1.3, 100-500ms latency</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Google TPU v4 Pod</td>
<td style="text-align: left;">4096 TPU v4 chips</td>
<td style="text-align: left;">128 TB+</td>
<td style="text-align: left;">Networked storage</td>
<td style="text-align: left;">~1-2 MW</td>
<td style="text-align: left;">Pay-per-use</td>
<td style="text-align: left;">Training foundation models, large-scale ML research</td>
<td colspan="2" style="text-align: left;">&gt;1000 TFLOPS compute, &gt;100GB/s memory bandwidth, | PUE 1.1-1.3, 100-500ms latency |</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Edge ML</td>
<td style="text-align: left;">NVIDIA Jetson AGX Orin</td>
<td style="text-align: left;">12-core Arm® Cortex®-A78AE, NVIDIA Ampere GPU</td>
<td style="text-align: left;">32 GB LPDDR5</td>
<td style="text-align: left;">64GB eMMC</td>
<td style="text-align: left;">15-60 W</td>
<td style="text-align: left;">$899</td>
<td style="text-align: left;">Computer vision, robotics, autonomous systems</td>
<td style="text-align: left;">1-100 TOPS compute, &lt;10W sustained power, &lt;100ms latency requirements</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Intel NUC 12 Pro</td>
<td style="text-align: left;">Intel Core i7-1260P, Intel Iris Xe</td>
<td style="text-align: left;">32 GB DDR4</td>
<td style="text-align: left;">1 TB SSD</td>
<td style="text-align: left;">28 W</td>
<td style="text-align: left;">$750</td>
<td style="text-align: left;">Edge AI servers, industrial automation</td>
<td style="text-align: left;">1-100 TOPS compute, &lt;10W sustained power, &lt;100ms latency requirements</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mobile ML</td>
<td style="text-align: left;">iPhone 15 Pro</td>
<td style="text-align: left;">A17 Pro (6-core CPU, 6-core GPU)</td>
<td style="text-align: left;">8 GB RAM</td>
<td style="text-align: left;">128 GB-1 TB</td>
<td style="text-align: left;">3-5 W</td>
<td style="text-align: left;">$999+</td>
<td style="text-align: left;">Face ID, computational photography, voice recognition</td>
<td style="text-align: left;">1-10 TOPS compute, &lt;2W sustained power, &lt;50ms UI response</td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: left;">Tiny ML</td>
<td style="text-align: left;">Arduino Nano 33 BLE Sense</td>
<td style="text-align: left;">Arm Cortex-M4 @ 64 MHz</td>
<td style="text-align: left;">256 KB RAM</td>
<td style="text-align: left;">1 MB Flash</td>
<td style="text-align: left;">0.02-0.04 W</td>
<td style="text-align: left;">$35</td>
<td style="text-align: left;">Gesture recognition, voice detection</td>
<td style="text-align: left;">&lt;1 TOPS compute, &lt;1mW power, microsecond response times</td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">ESP32-CAM</td>
<td style="text-align: left;">Dual-core @ 240MHz</td>
<td style="text-align: left;">520 KB RAM</td>
<td style="text-align: left;">4 MB Flash</td>
<td style="text-align: left;">0.05-0.25 W</td>
<td style="text-align: left;">$10</td>
<td style="text-align: left;">Image classification, motion detection</td>
<td style="text-align: left;">&lt;1 TOPS compute, &lt;1mW power, microsecond response times</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-vMLsizes" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-vMLsizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1b6a65f1ce1f88fffebdd36030c53ddacd39e03d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Device Memory Constraints: AI model deployment spans a wide range of devices with drastically different memory capacities, from cloud servers with 16 GB to microcontroller-based systems with only 320 kb. This progression necessitates specialized optimization techniques and efficient architectures to enable on-device intelligence with limited resources. Source: [@lin2023tiny]."><img src="ml_systems_files/mediabag/1b6a65f1ce1f88fffebdd36030c53ddacd39e03d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vMLsizes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Device Memory Constraints</strong>: AI model deployment spans a wide range of devices with drastically different memory capacities, from cloud servers with 16 GB to microcontroller-based systems with only 320 kb. This progression necessitates specialized optimization techniques and efficient architectures to enable on-device intelligence with limited resources. Source: <span class="citation" data-cites="lin2023tiny">(<a href="#ref-lin2023tiny" role="doc-biblioref">Lin et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lin2023tiny" class="csl-entry" role="listitem">
Lin, Ji, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, and Song Han. 2023. <span>“Tiny Machine Learning: Progress and Futures [Feature].”</span> <em>IEEE Circuits and Systems Magazine</em> 23 (3): 8–34. <a href="https://doi.org/10.1109/mcas.2023.3302182">https://doi.org/10.1109/mcas.2023.3302182</a>.
</div></div></figure>
</div>
<div id="quiz-question-sec-ml-systems-overview-db10" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary advantage of deploying machine learning models on edge devices?</p>
<ol type="a">
<li>Reduced latency and improved privacy</li>
<li>Maximum computational power</li>
<li>Unlimited storage capacity</li>
<li>No resource constraints</li>
</ol></li>
<li><p>Explain the trade-offs involved in choosing between cloud ML and Tiny ML for a real-time image classification task.</p></li>
<li><p>What is a key challenge when deploying machine learning models on mobile devices?</p>
<ol type="a">
<li>Lack of internet connectivity</li>
<li>Balancing model performance with battery life</li>
<li>Excessive computational power</li>
<li>Unlimited memory availability</li>
</ol></li>
<li><p>In a production system, how might you decide between using Edge ML and Mobile ML for a smart home application?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-overview-db10" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-cloudbased-machine-learning-7606" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-cloudbased-machine-learning-7606">Cloud-Based Machine Learning</h2>
<p>Having established the fundamental constraints and evolutionary progression that shape ML deployment paradigms, we now examine each paradigm in detail, beginning with Cloud ML—the foundation from which other paradigms emerged.</p>
<p>Cloud ML maximizes computational resources while accepting latency constraints. Cloud ML provides the optimal choice when computational power matters more than response time, making it ideal for complex training tasks and inference that can tolerate network delays.</p>
<p>Cloud Machine Learning leverages the scalability and power of centralized cloud infrastructures<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to handle computationally intensive tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers utilize distributed architectures and specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. This section focuses on the deployment characteristics that make cloud ML systems effective for large-scale applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Cloud Infrastructure Evolution</strong>: Cloud computing for ML emerged from Amazon’s decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending reached approximately $138 billion annually, with total public cloud services exceeding $675 billion.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>NLP Computational Demands</strong>: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training, equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days <span class="citation" data-cites="strubell2019energy">(<a href="#ref-strubell2019energy" role="doc-biblioref">Strubell, Ganesh, and McCallum 2019</a>)</span>. This computational scale drove the need for massive cloud infrastructure.</p><div id="ref-strubell2019energy" class="csl-entry" role="listitem">
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. <span>“Energy and Policy Considerations for Deep Learning in NLP.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 3645–50. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/p19-1355">https://doi.org/10.18653/v1/p19-1355</a>.
</div></div></div><div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of Cloud ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Cloud ML</summary><div><strong>Cloud Machine Learning (Cloud ML)</strong> refers to the deployment of machine learning models on <em>centralized computing infrastructures</em>, such as data centers. These systems operate in the <em>kilowatt to megawatt</em> power range and utilize <em>specialized computing systems</em> to handle <em>large scale datasets</em> and train <em>complex models</em>. Cloud ML offers <em>scalability</em> and <em>computational capacity</em>, making it well-suited for tasks requiring extensive resources and collaboration. However, it depends on <em>consistent connectivity</em> and may introduce <em>latency</em> for real-time applications.<p></p>
</div></details>
</div>
<p><a href="#fig-cloud-ml" class="quarto-xref">Figure&nbsp;3</a> provides an overview of Cloud ML’s capabilities, which we will discuss in greater detail throughout this section.</p>
<div id="fig-cloud-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cloud-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4fce3d88761f81181ec3328739a10b05a000b663.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Cloud ML Capabilities: Cloud machine learning systems address challenges related to scale, complexity, and resource management through centralized computing infrastructure and specialized hardware. This figure outlines key considerations for deploying models in the cloud, including the need for reliable infrastructure and efficient resource allocation to handle large datasets and complex computations."><img src="ml_systems_files/mediabag/4fce3d88761f81181ec3328739a10b05a000b663.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloud-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Cloud ML Capabilities</strong>: Cloud machine learning systems address challenges related to scale, complexity, and resource management through centralized computing infrastructure and specialized hardware. This figure outlines key considerations for deploying models in the cloud, including the need for reliable infrastructure and efficient resource allocation to handle large datasets and complex computations.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-b564" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-b564">Characteristics</h3>
<p>Cloud ML’s defining characteristic is its centralized infrastructure that operates at unprecedented scale. <a href="#fig-cloudml-example" class="quarto-xref">Figure&nbsp;4</a> illustrates this concept with an example from Google’s Cloud TPU<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> data center. As detailed in <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a>, cloud systems like the NVIDIA DGX A100 and Google’s TPU v4 Pod represent a 100-1000x computational advantage over mobile devices, with &gt;1000 TFLOPS compute power and megawatt-scale power consumption. Cloud service providers offer virtual platforms with &gt;100GB/s memory bandwidth housed in globally distributed data centers<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. These centralized facilities enable computational workloads impossible on resource-constrained devices. However, this centralization introduces critical trade-offs: network round-trip latency of 100-500ms eliminates real-time applications, while operational costs scale linearly with usage.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Tensor Processing Unit (TPU)</strong>: Google’s custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers 1.1 exaflops of peak performance, representing one of the world’s largest publicly available ML clusters.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Hyperscale Data Centers</strong>: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoft’s data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes.</p></div></div><div id="fig-cloudml-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cloudml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/cloud_ml_tpu.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Cloud Data Center Scale: Large-scale machine learning systems require centralized infrastructure with massive computational resources and storage capacity. Google’s cloud TPU data center provides this need, housing specialized AI accelerator hardware to efficiently manage the demands of training and deploying complex models. Source: [@google2024gemini]."><img src="images/jpg/cloud_ml_tpu.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cloudml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Cloud Data Center Scale</strong>: Large-scale machine learning systems require centralized infrastructure with massive computational resources and storage capacity. Google’s cloud TPU data center provides this need, housing specialized AI accelerator hardware to efficiently manage the demands of training and deploying complex models. Source: <span class="citation" data-cites="google2024gemini">(<a href="#ref-google2024gemini" role="doc-biblioref">DeepMind 2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-google2024gemini" class="csl-entry" role="listitem">
DeepMind, Google. 2024. <span>“Gemini: A Family of Highly Capable Multimodal Models.”</span> <a href="https://blog.google/technology/ai/google-gemini-ai/">https://blog.google/technology/ai/google-gemini-ai/</a>.
</div></div></figure>
</div>
<p>Cloud ML excels in processing massive data volumes through parallelized architectures. Through distributed training across hundreds of GPUs, cloud systems complete tasks in hours that would require months on single devices. This enables training on datasets requiring hundreds of terabytes of storage and petaflops of computation—resources impossible on constrained devices. The detailed memory bandwidth analysis and optimization techniques that enable this performance are covered in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong> and <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p>
<p>The centralized infrastructure creates three key advantages. First, exceptional deployment flexibility: trained models deploy through cloud APIs<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, becoming accessible worldwide and integrating into applications across mobile, web, and IoT platforms regardless of end user resources. Second, seamless collaboration: multiple teams access and contribute to projects simultaneously, with integrated version control accelerating development from experimentation to production. Third, economic flexibility: pay-as-you-go pricing models<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> eliminate upfront capital expenditure, with resources scaling up during intensive training and down during lower demand.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<strong>ML APIs</strong>: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google’s Vision API launched in 2016, processing over 1 billion images monthly within two years, enabling developers to add AI capabilities without ML expertise.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Pay-as-You-Go Pricing</strong>: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware.</p></div></div></section>
<section id="sec-ml-systems-benefits-e12c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-e12c">Benefits</h3>
<p>Cloud ML’s value extends beyond raw computational power to transformational operational capabilities. Dynamic scalability enables organizations to adapt seamlessly to changing needs—as data volume grows or model complexity increases, systems scale without hardware investments, ensuring consistent performance across varying workloads. This elasticity proves particularly valuable during experimentation phases, where resource needs fluctuate unpredictably.</p>
<p>The ecosystem of tools and services accelerates development dramatically. Access to prebuilt models, AutoML<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> capabilities, and specialized APIs enables developers to implement sophisticated solutions by building on established advancements rather than developing capabilities from scratch. This democratization of AI reduces time-to-deployment from months to days for many applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>AutoML (Automated Machine Learning)</strong>: Automated systems that democratize ML by handling model selection, hyperparameter tuning, and feature engineering. Google AutoML Vision achieved 93.9% accuracy on ImageNet with minimal human intervention, compared to months of expert work for similar results.</p></div></div><p>Perhaps most significantly, cloud deployment makes advanced AI accessible to organizations of all sizes. Without requiring specialized hardware expertise or significant capital investment, teams can leverage the same infrastructure that powers industry-leading systems, scaling their ambitions with their success rather than being constrained by upfront infrastructure decisions.</p>
</section>
<section id="sec-ml-systems-challenges-e73b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-e73b">Challenges</h3>
<p>These substantial benefits come with corresponding trade-offs that organizations must carefully consider when adopting cloud ML deployment strategies.</p>
<p>Latency represents the most fundamental physical constraint in Cloud ML (as detailed in <a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>). Network round-trip delays force architectural decisions where autonomous vehicles requiring sub-10ms emergency responses must use local processing despite 10x higher hardware costs. Beyond these technical constraints, cloud latency introduces operational complexity through unpredictable response times that complicate performance monitoring, cascading failures when network issues affect multiple services simultaneously, and substantial difficulty debugging performance issues across geographically distributed infrastructure.</p>
<p>The centralization of data processing and storage inherent to cloud deployment raises significant privacy and security concerns. Transmitting sensitive data to remote data centers creates potential vulnerabilities to cyber-attacks and unauthorized access, with cloud environments presenting attractive targets for adversaries seeking to exploit valuable information repositories. Organizations must implement rigorous security measures including encryption, strict access controls, and continuous monitoring to address these risks. Furthermore, regulatory compliance becomes more complex when handling sensitive data in cloud environments, with regulations like GDPR<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> and HIPAA<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> imposing stringent requirements on data handling and processing.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<strong>GDPR (General Data Protection Regulation)</strong>: European privacy law effective 2018, imposing fines up to €20 million or 4% of global revenue for violations. Forces ML systems to implement “right to be forgotten” and data processing transparency, technically challenging for neural networks.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>HIPAA (Health Insurance Portability and Accountability Act)</strong>: US healthcare privacy law requiring strict data security measures. ML systems handling medical data must implement encryption, access controls, and audit trails, adding 30-50% to development costs but enabling $150B+ healthcare AI market.</p></div></div><p>Cost management presents another dimension of complexity that significantly affects deployment decisions. While cloud infrastructure eliminates upfront capital expenditure, operational expenses scale with usage in ways that can prove difficult to predict. Consider a production system serving 1 million daily inferences at $0.001 each: annual costs reach $365,000, compared to $100,000 for equivalent edge hardware purchased once. The break-even point occurs around 100,000-1,000,000 requests, directly shaping deployment strategy. Beyond these direct costs, cloud pricing structures introduce operational challenges through unpredictable monthly bills that complicate budgeting, usage spikes that can exceed allocated budgets by orders of magnitude, and the necessity of sophisticated monitoring systems to track expenditures across multiple models and services. Maintaining economic viability requires implementing cost governance frameworks that include automated resource scaling, inference caching strategies, and model optimization pipelines.</p>
<p>Network dependency creates yet another constraint for cloud ML implementations. The requirement for stable and reliable internet connectivity means that any disruption in network availability directly impacts system performance and availability. This dependency becomes particularly problematic in environments where network access is limited, unreliable, or prohibitively expensive. Building truly resilient ML systems demands reliable network infrastructure complemented by appropriate failover mechanisms or offline processing capabilities. Systematic approaches to system resilience, failure mode analysis, and resilient ML system design are detailed in <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
<p>Finally, vendor lock-in emerges as organizations become increasingly dependent on specific tools, APIs, and services from their chosen cloud provider. This dependency complicates future transitions between providers or platform migrations, with organizations potentially encountering challenges related to portability, interoperability, and significant cost implications. Strategic planning must therefore include careful evaluation of vendor offerings, consideration of long-term organizational goals, and explicit preparation for potential migration scenarios to mitigate these risks.</p>
<p>Successfully navigating these challenges requires thorough planning, thoughtful architectural design, and systematic risk mitigation strategies. Organizations must carefully balance Cloud ML benefits against these constraints based on their specific requirements, data sensitivity concerns, and business objectives, recognizing that proactive approaches to these challenges enable effective use of Cloud ML while maintaining essential properties including data privacy, security, cost effectiveness, and system reliability.</p>
</section>
<section id="sec-ml-systems-use-cases-348c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-348c">Use Cases</h3>
<p>Despite these challenges, Cloud ML has achieved widespread adoption across diverse domains, demonstrating how organizations successfully navigate trade-offs to deploy centralized computing power for transformative applications.</p>
<p>Virtual assistants like Siri and Alexa exemplify cloud ML’s ability to handle computationally intensive natural language processing at scale. Leveraging extensive cloud computational capabilities, they process and analyze voice inputs in real-time, understanding user queries, extracting relevant information, and generating intelligent responses. The cloud’s scalability enables handling vast numbers of concurrent interactions while continuously improving through exposure to diverse linguistic patterns and use cases.</p>
<p>Netflix and Amazon’s recommendation engines demonstrate another compelling use case. Processing massive datasets to uncover patterns in user preferences and behavior, these platforms employ collaborative filtering<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> and other machine learning techniques to offer personalized suggestions. Cloud computational resources enable continuous updates and refinements as user data grows, directly enhancing engagement while handling the complexity of analyzing billions of interactions.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Collaborative Filtering</strong>: Recommendation technique analyzing user behavior patterns to predict preferences. Netflix’s algorithm processes 100+ billion data points daily, with collaborative filtering contributing to 80% of watched content and saving $1 billion annually in customer retention.</p></div></div><p>Financial fraud detection has been revolutionized by cloud ML capabilities. Analyzing vast amounts of transactional data in real-time, ML algorithms trained on historical fraud patterns detect anomalies and suspicious behavior, enabling proactive fraud prevention that minimizes financial losses. The cloud’s capacity to process and store large transaction volumes while analyzing patterns across millions of accounts makes this application particularly effective.</p>
<p>Beyond these flagship applications, cloud ML permeates everyday online experiences: personalized advertisements on social media, predictive text in email services, product recommendations in e-commerce, enhanced search results, and automated photo tagging. Continuously learning and adapting to user preferences through cloud resources, these applications create increasingly intuitive and personalized digital experiences.</p>
<p>Security applications further demonstrate cloud ML’s versatility. Anomaly detection systems continuously monitor user activities and system logs to identify unusual patterns, detecting potential cyber threats including unauthorized access attempts, malware infections, and data breaches. Cloud scalability proves essential for handling the growing complexity and volume of security data, enabling proactive protection against evolving threats across large-scale deployments.</p>
<div id="quiz-question-sec-ml-systems-cloudbased-machine-learning-7606" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary benefit of using Cloud ML for machine learning projects?</p>
<ol type="a">
<li>Reduced latency for real-time applications</li>
<li>Elimination of data privacy concerns</li>
<li>Complete independence from internet connectivity</li>
<li>Dynamic scalability and resource management</li>
</ol></li>
<li><p>True or False: Cloud ML eliminates the need for data privacy and security measures.</p></li>
<li><p>What are some challenges organizations face when implementing Cloud ML, and how might these be mitigated?</p></li>
<li><p>Order the following steps in deploying a machine learning model using Cloud ML: (1) Train model, (2) Deploy model, (3) Collect data, (4) Validate model.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-cloudbased-machine-learning-7606" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-edge-machine-learning-06ec" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-edge-machine-learning-06ec">Edge Machine Learning</h2>
<p>Building on our understanding of Cloud ML’s strengths and limitations, we now examine how Edge ML emerged as a direct response to latency and privacy constraints inherent in centralized processing.</p>
<p>Edge Machine Learning addresses cloud’s limitations by moving computation closer to data sources, trading unlimited computational resources for sub-100ms latency and local data sovereignty. This paradigm becomes essential for time-sensitive applications like autonomous systems and industrial IoT<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> that cannot tolerate cloud round-trip delays, and for applications where data privacy regulations prevent cloud processing. Edge devices, such as gateways and IoT hubs<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, maintain acceptable performance with intermediate resource constraints.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Industrial IoT</strong>: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025.</p></div><div id="fn16"><p><sup>16</sup>&nbsp;<strong>IoT Hubs</strong>: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making.</p></div></div><div id="callout-definition*-1.2" class="callout callout-definition" title="Definition of Edge ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Edge ML</summary><div><strong>Edge Machine Learning (Edge ML)</strong> describes the deployment of machine learning models at or near the <em>edge of the network</em>. These systems operate in the <em>tens to hundreds of watts</em> range and rely on <em>localized hardware</em> optimized for <em>real-time processing</em>. Edge ML minimizes <em>latency</em> and enhances <em>privacy</em> by processing data locally, but its primary limitation lies in <em>restricted computational resources</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Edge ML through four key dimensions. <a href="#fig-edge-ml" class="quarto-xref">Figure&nbsp;5</a> provides an overview of this section.</p>
<div id="fig-edge-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edge-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d6102b6914945d2473fe48f7db0c020fb8a79aca.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Edge ML Dimensions: This figure outlines key considerations for edge machine learning, contrasting challenges with benefits and providing representative examples and characteristics. Understanding these dimensions enables designing and deploying effective AI solutions on resource-constrained devices."><img src="ml_systems_files/mediabag/d6102b6914945d2473fe48f7db0c020fb8a79aca.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edge-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Edge ML Dimensions</strong>: This figure outlines key considerations for edge machine learning, contrasting challenges with benefits and providing representative examples and characteristics. Understanding these dimensions enables designing and deploying effective AI solutions on resource-constrained devices.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-09e1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-09e1">Characteristics</h3>
<p>Edge ML’s diversity spans wearables, industrial sensors, and smart home appliances—devices that process data locally<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> without depending on central servers (<a href="#fig-edgeml-example" class="quarto-xref">Figure&nbsp;6</a>).</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>IoT Device Growth</strong>: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management.</p></div></div><p>Edge ML operates under intermediate resource constraints that require careful architectural optimization. As shown in <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a>, edge devices like the NVIDIA Jetson AGX Orin and Intel NUC 12 Pro occupy the middle ground between cloud systems and mobile devices in terms of computational resources, power consumption, and cost. Memory bandwidth at 25-100 GB/s enables models requiring 100MB-1GB parameters but blocks larger models that dominate cloud applications. These constraints drive specific algorithmic choices: edge inference systems typically use model optimization techniques (explored in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) to reduce memory footprint by 4x and achieve 2-4x speedup compared to cloud models. Local processing eliminates network round-trip latency of 100-500ms, enabling real-time applications with &lt;100ms response requirements. The bandwidth savings become significant at scale: processing 1000 camera feeds locally at 1Mbps each avoids 1Gbps uplink costs and reduces cloud processing expenses by $10,000-100,000 annually.</p>
</section>
<section id="sec-ml-systems-benefits-4fb7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-4fb7">Benefits</h3>
<p>Edge ML’s quantifiable advantages stem from eliminating network dependencies and optimizing for local resource constraints. Latency reduction from 100-500ms (cloud) to 1-50ms (edge) represents a 4-40x improvement that enables entirely new application categories. This improvement becomes essential in safety-critical scenarios: autonomous vehicles requiring &lt;10ms emergency braking decisions, industrial robotics needing &lt;1ms precision control, and augmented reality demanding &lt;20ms motion-to-photon latency for comfort. Energy efficiency improves through localized computation: a smartphone processing 1000 images locally at 0.1J per inference consumes 100J total, while uploading to cloud requires 1-10J per image for wireless transmission alone, creating 10-100x energy penalty that eliminates cloud processing for battery-powered applications<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Latency-Critical Applications</strong>: Autonomous vehicles require &lt;10ms response times for emergency braking decisions. Industrial robotics needs &lt;1ms for precision control. Cloud round-trip latency typically ranges from 100-500ms, making edge processing essential for safety-critical applications.</p></div></div><p>Beyond latency improvements, edge ML delivers substantial bandwidth savings that translate directly to operational cost reductions. Consider a retail store with 50 cameras streaming video at 2 Mbps each: cloud processing would require 100 Mbps uplink bandwidth continuously, costing approximately $1,000-2,000 monthly for dedicated connectivity. Edge processing reduces this to transmitting only metadata and alerts, typically requiring &lt;1 Mbps total, reducing bandwidth costs by 99%. At scale, these savings become transformative—a city deploying 10,000 traffic cameras would face prohibitive connectivity costs with cloud processing, while edge deployment makes the system economically viable.</p>
<p>Edge ML provides improved data privacy through local processing, eliminating network transmission risks and simplifying regulatory compliance. Healthcare applications avoid HIPAA transmission complexities, financial institutions protect customer data, and manufacturing facilities secure proprietary information. This local processing proves essential for data sovereignty requirements across various jurisdictions.</p>
<p>The operational resilience of edge ML systems provides another significant advantage over cloud-dependent architectures. Edge systems continue functioning during network outages, maintaining critical operations when connectivity fails. A manufacturing plant with edge-based quality control systems maintains production during internet disruptions, while cloud-dependent systems would halt operations. Similarly, edge-based building management systems continue optimizing heating, cooling, and security even when external network connections fail, ensuring continuous facility operation and occupant safety.</p>
</section>
<section id="sec-ml-systems-challenges-2714" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-2714">Challenges</h3>
<p>These compelling benefits come with corresponding trade-offs that organizations must carefully navigate when implementing edge ML systems.</p>
<p>The primary technical constraint stems from limited computational resources compared to cloud-based solutions. Edge servers<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> typically provide 10-100x less processing power and storage capacity than cloud servers, fundamentally limiting the complexity and sophistication of deployable machine learning models. A cloud server might train or run models with billions of parameters, while edge servers must operate with models containing millions of parameters or fewer. This resource disparity forces difficult architectural decisions: organizations must either accept reduced model accuracy when deploying to edge servers or invest in expensive model compression and optimization efforts detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>. The constraint becomes particularly acute for applications requiring ensemble methods or multiple specialized models, where cloud deployments might run dozens of models concurrently while edge servers must carefully select which capabilities to include.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Edge Server Constraints</strong>: Typical edge servers have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques. These edge servers act as gateways between smaller IoT devices and cloud infrastructure.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Edge Network Coordination</strong>: For n edge devices, the number of potential communication paths is n(n-1)/2. A network of 1,000 devices has 499,500 possible connections to manage. Software-defined networking and edge orchestration platforms like Kubernetes K3s help manage this complexity.</p></div></div><p>Managing a distributed network of edge nodes introduces operational complexity that scales nonlinearly with deployment size. Ensuring all nodes operate efficiently and remain current with the latest algorithms and security protocols presents substantial logistical challenges. This distributed management problem grows with the square of device count: coordinating 1,000 edge devices requires managing 499,500 potential communication paths<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. Organizations must implement sophisticated orchestration systems to handle version control, gradual rollouts, and rollback capabilities across thousands or millions of devices. When a model update causes unexpected behavior, diagnosing and resolving issues across a distributed fleet proves far more complex than in centralized cloud deployments. Furthermore, edge devices often operate in diverse network conditions, requiring robust update mechanisms that can handle intermittent connectivity, limited bandwidth, and varying latency profiles.</p>
<p>Security challenges at the edge present unique concerns that differ substantially from cloud security paradigms. While Edge ML offers enhanced data privacy through local processing, edge nodes can be more vulnerable to physical tampering and theft, particularly in unsecured environments like retail stores, public infrastructure, or industrial facilities. An attacker gaining physical access to an edge device might extract models, manipulate inputs, or compromise the device entirely. Unlike cloud data centers with extensive physical security measures, edge devices deployed in accessible locations must rely on hardware-based security features like secure enclaves and encrypted storage, adding cost and complexity. Additionally, the distributed nature of edge deployments increases the attack surface: securing 10,000 edge devices requires addressing 10,000 potential entry points, compared to securing a handful of data centers. Organizations must implement defense-in-depth strategies including secure boot processes, encrypted communication channels, and anomaly detection systems to identify compromised nodes.</p>
<p>The heterogeneity of edge server hardware presents another significant challenge for deployment and maintenance. Unlike cloud environments where organizations control infrastructure specifications, edge deployments often must support diverse edge server platforms with varying capabilities. A retail deployment might include edge servers ranging from high-performance NVIDIA Jetson boards to lower-cost ARM-based systems, each requiring different model optimizations and deployment configurations. This hardware diversity complicates model development, testing, and validation: what works efficiently on one edge server type might perform poorly on another. Organizations must either maintain multiple model variants for different hardware profiles or develop models that can adapt dynamically to available resources, both approaches increasing development and maintenance overhead.</p>
<p>Finally, the initial deployment costs for edge ML can be substantial despite long-term operational savings. Each edge location requires upfront hardware investment, typically ranging from $500-2,000 per edge server for capable edge computing platforms like NVIDIA Jetson or Intel NUC systems. For a deployment spanning 1,000 locations, initial hardware costs alone reach $500,000-2,000,000 before considering installation, configuration, and integration expenses. While these costs are offset by reduced bandwidth expenses and cloud computing fees over time, organizations must have sufficient capital to fund the initial deployment and patience to realize returns over multi-year periods.</p>
<div id="fig-edgeml-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edgeml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/edge_ml_iot.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Edge Device Deployment: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse."><img src="images/jpg/edge_ml_iot.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edgeml-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Edge Device Deployment</strong>: Diverse IoT devices, from wearables to home appliances, enable decentralized machine learning by performing inference locally, reducing reliance on cloud connectivity and improving response times. Source: Edge Impulse.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ml-systems-use-cases-05eb" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-05eb">Use Cases</h3>
<p>Despite these challenges, Edge ML has achieved widespread deployment across diverse industries, demonstrating how organizations successfully balance the trade-offs between computational constraints and operational requirements. These real-world applications illustrate scenarios where low latency, data privacy, and operational resilience justify the additional complexity of edge deployment.</p>
<p>Autonomous vehicles represent perhaps the most demanding application of edge ML, where safety-critical decisions must occur within milliseconds based on sensor data that cannot be transmitted to remote servers. As introduced in <strong>?@sec-introduction-autonomous-vehicles-2910</strong>, systems like Waymo demonstrate how autonomous vehicles generate 4-20 terabytes of data daily from cameras, LiDAR, radar, and ultrasonic sensors, processing this information through multiple edge-based neural networks for object detection, path planning, and decision-making. Tesla’s Full Self-Driving system, for example, processes inputs from eight cameras at 36 frames per second through custom edge hardware, making driving decisions with latencies under 10ms. The physical impossibility of cloud processing for these applications stems from fundamental constraints: even at the speed of light, transmitting sensor data to a cloud server and receiving control commands would require 100-200ms minimum, far exceeding the &lt;10ms budget for emergency braking decisions. Furthermore, autonomous vehicles must maintain full functionality in areas with limited or no network connectivity, from rural highways to underground parking structures, making local processing not just preferable but essential for safe operation.</p>
<p>Smart retail environments demonstrate edge ML’s practical advantages for privacy-sensitive, bandwidth-intensive applications. Major retailers deploy edge-based computer vision systems that analyze customer behavior, optimize store layouts, and prevent theft without transmitting video footage off-site. Amazon Go stores process video from hundreds of cameras through local edge servers, tracking customer movements and item selections to enable checkout-free shopping. This edge-based approach addresses both technical and privacy concerns: transmitting high-resolution video from 100+ cameras would require prohibitive bandwidth (200+ Mbps sustained), while local processing ensures customer video never leaves the premises, addressing privacy concerns and regulatory requirements. The system analyzes approximately 100 million frames daily per store location, detecting and tracking thousands of shopper-item interactions with sub-second latency, demonstrating edge ML’s capability to handle complex, real-time computer vision tasks at scale.</p>
<p>Smart buildings and infrastructure utilize edge ML to optimize energy consumption, enhance security, and improve occupant comfort while maintaining operational continuity during network outages. Commercial buildings equipped with edge-based building management systems process data from thousands of sensors monitoring temperature, occupancy, air quality, and energy usage. These systems use ML models to predict heating and cooling needs, optimize lighting based on natural light and occupancy patterns, and identify equipment anomalies before failures occur. A typical 500,000 square foot office building might deploy 5,000-10,000 sensors generating data continuously, with edge processing reducing cloud transmission requirements by 95% while enabling sub-second response times for comfort adjustments. The resilience advantage becomes particularly valuable during network disruptions: edge-based building systems maintain climate control, security monitoring, and elevator management during internet outages that would disable cloud-dependent systems, ensuring both occupant comfort and safety.</p>
<p>The Industrial IoT<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> leverages edge ML for applications where millisecond-level responsiveness and operational continuity directly impact production efficiency and worker safety. Manufacturing facilities deploy edge ML systems for real-time quality control, predictive maintenance, and process optimization. In automotive manufacturing, edge-based vision systems inspect thousands of welds per vehicle, detecting defects as small as 0.1mm in real-time to prevent defective parts from progressing down the production line. These systems process high-resolution imagery at line speeds of 60+ parts per minute, making accept/reject decisions within 100ms to maintain production flow. Predictive maintenance applications analyze vibration, temperature, and acoustic data from rotating equipment to detect bearing wear, misalignment, and other failure precursors days or weeks before breakdown occurs. A single pulp and paper mill might monitor 10,000+ industrial assets through edge devices, analyzing sensor data locally to generate maintenance predictions without exposing proprietary production data to external networks. This approach has demonstrated 25-35% reductions in unplanned downtime and 20-25% reductions in maintenance costs across various manufacturing sectors<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Industry 4.0</strong>: Fourth industrial revolution integrating cyber-physical systems, IoT, and cloud computing into manufacturing. Expected to increase productivity by 20-30% and reduce costs by 15-25% globally, with Germany leading adoption (83% of manufacturers) followed by US (54%).</p></div><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Predictive Maintenance</strong>: ML-driven maintenance scheduling based on equipment condition rather than fixed intervals. Reduces unplanned downtime by 35-45% and maintenance costs by 20-25%. GE saves $1.5 billion annually using predictive analytics across its industrial equipment.</p></div></div><p>Healthcare applications of edge ML demonstrate how privacy requirements and latency constraints drive local processing despite the availability of powerful cloud infrastructure. Hospitals deploy edge-based systems for patient monitoring, diagnostic assistance, and clinical decision support. Patient monitoring systems analyze ECG, blood pressure, respiratory rate, and other vital signs through local ML models that detect anomalies and predict deterioration hours before clinically evident signs appear. These systems must maintain operation during network outages and cannot transmit raw patient data off-premises due to HIPAA regulations and hospital policies. Operating rooms utilize edge ML for surgical assistance, with systems analyzing video feeds from surgical cameras to provide real-time guidance, instrument tracking, and complication detection. The latency requirements for these applications (sub-100ms for real-time feedback) and privacy mandates (raw surgical video cannot be transmitted externally) make edge processing the only viable deployment approach.</p>
<p>Smart city infrastructure increasingly relies on edge ML for traffic management, public safety, and resource optimization across urban environments. Traffic management systems process video from thousands of intersection cameras through local edge servers, detecting vehicles, pedestrians, and cyclists to optimize signal timing dynamically. A medium-sized city might deploy 1,000-5,000 traffic cameras, with edge processing reducing bandwidth requirements from 2-10 Gbps (for raw video transmission) to under 100 Mbps (for metadata and alerts), making the system economically feasible while enabling real-time response to changing traffic conditions. Public safety applications use edge ML for anomaly detection in surveillance video, identifying abandoned packages, unusual crowd behaviors, or safety hazards without transmitting video to central servers, addressing both bandwidth constraints and privacy concerns. These systems must operate reliably during network disruptions, as connectivity failures during emergencies would render cloud-based systems useless precisely when most needed.</p>
<div id="quiz-question-sec-ml-systems-edge-machine-learning-06ec" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>What is a primary benefit of using Edge Machine Learning over Cloud ML?</p>
<ol type="a">
<li>Increased computational resources</li>
<li>Reduced latency</li>
<li>Centralized data processing</li>
<li>Unlimited storage capacity</li>
</ol></li>
<li><p>True or False: Edge ML enhances data privacy by processing data locally rather than sending it to centralized servers.</p></li>
<li><p>What challenges might arise when deploying machine learning models on edge devices, and how can they be addressed?</p></li>
<li><p>Edge Machine Learning is crucial for applications requiring real-time decision making, such as ____. This is important because it allows for immediate processing and response.</p></li>
<li><p>Order the following benefits of Edge ML in terms of their impact on system performance: (1) Reduced latency, (2) Enhanced data privacy, (3) Lower bandwidth usage.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-edge-machine-learning-06ec" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-mobile-machine-learning-f5b5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-mobile-machine-learning-f5b5">Mobile Machine Learning</h2>
<p>While Edge ML solved critical challenges for industrial and infrastructure applications, the proliferation of billions of personal computing devices created demand for ML capabilities optimized for user-centric applications. Mobile ML extends edge principles to personal devices, emphasizing privacy, offline operation, and personalized experiences.</p>
<p>Edge ML addresses latency and privacy concerns but still requires dedicated edge infrastructure and ongoing connectivity. Mobile Machine Learning extends intelligence directly to personal devices, prioritizing user proximity, offline capability, and privacy while operating under strict power and thermal constraints.</p>
<p>Mobile ML integrates machine learning directly into portable devices like smartphones and tablets, providing users with real-time, personalized capabilities. This paradigm excels when user privacy, offline operation, and immediate responsiveness matter more than computational sophistication. Mobile ML supports applications such as voice recognition<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>, computational photography<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>, and health monitoring while maintaining data privacy through on-device computation. These battery-powered devices must balance performance with power efficiency and thermal management, making them ideal for frequent, short-duration AI tasks.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>Voice Recognition Evolution</strong>: Apple’s Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to &lt;50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines.</p></div><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Computational Photography</strong>: Combines multiple exposures and ML algorithms to enhance image quality. Google’s Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time.</p></div></div><div id="callout-definition*-1.3" class="callout callout-definition" title="Definition of Mobile ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Mobile ML</summary><div><strong>Mobile Machine Learning (Mobile ML)</strong> enables machine learning models to run directly on <em>portable, battery-powered devices</em> like smartphones and tablets. Operating within the <em>single-digit to tens of watts</em> range, Mobile ML leverages <em>on-device computation</em> to provide <em>personalized and responsive applications</em>. This paradigm preserves <em>privacy</em> and ensures <em>offline functionality</em>, though it must balance <em>performance</em> with <em>battery and storage limitations</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Mobile ML across four key dimensions, revealing how this paradigm balances capability with constraints. <a href="#fig-mobile-ml" class="quarto-xref">Figure&nbsp;7</a> provides an overview of Mobile ML’s capabilities discussed throughout this section.</p>
<div id="fig-mobile-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mobile-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="edadf11d2e25a4f078f21c852982acbd85f96dff.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Mobile ML Capabilities: Mobile machine learning systems balance performance with resource constraints through on-device processing, specialized hardware acceleration, and optimized frameworks. This figure outlines key considerations for deploying ML models on mobile devices, including the trade-offs between computational efficiency, battery life, and model performance."><img src="ml_systems_files/mediabag/edadf11d2e25a4f078f21c852982acbd85f96dff.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mobile-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Mobile ML Capabilities</strong>: Mobile machine learning systems balance performance with resource constraints through on-device processing, specialized hardware acceleration, and optimized frameworks. This figure outlines key considerations for deploying ML models on mobile devices, including the trade-offs between computational efficiency, battery life, and model performance.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-9792" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-9792">Characteristics</h3>
<p>The iPhone 15 Pro exemplifies mobile capabilities: 8GB RAM, 128GB-1TB storage, and 1-10 TOPS AI compute through specialized Neural Processing Units consuming 3-5W power—achieving 10-100x better energy efficiency than general-purpose processors. System on Chip architectures<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> integrate computation and memory to minimize data movement costs, the primary energy bottleneck. Mobile memory bandwidth of 25-50 GB/s limits deployable models to 10-100MB parameter sets, requiring aggressive model compression and optimization techniques (detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) for practical deployment. Battery constraints prove critical: at 5000-6000mAh capacity (~18-22Wh), continuous 1W ML processing reduces device lifetime from 24 to 18 hours, making energy optimization essential for sustaining &lt;50ms UI response times<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Mobile System-on-Chip</strong>: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Apple’s A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor.</p></div><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Neural Processing Unit (NPU)</strong>: Specialized processors optimized for neural network operations. Apple’s Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcomm’s Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming &lt;1W.</p></div><div id="fn27"><p><sup>27</sup>&nbsp;<strong>TensorFlow Lite</strong>: Google’s mobile ML framework launched in 2017, designed to run models &lt;100MB with &lt;100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide.</p></div><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Core ML</strong>: Apple’s framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models.</p></div><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Model Optimization</strong>: Techniques that reduce model size and computational requirements while maintaining accuracy. These optimizations enable deployment on resource-constrained devices (detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>).</p></div></div><p>Mobile ML is supported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> for Android devices and Core ML<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimization techniques<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> to ensure smooth performance within mobile resource constraints.</p>
</section>
<section id="sec-ml-systems-benefits-99f9" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-99f9">Benefits</h3>
<p>Mobile ML delivers real-time processing capabilities that eliminate latency entirely, enabling instantaneous user interactions that feel magical in their responsiveness. Unlike cloud ML requiring 100-500ms network round-trips or even edge ML with 1-50ms local processing delays, mobile ML achieves sub-10ms response times for most tasks, with some operations completing in under 1ms. Face detection on modern smartphones processes camera frames at 60fps (16.7ms per frame) with detection latency under 5ms, enabling smooth, real-time augmented reality effects. Voice assistants respond to wake words within 2-3ms, faster than human perception of delay. Real-time translation applications process speech and display translated text with end-to-end latency under 100ms, enabling natural conversation flow<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>. This imperceptible latency creates user experiences impossible with remote processing, where even small delays break immersion and reduce usability.</p>
<div class="no-row-height column-margin column-container"><div id="fn30"><p><sup>30</sup>&nbsp;<strong>Real-Time Translation</strong>: Google Translate app can translate conversations in 40+ languages offline using on-device neural networks. The offline models are 35-45MB each versus 2GB+ for cloud versions, achieving 90% of cloud accuracy while enabling instant translation without internet.</p></div><div id="fn31"><p><sup>31</sup>&nbsp;<strong>Mobile Face Detection</strong>: Apple’s Face ID uses a 30,000-dot projector and neural networks to create 3D face maps in &lt;2 seconds. The system processes biometric data entirely on-device using the Secure Enclave, making it practically impossible to extract face data even with physical device access.</p></div></div><p>Privacy advantages of mobile ML extend beyond simple local processing to create fundamentally different trust models. Unlike edge servers that organizations control, mobile devices belong to users, ensuring complete data sovereignty. Face ID exemplifies this architecture: the system processes biometric data entirely on-device using the Secure Enclave<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>, a hardware-isolated processor where even the main OS cannot access data, making extraction practically impossible even with physical device access. This architecture proves so robust that banking applications trust Face ID for authentication despite initial skepticism about biometric security. Keyboard prediction models learn individual writing patterns, vocabulary preferences, and frequently used phrases entirely locally, with iOS and Android maintaining these models in sandboxed storage that applications cannot access. Health monitoring applications process sensitive vital signs, exercise data, and sleep patterns on-device, ensuring HIPAA compliance without complex infrastructure. The privacy guarantee extends to development: many mobile ML models train using federated learning approaches where model updates occur on-device and only encrypted gradients are transmitted for aggregation, never exposing raw user data.</p>
<p>Mobile ML’s offline functionality transforms how users interact with AI-powered features, eliminating dependency on network availability while reducing operational costs. Applications function identically whether users have high-speed WiFi, poor cellular connectivity, or no connection whatsoever. Google Maps processes complex navigation queries, real-time traffic rerouting, and place searches entirely offline once map data downloads, analyzing millions of road segments without server communication. Offline translation supports conversations in 40+ language pairs with 90% of online accuracy, requiring only 35-45MB per language versus 2GB+ for cloud models. Music identification apps like Shazam process audio fingerprints locally, matching against databases of millions of songs stored in compressed form on-device. This offline capability proves particularly valuable in developing markets where connectivity remains expensive or unreliable, and in developed markets during travel where roaming costs discourage data usage. Airlines deploy mobile ML for aircraft maintenance, enabling technicians to identify parts, access repair procedures, and document work in environments where network access is restricted or unavailable.</p>
<p>Personalization reaches unprecedented depth through mobile ML’s direct access to user context and behavior patterns. Unlike cloud systems that anonymize data or edge systems with limited user history, mobile devices accumulate rich behavioral data spanning months or years of interaction. Mobile operating systems learn when users typically wake up, commute patterns, frequently visited locations, app usage rhythms, and communication patterns to optimize device behavior proactively. iOS predicts which app users will open next with 70-80% accuracy by analyzing time of day, location, connected accessories, and recent activities, preloading app data to eliminate startup delays. Notification management systems use ML to determine optimal delivery times based on when users typically interact with each app type, reducing interruptions during focus periods while ensuring important messages arrive promptly. Camera systems adapt processing pipelines to individual preferences learned from which photos users keep versus delete, automatically adjusting color temperature, contrast, and detail based on implicit feedback. These personalization systems operate continuously in background, adapting to changing user patterns while consuming minimal battery through efficient inference and smart scheduling.</p>
</section>
<section id="sec-ml-systems-challenges-aa62" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-aa62">Challenges</h3>
<p>These advantages come with significant trade-offs that require careful engineering to navigate successfully.</p>
<p>Resource constraints on mobile devices impose hard limits that fundamentally shape deployable model architectures. Memory constraints prove particularly restrictive: flagship phones with 12-24GB RAM allocate only 100MB-1GB to individual ML applications due to OS memory management and the need to support dozens of concurrent apps. This allocation represents 0.5-5% of total memory, forcing model architectures under 100-500MB total size including weights, activations, and intermediate buffers. For perspective, GPT-3 requires 350GB just for model parameters, making such architectures completely infeasible on mobile devices even with aggressive compression. Processing power limitations compound these memory constraints: mobile NPUs delivering 1-10 TOPS compare unfavorably to cloud GPUs providing 100-1000 TOPS, and critically, mobile NPUs cannot sustain peak performance continuously due to thermal constraints. A mobile device might burst to 10 TOPS for 30 seconds before thermal throttling reduces performance to 3-5 TOPS sustained<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>, forcing careful workload scheduling and optimization. Storage limitations create additional complexity: while flagship devices offer 512GB-2TB storage, app stores impose size limits (typically 150MB maximum initial download, 4GB total with additional assets) to ensure reasonable download times. These limits force developers to choose between model sophistication and feature breadth, or implement complex progressive download schemes that fetch models on-demand.</p>
<div class="no-row-height column-margin column-container"><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Mobile Device Constraints</strong>: Flagship phones typically have 12-24GB RAM and 512GB-2TB storage, versus cloud servers with 256-2048GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W.</p></div></div><p>Battery life presents the most visible constraint to users and the most complex engineering challenge to developers. ML inference power consumption varies dramatically by operation type: a single inference might consume 0.01-1.0 joules depending on model complexity, with larger, unoptimized models at the high end and compressed models at the low end. Consider a typical usage scenario: processing 100 inferences per hour at 0.1J each consumes 10J hourly, or 240J daily. With a 5000mAh battery at 3.7V providing 66,600J total capacity, ML operations alone could consume 0.36% of battery daily—but this compounds with the baseline 20-30% daily battery drain from all other operations, making ML efficiency critical. The power budget becomes particularly constrained during intensive use: processing video in real-time at 30fps requires 30 inferences per second, consuming 90-180J per minute at 0.1-0.2J per inference. At this rate, battery life drops from 24 hours to 6-8 hours, making thermal management and power optimization essential. Developers must balance model sophistication against power consumption through techniques including dynamic model selection (using simpler models when battery is low), inference rate throttling, and careful scheduling of background processing during charging periods.</p>
<p>The model development and deployment workflow for mobile ML introduces complexity absent in cloud development. Cross-platform deployment proves particularly challenging: developers must maintain separate model variants for iOS (using Core ML format) and Android (using TensorFlow Lite), each with platform-specific optimizations and different supported operations. A model working perfectly on iOS might require significant rework for Android due to differences in available hardware accelerators, optimization approaches, and framework capabilities. Worse, heterogeneity within platforms creates additional complexity: Android devices span an enormous performance range from $100 budget phones with minimal AI acceleration to $1500 flagships with sophisticated NPUs, requiring multiple model variants or adaptive architecture selection based on detected capabilities. Testing compounds these challenges: validating model behavior across hundreds of device configurations proves impractical, forcing developers to establish representative device matrices covering performance tiers, but edge cases inevitably emerge when users encounter unsupported hardware configurations or OS versions. Deployment and updates face additional friction: unlike cloud deployments where updates roll out instantly, mobile model updates require app store approval processes taking 1-7 days, during which bugs discovered in production cannot be fixed without jailbreak or complex dynamic loading schemes that most platforms restrict for security reasons.</p>
<p>Thermal management creates constraints unique to mobile deployment that don’t exist in cloud or edge contexts. Mobile devices must maintain surface temperatures under 45°C for user comfort and safety, but sustained ML operations generate substantial heat. The Apple A17 Pro chip, for example, can briefly achieve 35 TOPS but thermal limits force throttling to 10-15 TOPS sustained to prevent overheating. This thermal throttling occurs unpredictably based on ambient temperature, device usage history, and enclosure design, making performance guarantees impossible. Applications must implement adaptive strategies including reduced inference rates when devices heat up, model complexity reduction during thermal stress, and cooldown periods between intensive operations. The problem becomes acute for AR applications requiring sustained processing: Meta’s Quest headsets, operating in mobile-like thermal envelopes, must carefully balance visual quality, frame rate, and hand tracking against thermal budgets, with automatic quality reduction when temperatures exceed thresholds. Developers cannot rely on theoretical peak performance numbers from device specifications; production systems must monitor thermal state and adapt dynamically, adding substantial implementation complexity and requiring extensive testing across temperature ranges and usage patterns.</p>
<p>Finally, the fragmentation of mobile hardware and software ecosystems creates ongoing maintenance burdens. New device releases occur annually, each with different NPU architectures, memory configurations, and performance characteristics. Models optimized for one generation may perform suboptimally on the next, requiring continuous re-optimization. OS updates introduce additional complexity: iOS and Android major releases annually change underlying ML frameworks, add new capabilities, deprecate old APIs, and modify performance characteristics. A model deployed in iOS 15 might need rework for iOS 16 due to framework changes, and again for iOS 17 to leverage new hardware capabilities. This continuous adaptation requires dedicated engineering resources unlike cloud deployments where hardware and software updates happen transparently, or edge deployments where hardware typically remains stable for years.</p>
</section>
<section id="sec-ml-systems-use-cases-c808" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-c808">Use Cases</h3>
<p>Despite these constraints, Mobile ML has achieved transformative success across diverse applications, demonstrating how careful optimization enables sophisticated capabilities within mobile device limitations. These applications showcase the unique advantages of on-device processing for billions of users worldwide.</p>
<p>Computational photography represents perhaps the most visible success of mobile ML, transforming smartphone cameras into sophisticated imaging systems rivaling dedicated cameras costing thousands of dollars. Modern flagships process every photo through multiple ML pipelines operating in real-time. Portrait mode photography<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> uses depth estimation networks processing dual-camera or LiDAR data to generate accurate depth maps, then applies segmentation networks to separate subjects from backgrounds with pixel-level precision, enabling DSLR-quality bokeh effects without expensive optics. Night mode captures and aligns multiple exposures (typically 9-15 frames over 3-5 seconds), using ML-based image registration to compensate for hand motion, then applies denoising networks that preserve detail while reducing noise by 10-20dB compared to single shots. Google’s Pixel phones demonstrate the extreme: the same camera sensor used in Pixel 2 through Pixel 5 produced dramatically different image quality through ML advancements alone, with each generation processing 10-15 distinct ML models per photo for HDR merging, super-resolution, face retouching, and scene optimization. Document scanning applications detect paper edges with sub-pixel accuracy, apply perspective correction, enhance text contrast, and perform OCR entirely on-device, processing a full page in under 1 second. These capabilities democratized professional photography, with smartphone images now regularly appearing in major publications and even winning photography competitions.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Portrait Mode Photography</strong>: Uses dual cameras or LiDAR to create depth maps, then applies ML-based segmentation to separate subjects from backgrounds. iPhone’s Portrait mode processes multiple exposures in real-time, achieving DSLR-quality depth-of-field effects that would require expensive lenses and professional editing.</p></div></div><p>Voice-driven interactions demonstrate mobile ML’s transformation of human-device communication. Always-on wake word detection operates at &lt;1mW power consumption, activating sophisticated speech recognition, natural language understanding, and response generation—all within 100-200ms on-device latency. Siri achieves &lt;10ms recognition for simple commands while maintaining complete privacy. Keyboard prediction evolved from simple n-grams to context-aware neural models achieving 60-70% phrase prediction accuracy, reducing typing by 30-40% through continuous on-device learning. Google Translate exemplifies sophistication: real-time camera translation processes 100+ languages at 15-30fps on-device, overlaying translations in original positions and fonts, while conversation mode enables bilingual dialogue with sub-200ms translation latency for 40+ language pairs.</p>
<p>Apple Watch and similar wearables extract sophisticated health insights from simple sensors. Activity detection (running, swimming, cycling, yoga) exceeds 95% accuracy while counting repetitions and measuring intensity. FDA-cleared atrial fibrillation detection achieves 98%+ sensitivity/specificity by continuously analyzing heart rhythms. Fall detection distinguishes actual falls from similar motions (jumping, sitting), automatically contacting emergency services—a capability credited with saving lives. Sleep tracking identifies sleep stages with accuracy approaching dedicated labs, while camera-based pulse measurement extracts heart rates within 2-3 BPM of medical devices. Processing this extraordinarily sensitive data on-device maintains HIPAA compliance while providing clinical-grade monitoring accessible to billions.</p>
<p>Accessibility applications demonstrate transformative social impact through continuous, privacy-preserving local processing. Live Text detects and recognizes text from camera feeds in &lt;100ms, enabling instant text copying, translation, or information extraction. Sound Recognition alerts deaf users to critical environmental cues (fire alarms, doorbells, crying babies) through haptic feedback. VoiceOver generates natural language image descriptions, enabling blind users to understand visual content. Operating entirely locally with minimal battery impact, these features provide critical functionality without requiring internet connectivity.</p>
<p>ARCore and ARKit leverage mobile ML for real-time environment understanding at 60fps. Visual-inertial odometry tracks device position with centimeter-level accuracy while simultaneously mapping 3D surroundings. Object detection identifies surfaces and semantic features, enabling virtual objects to interact realistically with physical environments. Hand tracking extracts 21-joint 3D poses at 60fps for controller-free gesture interaction. Face tracking analyzes 50+ landmark meshes for real-time AR effects (Snapchat filters, Animoji). These applications demand consistent sub-16ms frame times with predictable latency, making only on-device processing viable.</p>
<p>Intelligent assistants that understand personal context represent mobile ML’s most ambitious application. Modern mobile operating systems use hundreds of small, specialized models working together to create cohesive user experiences. App prediction models analyze time, location, calendar events, and recent activities to predict which apps users will open with 70-80% accuracy, preloading them in memory to eliminate launch delays. Notification management systems use ML to determine optimal delivery times, grouping related notifications, surfacing important messages, and deferring less urgent items to avoid interruptions during focus periods. Battery optimization uses ML to predict when users will next charge their devices, adjusting background activity and process scheduling to ensure devices last until predicted charge times. These personalization systems process extraordinarily detailed behavior data—location history, app usage patterns, communication graphs, content preferences—that would raise severe privacy concerns if transmitted to cloud servers. On-device processing makes this deep personalization possible while maintaining user trust, as individuals retain complete control over their personal data with mathematical guarantees that it never leaves their devices.</p>
<div id="quiz-question-sec-ml-systems-mobile-machine-learning-f5b5" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary benefit of Mobile Machine Learning?</p>
<ol type="a">
<li>Unlimited computational resources</li>
<li>Reduced data privacy concerns</li>
<li>Increased dependency on cloud connectivity</li>
<li>Simplified model deployment</li>
</ol></li>
<li><p>True or False: Mobile ML can operate effectively without internet connectivity.</p></li>
<li><p>Discuss the trade-offs involved in optimizing machine learning models for mobile devices.</p></li>
<li><p>____ is a technique used in Mobile ML to reduce model size and speed up inference while maintaining accuracy.</p></li>
<li><p>In a production system, how might Mobile ML enhance user experience in real-time applications?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-mobile-machine-learning-f5b5" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-tiny-machine-learning-9d4a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-tiny-machine-learning-9d4a">Tiny Machine Learning</h2>
<p>The progression from Cloud to Edge to Mobile ML demonstrates increasing distribution of intelligence, but each still requires watts of power and hundreds of dollars in hardware cost. Tiny ML completes the deployment spectrum by enabling intelligence on devices costing &lt;$10 and consuming &lt;1mW, making ubiquitous sensing economically and practically feasible.</p>
<p>Mobile ML delivers personal AI but still requires sophisticated hardware with significant power and memory resources. Tiny Machine Learning pushes the deployment spectrum to its extreme, bringing intelligence to the smallest, most resource-constrained devices where traditional computing approaches become impossible. This paradigm prioritizes ultra-low power consumption and minimal cost over computational sophistication.</p>
<p>Tiny ML brings intelligence to the smallest devices, from microcontrollers<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> to embedded sensors, enabling real-time computation in severely resource-constrained environments. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and extreme energy efficiency. Tiny ML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>, often running for months or years on limited power sources such as coin-cell batteries<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;<strong>Microcontrollers</strong>: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM, still thousands of times less than a smartphone.</p></div><div id="fn35"><p><sup>35</sup>&nbsp;<strong>Energy Efficiency in TinyML</strong>: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume &lt;1µW in sleep mode and 100-300µW/MHz when active. Efficient ML inference can run for years on a single coin-cell battery.</p></div><div id="fn36"><p><sup>36</sup>&nbsp;<strong>Coin-Cell Batteries</strong>: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling “deploy-and-forget” IoT applications.</p></div></div><div id="callout-definition*-1.4" class="callout callout-definition" title="Definition of Tiny ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Tiny ML</summary><div><strong>Tiny Machine Learning (Tiny ML)</strong> refers to the execution of machine learning models on <em>ultra-constrained devices</em>, such as microcontrollers and sensors. These devices operate in the <em>milliwatt to sub-watt</em> power range, prioritizing <em>energy efficiency</em> and <em>compactness</em>. Tiny ML enables <em>localized decision making</em> in resource constrained environments, excelling in applications where <em>extended operation on limited power sources</em> is required. However, it is limited by <em>severely restricted computational resources</em>.<p></p>
</div></details>
</div>
<p>The analysis examines Tiny ML through four critical dimensions that define its unique position in the ML deployment spectrum. <a href="#fig-tiny-ml" class="quarto-xref">Figure&nbsp;8</a> encapsulates the key aspects of Tiny ML discussed in this section.</p>
<div id="fig-tiny-ml" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tiny-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="8ffc2616fc4390e0fec58576b8f19dc1081feeea.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: TinyML System Characteristics: Constrained devices necessitate a focus on efficiency, driving trade-offs between model complexity, accuracy, and energy consumption, while enabling localized intelligence and real-time responsiveness in embedded applications. This figure outlines key aspects of TinyML, including the challenges of resource limitations, example applications, and the benefits of on-device machine learning."><img src="ml_systems_files/mediabag/8ffc2616fc4390e0fec58576b8f19dc1081feeea.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tiny-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>TinyML System Characteristics</strong>: Constrained devices necessitate a focus on efficiency, driving trade-offs between model complexity, accuracy, and energy consumption, while enabling localized intelligence and real-time responsiveness in embedded applications. This figure outlines key aspects of TinyML, including the challenges of resource limitations, example applications, and the benefits of on-device machine learning.
</figcaption>
</figure>
</div>
<section id="sec-ml-systems-characteristics-d52d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-characteristics-d52d">Characteristics</h3>
<p>TinyML operates at hardware extremes<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>: Arduino Nano 33 BLE Sense (256KB RAM, 1MB Flash, 0.02-0.04W, $35) and ESP32-CAM (520KB RAM, 4MB Flash, 0.05-0.25W, $10) represent 30,000-50,000x memory reduction versus cloud systems (1TB RAM) and 160,000x power reduction (6.5kW). <a href="#fig-TinyML-example" class="quarto-xref">Figure&nbsp;9</a> illustrates these compact devices, some fitting in a palm, others as small as a fingernail<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>. These constraints enable months or years of autonomous operation but demand specialized algorithms delivering acceptable performance at &lt;1 TOPS compute with microsecond response times.</p>
<div class="no-row-height column-margin column-container"><div id="fn37"><p><sup>37</sup>&nbsp;<strong>On-Device Training Constraints</strong>: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation where multiple devices collaboratively train a shared model.</p></div><div id="fn38"><p><sup>38</sup>&nbsp;<strong>TinyML Device Scale</strong>: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Google’s Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously “dumb” objects like smart dust sensors.</p></div></div><div id="fig-TinyML-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-TinyML-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/tiny_ml.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: TinyML System Scale: These device kits exemplify the extreme miniaturization achievable with TinyML, enabling deployment of machine learning on resource-constrained devices with limited power and memory. such compact systems broaden the applicability of ML to previously inaccessible edge applications, including wearable sensors and embedded IoT devices. Source: [@warden2018speech]"><img src="images/png/tiny_ml.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-TinyML-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>TinyML System Scale</strong>: These device kits exemplify the extreme miniaturization achievable with TinyML, enabling deployment of machine learning on resource-constrained devices with limited power and memory. such compact systems broaden the applicability of ML to previously inaccessible edge applications, including wearable sensors and embedded IoT devices. Source: <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.”</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="https://doi.org/10.48550/arXiv.1804.03209">https://doi.org/10.48550/arXiv.1804.03209</a>.
</div></div></figure>
</div>
</section>
<section id="sec-ml-systems-benefits-020f" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-benefits-020f">Benefits</h3>
<p>Tiny ML’s primary benefit stems from eliminating data transmission entirely, achieving response latencies measured in microseconds rather than milliseconds. Without network stacks, protocol overhead, or even inter-process communication delays, TinyML systems respond to sensor inputs within 10-100 microseconds—fast enough to control real-time systems requiring sub-millisecond response times. Industrial vibration monitoring processes accelerometer data at 10kHz sampling rates with &lt;50μs processing latency per sample, detecting bearing failures before they cause damage. Audio wake-word detection analyzes 16kHz audio streams in real-time with total algorithmic latency under 100μs, enabling instant response to voice commands. This microsecond-scale responsiveness opens applications impossible even with mobile ML: precision manufacturing systems use TinyML for real-time quality control at production speeds exceeding 1000 parts per minute, where even 1ms processing delays would miss defects.</p>
<p>The economic advantages of TinyML prove transformative for massive-scale deployments where per-unit costs dominate total system economics. A complete TinyML system on a coin (an ESP32-CAM module with camera, WiFi, and microcontroller) costs $8-12 in volume, enabling deployment scenarios impossible at higher price points. Agricultural monitoring systems can deploy 1000 soil sensors for $10,000 total hardware cost versus $500,000-1,000,000 for equivalent cellular-connected mobile or edge solutions. Building occupancy monitoring can instrument every room with individual sensors for under $5,000 versus $50,000+ for camera-based edge solutions. At city scale, deploying 100,000 environmental sensors becomes economically viable at $1-2M for TinyML versus $50-100M for edge-based approaches. This cost structure enables entirely new applications: disposable smart packaging that monitors food freshness, biodegradable environmental sensors for temporary deployments, and ubiquitous sensing in developing regions where connectivity costs prohibit cloud or edge solutions.</p>
<p>Energy efficiency enables deployment scenarios literally impossible with higher-power alternatives. TinyML devices consuming 1-10mW average power can operate for 1-10 years on coin-cell batteries (CR2032: 200mAh at 3V ≈ 2,160 joules), versus mobile devices requiring daily charging or edge devices needing continuous power. This ultra-low-power operation enables applications including wildlife tracking collars that monitor animal behavior for years without recapture, structural health sensors embedded in concrete during construction that monitor building integrity for decades, and agricultural soil sensors distributed across remote farmland where power infrastructure doesn’t exist. The operational advantage extends beyond battery life: devices can energy-harvest from ambient sources including solar (indoor lighting provides 10-100μW/cm²), vibration (industrial equipment generates 100μW-10mW), and thermal differentials (5-10°C differences generate 10-100μW), enabling perpetual operation without batteries. Maintenance costs drop to near-zero when devices never require battery replacement or charging, transforming the economics of large-scale sensing deployments.</p>
<p>Privacy and security advantages of TinyML surpass even mobile ML through physical data confinement. Data never leaves the sensor—not through wireless transmission, not through wired connections, not even through debug interfaces if properly configured. Medical wearables process cardiac rhythms, detecting arrhythmias through local analysis without transmitting raw ECG data. Smart home sensors detect occupancy, motion, and activities without transmitting video or audio. Factory quality control systems identify defects through local analysis without exposing proprietary manufacturing data or product designs. This extreme privacy proves particularly valuable for sensitive applications where even encrypted transmission poses unacceptable risks: voting systems, nuclear facility monitoring, military applications, and financial transaction processing. The privacy guarantee is mathematical: if data never leaves the device and device memory is volatile, data physically cannot leak through communication channels—something impossible to guarantee in networked systems regardless of encryption strength.</p>
</section>
<section id="sec-ml-systems-challenges-297b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-challenges-297b">Challenges</h3>
<p>Achieving TinyML’s remarkable benefits requires navigating extreme resource constraints that fundamentally limit model sophistication and application scope.</p>
<p>Computational and memory constraints impose hard limits orders of magnitude more severe than mobile ML. Typical microcontrollers provide 256KB-2MB RAM and 1-8MB flash storage, versus smartphones with 12-24GB RAM and 512GB+ storage—a 5,000-50,000x difference. This extreme disparity means models that fit comfortably on mobile devices become completely infeasible for TinyML: a 50MB mobile model would consume all available storage on most microcontrollers just for weights, leaving no memory for code, data, or inference execution. In practice, TinyML models must stay under 100-500KB total size including weights, activations, and intermediate buffers. Processing power proves equally constrained: microcontrollers deliver 1-100 MOPS (million operations per second) versus mobile NPUs providing 1,000-10,000 GOPS (billion operations per second)—a 10,000-1,000,000x difference. This forces model architectures with dramatically fewer parameters: while mobile models might contain 1-10 million parameters, TinyML models typically contain 10,000-100,000 parameters, a 100x reduction that fundamentally limits model capacity and accuracy. The memory constraint cascades through the entire model architecture: not only must weights be small, but intermediate activations during inference must fit in remaining memory, forcing shallow networks with narrow layers that sacrifice accuracy for feasibility.</p>
<p>The development workflow for TinyML introduces complexity absent in higher-level paradigms, requiring expertise spanning multiple specialized domains. Developers must understand neural network architectures at the lowest level: which operations specific microcontrollers support in hardware, how memory layouts affect cache performance, how optimization techniques affect different layer types, and how to balance model accuracy against inference latency and power consumption. Unlike mobile or cloud development where frameworks abstract these details, TinyML developers must manually optimize models at the operator level. The toolchain complexity compounds challenges: models typically train in Python using TensorFlow or PyTorch, convert to TensorFlow Lite format, apply aggressive compression and optimization techniques (detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>), optimize for specific hardware using vendor tools (CMSIS-NN for ARM Cortex-M, TensorRT for embedded GPUs), and finally compile to device-specific binaries—with each step potentially introducing accuracy degradation or compatibility issues. Debugging proves particularly challenging: microcontrollers lack debugging infrastructure common in desktop development, forcing developers to use oscilloscopes, logic analyzers, and JTAG debuggers to diagnose timing issues, memory corruption, and numerical precision problems. Model validation requires extensive testing across temperature ranges (-40°C to +85°C for industrial applications), supply voltage variations (batteries degrade over time), and electromagnetic interference conditions that don’t affect software running on stable power supplies and controlled environments.</p>
<p>Model accuracy suffers inherently from the extreme compression required for TinyML deployment. While mobile models might maintain 90-95% of cloud model accuracy, TinyML models typically achieve 70-85% accuracy of equivalent full-precision models—a degradation that makes them unsuitable for many applications requiring high precision. This accuracy loss stems from multiple sources that we’ll explore in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>: the dramatic parameter reduction (100-1000x fewer parameters than cloud models) limits model capacity, aggressive numerical precision reduction introduces approximation errors, and simplified architectures (shallow networks, narrow layers, restricted operations) reduce model expressiveness. Developers must carefully evaluate whether reduced accuracy remains acceptable for specific applications: 80% accuracy detecting manufacturing defects might be useless (missing 20% of defects), while 80% accuracy for voice wake-word detection proves entirely acceptable (occasional missed activations are annoying but not critical). The challenge becomes acute when TinyML models must compete against cloud or mobile alternatives: users comparing a 95% accurate cloud model against an 80% accurate TinyML version will perceive the on-device option as inferior unless latency, privacy, or offline operation advantages overwhelmingly compensate for accuracy loss<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn39"><p><sup>39</sup>&nbsp;<strong>TinyML Model Optimization</strong>: Specialized techniques that dramatically reduce model size and computational requirements. A typical smartphone model of 50MB might optimize to 250KB for microcontroller deployment while retaining 95% accuracy (techniques detailed in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>).</p></div></div><p>Limited computational resources create inflexibility that complicates deployment and maintenance. Unlike cloud systems where models scale elastically or mobile devices that support multiple concurrent models, TinyML devices typically run a single fixed model determined at deployment time. Updating models requires firmware flashing through physical connections or over-the-air updates that consume substantial power and pose bricking risks if interrupted. This inflexibility forces difficult trade-offs: deploying a generalized model that handles multiple scenarios with mediocre performance, or deploying specialized models that excel at specific tasks but cannot adapt to changing requirements. Environmental sensors exemplify this challenge: should a wildlife monitoring device optimize for species common in the deployment region (limiting detection to known species) or deploy a more general but less accurate model that might detect unexpected species? Agricultural sensors face similar dilemmas: optimize for crops currently planted or deploy general models that work suboptimally for all crops? The deployment permanence makes these decisions critical—unlike cloud or edge systems where models update frequently, TinyML devices might operate for years with initial models.</p>
<p>Finally, ecosystem fragmentation across microcontroller vendors, toolchains, and frameworks creates substantial development overhead. Each vendor provides different development tools, debugging interfaces, and optimization libraries that require specialized expertise. ARM Cortex-M series dominates but encompasses numerous variants (M0, M0+, M3, M4, M7, M33, M55) with different instruction sets, optional hardware floating-point units, and varying DSP capabilities. Microchip, STMicroelectronics, Nordic Semiconductor, Espressif, and others each provide proprietary peripherals and development environments. This fragmentation means code targeting one microcontroller family often requires significant rework for others, unlike mobile development where iOS and Android provide relatively stable platforms or cloud development where infrastructure differences remain abstracted. The toolchain fragmentation compounds challenges: TensorFlow Lite Micro, CMSIS-NN, uTensor, and numerous vendor-specific frameworks provide overlapping but incompatible capabilities, forcing developers to choose platforms early and accept lock-in or invest heavily in multi-platform support.</p>
</section>
<section id="sec-ml-systems-use-cases-3c3f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-use-cases-3c3f">Use Cases</h3>
<p>Despite these formidable challenges, Tiny ML has achieved remarkable success across diverse domains where its unique advantages—ultra-low power, minimal cost, and complete data privacy—enable applications impossible with other paradigms.</p>
<p>Industrial predictive maintenance demonstrates TinyML’s ability to transform traditional infrastructure through distributed intelligence. Manufacturing facilities deploy thousands of vibration sensors on motors, pumps, bearings, and other rotating equipment, with each sensor running TinyML models that analyze vibration signatures in real-time to detect early failure indicators. These sensors operate continuously for 5-10 years on coin-cell batteries, consuming &lt;2mW average power by duty-cycling between active sensing and sleep modes. The economic advantages prove compelling: a traditional condition monitoring system using wired sensors costs $500-2,000 per measurement point including installation, while TinyML wireless sensors cost $15-50 installed. At factory scale with 10,000 monitoring points, traditional systems require $5-20M capital investment versus $150,000-500,000 for TinyML deployments. The distributed intelligence enables local anomaly detection with &lt;100ms latency, triggering immediate alerts when vibration patterns indicate impending failure, providing 7-14 days advance warning before breakdown occurs. Companies deploying these systems report 25-45% reductions in unplanned downtime and 30-50% reductions in maintenance costs, with payback periods under 6 months for typical installations.</p>
<p>Billions of devices employ always-listening wake-word detection at &lt;1mW continuous power—TinyML’s most visible consumer application. Analyzing 16kHz audio through 5,000-20,000 parameter networks compressed to 10-50KB, these systems detect wake phrases (“Hey Siri”, “Ok Google”, “Alexa”) with &gt;95% accuracy and &lt;0.01% false activation rates. At &lt;1mW consumption, wake-word detection adds &lt;1% to battery drain, making always-on listening practical. Amazon Echo devices use dedicated TinyML chips (AML05) consuming &lt;10mW for detection, only activating the main ARM Cortex-A processor when wakewords trigger—reducing average power by 10-20x. The privacy advantage: raw audio never leaves devices until wakeword activation<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn40"><p><sup>40</sup>&nbsp;<strong>TinyML in Fitness Trackers</strong>: Modern fitness trackers use TinyML for activity recognition, sleep analysis, and health monitoring. Apple Watch can detect falls using accelerometer data and on-device ML, automatically calling emergency services. The algorithm analyzes motion patterns in real-time using &lt;1mW power.</p></div></div><p>Precision agriculture demonstrates TinyML’s economic transformation where traditional solutions prove cost-prohibitive. Monitoring 100 hectares at 10-meter grid spacing requires ~1,000 monitoring points. Traditional cellular sensors ($100-200 each, $5-10/month connectivity) create $100,000-200,000 upfront costs plus $60,000-120,000 annually. TinyML sensors ($15-30) enable $15,000-30,000 deployments with zero recurring costs, operating 3-5 years on AA batteries (5-10 years with solar harvesting) through aggressive duty cycling. Rather than transmitting raw readings, local intelligence analyzes temporal patterns for irrigation needs, pest indicators, or disease signatures, reducing wireless communication by 100x while transmitting only actionable alerts.</p>
<p>Occupancy sensing reveals privacy advantages making TinyML preferable to cameras despite lower accuracy. Using PIR, ultrasonic, or low-resolution (32x32) thermal sensors with TinyML models, buildings achieve 85-90% occupancy detection accuracy versus &gt;95% for camera systems. But TinyML sensors cost $20-40 versus $200-500 for cameras while providing mathematical privacy guarantees cameras cannot: 32x32 thermal arrays remain uninterpretable as images, and compressed representations never leave devices. Organizations deploy these sensors at 10-20x density (one per room versus one per floor), achieving equivalent coverage at similar cost with superior privacy. Battery operation (2-5 years on coin cells) eliminates power wiring installation costs that make camera retrofit installations expensive.</p>
<p>Wildlife conservation and environmental monitoring leverage TinyML’s combination of ultra-low power, ruggedness, and privacy for remote deployments where traditional systems prove impractical<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>. Researchers deploy TinyML-enabled audio sensors in rainforests to monitor biodiversity through autonomous species identification from vocalizations. These sensors analyze continuous audio streams through on-device neural networks trained to recognize hundreds of species calls, logging detections locally and transmitting periodic summaries via satellite or when researchers physically retrieve devices. Operating on solar panels providing 100-500mW average power in canopy conditions, devices run perpetually while processing 24/7 audio monitoring. Traditional approaches—recording audio and transmitting to cloud for analysis—prove impossible at scale: recording 48kHz stereo audio generates 180MB/hour, requiring 4.3GB/day storage and prohibitive satellite transmission costs ($1-5 per MB). TinyML reduces data by 10,000x through local processing: rather than transmitting 4GB daily, devices transmit 400KB of detection summaries. At deployment scale (100-1,000 sensors per study site), this reduction transforms project economics from impossible to practical. Similar systems monitor coral reef health through underwater audio, detect illegal logging through chainsaw sound recognition, and track endangered species through camera-trap classification—all leveraging TinyML’s ability to operate autonomously for years while processing sensitive data locally.</p>
<div class="no-row-height column-margin column-container"><div id="fn41"><p><sup>41</sup>&nbsp;<strong>Smart Glasses with TinyML</strong>: Google Glass Enterprise uses TinyML for real-time object recognition and barcode scanning. The glasses process visual data locally using specialized vision chips consuming &lt;500mW, enabling 8+ hour operation while providing instant augmented reality overlays.</p></div></div><p>Medical wearables achieve clinical-grade health monitoring through TinyML processing of biosignals entirely on-device, addressing both privacy requirements and power constraints. Continuous cardiac monitoring analyzes ECG signals through neural networks that detect arrhythmias, atrial fibrillation, and other cardiac abnormalities with sensitivity and specificity exceeding 95-98% compared to cardiologist interpretation. These systems process 250-500 samples/second (standard ECG sampling rates) through convolutional networks containing 10,000-50,000 parameters, consuming &lt;5mW for continuous monitoring. The low power enables week-long continuous monitoring from button-cell batteries in patch-format devices, versus hours for smartphone-based monitoring. FDA clearances for several TinyML-based arrhythmia detectors validate clinical accuracy, with devices approved for diagnosis rather than merely screening—a distinction reflecting confidence in on-device processing accuracy. Sleep apnea detection provides another compelling application: wearable devices analyze respiratory patterns, oxygen saturation, and heart rate variability to detect apneic events with 90-95% agreement with polysomnography (the clinical gold standard requiring overnight hospital stays with 20+ sensors). By enabling at-home monitoring through single-device wearables, TinyML dramatically reduces diagnostic costs from $2,000-5,000 for in-lab studies to &lt;$100 for at-home testing, improving accessibility while maintaining clinical accuracy.</p>
<div id="quiz-question-sec-ml-systems-tiny-machine-learning-9d4a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary challenge when implementing Tiny ML on microcontrollers?</p>
<ol type="a">
<li>Complex development cycle</li>
<li>High computational power availability</li>
<li>Unlimited memory resources</li>
<li>High energy consumption</li>
</ol></li>
<li><p>True or False: Tiny ML devices typically require constant connectivity to external servers for data processing.</p></li>
<li><p>Explain how Tiny ML enhances data security in IoT applications.</p></li>
<li><p>Order the following benefits of Tiny ML in terms of their impact on system performance: (1) Ultra-low latency, (2) High data security, (3) Energy efficiency.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-tiny-machine-learning-9d4a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-hybrid-machine-learning-1bbf" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ml-systems-hybrid-machine-learning-1bbf">Hybrid Machine Learning</h2>
<p>As established in the Overview (<a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>), production systems increasingly combine paradigms rather than deploying any single approach in isolation. Hybrid Machine Learning formalizes this integration, creating unified systems that leverage each paradigm’s complementary strengths.</p>
<div id="callout-definition*-1.5" class="callout callout-definition" title="Definition of Hybrid ML">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Hybrid ML</summary><div><strong>Hybrid Machine Learning (Hybrid ML)</strong> refers to the integration of multiple ML paradigms—Cloud, Edge, Mobile, and Tiny ML—to form unified, distributed systems. These systems leverage complementary strengths while mitigating individual limitations through strategic workload distribution across computational tiers, achieving scalability, adaptability, and privacy-preservation impossible with single-paradigm approaches.<p></p>
</div></details>
</div>
<section id="sec-ml-systems-design-patterns-ade8" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ml-systems-design-patterns-ade8">Design Patterns</h3>
<p>Hybrid ML design patterns provide reusable architectural solutions for integrating paradigms effectively. Each pattern represents a strategic approach to distributing ML workloads across computational tiers, optimized for specific trade-offs in latency, privacy, resource efficiency, and scalability.</p>
<p>The following sections examine five essential patterns that address common integration challenges in hybrid ML systems.</p>
<section id="sec-ml-systems-trainserve-split-0d17" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-trainserve-split-0d17">Train-Serve Split</h4>
<p>One of the most common hybrid patterns is the train-serve split, where model training occurs in the cloud but inference happens on edge, mobile, or tiny devices. This pattern takes advantage of the cloud’s vast computational resources for the training phase while benefiting from the low latency and privacy advantages of on-device inference<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a>. For example, smart home devices often use models trained on large datasets in the cloud but run inference locally to ensure quick response times and protect user privacy. In practice, this might involve training models on powerful systems like the NVIDIA DGX A100, utilizing its 8 A100 GPUs and terabyte-scale memory, before deploying optimized versions to edge devices like the NVIDIA Jetson AGX Orin for efficient inference. Similarly, mobile vision models for computational photography are typically trained on powerful cloud infrastructure but deployed to run efficiently on phone hardware.</p>
<div class="no-row-height column-margin column-container"><div id="fn42"><p><sup>42</sup>&nbsp;<strong>Train-Serve Split Economics</strong>: Training large models can cost $1-10M (GPT-3: $4.6M in compute costs) but inference costs &lt;$0.01 per query when deployed efficiently <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. This 1,000,000x cost difference drives the pattern of expensive cloud training with cost-effective edge inference.</p><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901. <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</a>.
</div></div></div></section>
<section id="sec-ml-systems-hierarchical-processing-6114" class="level4">
<h4 class="anchored" data-anchor-id="sec-ml-systems-hierarchical-processing-6114">Hierarchical Processing</h4>
<p>Hierarchical processing creates a multi-tier system where data and intelligence flow between different levels of the ML stack. In industrial IoT applications, tiny sensors might perform basic anomaly detection, edge devices aggregate and analyze data from multiple sensors, and cloud systems handle complex analytics and model updates. For instance, we might see ESP32-CAM devices performing basic image classification at the sensor level with their minimal 520 KB RAM, feeding data up to Jetson AGX Orin devices for more sophisticated computer vision tasks, and ultimately connecting to cloud infrastructure for complex analytics and model updates.</p>
<p>This hierarchy allows each tier to handle tasks appropriate to its capabilities. Tiny ML devices handle immediate, simple decisions; edge devices manage local coordination; and cloud systems tackle complex analytics and learning tasks. Smart city installations often use this pattern, with street-level sensors feeding data to neighborhood-level edge processors, which in turn connect to city-wide cloud analytics.</p>
</section>
<section id="sec-ml-systems-progressive-deployment-2570" class="level4">
<h4 class="anchored" data-anchor-id="sec-ml-systems-progressive-deployment-2570">Progressive Deployment</h4>
<p>Progressive deployment strategies create tiered intelligence architectures that balance computational capability with resource constraints through systematic model compression across deployment tiers. These strategies adapt models for different computational tiers, creating a cascade of increasingly lightweight versions. A model might start as a large, complex version in the cloud, then be progressively compressed and optimized for edge servers, mobile devices, and finally tiny sensors. The optimization techniques explored in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong> enable this progressive deployment strategy.</p>
<p>Consider Amazon Alexa’s production implementation: wake-word detection uses &lt;1KB models on TinyML devices consuming &lt;1mW, edge processing handles simple commands with 1-10MB models at 1-10W power consumption, while complex natural language understanding requires GB+ models in cloud infrastructure. Voice assistant systems exemplify this pattern, where full natural language processing runs in the cloud, while simplified wake-word detection runs on-device. This tiered approach reduces cloud inference costs by 95% while maintaining user experience, demonstrating how production systems achieve both technical performance and economic viability by balancing capability and resource constraints across the ML stack.</p>
<p>While this approach offers significant technical and economic benefits, it also introduces operational complexity including model versioning across tiers, ensuring consistency between model generations, managing failure cascades when network connectivity is lost, and coordinating updates across millions of deployed devices. Production teams must maintain specialized expertise for TinyML optimization, edge orchestration, and cloud scaling, creating significant organizational challenges that affect deployment decisions beyond pure technical requirements.</p>
</section>
<section id="sec-ml-systems-federated-learning-bf9c" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-federated-learning-bf9c">Federated Learning</h4>
<p>Federated learning<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> represents a sophisticated hybrid approach that addresses the production challenge of learning from distributed data while maintaining privacy compliance. Google’s production federated learning system processes 6 billion mobile keyboards, training improved models while keeping all typed text local, achieving both privacy compliance and model improvement at scale. Each federated learning round involves 100-10,000 devices contributing model updates, requiring sophisticated orchestration to manage device availability, network conditions, and computational heterogeneity. Production federated systems face unique operational challenges including device dropout rates of 50-90% during training rounds, network bandwidth constraints that limit update frequency, and the need for differential privacy mechanisms to prevent information leakage. The aggregation servers must handle intermittent connectivity, varying computational capabilities across device types, and ensure robust convergence despite non-IID data distributions. These systems require specialized monitoring infrastructure to track training progress across distributed populations, debug convergence issues without accessing raw data, and manage the complex interplay between local learning rates and global aggregation strategies, creating operational complexity that significantly exceeds traditional centralized training deployments.</p>
<div class="no-row-height column-margin column-container"><div id="fn43"><p><sup>43</sup>&nbsp;<strong>Federated Learning Architecture</strong>: Coordinates learning across millions of devices without centralizing data <span class="citation" data-cites="mcmahan2017federated">(<a href="#ref-mcmahan2017federated" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Google’s federated learning processes 6 billion mobile keyboards, training improved models while keeping all typed text local. Each round involves 100-10,000 devices contributing model updates.</p><div id="ref-mcmahan2017federated" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. <span>“Communication-Efficient Learning of Deep Networks from Decentralized Data.”</span> In <em>Artificial Intelligence and Statistics</em>, 1273–82. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div></div></section>
<section id="sec-ml-systems-collaborative-learning-7f59" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ml-systems-collaborative-learning-7f59">Collaborative Learning</h4>
<p>Collaborative learning enables peer-to-peer learning between devices at the same tier, often complementing hierarchical structures.<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> Autonomous vehicle fleets, for example, might share learning about road conditions or traffic patterns directly between vehicles while also communicating with cloud infrastructure. This horizontal collaboration allows systems to share time-sensitive information and learn from each other’s experiences without always routing through central servers.</p>
<div class="no-row-height column-margin column-container"><div id="fn44"><p><sup>44</sup>&nbsp;<strong>Tiered Voice Processing</strong>: Amazon Alexa uses a 3-tier system: tiny wake-word detection on-device (&lt;1KB model), edge processing for simple commands (1-10MB models), and cloud processing for complex queries (GB+ models). This reduces cloud costs by 95% while maintaining functionality.</p></div></div></section>
</section>
<section id="sec-ml-systems-realworld-integration-0815" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-realworld-integration-0815">Real-World Integration</h3>
<p>While these design patterns provide valuable templates for hybrid system architecture, real-world implementations require integrating multiple patterns into cohesive solutions. Design patterns establish a foundation for organizing and optimizing ML workloads across distributed systems. However, the practical application of these patterns often requires combining multiple paradigms into integrated workflows. In practice, ML systems rarely operate in isolation. Instead, they form interconnected networks where each paradigm plays a specific role while communicating with other parts of the system. These interconnected networks follow integration patterns that assign specific roles based on the unique strengths and limitations of each paradigm as outlined in our four-paradigm framework (<a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>).</p>
<p><a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a> illustrates these key interactions through specific connection types: “Deploy” paths show how models flow from cloud training to various devices, “Data” and “Results” show information flow from sensors through processing stages, “Analyze” shows how processed information reaches cloud analytics, and “Sync” demonstrates device coordination. Notice how data generally flows upward from sensors through processing layers to cloud analytics, while model deployments flow downward from cloud training to various inference points. The interactions aren’t strictly hierarchical. Mobile devices might communicate directly with both cloud services and tiny sensors, while edge systems can assist mobile devices with complex processing tasks.</p>
<div id="fig-hybrid" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hybrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="047b9fba8d702fcb8a171c8518556b1e6617c672.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Hybrid System Interactions: Data flows upward from sensors through processing layers to cloud analytics for insights, while trained models deploy downward from the cloud to enable inference at the edge, mobile, and Tiny ML devices. These connection types (deploy, data/results, analyze, and sync) establish a distributed architecture where each paradigm contributes unique capabilities to the overall machine learning system."><img src="ml_systems_files/mediabag/047b9fba8d702fcb8a171c8518556b1e6617c672.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hybrid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Hybrid System Interactions</strong>: Data flows upward from sensors through processing layers to cloud analytics for insights, while trained models deploy downward from the cloud to enable inference at the edge, mobile, and Tiny ML devices. These connection types (deploy, data/results, analyze, and sync) establish a distributed architecture where each paradigm contributes unique capabilities to the overall machine learning system.
</figcaption>
</figure>
</div>
<p>To understand how these labeled interactions manifest in real applications, consider how industrial defect detection systems exemplify model deployment patterns across computational tiers. When a manufacturing company develops a computer vision model for quality control, the cloud infrastructure trains the model on extensive datasets from multiple production facilities. Following the “Deploy” paths shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a>, this cloud-trained model distributes to edge servers managing factory-floor operations, quality control tablets used by inspectors on the production line, and tiny cameras embedded directly in the manufacturing equipment. This distribution pattern demonstrates how a single ML solution flows from centralized training infrastructure to inference points at multiple computational scales, each optimized for its specific operational context.</p>
<p>Data flow patterns emerge clearly in agricultural monitoring systems, where hierarchical processing spans the entire computational spectrum. Soil sensors running Tiny ML collect moisture and nutrient measurements at field level, performing local inference to detect immediate anomalies. These sensors transmit inference results to edge processors installed in farm stations, which aggregate data from dozens of sensors and perform regional analysis. The edge systems then route processed insights to cloud infrastructure via the “Analyze” path shown in <a href="#fig-hybrid" class="quarto-xref">Figure&nbsp;10</a>, enabling farm-wide analytics and long-term trend analysis. Simultaneously, edge processors share relevant metrics with farmers’ mobile applications, creating a multi-tier data flow that moves from distributed sensing through local aggregation to centralized analytics. This architecture illustrates how information traverses upward through processing layers, with each tier adding analytical sophistication appropriate to its computational resources.</p>
<p>Cross-tier assistance patterns enable mobile applications to extend their capabilities through temporary use of edge resources when local processing proves insufficient. When a mobile application encounters image processing tasks exceeding the device’s thermal or computational budget—such as real-time 3D reconstruction or complex scene understanding—it invokes the “Assist” connection to nearby edge infrastructure. The edge system processes the computationally intensive operation and returns results to the mobile device, effectively augmenting the phone’s capabilities without draining its battery or triggering thermal throttling. This collaboration pattern demonstrates how different computational tiers cooperate dynamically based on workload characteristics rather than following rigid hierarchies.</p>
<p>Integration between Tiny ML and mobile devices commonly follows gateway patterns, where resource-constrained sensors use personal devices as bridges to broader computing infrastructure. A fitness tracker exemplifies this architecture: the device continuously monitors activity patterns and vital signs using Tiny ML algorithms optimized for microcontroller execution. Throughout the day, the tracker uses the “Sync” pathway to transfer processed data to the user’s smartphone, which combines fitness metrics with other health information from multiple sources. The mobile device then consolidates this data and transmits periodic updates to cloud infrastructure via the “Analyze” path, enabling long-term health analysis and population-level insights while maintaining user privacy through local processing of raw sensor data. This pattern enables tiny devices to participate in large-scale systems despite lacking direct network connectivity or sufficient resources for cloud communication protocols.</p>
<p>Multi-layer processing scenarios demonstrate the full complexity of hybrid system integration, with multiple paradigms operating simultaneously across different aspects of a single application. Smart retail environments deploy tiny sensors throughout the store to monitor inventory levels on shelves, performing continuous inference to detect stock depletion. These sensors transmit inference results following both “Data” and “Results” paths: edge systems receive updates for immediate stock management decisions, while mobile devices used by store staff receive notifications requiring human intervention. The edge infrastructure aggregates sensor data with point-of-sale systems, foot traffic analysis, and environmental monitoring, processing this combined information stream to optimize store operations. Concurrently, this processed data flows to cloud infrastructure where machine learning models analyze trends across hundreds of store locations, identifying patterns in consumer behavior and supply chain efficiency. This multi-tier architecture demonstrates how hybrid ML systems integrate sensing, local decision making, human interaction, and large-scale analytics into unified operational frameworks.</p>
<p>These real-world integration patterns reveal how deployment paradigms complement each other through careful orchestration of data flows, model deployments, and cross-tier assistance. Industrial systems rarely confine themselves to single paradigms; instead, they compose capabilities from Cloud, Edge, Mobile, and Tiny ML into distributed architectures that optimize for latency, privacy, cost, and operational requirements simultaneously. Understanding these integration patterns proves essential for system architects designing production deployments, as the interactions between paradigms often determine system success more than the capabilities of individual components. Effective hybrid systems balance local autonomy with centralized coordination, distribute intelligence according to computational constraints, and maintain consistent operation across network interruptions and hardware heterogeneity.</p>
<div id="quiz-question-sec-ml-systems-hybrid-machine-learning-1bbf" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>What is the primary advantage of using a Hybrid Machine Learning approach?</p>
<ol type="a">
<li>It reduces the need for data privacy measures.</li>
<li>It eliminates the need for cloud-based training.</li>
<li>It combines the strengths of different ML paradigms while addressing their limitations.</li>
<li>It focuses solely on edge device processing.</li>
</ol></li>
<li><p>Explain how the train-serve split pattern in Hybrid ML benefits real-time applications.</p></li>
<li><p>Order the following ML paradigms based on their typical role in a Hybrid ML system from data collection to complex analytics: (1) Cloud ML, (2) Edge ML, (3) Tiny ML.</p></li>
<li><p>True or False: Federated learning in Hybrid ML allows for model training on edge devices while maintaining data privacy.</p></li>
<li><p>In a production system, how might you apply the hierarchical processing pattern to optimize resource utilization?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-hybrid-machine-learning-1bbf" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-shared-principles-34fe" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-shared-principles-34fe">Shared Principles</h2>
<p>Having examined each deployment paradigm individually, we can identify the underlying principles that unite all ML systems. Understanding these shared foundations explains why hybrid approaches work effectively and how techniques transfer between paradigms, providing the foundation for systematic comparison and decision making.</p>
<p>The design and integration patterns illustrate how ML paradigms interact to address real-world challenges. While each paradigm is tailored to specific roles, their interactions reveal recurring principles that guide effective system design. These shared principles provide a unifying framework for understanding both individual ML paradigms and their hybrid combinations. These principles reveal a deeper system design perspective, showing how different ML implementations optimized for distinct contexts converge around core concepts. This convergence enables systematic understanding of ML systems, despite their diversity and breadth.</p>
<p><a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> illustrates this convergence, highlighting the relationships that underpin practical system design and implementation. Understanding these principles is valuable for working with individual ML systems and for developing hybrid solutions that leverage their strengths, mitigate their limitations, and create cohesive, efficient ML workflows.</p>
<div id="fig-ml-systems-convergence" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-systems-convergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5fedc48a7e99a893f124b30012db0e32753613d4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Convergence of ML Systems: Diverse machine learning deployments (cloud, edge, mobile, and tiny) share foundational principles in data pipelines, resource management, and system architecture, enabling hybrid solutions and systematic design approaches. Understanding these shared principles allows practitioners to adapt techniques across different paradigms and build cohesive, efficient ML workflows despite varying constraints and optimization goals."><img src="ml_systems_files/mediabag/5fedc48a7e99a893f124b30012db0e32753613d4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-systems-convergence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Convergence of ML Systems</strong>: Diverse machine learning deployments (cloud, edge, mobile, and tiny) share foundational principles in data pipelines, resource management, and system architecture, enabling hybrid solutions and systematic design approaches. Understanding these shared principles allows practitioners to adapt techniques across different paradigms and build cohesive, efficient ML workflows despite varying constraints and optimization goals.
</figcaption>
</figure>
</div>
<p>This layered analysis reveals why seemingly different ML paradigms share core principles, enabling hybrid approaches and technique transfer between paradigms. At the top, we see the diverse implementations across the deployment spectrum as established in our paradigm framework (<a href="#sec-ml-systems-overview-db10" class="quarto-xref">Section&nbsp;1.1</a>).</p>
<p>Despite their distinct characteristics, the arrows in the figure show how all these implementations connect to the same core system principles. This reflects an important reality in ML systems: even though they may operate at dramatically different scales, from cloud systems processing petabytes to tiny devices handling kilobytes, they all must solve similar fundamental challenges:</p>
<ul>
<li>Managing data pipelines from collection through processing to deployment as detailed in <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong></li>
<li>Balancing resource utilization across compute, memory, energy, and network</li>
<li>Implementing system architectures that effectively integrate models, hardware, and software</li>
</ul>
<p>Core principles lead to shared system considerations around optimization, operations, and trustworthiness. This progression explains why techniques developed for one scale of ML system often transfer effectively to others. The underlying problems (efficiently processing data, managing resources, and ensuring reliable operation) remain consistent even as specific solutions vary based on scale and context.</p>
<p>This convergence becomes particularly valuable as systems move toward hybrid ML architectures. When different ML implementations share fundamental principles, combining them effectively becomes more intuitive. This explains why, for example, a cloud-trained model can be effectively deployed to edge devices, or why mobile and Tiny ML systems can complement each other in IoT applications. This cross-deployment flexibility relies on model optimization techniques (<strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) and careful management of hybrid system operations (<strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>).</p>
<section id="sec-ml-systems-implementation-layer-9002" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-implementation-layer-9002">Implementation Layer</h3>
<p>The top layer of <a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> represents the diverse landscape of ML systems across the deployment spectrum. Each implementation addresses specific needs and operational contexts, yet all contribute to the broader ecosystem of ML deployment options.</p>
<p>Cloud ML, centered in data centers, provides the foundation for large scale training and complex model serving. With access to vast computational resources like the NVIDIA DGX A100 systems we saw in <a href="#tbl-representative-systems" class="quarto-xref">Table&nbsp;1</a>, cloud implementations excel at handling massive datasets and training sophisticated models. This makes them particularly suited for tasks requiring extensive computational power, such as training foundation models or processing large-scale analytics. The specialized hardware architectures that enable this computational power are explored as detailed in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<p>Edge ML shifts the focus to local processing, prioritizing inference capabilities closer to data sources. Using devices like the NVIDIA Jetson AGX Orin, edge implementations balance computational power with reduced latency and improved privacy. This approach proves especially valuable in scenarios requiring quick decisions based on local data, such as industrial automation or real-time video analytics.</p>
<p>Mobile ML leverages the capabilities of personal devices, particularly smartphones and tablets. With specialized hardware like Apple’s A17 Pro chip, mobile implementations enable sophisticated ML capabilities while maintaining user privacy and providing offline functionality. This paradigm has revolutionized applications from computational photography to on-device speech recognition while ensuring user data remains on-device for enhanced privacy.</p>
<p>Tiny ML represents the frontier of embedded ML, bringing intelligence to highly constrained devices. Operating on microcontrollers like the Arduino Nano 33 BLE Sense, tiny implementations must carefully balance functionality with severe resource constraints. Despite these limitations, Tiny ML enables ML capabilities in scenarios where power efficiency and size constraints are paramount.</p>
</section>
<section id="sec-ml-systems-system-principles-layer-db81" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-system-principles-layer-db81">System Principles Layer</h3>
<p>The middle layer reveals the fundamental principles that unite all ML systems, regardless of their implementation scale. These core principles remain consistent even as their specific manifestations vary dramatically across different deployments.</p>
<p>Data Pipeline principles govern how systems handle information flow, from initial collection through processing to final deployment. In cloud systems, this might mean processing petabytes of data through distributed pipelines. For tiny systems, it could involve carefully managing sensor data streams within limited memory. Despite these scale differences, all systems must address the same fundamental challenges of data ingestion, transformation, and utilization.</p>
<p>Resource Management emerges as a universal challenge across all implementations. Whether managing thousands of GPUs in a data center or optimizing battery life on a microcontroller, all systems must balance competing demands for computation, memory, energy, and network resources. The quantities involved may differ by orders of magnitude, but the core principles of resource allocation and optimization remain remarkably consistent.</p>
<p>System Architecture principles guide how ML systems integrate models, hardware, and software components. Cloud architectures might focus on distributed computing and scalability, while tiny systems emphasize efficient memory mapping and interrupt handling. Yet all must solve fundamental problems of component integration, data flow optimization, and processing coordination.</p>
</section>
<section id="sec-ml-systems-system-considerations-layer-660c" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-system-considerations-layer-660c">System Considerations Layer</h3>
<p>The bottom layer of <a href="#fig-ml-systems-convergence" class="quarto-xref">Figure&nbsp;11</a> illustrates how fundamental principles manifest in practical system-wide considerations. These considerations span all ML implementations, though their specific challenges and solutions vary based on scale and context.</p>
<p><strong>Optimization and Efficiency</strong> shape how ML systems balance performance with resource utilization. In cloud environments, this often means optimizing model training across GPU clusters while managing energy consumption in data centers. Edge systems focus on reducing model size and accelerating inference without compromising accuracy. Mobile implementations must balance model performance with battery life and thermal constraints. Tiny ML pushes optimization to its limits, requiring extensive model compression and numerical precision techniques (explored in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>) to fit within severely constrained environments. Despite these different emphases, all implementations grapple with the core challenge of maximizing performance within their available resources.</p>
<p><strong>Operational Aspects</strong> affect how ML systems are deployed, monitored, and maintained in production environments. Cloud systems must handle continuous deployment across distributed infrastructure while monitoring model performance at scale. Edge implementations need robust update mechanisms and health monitoring across potentially thousands of devices. Mobile systems require seamless app updates and performance monitoring without disrupting user experience. Tiny ML faces unique challenges in deploying updates to embedded devices while ensuring continuous operation. Across all scales, the fundamental problems of deployment, monitoring, and maintenance remain consistent, even as solutions vary.</p>
<p><strong>Trustworthy AI</strong> considerations ensure ML systems operate reliably, securely, and with appropriate privacy protections. Cloud implementations must secure massive amounts of data while ensuring model predictions remain reliable at scale. Edge systems need to protect local data processing while maintaining model accuracy in diverse environments. Mobile ML must preserve user privacy while delivering consistent performance. Tiny ML systems, despite their size, must still ensure secure operation and reliable inference. These trustworthiness considerations cut across all implementations, reflecting the critical importance of building ML systems that users can depend on.</p>
<p>The progression through these layers, from diverse implementations through core principles to shared considerations, reveals why ML systems can be studied as a unified field despite their apparent differences. While specific solutions may vary dramatically based on scale and context, the fundamental challenges remain remarkably consistent. This understanding becomes particularly valuable as we move toward increasingly sophisticated hybrid systems that combine multiple implementation approaches.</p>
<p>This convergence of fundamental principles across ML implementations helps explain why hybrid approaches work so effectively in practice. As we saw in our discussion of hybrid ML, different implementations naturally complement each other precisely because they share these core foundations. Whether we’re looking at train-serve splits that leverage cloud resources for training and edge devices for inference, or hierarchical processing that combines Tiny ML sensors with edge aggregation and cloud analytics, the shared principles enable seamless integration across scales.</p>
</section>
<section id="sec-ml-systems-principles-practice-907d" class="level3">
<h3 class="anchored" data-anchor-id="sec-ml-systems-principles-practice-907d">Principles to Practice</h3>
<p>Convergence of principles explains why techniques and insights transfer well between different scales of ML systems. Understanding data pipelines in cloud environments informs data flow structure in embedded systems. Resource management strategies developed for mobile devices inspire new approaches to cloud optimization. System architecture patterns effective at one scale often adapt well to others.</p>
<p>Understanding these fundamental principles and shared considerations provides a foundation for comparing different ML implementations effectively. While each approach has distinct characteristics and optimal use cases, they all build upon the same core elements. This foundation of shared principles makes systematic comparison possible, revealing the precise trade-offs that should guide architectural decisions.</p>
<div id="quiz-question-sec-ml-systems-shared-principles-34fe" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the shared principles that unify different ML paradigms?</p>
<ol type="a">
<li>They provide a framework for understanding and integrating diverse ML implementations.</li>
<li>They focus solely on optimizing computational resources.</li>
<li>They are specific to cloud-based ML systems.</li>
<li>They are applicable only to resource-constrained environments.</li>
</ol></li>
<li><p>How do shared principles in ML systems facilitate the development of hybrid solutions?</p></li>
<li><p>True or False: The core principles of ML systems, such as resource management and system architecture, vary significantly between cloud and tiny ML implementations.</p></li>
<li><p>What is a key benefit of understanding the convergence of ML system principles?</p>
<ol type="a">
<li>It allows for the exclusive use of cloud resources.</li>
<li>It limits the application of ML to specific environments.</li>
<li>It enables the development of more efficient and cohesive ML workflows.</li>
<li>It simplifies the design of ML systems by focusing only on hardware constraints.</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-shared-principles-34fe" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ml-systems-system-comparison-8b05" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-system-comparison-8b05">System Comparison</h2>
<p>Building from this understanding of shared principles, systematic comparison across deployment paradigms reveals the precise trade-offs that should drive deployment decisions and highlights scenarios where each paradigm excels, providing practitioners with analytical frameworks for making informed architectural choices.</p>
<p>The relationship between computational resources and deployment location forms one of the most fundamental comparisons across ML systems. As we move from cloud deployments to tiny devices, we observe a dramatic reduction in available computing power, storage, and energy consumption. Cloud ML systems, with their data center infrastructure, can leverage virtually unlimited resources, processing data at the scale of petabytes and training models with billions of parameters. Edge ML systems, while more constrained, still offer significant computational capability through specialized hardware like edge GPUs and neural processing units. Mobile ML represents a middle ground, balancing computational power with energy efficiency on devices like smartphones and tablets. At the far end of the spectrum, TinyML operates under severe resource constraints, often limited to kilobytes of memory and milliwatts of power consumption.</p>
<div id="tbl-big_vs_tiny" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-big_vs_tiny-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Deployment Locations</strong>: Machine learning systems vary in where computation occurs, from centralized cloud servers to local edge devices and ultra-low-power TinyML chips, each impacting latency, bandwidth, and energy consumption. This table categorizes these deployments by their processing location and associated characteristics, enabling informed decisions about system architecture and resource allocation.
</figcaption>
<div aria-describedby="tbl-big_vs_tiny-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Cloud ML</th>
<th style="text-align: left;">Edge ML</th>
<th style="text-align: left;">Mobile ML</th>
<th style="text-align: left;">Tiny ML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Performance</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Processing Location</td>
<td style="text-align: left;">Centralized cloud servers (Data Centers)</td>
<td style="text-align: left;">Local edge devices (gateways, servers)</td>
<td style="text-align: left;">Smartphones and tablets</td>
<td style="text-align: left;">Ultra-low-power microcontrollers and embedded systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Latency</td>
<td style="text-align: left;">High (100 ms-1000 ms+)</td>
<td style="text-align: left;">Moderate (10-100 ms)</td>
<td style="text-align: left;">Low-Moderate (5-50 ms)</td>
<td style="text-align: left;">Very Low (1-10 ms)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Compute Power</td>
<td style="text-align: left;">Very High (Multiple GPUs/TPUs)</td>
<td style="text-align: left;">High (Edge GPUs)</td>
<td style="text-align: left;">Moderate (Mobile NPUs/GPUs)</td>
<td style="text-align: left;">Very Low (MCU/tiny processors)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Storage Capacity</td>
<td style="text-align: left;">Unlimited (petabytes+)</td>
<td style="text-align: left;">Large (terabytes)</td>
<td style="text-align: left;">Moderate (gigabytes)</td>
<td style="text-align: left;">Very Limited (kilobytes-megabytes)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Energy Consumption</td>
<td style="text-align: left;">Very High (kW-MW range)</td>
<td style="text-align: left;">High (100 s W)</td>
<td style="text-align: left;">Moderate (1-10 W)</td>
<td style="text-align: left;">Very Low (mW range)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scalability</td>
<td style="text-align: left;">Excellent (virtually unlimited)</td>
<td style="text-align: left;">Good (limited by edge hardware)</td>
<td style="text-align: left;">Moderate (per-device scaling)</td>
<td style="text-align: left;">Limited (fixed hardware)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Operational</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Privacy</td>
<td style="text-align: left;">Basic-Moderate (Data leaves device)</td>
<td style="text-align: left;">High (Data stays in local network)</td>
<td style="text-align: left;">High (Data stays on phone)</td>
<td style="text-align: left;">Very High (Data never leaves sensor)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Connectivity Required</td>
<td style="text-align: left;">Constant high-bandwidth</td>
<td style="text-align: left;">Intermittent</td>
<td style="text-align: left;">Optional</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Offline Capability</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">Excellent</td>
<td style="text-align: left;">Complete</td>
</tr>
<tr class="even">
<td style="text-align: left;">Real-time Processing</td>
<td style="text-align: left;">Dependent on network</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">Very Good</td>
<td style="text-align: left;">Excellent</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Deployment</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Cost</td>
<td style="text-align: left;">High ($1000s+/month)</td>
<td style="text-align: left;">Moderate ($100s-1000s)</td>
<td style="text-align: left;">Low ($0-10s)</td>
<td style="text-align: left;">Very Low ($1-10s)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hardware Requirements</td>
<td style="text-align: left;">Cloud infrastructure</td>
<td style="text-align: left;">Edge servers/gateways</td>
<td style="text-align: left;">Modern smartphones</td>
<td style="text-align: left;">MCUs/embedded systems</td>
</tr>
<tr class="even">
<td style="text-align: left;">Development Complexity</td>
<td style="text-align: left;">High (cloud expertise needed)</td>
<td style="text-align: left;">Moderate-High (edge+networking)</td>
<td style="text-align: left;">Moderate (mobile SDKs)</td>
<td style="text-align: left;">High (embedded expertise)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deployment Speed</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Slow</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-big_vs_tiny" class="quarto-xref">Table&nbsp;2</a> quantifies these paradigm differences across performance, operational, and deployment dimensions, revealing clear gradients in latency (cloud: 100-1000ms → edge: 10-100ms → mobile: 5-50ms → tiny: 1-10ms) and privacy guarantees (strongest with TinyML’s complete local processing).</p>
<p><a href="#fig-op_char" class="quarto-xref">Figure&nbsp;12</a> visualizes performance and operational characteristics through radar plots. Plot a) contrasts compute power and scalability (Cloud ML’s strengths) against latency and energy efficiency (TinyML’s advantages), with Edge and Mobile ML occupying intermediate positions.</p>
<div id="fig-op_char" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-op_char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bc71b7cb86d1a14fb63efcb143e44ef94bdea315.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: ML System Trade-Offs: Radar plots quantify performance and operational characteristics across cloud, edge, mobile, and Tiny ML paradigms, revealing inherent trade-offs between compute power, latency, energy consumption, and scalability. These visualizations enable informed selection of the most suitable deployment approach based on application-specific constraints and priorities."><img src="ml_systems_files/mediabag/bc71b7cb86d1a14fb63efcb143e44ef94bdea315.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-op_char-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>ML System Trade-Offs</strong>: Radar plots quantify performance and operational characteristics across cloud, edge, mobile, and Tiny ML paradigms, revealing inherent trade-offs between compute power, latency, energy consumption, and scalability. These visualizations enable informed selection of the most suitable deployment approach based on application-specific constraints and priorities.
</figcaption>
</figure>
</div>
<p>Plot b) emphasizes operational dimensions where TinyML excels (privacy, connectivity independence, offline capability) versus Cloud ML’s dependency on centralized infrastructure and constant connectivity.</p>
<p>Development complexity varies inversely with hardware capability: Cloud and TinyML require deep expertise (cloud infrastructure and embedded systems respectively), while Mobile and Edge leverage more accessible SDKs and tooling. Cost structures show similar inversion: Cloud incurs ongoing operational expenses ($1000s+/month), Edge requires moderate upfront investment ($100s-1000s), Mobile leverages existing devices ($0-10s), and TinyML minimizes hardware costs ($1-10s) while demanding higher development investment.</p>
<p>Understanding these trade-offs proves crucial for selecting appropriate deployment strategies that align application requirements with paradigm capabilities.</p>
<div id="quiz-question-sec-ml-systems-system-comparison-8b05" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following ML deployment options is most suitable for applications requiring ultra-low latency and minimal energy consumption?</p>
<ol type="a">
<li>Tiny ML</li>
<li>Edge ML</li>
<li>Mobile ML</li>
<li>Cloud ML</li>
</ol></li>
<li><p>Explain the trade-offs involved in choosing Edge ML over Cloud ML for a real-time video processing application.</p></li>
<li><p>In a scenario where data privacy is a top priority, the most suitable ML deployment option is ____, as it ensures data never leaves the device.</p></li>
<li><p>Order the following ML deployment options by their typical latency from highest to lowest: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-system-comparison-8b05" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-ml-systems-deployment-decision-framework-824f" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-deployment-decision-framework-824f">Deployment Decision Framework</h2>
<p>Understanding deployment paradigms individually represents only half the challenge; systematically choosing the right paradigm for specific applications requires structured decision-making. In practice, organizations often make deployment decisions based on organizational inertia (“we’re a cloud-first company”), technology trends (“edge AI is the future”), or developer familiarity (“our team knows mobile development”) rather than rigorous analysis of application requirements and constraints. These heuristic-driven decisions frequently lead to suboptimal deployments: cloud systems that fail latency requirements, edge deployments that exceed budget constraints, or mobile solutions that drain batteries unacceptably fast.</p>
<p>The framework presented in <a href="#fig-mlsys-playbook-flowchart" class="quarto-xref">Figure&nbsp;13</a> provides a systematic alternative, converting this chapter’s comparative analysis into actionable decision logic. Rather than evaluating all paradigm characteristics simultaneously—an overwhelming task given the dozens of dimensions explored earlier—the framework sequences decisions hierarchically. It filters options progressively through critical constraints: privacy requirements that eliminate paradigms requiring data transmission, latency demands that rule out high-latency approaches, computational needs that determine minimum resource thresholds, and cost constraints that establish economic viability. This structured approach ensures deployment decisions emerge from application requirements rather than organizational biases, increasing the likelihood of successful system deployment.</p>
<p><a href="#fig-mlsys-playbook-flowchart" class="quarto-xref">Figure&nbsp;13</a> presents this decision framework, distilling the chapter’s key insights into a systematic approach for determining the most suitable deployment paradigm based on specific requirements and constraints.</p>
<div id="fig-mlsys-playbook-flowchart" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="!t" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-playbook-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="216fb8731d81432c618b0b4e7b8e1c1ac3ec6537.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Deployment Decision Logic: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application."><img src="ml_systems_files/mediabag/216fb8731d81432c618b0b4e7b8e1c1ac3ec6537.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-playbook-flowchart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>Deployment Decision Logic</strong>: This flowchart guides selection of an appropriate machine learning deployment paradigm by systematically evaluating privacy requirements and processing constraints, ultimately balancing performance, cost, and data security. Navigating the decision tree helps practitioners determine whether cloud, edge, mobile, or tiny machine learning best suits a given application.
</figcaption>
</figure>
</div>
<p>The framework organizes deployment decisions into interconnected layers spanning both technical and organizational considerations, recognizing that successful ML system deployment depends equally on technical feasibility and organizational capability.</p>
<p>Technical constraints form the foundation of deployment decisions, establishing hard boundaries that eliminate infeasible options before organizational factors enter consideration. Privacy requirements determine whether data can leave the local environment at all, effectively dividing deployment options between cloud-based processing and local alternatives. Applications handling sensitive personal data under GDPR, medical records under HIPAA, or financial transactions under SOX face regulatory requirements that mandate local processing, immediately eliminating pure cloud deployment regardless of its technical advantages. Even absent explicit regulations, privacy concerns often drive local processing: consumer applications handling biometric data, enterprise systems processing proprietary information, and government systems managing classified data typically require on-device or edge processing to maintain data sovereignty. This privacy dimension functions as a binary filter—once determined that data cannot be transmitted externally, cloud ML becomes infeasible regardless of its computational advantages.</p>
<p>Latency requirements create the second critical technical constraint, establishing response time budgets that determine viable deployment paradigms. Safety-critical applications requiring sub-10ms response times—autonomous vehicle emergency braking, industrial robot collision avoidance, medical device intervention—physically cannot use cloud processing where network round-trips alone consume 100-500ms. These applications must deploy on edge servers or mobile devices where local processing achieves 1-50ms latencies, or on TinyML devices where microsecond-scale responses become possible. Interactive applications requiring sub-100ms response for acceptable user experience—voice assistants, augmented reality, real-time translation—can potentially use edge or mobile deployment but find cloud processing marginal at best. Batch processing applications tolerating multi-second response times—recommendation systems, content moderation, large-scale analytics—can leverage cloud processing without latency penalties. The latency constraint proves particularly unforgiving because physics imposes absolute limits: no amount of optimization can circumvent light-speed delays in data transmission, making latency requirements a hard filter on deployment options.</p>
<p>Network reliability constraints determine whether systems can depend on connectivity for operation or must function autonomously during outages. Applications requiring high availability during network failures—building management systems maintaining life safety, medical devices providing continuous monitoring, manufacturing systems controlling production lines—must deploy on edge or local devices capable of autonomous operation. Systems tolerating occasional outages during connectivity loss—analytics dashboards, batch processing jobs, non-critical notifications—can depend on cloud connectivity without compromising core functionality. The reliability assessment extends beyond simple availability to consider bandwidth constraints, connection costs, and data sovereignty requirements in regions with poor infrastructure. Applications operating in remote locations, developing markets, or mobile environments often find network dependencies impractical regardless of theoretical cloud advantages, forcing local processing through necessity rather than preference.</p>
<p>Computational requirements establish the final technical constraint, determining whether applications need high-performance infrastructure exceeding local device capabilities or can operate within resource-constrained environments. Training large language models, processing high-resolution video at scale, or running complex ensemble methods require computational resources available only in cloud environments with thousands of GPUs and terabytes of memory. Applications with modest computational needs—simple classification, anomaly detection, or pattern matching—can deploy effectively on edge servers, mobile devices, or even TinyML platforms depending on model complexity and inference frequency. The computational assessment must consider both peak requirements during maximum load and sustained performance over extended periods, as many local devices can burst to high performance briefly but cannot maintain peak throughput due to thermal constraints.</p>
<p>Organizational factors often prove as decisive as technical constraints in determining deployment success, as technically optimal solutions fail when organizations lack capabilities to implement and maintain them effectively. Team expertise shapes deployment feasibility through the specialized knowledge required for each paradigm. Cloud ML deployment requires expertise in distributed systems, container orchestration, auto-scaling policies, and cost optimization—skills typically found in organizations with established DevOps practices and cloud engineering teams. Edge ML demands knowledge of distributed device management, network edge architecture, and orchestration platforms like Kubernetes K3s—expertise less common than cloud skills and often requiring dedicated edge computing teams. Mobile ML development requires mobile-specific optimization techniques, platform-specific frameworks (Core ML for iOS, TensorFlow Lite for Android), and understanding of mobile OS constraints—skills distinct from cloud or edge development. TinyML pushes specialization further, requiring embedded systems expertise, hardware-level optimization, and electrical engineering knowledge rare in software-focused organizations. Organizations choosing deployment paradigms misaligned with team capabilities face extended development timelines, quality issues, and ongoing maintenance challenges that undermine technical advantages.</p>
<p>Monitoring and maintenance capabilities determine whether organizations can successfully operate deployed systems at scale over extended periods. Cloud systems demand DevOps expertise for managing infrastructure as code, monitoring distributed services, optimizing cloud costs, and responding to scaling events—capabilities requiring dedicated operations teams that smaller organizations may lack. Edge deployments require managing thousands or millions of distributed devices, coordinating software updates across heterogeneous hardware, and diagnosing issues remotely without physical access—operational complexity exceeding cloud management despite lower per-device sophistication. Mobile deployments face app store review processes, fragmentation across device capabilities and OS versions, and inability to rapidly update deployed models without user consent—constraints requiring careful release planning and validation processes. TinyML deployments present unique operational challenges including specialized debugging for resource-constrained environments, managing firmware updates without bricking devices in remote locations, and diagnosing issues without traditional debugging infrastructure—challenges requiring embedded systems expertise often absent in ML-focused teams.</p>
<p>Cost and energy efficiency considerations shape deployment decisions through both capital expenditure requirements and ongoing operational costs. Cloud deployment eliminates upfront hardware investment but creates operational expenses scaling with usage, making it economically attractive for applications with unpredictable load or short operational lifetimes but potentially expensive for high-volume, long-running systems. Edge deployment requires substantial upfront hardware investment ($500-2,000 per location) but reduces ongoing costs through lower bandwidth consumption and reduced cloud fees, creating payback periods of months to years depending on usage patterns. Mobile deployment typically involves no infrastructure costs for organizations (users provide devices) but faces app store fees, certification requirements, and support costs for diverse device populations. TinyML deployment minimizes both hardware costs ($10-50 per device) and operational expenses (no connectivity fees, minimal maintenance) but demands significant upfront development investment for specialized optimization. Energy efficiency considerations extend these cost calculations by determining operational viability in power-constrained environments: cloud and edge systems require continuous power infrastructure, mobile systems demand daily charging, while TinyML enables years of battery operation or energy harvesting—distinctions that determine feasibility for remote deployments regardless of nominal cost advantages.</p>
<p>These technical and organizational factors interact in complex ways that make deployment decisions genuinely multidimensional engineering challenges. Organizations must systematically evaluate how privacy requirements, latency constraints, reliability needs, and computational demands interact with team capabilities, operational capacity, and cost structures to determine viable deployment paradigms. A technically optimal cloud deployment fails if organizational expertise lies in embedded systems. A cost-effective edge deployment becomes impractical if the organization lacks distributed device management capabilities. A privacy-preserving mobile deployment proves infeasible if the application requires computational resources exceeding mobile device capabilities. These decisions are fundamentally constrained by scaling laws that determine resource requirements, energy consumption, and performance characteristics across different deployment contexts (<strong>?@sec-efficient-ai-ai-scaling-laws-a043</strong>). Successful paradigm selection requires balancing technical optimization with organizational reality, recognizing that deployment decisions represent systems engineering challenges extending well beyond pure technical requirements. The operational aspects of managing these deployments in production are explored in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, while benchmarking techniques for evaluating deployment performance are detailed in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>. Comprehensive energy efficiency strategies across the entire ML lifecycle are discussed in <strong><a href="../core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 17: Sustainable AI</a></strong>.</p>
<p>This framework provides a practical roadmap for navigating deployment decisions. Following this structured approach, system designers can evaluate trade-offs and align their deployment choices with technical, financial, and operational priorities while addressing the unique challenges of each application. The operational aspects of managing these deployments in production are covered as explored in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, while the benchmarking techniques for evaluating deployment performance are detailed as discussed in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
<div id="quiz-question-sec-ml-systems-deployment-decision-framework-824f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which of the following factors is NOT one of the fundamental layers considered in the deployment decision framework?</p>
<ol type="a">
<li>Scalability</li>
<li>Latency</li>
<li>Privacy</li>
<li>Cost and Energy Efficiency</li>
</ol></li>
<li><p>Explain how the deployment decision framework can guide the choice between Cloud ML and Edge ML for a privacy-sensitive application.</p></li>
<li><p>True or False: The deployment decision framework suggests that applications with strict cost constraints should prioritize Cloud ML over Tiny ML.</p></li>
<li><p>Order the following deployment decision layers from first to last as they appear in the decision-making process: (1) Compute Needs, (2) Privacy, (3) Cost and Energy Efficiency, (4) Latency.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-deployment-decision-framework-824f" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="fallacies-and-pitfalls" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="fallacies-and-pitfalls">Fallacies and Pitfalls</h2>
<p>The diversity of ML deployment paradigms, from cloud to edge to mobile to tiny, creates a complex decision space where engineers must navigate trade-offs between computational power, latency, privacy, and resource constraints. This complexity leads to several persistent misconceptions about deployment choices and their implications for system design.</p>
<p>⚠️ <strong>Fallacy:</strong> <em>Cloud ML is always superior to edge or embedded deployment because of unlimited computational resources.</em></p>
<p>While cloud infrastructure offers vast computational power and storage, this doesn’t automatically make it the optimal choice for all ML applications. Cloud deployment introduces fundamental trade-offs including network latency (often 100-500ms round trip), privacy concerns when transmitting sensitive data, ongoing operational costs that scale with usage, and complete dependence on network connectivity. Edge and embedded deployments excel in scenarios requiring real-time response (autonomous vehicles need sub-10ms decision making), strict data privacy (medical devices processing patient data), predictable costs (one-time hardware investment versus recurring cloud fees), or operation in disconnected environments (industrial equipment in remote locations). The optimal deployment paradigm depends on specific application requirements rather than raw computational capability.</p>
<p>⚠️ <strong>Pitfall:</strong> <em>Choosing a deployment paradigm based solely on model accuracy metrics without considering system-level constraints.</em></p>
<p>Teams often select deployment strategies by comparing model accuracy in isolation, overlooking critical system requirements that determine real-world viability. A cloud-deployed model achieving 99% accuracy becomes useless for autonomous emergency braking if network latency exceeds reaction time requirements. Similarly, a sophisticated edge model that drains a mobile device’s battery in minutes fails despite superior accuracy. Successful deployment requires evaluating multiple dimensions simultaneously: latency requirements, power budgets, network reliability, data privacy regulations, and total cost of ownership. Establish these constraints before model development to avoid expensive architectural pivots late in the project.</p>
<p>⚠️ <strong>Pitfall:</strong> <em>Attempting to deploy desktop-trained models directly to edge or mobile devices without architecture modifications.</em></p>
<p>Models developed on powerful workstations often fail dramatically when deployed to resource-constrained devices. A ResNet-50 model requiring 4GB memory for inference (including activations and batch processing) and 4 billion FLOPs per inference cannot run on a device with 512MB of RAM and a 1 GFLOP/s processor. Beyond simple resource violations, desktop-optimized models may use operations unsupported by mobile hardware (specialized mathematical operations), assume floating-point precision unavailable on embedded systems, or require batch processing incompatible with single-sample inference. Successful deployment demands architecture-aware design from the beginning, including specialized architectural techniques for mobile devices <span class="citation" data-cites="howard2017mobilenets">(<a href="#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span>, integer-only operations for microcontrollers, and optimization strategies that maintain accuracy while reducing computation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>“MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,”</span> April. <a href="https://doi.org/10.48550/arXiv.1704.04861">https://doi.org/10.48550/arXiv.1704.04861</a>.
</div></div></section>
<section id="sec-ml-systems-summary-473b" class="level2">
<h2 class="anchored" data-anchor-id="sec-ml-systems-summary-473b">Summary</h2>
<p>This chapter explored the diverse landscape of machine learning systems, revealing how deployment context directly shapes every aspect of system design. From cloud environments with vast computational resources to tiny devices operating under extreme constraints, each paradigm presents unique opportunities and challenges that directly influence architectural decisions, algorithmic choices, and performance trade-offs. The spectrum from cloud to edge to mobile to tiny ML represents more than just different scales of computation; it reflects a fundamental evolution in how we distribute intelligence across computing infrastructure.</p>
<p>The progression from centralized cloud systems to distributed edge and mobile deployments demonstrates how resource constraints drive innovation rather than simply limiting capabilities. Cloud ML leverages centralized power for complex processing but must navigate latency and privacy concerns. Edge ML brings computation closer to data sources, reducing latency while introducing intermediate resource constraints. Mobile ML extends these capabilities to personal devices, balancing user experience with battery life and thermal management. Tiny ML pushes the boundaries of what’s possible with minimal resources, enabling ubiquitous sensing and intelligence in previously impossible deployment contexts. This evolution showcases how thoughtful system design can transform limitations into opportunities for specialized optimization.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Deployment context drives architectural decisions more than algorithmic preferences</li>
<li>Resource constraints create opportunities for innovation, not just limitations</li>
<li>Hybrid approaches are emerging as the future of ML system design</li>
<li>Privacy and latency considerations increasingly favor distributed intelligence</li>
</ul>
</div>
</div>
<p>These paradigms reflect an ongoing shift toward systems that are finely tuned to specific operational requirements, moving beyond one-size-fits-all approaches toward context-aware system design. As these deployment models mature, hybrid architectures emerge that blend their strengths: cloud-based training paired with edge inference, federated learning across mobile devices, and hierarchical processing that optimizes across the entire spectrum. This evolution demonstrates how deployment contexts will continue driving innovation in system architecture, training methodologies, and optimization techniques, creating more sophisticated and context-aware ML systems.</p>
<section id="looking-ahead" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="looking-ahead">Looking Ahead</h3>
<p>We have established WHERE machine learning systems run—the deployment contexts that shape architectural decisions from cloud data centers to embedded microcontrollers. But deployment constraints alone don’t determine system design. The algorithms these systems execute fundamentally influence resource requirements, computational patterns, and optimization strategies. A neural network requiring gigabytes of memory and billions of floating-point operations demands different deployment approaches than a decision tree requiring kilobytes and integer comparisons. The next chapter (<strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>) examines the mathematical foundations of neural networks, the dominant computational paradigm powering modern ML systems. Understanding how neural networks process data, learn from examples, and make predictions reveals why certain deployment paradigms suit specific algorithms and how algorithmic choices propagate through the entire system stack. This algorithmic perspective complements the deployment perspective established here, together forming the foundation for systematic ML systems engineering.</p>


<div id="quiz-question-sec-ml-systems-summary-473b" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.10</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the progression of machine learning deployment paradigms?</p>
<ol type="a">
<li>From edge devices to cloud-based solutions</li>
<li>Centralized systems to increasingly distributed deployments</li>
<li>From mobile devices to tiny devices</li>
<li>From resource-constrained devices to centralized data centers</li>
</ol></li>
<li><p>Explain how hybrid machine learning approaches blend the strengths of different paradigms.</p></li>
<li><p>True or False: Tiny ML is primarily focused on leveraging large-scale computational resources for model training.</p></li>
<li><p>In a scenario where real-time responsiveness and privacy are critical, the most suitable ML deployment option is ____.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ml-systems-summary-473b" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ml-systems-overview-db10" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary advantage of deploying machine learning models on edge devices?</strong></p>
<ol type="a">
<li>Reduced latency and improved privacy</li>
<li>Maximum computational power</li>
<li>Unlimited storage capacity</li>
<li>No resource constraints</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Reduced latency and improved privacy. Edge devices process data locally, which reduces latency and keeps data close to the source, enhancing privacy. Options A, C, and D are incorrect as they describe characteristics of cloud or idealized scenarios.</p>
<p><em>Learning Objective</em>: Understand the advantages of edge ML deployment.</p></li>
<li><p><strong>Explain the trade-offs involved in choosing between cloud ML and Tiny ML for a real-time image classification task.</strong></p>
<p><em>Answer</em>: Cloud ML offers powerful computational resources and storage, suitable for complex models, but may introduce latency due to network dependency. Tiny ML, with minimal resources, allows real-time processing with low latency but requires significant model optimization. The choice depends on the application’s latency tolerance and resource availability.</p>
<p><em>Learning Objective</em>: Analyze trade-offs between different ML deployment options.</p></li>
<li><p><strong>What is a key challenge when deploying machine learning models on mobile devices?</strong></p>
<ol type="a">
<li>Lack of internet connectivity</li>
<li>Balancing model performance with battery life</li>
<li>Excessive computational power</li>
<li>Unlimited memory availability</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Balancing model performance with battery life. Mobile devices must manage power consumption to maintain battery life while running ML models. Options A, C, and D are incorrect as they do not accurately reflect the challenges of mobile ML deployment.</p>
<p><em>Learning Objective</em>: Identify challenges in mobile ML deployment.</p></li>
<li><p><strong>In a production system, how might you decide between using Edge ML and Mobile ML for a smart home application?</strong></p>
<p><em>Answer</em>: The decision would depend on factors such as the need for real-time processing, privacy concerns, and device capabilities. Edge ML is suitable for low-latency, local data processing, while Mobile ML offers portability and personal data handling. The choice should align with the application’s specific requirements and constraints.</p>
<p><em>Learning Objective</em>: Apply deployment concepts to real-world ML system scenarios.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-overview-db10" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-cloudbased-machine-learning-7606" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary benefit of using Cloud ML for machine learning projects?</strong></p>
<ol type="a">
<li>Reduced latency for real-time applications</li>
<li>Elimination of data privacy concerns</li>
<li>Complete independence from internet connectivity</li>
<li>Dynamic scalability and resource management</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Dynamic scalability and resource management. Cloud ML provides dynamic scalability, allowing organizations to scale resources up or down based on computational needs. Options A, C, and D are incorrect because Cloud ML can introduce latency, relies on internet connectivity, and poses data privacy challenges.</p>
<p><em>Learning Objective</em>: Understand the key benefits of Cloud ML in terms of scalability and resource management.</p></li>
<li><p><strong>True or False: Cloud ML eliminates the need for data privacy and security measures.</strong></p>
<p><em>Answer</em>: False. Cloud ML requires robust data privacy and security measures because data is stored and processed in centralized data centers, which can be vulnerable to cyber-attacks.</p>
<p><em>Learning Objective</em>: Recognize the importance of data privacy and security in Cloud ML environments.</p></li>
<li><p><strong>What are some challenges organizations face when implementing Cloud ML, and how might these be mitigated?</strong></p>
<p><em>Answer</em>: Challenges include latency issues, data privacy concerns, cost management, network dependency, and vendor lock-in. Mitigation strategies involve optimizing system design for latency, implementing robust security measures, monitoring and optimizing resource usage, ensuring reliable network infrastructure, and planning for potential vendor transitions.</p>
<p><em>Learning Objective</em>: Identify and describe the challenges of Cloud ML implementation and possible mitigation strategies.</p></li>
<li><p><strong>Order the following steps in deploying a machine learning model using Cloud ML: (1) Train model, (2) Deploy model, (3) Collect data, (4) Validate model.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Collect data, (1) Train model, (4) Validate model, (2) Deploy model. Data collection is the first step, followed by training the model. Validation ensures the model’s accuracy before deployment.</p>
<p><em>Learning Objective</em>: Understand the typical workflow for deploying a machine learning model using Cloud ML.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-cloudbased-machine-learning-7606" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-edge-machine-learning-06ec" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary benefit of using Edge Machine Learning over Cloud ML?</strong></p>
<ol type="a">
<li>Increased computational resources</li>
<li>Reduced latency</li>
<li>Centralized data processing</li>
<li>Unlimited storage capacity</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Reduced latency. Edge ML processes data locally, which minimizes the time data takes to travel to and from remote servers, thus reducing latency. Options A, C, and D are incorrect because Edge ML typically has limited resources and focuses on decentralized processing.</p>
<p><em>Learning Objective</em>: Understand the key advantages of Edge ML compared to Cloud ML.</p></li>
<li><p><strong>True or False: Edge ML enhances data privacy by processing data locally rather than sending it to centralized servers.</strong></p>
<p><em>Answer</em>: True. This is true because processing data locally on edge devices reduces the need to transmit sensitive information over networks, thus minimizing the risk of data breaches.</p>
<p><em>Learning Objective</em>: Recognize the privacy benefits of Edge ML.</p></li>
<li><p><strong>What challenges might arise when deploying machine learning models on edge devices, and how can they be addressed?</strong></p>
<p><em>Answer</em>: Challenges include limited computational resources and increased complexity in managing edge nodes. Addressing these may involve using model compression techniques and implementing robust management protocols for updates and security. For example, quantization can reduce model size, and automated update systems can ensure nodes remain current.</p>
<p><em>Learning Objective</em>: Identify and propose solutions for challenges in Edge ML deployment.</p></li>
<li><p><strong>Edge Machine Learning is crucial for applications requiring real-time decision making, such as ____. This is important because it allows for immediate processing and response.</strong></p>
<p><em>Answer</em>: autonomous vehicles. This is important because it allows for immediate processing and response, which is critical for safety and operational efficiency.</p>
<p><em>Learning Objective</em>: Recall specific applications where Edge ML is essential.</p></li>
<li><p><strong>Order the following benefits of Edge ML in terms of their impact on system performance: (1) Reduced latency, (2) Enhanced data privacy, (3) Lower bandwidth usage.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Reduced latency, (3) Lower bandwidth usage, (2) Enhanced data privacy. Reduced latency has the most immediate impact on performance, followed by lower bandwidth usage which affects cost and efficiency, and finally enhanced data privacy, which, while crucial, is more of a security and compliance benefit.</p>
<p><em>Learning Objective</em>: Understand and prioritize the benefits of Edge ML for system performance.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-edge-machine-learning-06ec" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-mobile-machine-learning-f5b5" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary benefit of Mobile Machine Learning?</strong></p>
<ol type="a">
<li>Unlimited computational resources</li>
<li>Reduced data privacy concerns</li>
<li>Increased dependency on cloud connectivity</li>
<li>Simplified model deployment</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Reduced data privacy concerns. Mobile ML processes data locally, minimizing the risk of data breaches by keeping sensitive information on the device. Other options are incorrect as Mobile ML operates within resource constraints and aims to reduce cloud dependency.</p>
<p><em>Learning Objective</em>: Understand the benefits of Mobile ML in terms of privacy and offline functionality.</p></li>
<li><p><strong>True or False: Mobile ML can operate effectively without internet connectivity.</strong></p>
<p><em>Answer</em>: True. Mobile ML is designed to function offline, ensuring applications remain responsive and reliable even without network access.</p>
<p><em>Learning Objective</em>: Recognize the offline capabilities of Mobile ML systems.</p></li>
<li><p><strong>Discuss the trade-offs involved in optimizing machine learning models for mobile devices.</strong></p>
<p><em>Answer</em>: Optimizing ML models for mobile devices involves balancing model complexity and performance with constraints like battery life, storage, and computational power. For example, model quantization reduces size and power consumption but may affect accuracy. This is important because it ensures efficient on-device processing without compromising user experience.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs in optimizing ML models for mobile deployment.</p></li>
<li><p><strong>____ is a technique used in Mobile ML to reduce model size and speed up inference while maintaining accuracy.</strong></p>
<p><em>Answer</em>: Quantization. This technique reduces model precision, typically from 32-bit to 8-bit integers, significantly decreasing model size and improving inference speed.</p>
<p><em>Learning Objective</em>: Recall the model optimization techniques used in Mobile ML.</p></li>
<li><p><strong>In a production system, how might Mobile ML enhance user experience in real-time applications?</strong></p>
<p><em>Answer</em>: Mobile ML enhances user experience by providing real-time processing capabilities directly on the device. For example, in computational photography, it allows for immediate image enhancements and effects. This is important because it ensures fast, responsive applications that adapt to user needs without relying on cloud processing.</p>
<p><em>Learning Objective</em>: Apply Mobile ML concepts to real-time application scenarios.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-mobile-machine-learning-f5b5" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-tiny-machine-learning-9d4a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary challenge when implementing Tiny ML on microcontrollers?</strong></p>
<ol type="a">
<li>Complex development cycle</li>
<li>High computational power availability</li>
<li>Unlimited memory resources</li>
<li>High energy consumption</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Complex development cycle. This is correct because developing Tiny ML models requires specialized knowledge in both machine learning and embedded systems due to resource constraints. Options A, C, and D are incorrect as they do not reflect the challenges of Tiny ML.</p>
<p><em>Learning Objective</em>: Understand the challenges associated with deploying Tiny ML in resource-constrained environments.</p></li>
<li><p><strong>True or False: Tiny ML devices typically require constant connectivity to external servers for data processing.</strong></p>
<p><em>Answer</em>: False. This is false because Tiny ML devices are designed to process data locally on the device, eliminating the need for constant connectivity to external servers.</p>
<p><em>Learning Objective</em>: Recognize the independence of Tiny ML devices from constant server connectivity.</p></li>
<li><p><strong>Explain how Tiny ML enhances data security in IoT applications.</strong></p>
<p><em>Answer</em>: Tiny ML enhances data security by processing data locally on the device, which reduces the risk of data interception during transmission. For example, in a smart home system, data related to user behavior is processed directly on the device, minimizing exposure to external threats. This is important because it ensures sensitive information remains secure.</p>
<p><em>Learning Objective</em>: Analyze the benefits of local data processing in enhancing security for Tiny ML applications.</p></li>
<li><p><strong>Order the following benefits of Tiny ML in terms of their impact on system performance: (1) Ultra-low latency, (2) High data security, (3) Energy efficiency.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Ultra-low latency, (3) Energy efficiency, (2) High data security. Ultra-low latency directly impacts real-time responsiveness, energy efficiency ensures long-term operation, and high data security protects sensitive information.</p>
<p><em>Learning Objective</em>: Understand the prioritized benefits of Tiny ML in terms of system performance.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-tiny-machine-learning-9d4a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-hybrid-machine-learning-1bbf" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary advantage of using a Hybrid Machine Learning approach?</strong></p>
<ol type="a">
<li>It reduces the need for data privacy measures.</li>
<li>It eliminates the need for cloud-based training.</li>
<li>It combines the strengths of different ML paradigms while addressing their limitations.</li>
<li>It focuses solely on edge device processing.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. It combines the strengths of different ML paradigms while addressing their limitations. Hybrid ML leverages the computational power of the cloud, efficiency of edge devices, and capabilities of Tiny ML to create a balanced system. Options A, C, and D are incorrect as they do not capture the essence of Hybrid ML.</p>
<p><em>Learning Objective</em>: Understand the fundamental benefit of integrating multiple ML paradigms in Hybrid ML.</p></li>
<li><p><strong>Explain how the train-serve split pattern in Hybrid ML benefits real-time applications.</strong></p>
<p><em>Answer</em>: The train-serve split pattern benefits real-time applications by leveraging the cloud for model training, which requires significant computational resources, and deploying the trained model to edge or mobile devices for inference. This approach ensures low latency and privacy by processing data locally, which is crucial for applications like smart home devices where quick response times are essential. This is important because it balances the need for powerful training infrastructure with the practical requirements of real-time operation.</p>
<p><em>Learning Objective</em>: Analyze the advantages of the train-serve split pattern in Hybrid ML for real-time applications.</p></li>
<li><p><strong>Order the following ML paradigms based on their typical role in a Hybrid ML system from data collection to complex analytics: (1) Cloud ML, (2) Edge ML, (3) Tiny ML.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Tiny ML, (2) Edge ML, (1) Cloud ML. Tiny ML devices typically handle immediate data collection and basic processing, Edge ML aggregates and analyzes data from multiple sources, and Cloud ML manages complex analytics and model updates. This order reflects the hierarchical processing pattern where each tier handles tasks suited to its capabilities.</p>
<p><em>Learning Objective</em>: Understand the hierarchical processing pattern in Hybrid ML systems.</p></li>
<li><p><strong>True or False: Federated learning in Hybrid ML allows for model training on edge devices while maintaining data privacy.</strong></p>
<p><em>Answer</em>: True. Federated learning enables model training across many edge or mobile devices by sharing model updates rather than raw data with cloud servers, thus preserving data privacy. This is important for applications where privacy is critical but collective learning is beneficial.</p>
<p><em>Learning Objective</em>: Understand the role of federated learning in enhancing privacy in Hybrid ML systems.</p></li>
<li><p><strong>In a production system, how might you apply the hierarchical processing pattern to optimize resource utilization?</strong></p>
<p><em>Answer</em>: In a production system, the hierarchical processing pattern can be applied by assigning tasks based on the capabilities of each ML tier. For example, Tiny ML devices can handle basic anomaly detection, Edge ML can perform local data aggregation and analysis, and Cloud ML can manage complex analytics and model updates. This ensures that each tier utilizes its resources efficiently, reducing latency and improving overall system performance. This is important because it allows for scalable and adaptive ML solutions tailored to specific application needs.</p>
<p><em>Learning Objective</em>: Apply the hierarchical processing pattern to optimize resource utilization in Hybrid ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-hybrid-machine-learning-1bbf" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-shared-principles-34fe" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the shared principles that unify different ML paradigms?</strong></p>
<ol type="a">
<li>They provide a framework for understanding and integrating diverse ML implementations.</li>
<li>They focus solely on optimizing computational resources.</li>
<li>They are specific to cloud-based ML systems.</li>
<li>They are applicable only to resource-constrained environments.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. They provide a framework for understanding and integrating diverse ML implementations. This is correct because the shared principles help unify various ML paradigms, offering a cohesive framework for system design across different contexts. Options A, C, and D are incorrect as they limit the scope of these principles to specific aspects or environments.</p>
<p><em>Learning Objective</em>: Understand the role of shared principles in unifying different ML paradigms.</p></li>
<li><p><strong>How do shared principles in ML systems facilitate the development of hybrid solutions?</strong></p>
<p><em>Answer</em>: Shared principles in ML systems, such as data pipeline management and resource optimization, provide a common foundation that allows different ML implementations to be integrated effectively. For example, a cloud-trained model can be deployed on edge devices because both systems adhere to these core principles. This is important because it enables seamless integration and efficient workflows across diverse ML environments.</p>
<p><em>Learning Objective</em>: Explain the role of shared principles in enabling hybrid ML solutions.</p></li>
<li><p><strong>True or False: The core principles of ML systems, such as resource management and system architecture, vary significantly between cloud and tiny ML implementations.</strong></p>
<p><em>Answer</em>: False. This is false because, despite differences in scale and context, the core principles like resource management and system architecture remain consistent across ML implementations. The specific solutions might vary, but the underlying challenges and principles are shared.</p>
<p><em>Learning Objective</em>: Recognize the consistency of core principles across different ML implementations.</p></li>
<li><p><strong>What is a key benefit of understanding the convergence of ML system principles?</strong></p>
<ol type="a">
<li>It allows for the exclusive use of cloud resources.</li>
<li>It limits the application of ML to specific environments.</li>
<li>It enables the development of more efficient and cohesive ML workflows.</li>
<li>It simplifies the design of ML systems by focusing only on hardware constraints.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. It enables the development of more efficient and cohesive ML workflows. This is correct because understanding the convergence of principles allows for the integration of diverse ML paradigms, leading to more efficient and cohesive system designs. Options A, C, and D are incorrect as they either limit the scope of ML applications or oversimplify the design process.</p>
<p><em>Learning Objective</em>: Identify the benefits of understanding the convergence of ML system principles.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-shared-principles-34fe" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-system-comparison-8b05" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following ML deployment options is most suitable for applications requiring ultra-low latency and minimal energy consumption?</strong></p>
<ol type="a">
<li>Tiny ML</li>
<li>Edge ML</li>
<li>Mobile ML</li>
<li>Cloud ML</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Tiny ML. This is correct because Tiny ML systems are designed for ultra-low latency and minimal energy consumption, making them ideal for real-time, low-power applications. Cloud ML, Edge ML, and Mobile ML have higher energy and latency requirements.</p>
<p><em>Learning Objective</em>: Understand the suitability of different ML deployment options based on latency and energy requirements.</p></li>
<li><p><strong>Explain the trade-offs involved in choosing Edge ML over Cloud ML for a real-time video processing application.</strong></p>
<p><em>Answer</em>: Edge ML offers lower latency and improved data privacy by processing data locally, making it suitable for real-time applications. However, it may have limited computational resources compared to Cloud ML, which can handle larger data volumes and more complex models. This trade-off is important for applications where immediate processing and data privacy are critical.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between Edge ML and Cloud ML in real-time applications.</p></li>
<li><p><strong>In a scenario where data privacy is a top priority, the most suitable ML deployment option is ____, as it ensures data never leaves the device.</strong></p>
<p><em>Answer</em>: Tiny ML. Tiny ML ensures data never leaves the device, providing the highest level of data privacy.</p>
<p><em>Learning Objective</em>: Identify the ML deployment option that maximizes data privacy.</p></li>
<li><p><strong>Order the following ML deployment options by their typical latency from highest to lowest: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Cloud ML, (2) Edge ML, (3) Mobile ML, (4) Tiny ML. Cloud ML has the highest latency due to network communication, followed by Edge ML, which processes data locally but still has moderate latency. Mobile ML has low to moderate latency, while Tiny ML offers the lowest latency due to localized processing.</p>
<p><em>Learning Objective</em>: Understand the latency characteristics of different ML deployment options.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-system-comparison-8b05" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-deployment-decision-framework-824f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following factors is NOT one of the fundamental layers considered in the deployment decision framework?</strong></p>
<ol type="a">
<li>Scalability</li>
<li>Latency</li>
<li>Privacy</li>
<li>Cost and Energy Efficiency</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Scalability. This is correct because the framework focuses on Privacy, Latency, Compute Needs, and Cost and Energy Efficiency, not Scalability.</p>
<p><em>Learning Objective</em>: Identify the key factors considered in the deployment decision framework.</p></li>
<li><p><strong>Explain how the deployment decision framework can guide the choice between Cloud ML and Edge ML for a privacy-sensitive application.</strong></p>
<p><em>Answer</em>: The framework suggests prioritizing local processing for privacy-sensitive applications, leading to a preference for Edge ML over Cloud ML. For example, healthcare applications often require data to remain on local devices to protect patient privacy. This is important because it ensures compliance with data protection regulations while maintaining system functionality.</p>
<p><em>Learning Objective</em>: Apply the deployment decision framework to a specific scenario, considering privacy requirements.</p></li>
<li><p><strong>True or False: The deployment decision framework suggests that applications with strict cost constraints should prioritize Cloud ML over Tiny ML.</strong></p>
<p><em>Answer</em>: False. This is false because the framework indicates that applications with strict cost constraints should consider low-cost options like Tiny ML, which are more resource-efficient and budget-friendly.</p>
<p><em>Learning Objective</em>: Understand how cost constraints influence deployment decisions within the framework.</p></li>
<li><p><strong>Order the following deployment decision layers from first to last as they appear in the decision-making process: (1) Compute Needs, (2) Privacy, (3) Cost and Energy Efficiency, (4) Latency.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Privacy, (4) Latency, (1) Compute Needs, (3) Cost and Energy Efficiency. This order reflects the sequence in which each layer is considered to systematically narrow down deployment options based on specific requirements.</p>
<p><em>Learning Objective</em>: Sequence the layers of the decision framework to understand their role in narrowing deployment options.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-deployment-decision-framework-824f" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ml-systems-summary-473b" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.10</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the progression of machine learning deployment paradigms?</strong></p>
<ol type="a">
<li>From edge devices to cloud-based solutions</li>
<li>Centralized systems to increasingly distributed deployments</li>
<li>From mobile devices to tiny devices</li>
<li>From resource-constrained devices to centralized data centers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Centralized systems to increasingly distributed deployments. This progression reflects the shift from cloud-based systems to more distributed solutions like Edge ML, Mobile ML, and Tiny ML, which are tailored to specific contexts.</p>
<p><em>Learning Objective</em>: Understand the evolution of ML deployment paradigms from centralized to distributed systems.</p></li>
<li><p><strong>Explain how hybrid machine learning approaches blend the strengths of different paradigms.</strong></p>
<p><em>Answer</em>: Hybrid machine learning approaches combine the computational power of cloud-based systems with the low-latency, privacy-preserving features of edge and mobile systems. For example, cloud-based training can be paired with edge inference to optimize resource use and enhance real-time responsiveness. This is important because it allows for flexible and efficient ML solutions tailored to specific application needs.</p>
<p><em>Learning Objective</em>: Analyze the benefits of hybrid ML approaches in leveraging multiple paradigms.</p></li>
<li><p><strong>True or False: Tiny ML is primarily focused on leveraging large-scale computational resources for model training.</strong></p>
<p><em>Answer</em>: False. Tiny ML focuses on deploying machine learning models on resource-constrained devices, not on leveraging large-scale computational resources. It aims to enable ML applications in environments with limited power and computational capabilities.</p>
<p><em>Learning Objective</em>: Correct misconceptions about the focus of Tiny ML.</p></li>
<li><p><strong>In a scenario where real-time responsiveness and privacy are critical, the most suitable ML deployment option is ____. </strong></p>
<p><em>Answer</em>: Edge ML. Edge ML processes data locally, reducing latency and enhancing privacy, making it suitable for real-time applications.</p>
<p><em>Learning Objective</em>: Identify the most appropriate ML paradigm for specific application requirements.</p></li>
</ol>
<p><a href="#quiz-question-sec-ml-systems-summary-473b" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/introduction/introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <span class="nav-page-text">DL Primer</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>