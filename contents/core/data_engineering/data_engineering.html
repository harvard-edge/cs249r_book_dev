<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/frameworks/frameworks.html" rel="next">
<link href="../../../contents/core/workflow/workflow.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/data_engineering/data_engineering.html">Data Engineering</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>🚧 DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">38b88181</code> • 2025-09-29 23:20 UTC • <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version →</em></a></p>
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-data-engineering" id="toc-sec-data-engineering" class="nav-link active" data-scroll-target="#sec-data-engineering">Data Engineering</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-data-engineering-systems-framework" id="toc-sec-data-engineering-systems-framework" class="nav-link" data-scroll-target="#sec-data-engineering-systems-framework">Data Engineering Systems Framework</a>
  <ul class="collapse">
  <li><a href="#the-four-pillars-of-data-engineering-systems" id="toc-the-four-pillars-of-data-engineering-systems" class="nav-link" data-scroll-target="#the-four-pillars-of-data-engineering-systems">The Four Pillars of Data Engineering Systems</a></li>
  <li><a href="#integrating-the-pillars-through-systems-thinking" id="toc-integrating-the-pillars-through-systems-thinking" class="nav-link" data-scroll-target="#integrating-the-pillars-through-systems-thinking">Integrating the Pillars Through Systems Thinking</a></li>
  <li><a href="#framework-application-throughout-the-data-lifecycle" id="toc-framework-application-throughout-the-data-lifecycle" class="nav-link" data-scroll-target="#framework-application-throughout-the-data-lifecycle">Framework Application Throughout the Data Lifecycle</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-problem-definition-f820" id="toc-sec-data-engineering-problem-definition-f820" class="nav-link" data-scroll-target="#sec-data-engineering-problem-definition-f820">Problem Definition and Governance Foundations</a>
  <ul class="collapse">
  <li><a href="#governance-principles-for-data-engineering" id="toc-governance-principles-for-data-engineering" class="nav-link" data-scroll-target="#governance-principles-for-data-engineering">Governance Principles for Data Engineering</a></li>
  <li><a href="#systematic-problem-definition-process" id="toc-systematic-problem-definition-process" class="nav-link" data-scroll-target="#systematic-problem-definition-process">Systematic Problem Definition Process</a></li>
  <li><a href="#applying-the-framework-keyword-spotting-case-study" id="toc-applying-the-framework-keyword-spotting-case-study" class="nav-link" data-scroll-target="#applying-the-framework-keyword-spotting-case-study">Applying the Framework: Keyword Spotting Case Study</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-pipeline-basics-31ba" id="toc-sec-data-engineering-pipeline-basics-31ba" class="nav-link" data-scroll-target="#sec-data-engineering-pipeline-basics-31ba">Data Pipeline Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-pipeline-monitoring-debugging" id="toc-sec-data-engineering-pipeline-monitoring-debugging" class="nav-link" data-scroll-target="#sec-data-engineering-pipeline-monitoring-debugging">Pipeline Monitoring and Debugging</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-sources-c8d9" id="toc-sec-data-engineering-data-sources-c8d9" class="nav-link" data-scroll-target="#sec-data-engineering-data-sources-c8d9">Strategic Data Acquisition</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-existing-datasets-4f21" id="toc-sec-data-engineering-existing-datasets-4f21" class="nav-link" data-scroll-target="#sec-data-engineering-existing-datasets-4f21">Existing Datasets</a></li>
  <li><a href="#sec-data-engineering-web-scraping-fa9f" id="toc-sec-data-engineering-web-scraping-fa9f" class="nav-link" data-scroll-target="#sec-data-engineering-web-scraping-fa9f">Web Scraping</a></li>
  <li><a href="#sec-data-engineering-crowdsourcing-e093" id="toc-sec-data-engineering-crowdsourcing-e093" class="nav-link" data-scroll-target="#sec-data-engineering-crowdsourcing-e093">Crowdsourcing</a></li>
  <li><a href="#sec-data-engineering-anonymization-techniques-b90b" id="toc-sec-data-engineering-anonymization-techniques-b90b" class="nav-link" data-scroll-target="#sec-data-engineering-anonymization-techniques-b90b">Anonymization Techniques</a></li>
  <li><a href="#sec-data-engineering-synthetic-data-creation-f53d" id="toc-sec-data-engineering-synthetic-data-creation-f53d" class="nav-link" data-scroll-target="#sec-data-engineering-synthetic-data-creation-f53d">Synthetic Data Creation</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-1222" id="toc-sec-data-engineering-continuing-kws-example-1222" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-1222">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-ingestion-5dfc" id="toc-sec-data-engineering-data-ingestion-5dfc" class="nav-link" data-scroll-target="#sec-data-engineering-data-ingestion-5dfc">Data Ingestion</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-ingestion-patterns-b977" id="toc-sec-data-engineering-ingestion-patterns-b977" class="nav-link" data-scroll-target="#sec-data-engineering-ingestion-patterns-b977">Ingestion Patterns</a></li>
  <li><a href="#sec-data-engineering-etl-elt-comparison-bbb7" id="toc-sec-data-engineering-etl-elt-comparison-bbb7" class="nav-link" data-scroll-target="#sec-data-engineering-etl-elt-comparison-bbb7">ETL and ELT Comparison</a></li>
  <li><a href="#sec-data-engineering-data-source-integration-53f9" id="toc-sec-data-engineering-data-source-integration-53f9" class="nav-link" data-scroll-target="#sec-data-engineering-data-source-integration-53f9">Data Source Integration</a></li>
  <li><a href="#sec-data-engineering-validation-techniques-5b83" id="toc-sec-data-engineering-validation-techniques-5b83" class="nav-link" data-scroll-target="#sec-data-engineering-validation-techniques-5b83">Validation Techniques</a></li>
  <li><a href="#sec-data-engineering-error-management-e2e9" id="toc-sec-data-engineering-error-management-e2e9" class="nav-link" data-scroll-target="#sec-data-engineering-error-management-e2e9">Error Management</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-698c" id="toc-sec-data-engineering-continuing-kws-example-698c" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-698c">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-processing-c336" id="toc-sec-data-engineering-data-processing-c336" class="nav-link" data-scroll-target="#sec-data-engineering-data-processing-c336">Systematic Data Processing</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-cleaning-techniques-e81b" id="toc-sec-data-engineering-cleaning-techniques-e81b" class="nav-link" data-scroll-target="#sec-data-engineering-cleaning-techniques-e81b">Cleaning Techniques</a></li>
  <li><a href="#sec-data-engineering-data-quality-assessment-b8b7" id="toc-sec-data-engineering-data-quality-assessment-b8b7" class="nav-link" data-scroll-target="#sec-data-engineering-data-quality-assessment-b8b7">Data Quality Assessment</a></li>
  <li><a href="#sec-data-engineering-transformation-techniques-2d54" id="toc-sec-data-engineering-transformation-techniques-2d54" class="nav-link" data-scroll-target="#sec-data-engineering-transformation-techniques-2d54">Transformation Techniques</a></li>
  <li><a href="#sec-data-engineering-feature-engineering-ea20" id="toc-sec-data-engineering-feature-engineering-ea20" class="nav-link" data-scroll-target="#sec-data-engineering-feature-engineering-ea20">Feature Engineering</a></li>
  <li><a href="#sec-data-engineering-processing-pipeline-design-48d4" id="toc-sec-data-engineering-processing-pipeline-design-48d4" class="nav-link" data-scroll-target="#sec-data-engineering-processing-pipeline-design-48d4">Processing Pipeline Design</a></li>
  <li><a href="#sec-data-engineering-scalability-considerations-1083" id="toc-sec-data-engineering-scalability-considerations-1083" class="nav-link" data-scroll-target="#sec-data-engineering-scalability-considerations-1083">Scalability Considerations</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-8ed1" id="toc-sec-data-engineering-continuing-kws-example-8ed1" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-8ed1">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-labeling-95e7" id="toc-sec-data-engineering-data-labeling-95e7" class="nav-link" data-scroll-target="#sec-data-engineering-data-labeling-95e7">Data Labeling</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-types-labels-76ae" id="toc-sec-data-engineering-types-labels-76ae" class="nav-link" data-scroll-target="#sec-data-engineering-types-labels-76ae">Types of Labels</a></li>
  <li><a href="#sec-data-engineering-annotation-techniques-6ebe" id="toc-sec-data-engineering-annotation-techniques-6ebe" class="nav-link" data-scroll-target="#sec-data-engineering-annotation-techniques-6ebe">Annotation Techniques</a></li>
  <li><a href="#sec-data-engineering-label-quality-assessment-9c43" id="toc-sec-data-engineering-label-quality-assessment-9c43" class="nav-link" data-scroll-target="#sec-data-engineering-label-quality-assessment-9c43">Label Quality Assessment</a></li>
  <li><a href="#sec-data-engineering-ai-annotation-41b4" id="toc-sec-data-engineering-ai-annotation-41b4" class="nav-link" data-scroll-target="#sec-data-engineering-ai-annotation-41b4">AI in Annotation</a></li>
  <li><a href="#sec-data-engineering-labeling-challenges-d658" id="toc-sec-data-engineering-labeling-challenges-d658" class="nav-link" data-scroll-target="#sec-data-engineering-labeling-challenges-d658">Labeling Challenges</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-52dc" id="toc-sec-data-engineering-continuing-kws-example-52dc" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-52dc">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-storage-6296" id="toc-sec-data-engineering-data-storage-6296" class="nav-link" data-scroll-target="#sec-data-engineering-data-storage-6296">Strategic Storage Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-storage-system-types-f802" id="toc-sec-data-engineering-storage-system-types-f802" class="nav-link" data-scroll-target="#sec-data-engineering-storage-system-types-f802">Storage System Types</a>
  <ul class="collapse">
  <li><a href="#storage-architecture-decision-matrix" id="toc-storage-architecture-decision-matrix" class="nav-link" data-scroll-target="#storage-architecture-decision-matrix">Storage Architecture Decision Matrix</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-storage-considerations-5f3e" id="toc-sec-data-engineering-storage-considerations-5f3e" class="nav-link" data-scroll-target="#sec-data-engineering-storage-considerations-5f3e">Storage Considerations</a></li>
  <li><a href="#sec-data-engineering-performance-factors-56f2" id="toc-sec-data-engineering-performance-factors-56f2" class="nav-link" data-scroll-target="#sec-data-engineering-performance-factors-56f2">Performance Factors</a></li>
  <li><a href="#sec-data-engineering-storage-ml-lifecycle-a3a7" id="toc-sec-data-engineering-storage-ml-lifecycle-a3a7" class="nav-link" data-scroll-target="#sec-data-engineering-storage-ml-lifecycle-a3a7">Storage in ML Lifecycle</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-development-phase-ebf9" id="toc-sec-data-engineering-development-phase-ebf9" class="nav-link" data-scroll-target="#sec-data-engineering-development-phase-ebf9">Development Phase</a></li>
  <li><a href="#sec-data-engineering-training-phase-b13d" id="toc-sec-data-engineering-training-phase-b13d" class="nav-link" data-scroll-target="#sec-data-engineering-training-phase-b13d">Training Phase</a></li>
  <li><a href="#sec-data-engineering-deployment-phase-a4c1" id="toc-sec-data-engineering-deployment-phase-a4c1" class="nav-link" data-scroll-target="#sec-data-engineering-deployment-phase-a4c1">Deployment Phase</a></li>
  <li><a href="#sec-data-engineering-maintenance-phase-86d3" id="toc-sec-data-engineering-maintenance-phase-86d3" class="nav-link" data-scroll-target="#sec-data-engineering-maintenance-phase-86d3">Maintenance Phase</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-feature-storage-3423" id="toc-sec-data-engineering-feature-storage-3423" class="nav-link" data-scroll-target="#sec-data-engineering-feature-storage-3423">Feature Storage</a></li>
  <li><a href="#sec-data-engineering-caching-techniques-ac59" id="toc-sec-data-engineering-caching-techniques-ac59" class="nav-link" data-scroll-target="#sec-data-engineering-caching-techniques-ac59">Caching Techniques</a></li>
  <li><a href="#sec-data-engineering-data-access-patterns-f6e7" id="toc-sec-data-engineering-data-access-patterns-f6e7" class="nav-link" data-scroll-target="#sec-data-engineering-data-access-patterns-f6e7">Data Access Patterns</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-11a6" id="toc-sec-data-engineering-continuing-kws-example-11a6" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-11a6">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-governance-f561" id="toc-sec-data-engineering-data-governance-f561" class="nav-link" data-scroll-target="#sec-data-engineering-data-governance-f561">Data Governance</a></li>
  <li><a href="#fallacies-and-pitfalls" id="toc-fallacies-and-pitfalls" class="nav-link" data-scroll-target="#fallacies-and-pitfalls">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-data-engineering-summary-9702" id="toc-sec-data-engineering-summary-9702" class="nav-link" data-scroll-target="#sec-data-engineering-summary-9702">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/data_engineering/data_engineering.html">Data Engineering</a></li></ol></nav></header>




<section id="sec-data-engineering" class="level1 page-columns page-full">
<h1>Data Engineering</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Create a rectangular illustration visualizing the concept of data engineering. Include elements such as raw data sources, data processing pipelines, storage systems, and refined datasets. Show how raw data is transformed through cleaning, processing, and storage to become valuable information that can be analyzed and used for decision-making.</em></p>
</div></div><p> <img src="images/png/cover_data_engineering.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>Why do successful machine learning systems depend more on data engineering excellence than algorithmic sophistication?</em></p>
<p>In machine learning systems, data quality determines success more than algorithmic sophistication. Poor data leads to unreliable models regardless of architectural complexity, while well-engineered data pipelines enable simpler models to achieve superior results. Data engineering encompasses the complete lifecycle: sourcing reliable datasets, ensuring label accuracy, managing storage formats, implementing quality validation, and maintaining governance standards. These engineering decisions directly impact model accuracy, system reliability, computational requirements, and regulatory compliance. Data engineering principles enable the construction of ML systems that perform consistently in production environments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Analyze different data sourcing methods (datasets, web scraping, crowdsourcing, synthetic data).</p></li>
<li><p>Explain the importance of data labeling and ensure label quality.</p></li>
<li><p>Evaluate data storage systems for ML workloads (databases, data warehouses, data lakes).</p></li>
<li><p>Describe the role of data pipelines in ML systems.</p></li>
<li><p>Explain the importance of data governance in ML (security, privacy, ethics).</p></li>
<li><p>Identify key challenges in data engineering for ML.</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-data-engineering-systems-framework" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-systems-framework">Data Engineering Systems Framework</h2>
<p>Data engineering provides a systematic approach to building reliable, scalable, and maintainable data infrastructure for machine learning systems. This chapter presents data engineering as a coherent engineering discipline organized around four foundational pillars: <strong>Quality</strong>, <strong>Reliability</strong>, <strong>Scalability</strong>, and <strong>Governance</strong>. These pillars work together to ensure that data systems can support the full lifecycle of ML applications, from initial problem definition through production deployment.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of Data Engineering">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Data Engineering</summary><div><strong>Data Engineering</strong> is the <em>systematic discipline</em> of designing, building, and maintaining data infrastructure that transforms raw information into reliable, accessible, and ML-ready datasets. This discipline encompasses <em>data acquisition, processing, storage, and governance</em>, ensuring systems are <em>scalable, secure, and aligned</em> with both technical requirements and business objectives. Data engineering focuses on <em>creating stable foundations</em> that enable successful machine learning systems through principled data management.<p></p>
</div></details>
</div>
<section id="the-four-pillars-of-data-engineering-systems" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-four-pillars-of-data-engineering-systems">The Four Pillars of Data Engineering Systems</h3>
<p>Every data engineering decision, from choosing storage formats to designing ingestion pipelines, should be evaluated against these four foundational principles. Each pillar contributes to system success through systematic decision-making:</p>
<p><strong>Quality Foundation</strong>: Data quality determines system success more than algorithmic sophistication. The concept of “Data Cascades,” introduced by <span class="citation" data-cites="sambasivan2021everyone">Sambasivan et al. (<a href="#ref-sambasivan2021everyone" role="doc-biblioref">2021</a>)</span>, demonstrates how quality issues compound throughout the ML lifecycle. When IBM Watson Health’s oncology recommendations came under scrutiny in 2019 for producing potentially unsafe cancer treatment recommendations <span class="citation" data-cites="strickland2019ibm">(<a href="#ref-strickland2019ibm" role="doc-biblioref">Strickland 2019</a>)</span>, investigations revealed systematic data quality failures as root causes that cascaded through the entire system<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Quality encompasses accuracy, completeness, consistency, and fitness for the intended ML task. High-quality data is essential for model success, with the mathematical foundations of this relationship explored in <strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong> and <strong><a href="../core/dnn_architectures/dnn_architectures.html#sec-dnn-architectures">Chapter 4: DNN Architectures</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-strickland2019ibm" class="csl-entry" role="listitem">
Strickland, Eliza. 2019. <span>“IBM Watson, Heal Thyself: How IBM Overpromised and Underdelivered on AI Health Care.”</span> <em>IEEE Spectrum</em> 56 (4): 24–31. <a href="https://doi.org/10.1109/mspec.2019.8678513">https://doi.org/10.1109/mspec.2019.8678513</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;<strong>IBM Watson Health</strong>: IBM’s AI health initiative, which accumulated approximately $4 billion in investment over its lifetime, was sold to Francisco Partners in 2022 after failing to deliver promised breakthroughs due to core data quality issues and over-hyped capabilities.</p></div></div><p><strong>Reliability Infrastructure</strong>: ML systems require consistent, predictable data processing that handles failures gracefully. Reliability means building systems that continue operating despite component failures, data anomalies, or unexpected load patterns. This includes implementing proper error handling, monitoring, and recovery mechanisms throughout the data pipeline.</p>
<p><strong>Scalability Architecture</strong>: While reliability ensures consistent operation, scalability addresses the challenge of growth. As ML systems grow from prototypes to production services, data volumes and processing requirements increase dramatically. Scalability involves designing systems that can handle growing data volumes, user bases, and computational demands without requiring complete system redesigns.</p>
<p><strong>Governance Principles</strong>: Finally, governance provides the framework within which quality, reliability, and scalability operate. Data governance ensures systems operate within legal, ethical, and business constraints while maintaining transparency and accountability. This includes privacy protection, bias mitigation, regulatory compliance, and establishing clear data ownership and access controls.</p>
</section>
<section id="integrating-the-pillars-through-systems-thinking" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="integrating-the-pillars-through-systems-thinking">Integrating the Pillars Through Systems Thinking</h3>
<p>While understanding each pillar individually provides important insights, recognizing their individual importance is only the first step toward effective data engineering. These four pillars are not independent components but interconnected aspects of a unified system where decisions in one area cascade through all others. Quality improvements must consider scalability constraints, reliability requirements affect governance implementations, and governance policies influence quality metrics. This systems perspective guides the organization of this chapter, where each technical topic is examined through the lens of how it supports and balances these foundational principles while managing their inherent tensions.</p>
<p>As <a href="#fig-ds-time" class="quarto-xref">Figure&nbsp;1</a> illustrates, data scientists spend up to 60% of their time on data preparation tasks <span class="citation" data-cites="kaggle2021state">(<a href="#ref-kaggle2021state" role="doc-biblioref">Yedjou et al. 2021</a>)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. This statistic reflects the current state where data engineering practices are often ad-hoc rather than systematic. By applying the four-pillar framework consistently, teams can reduce this overhead while building more reliable and maintainable systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kaggle2021state" class="csl-entry" role="listitem">
Yedjou, CG, SS Tchounwou, RA Aló, R Elhag, B Mochona, and L Latinwo. 2021. <span>“Application of Machine Learning Algorithms in Breast Cancer Diagnosis and Classification.”</span> <em>International Journal of Science Academic Research</em> 2 (1): 3081–86. <a href="https://www.kaggle.com/kaggle-survey-2021">https://www.kaggle.com/kaggle-survey-2021</a>.
</div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Data Quality Reality</strong>: The famous “garbage in, garbage out” principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output. This principle remains critically relevant in modern ML systems.</p></div></div><div id="fig-ds-time" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ds-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bfa17ab7428a59073217342626b3309e90c451c4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Data Scientist Time Allocation: Data preparation consumes a majority of data science effort, up to 60%, underscoring the need for systematic data engineering practices to prevent downstream model failures and ensure project success. Prioritizing data quality and pipeline development yields greater returns than solely focusing on advanced algorithms. Source: Various industry reports."><img src="data_engineering_files/mediabag/bfa17ab7428a59073217342626b3309e90c451c4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ds-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Data Scientist Time Allocation</strong>: Data preparation consumes a majority of data science effort, up to 60%, underscoring the need for systematic data engineering practices to prevent downstream model failures and ensure project success. Prioritizing data quality and pipeline development yields greater returns than solely focusing on advanced algorithms. Source: Various industry reports.
</figcaption>
</figure>
</div>
</section>
<section id="framework-application-throughout-the-data-lifecycle" class="level3">
<h3 class="anchored" data-anchor-id="framework-application-throughout-the-data-lifecycle">Framework Application Throughout the Data Lifecycle</h3>
<p>This four-pillar framework guides our exploration of data engineering systems from problem definition through production operations. We begin by establishing clear problem definitions and governance principles that shape all subsequent technical decisions. The framework then guides us through data acquisition strategies, where quality and reliability requirements determine how we source and validate data. Processing and storage decisions follow naturally from scalability and governance constraints, while operational practices ensure all four pillars are maintained throughout the system lifecycle.</p>
<p>Building on this foundation, each major section of this chapter demonstrates how the four pillars work together in practice. Data sourcing techniques must balance quality requirements with scalability constraints. Processing pipelines must ensure reliability while meeting governance standards. Storage architectures must support both performance needs and privacy requirements. This integrated approach transforms data engineering from a collection of ad-hoc techniques into a systematic engineering discipline.</p>
<p>To make these abstract principles concrete, we will illustrate them through a detailed case study: building a Keyword Spotting (KWS) system for voice-activated devices. This example demonstrates how the four-pillar framework guides real-world engineering decisions, from initial data collection through production deployment, showing how quality, reliability, scalability, and governance requirements shape every aspect of system design.</p>
</section>
</section>
<section id="sec-data-engineering-problem-definition-f820" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-problem-definition-f820">Problem Definition and Governance Foundations</h2>
<p>Building on the four-pillar framework established above, clear problem definitions and governance principles must guide all subsequent engineering decisions. These foundational choices determine system architecture and operational characteristics throughout the ML lifecycle.</p>
<p>The importance of this foundational work becomes clear when examining “Data Cascades,” the phenomenon identified by <span class="citation" data-cites="sambasivan2021everyone">Sambasivan et al. (<a href="#ref-sambasivan2021everyone" role="doc-biblioref">2021</a>)</span> where quality issues compound throughout ML systems, leading to model failures, project termination, and potential harm to users. These cascades occur when teams skip establishing clear quality criteria, reliability requirements, and governance principles before beginning data collection and processing work.</p>
<div class="no-row-height column-margin column-container"></div><p><a href="#fig-cascades" class="quarto-xref">Figure&nbsp;2</a> illustrates these potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. As illustrated in the figure, any lapses in this initial stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early, mitigating these cascading effects.</p>
<div id="fig-cascades" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cascades-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="42e7fff97c3a3c84a75ca00f3e1adc11fddbd994.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Data Quality Cascades: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Recognizing these cascades motivates proactive investment in data engineering and quality control to mitigate risks and ensure reliable system performance. Source: [@sambasivan2021everyone]."><img src="data_engineering_files/mediabag/42e7fff97c3a3c84a75ca00f3e1adc11fddbd994.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cascades-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Data Quality Cascades</strong>: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Recognizing these cascades motivates proactive investment in data engineering and quality control to mitigate risks and ensure reliable system performance. Source: <span class="citation" data-cites="sambasivan2021everyone">(<a href="#ref-sambasivan2021everyone" role="doc-biblioref">Sambasivan et al. 2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-sambasivan2021everyone" class="csl-entry" role="listitem">
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. <span>“<span>‘Everyone Wants to Do the Model Work, Not the Data Work’</span>: Data Cascades in High-Stakes AI.”</span> In <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 1–15. ACM. <a href="https://doi.org/10.1145/3411764.3445518">https://doi.org/10.1145/3411764.3445518</a>.
</div></div></figure>
</div>
<section id="governance-principles-for-data-engineering" class="level3">
<h3 class="anchored" data-anchor-id="governance-principles-for-data-engineering">Governance Principles for Data Engineering</h3>
<p>With this understanding of how quality issues cascade through ML systems, we must establish governance principles that ensure our data engineering systems operate within ethical, legal, and business constraints. These principles are not afterthoughts to be applied later but foundational requirements that shape every technical decision from the outset.</p>
<p>Central to these governance principles, data systems must protect user privacy and maintain security throughout their lifecycle. This means implementing access controls, encryption, and data minimization practices from the initial system design, not adding them as later enhancements. Privacy requirements directly influence data collection methods, storage architectures, and processing approaches.</p>
<p>Beyond privacy protection, data engineering systems must actively work to identify and mitigate bias in data collection, labeling, and processing. This requires diverse data collection strategies, representative sampling approaches, and systematic bias detection throughout the pipeline. Technical choices about data sources, labeling methodologies, and quality metrics all impact system fairness.</p>
<p>Complementing these fairness efforts, systems must maintain clear documentation about data sources, processing decisions, and quality criteria. This includes implementing data lineage tracking, maintaining processing logs, and establishing clear ownership and responsibility for data quality decisions.</p>
<p>Finally, data systems must comply with relevant regulations such as GDPR, CCPA, and domain-specific requirements. Compliance requirements influence data retention policies, user consent mechanisms, and cross-border data transfer protocols.</p>
<p>These governance principles work hand-in-hand with our technical pillars of quality, reliability, and scalability. A system cannot be truly reliable if it violates user privacy, and quality metrics are meaningless if they perpetuate unfair outcomes.</p>
</section>
<section id="systematic-problem-definition-process" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="systematic-problem-definition-process">Systematic Problem Definition Process</h3>
<p>Building on these governance foundations, we need a systematic approach to problem definition. As <span class="citation" data-cites="sculley2015hidden">Sculley et al. (<a href="#ref-sculley2015hidden" role="doc-biblioref">2021</a>)</span> emphasize, ML systems require problem framing that goes beyond traditional software development approaches. Whether developing recommendation engines processing millions of user interactions, computer vision systems analyzing medical images, or natural language models handling diverse text data, each system brings unique challenges that must be carefully considered within our governance and technical framework.</p>
<div class="no-row-height column-margin column-container"></div><p>Within this context, establishing clear objectives provides unified direction that guides the entire project, from data collection strategies through deployment operations. These objectives must balance technical performance with governance requirements, creating measurable outcomes that include both accuracy metrics and fairness criteria.</p>
<p>This systematic approach to problem definition ensures that governance principles and technical requirements are integrated from the start rather than retrofitted later. To achieve this integration, we identify the key steps that must precede any data collection effort:</p>
<ol type="1">
<li>Identify and clearly state the problem definition</li>
<li>Set clear objectives to meet</li>
<li>Establish success benchmarks</li>
<li>Understand end-user engagement/use</li>
<li>Understand the constraints and limitations of deployment</li>
<li>Perform data collection.</li>
<li>Iterate and refine.</li>
</ol>
</section>
<section id="applying-the-framework-keyword-spotting-case-study" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="applying-the-framework-keyword-spotting-case-study">Applying the Framework: Keyword Spotting Case Study</h3>
<p>To demonstrate how these systematic principles work in practice, Keyword Spotting (KWS) systems provide an ideal case study for applying our four-pillar framework to real-world data engineering challenges. These systems, which power voice-activated devices like smartphones and smart speakers, must detect specific wake words (such as “OK, Google” or “Alexa”) within continuous audio streams while operating under strict resource constraints.</p>
<p>As shown in <a href="#fig-keywords" class="quarto-xref">Figure&nbsp;3</a>, KWS systems operate as lightweight, always-on front-ends that trigger more complex voice processing systems. This deployment context creates specific requirements for each of our four pillars:</p>
<div id="fig-keywords" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-keywords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/data_engineering_kws.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Keyword Spotting System: A typical deployment of keyword spotting (KWS) technology in a voice-activated device, where a constantly-listening system detects a wake word to initiate further processing. this example demonstrates how KWS serves as a lightweight, always-on front-end for more complex voice interfaces."><img src="images/png/data_engineering_kws.png" class="img-fluid figure-img" style="width:55.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-keywords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Keyword Spotting System</strong>: A typical deployment of keyword spotting (KWS) technology in a voice-activated device, where a constantly-listening system detects a wake word to initiate further processing. this example demonstrates how KWS serves as a lightweight, always-on front-end for more complex voice interfaces.
</figcaption>
</figure>
</div>
<p><strong>Quality Challenges</strong>: KWS systems must achieve high accuracy across diverse acoustic environments, speaker accents, and background noise conditions. Quality requirements include distinguishing target keywords from similar-sounding words, handling whispered commands and shouted instructions with equal precision, and maintaining performance across demographic and linguistic diversity.</p>
<p><strong>Reliability Requirements</strong>: Building on these quality demands, these systems operate continuously on battery-powered devices, requiring consistent performance over extended periods. Reliability encompasses graceful handling of audio quality variations, consistent behavior across device wake/sleep cycles, and robust operation despite limited computational resources.</p>
<p><strong>Scalability Constraints</strong>: Beyond reliability considerations, KWS models must operate on low-power microcontrollers with severe memory and computational limitations, while potentially serving millions of devices worldwide. Scalability challenges include model size optimization, efficient inference algorithms, and distributed deployment across diverse hardware platforms.</p>
<p><strong>Governance Complexities</strong>: Overlaying these technical constraints, always-listening systems raise significant privacy concerns, requiring data minimization, local processing, and transparent user controls. Governance requirements include protecting recorded voice data, ensuring fair performance across demographic groups, and complying with privacy regulations across global markets.</p>
<p>These interconnected challenges become evident in the current landscape: many KWS systems support only a limited number of languages, primarily due to the governance and scalability challenges of collecting high-quality, representative voice data for smaller linguistic populations. This limitation demonstrates how all four pillars must work together to achieve successful deployment.</p>
<p>With this framework understanding established, we can apply the systematic problem definition process to our KWS example, demonstrating how the four pillars guide practical engineering decisions:</p>
<ol type="1">
<li><p><strong>Identifying the Problem</strong>: KWS detects specific keywords amidst ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources. A well-specified problem definition for developing a new KWS model should identify the desired keywords along with the envisioned application and deployment scenario.</p></li>
<li><p><strong>Setting Clear Objectives</strong>: The objectives for a KWS system might include:</p>
<ul>
<li>Achieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).</li>
<li>Ensuring low latency (e.g., keyword detection and response within 200 milliseconds).</li>
<li>Minimizing power consumption to extend battery life on embedded devices.</li>
<li>Ensuring the model’s size is optimized for the available memory on the device.</li>
</ul></li>
<li><p><strong>Benchmarks for Success</strong>: Establish clear metrics to measure the success of the KWS system. This could include:</p>
<ul>
<li><em>True Positive Rate:</em> The percentage of correctly identified keywords relative to all spoken keywords.</li>
<li><em>False Positive Rate:</em> The percentage of non-keywords (including silence, background noise, and out-of-vocabulary words) incorrectly identified as keywords.</li>
<li><em>Detection/Error Tradeoff</em> These curves evaluate KWS on streaming audio representative of a real-world deployment scenario, by comparing the number of false accepts per hour (the number of false positives over the total duration of the evaluation audio) against the false rejection rate (the number of missed keywords relative to the number of spoken keywords in the evaluation audio). <span class="citation" data-cites="nayak2022improving">Nayak et al. (<a href="#ref-nayak2022improving" role="doc-biblioref">2022</a>)</span> provides one example of this.</li>
<li><em>Response Time:</em> The time taken from keyword utterance to system response.</li>
<li><em>Power Consumption:</em> Average power used during keyword detection.</li>
</ul></li>
<li><p><strong>Stakeholder Engagement and Understanding</strong>: Engage with stakeholders, which include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:</p>
<ul>
<li>Device manufacturers might prioritize low power consumption.</li>
<li>Software developers might emphasize ease of integration.</li>
<li>End-users would prioritize accuracy and responsiveness.</li>
</ul></li>
<li><p><strong>Understanding the Constraints and Limitations of Embedded Systems</strong>: Embedded devices come with their own set of challenges:</p>
<ul>
<li><em>Memory Limitations:</em> KWS models must be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models need to be as small as 16 KB to fit in the always-on island of the SoC. This represents only the model size. Application code for preprocessing must also fit within the memory constraints.</li>
<li>Processing Power: The computational capabilities of embedded devices are limited (a few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.</li>
<li><em>Power Consumption:</em> Since many embedded devices are battery-powered, the KWS system must be power-efficient.</li>
<li><em>Environmental Challenges:</em> Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must function effectively across these scenarios.</li>
</ul></li>
<li><p><strong>Data Collection and Analysis</strong>: For a KWS system, data quality and diversity determine success. Considerations include:</p>
<ul>
<li><em>Demographics:</em> Collect data from speakers with various accents across age and gender to ensure wide-ranging recognition support.</li>
<li><em>Keyword Variations:</em> People might pronounce keywords differently or express slight variations in the wake word itself. Ensure the dataset captures these nuances.</li>
<li><em>Background Noises:</em> Include or augment data samples with different ambient noises to train the model for real-world scenarios.</li>
</ul></li>
<li><p><strong>Iterative Feedback and Refinement</strong>: Finally, once a prototype KWS system is developed, teams must ensure the system remains aligned with the defined problem and objectives as deployment scenarios change over time and use-cases evolve.</p>
<ul>
<li>Test it in real-world scenarios</li>
<li>Gather feedback - are some users or deployment scenarios encountering underperformance relative to others?</li>
<li>Iteratively refine the dataset and model</li>
</ul></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-nayak2022improving" class="csl-entry" role="listitem">
Nayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen Shum, Siddharth Sigtia, Erik Marchi, et al. 2022. <span>“Improving Voice Trigger Detection with Metric Learning.”</span> <em>arXiv Preprint arXiv:2204.02455</em>, April. <a href="http://arxiv.org/abs/2204.02455v2">http://arxiv.org/abs/2204.02455v2</a>.
</div></div><p>This comprehensive KWS example illustrates the broader principles of problem definition, showing how initial decisions about data requirements ripple throughout a project’s lifecycle. By carefully considering each aspect, from core problem identification, through performance benchmarks, to deployment constraints, teams can build a strong foundation for their ML systems. The methodical problem definition process provides a framework applicable across the ML spectrum. Whether developing computer vision systems for medical diagnostics, recommendation engines processing millions of user interactions, or natural language models analyzing diverse text corpora, this structured approach helps teams anticipate and plan for their data needs.</p>
<p>Having established our problem definition and governance principles, we now turn to the systematic implementation of these requirements through data pipeline architecture. Pipelines translate our four-pillar framework into operational reality, ensuring that quality, reliability, scalability, and governance principles are maintained throughout the data transformation process.</p>
<div id="quiz-question-sec-data-engineering-problem-definition-f820" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>What is a ‘Data Cascade’ in the context of machine learning systems?</p>
<ol type="a">
<li>A process where data errors compound, affecting downstream stages</li>
<li>A series of algorithmic improvements</li>
<li>A method of data augmentation</li>
<li>A type of neural network architecture</li>
</ol></li>
<li><p>Why is it crucial to define the problem clearly before collecting data in an ML project?</p></li>
<li><p>Which of the following is NOT a benchmark for success in a Keyword Spotting (KWS) system?</p>
<ol type="a">
<li>True Positive Rate</li>
<li>False Positive Rate</li>
<li>Model Training Time</li>
<li>Power Consumption</li>
</ol></li>
<li><p>In a production system, how might poor data quality at the data collection stage affect the deployment phase?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-problem-definition-f820" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-pipeline-basics-31ba" class="level2">
<h2 class="anchored" data-anchor-id="sec-data-engineering-pipeline-basics-31ba">Data Pipeline Architecture</h2>
<p>Data pipelines serve as the systematic implementation of our four-pillar framework, transforming raw data into ML-ready formats while maintaining quality, reliability, scalability, and governance standards. Rather than simple linear data flows, these are complex systems that must orchestrate multiple data sources, transformation processes, and storage systems while ensuring consistent performance under varying load conditions.</p>
<p>To illustrate these concepts, our KWS system pipeline architecture must handle continuous audio streams, maintain low-latency processing for real-time keyword detection, and ensure privacy-preserving data handling. The pipeline must scale from development environments processing sample audio files to production deployments handling millions of concurrent audio streams while maintaining strict quality and governance standards.</p>
<div id="fig-pipeline-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4213eeaeb926472ac24f617144bb4a8344e1ec2b.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Data Pipeline Architecture: Modular pipelines ingest, process, and deliver data for machine learning tasks, enabling independent scaling of components and improved data quality control. Distinct stages (ingestion, storage, and preparation) transform raw data into a format suitable for model training and validation, forming the foundation of reliable ML systems."><img src="data_engineering_files/mediabag/4213eeaeb926472ac24f617144bb4a8344e1ec2b.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Data Pipeline Architecture</strong>: Modular pipelines ingest, process, and deliver data for machine learning tasks, enabling independent scaling of components and improved data quality control. Distinct stages (ingestion, storage, and preparation) transform raw data into a format suitable for model training and validation, forming the foundation of reliable ML systems.
</figcaption>
</figure>
</div>
<p>As shown in the architecture diagram, ML data pipelines consist of several distinct layers: data sources, ingestion, processing, labeling, storage, and ML training (<a href="#fig-pipeline-flow" class="quarto-xref">Figure&nbsp;4</a>). Each layer plays a specific role in the data preparation workflow, and selecting appropriate technologies for each layer requires understanding core trade-offs.</p>
<p>Central to these design decisions, data pipeline design is constrained by storage hierarchies and I/O bandwidth limitations rather than CPU capacity. Understanding these constraints enables building efficient systems that can handle modern ML workloads. The storage hierarchy creates distinct tiers with different performance characteristics: object storage provides high latency but low cost with virtually unlimited scale, making it ideal for archival and batch processing. Distributed filesystems offer medium latency at moderate cost with petabyte scale, optimized for analytics workloads. In-memory stores deliver low latency but at high cost with capacity limited by RAM, essential for real-time serving.</p>
<p>Building on these storage considerations, the bandwidth hierarchy further shapes pipeline performance, with concrete performance tiers that directly influence architectural decisions. Traditional spinning disks deliver 100-200 MB/s sequential throughput, sufficient for batch ETL processing but inadequate for real-time model serving. SSD storage achieves 1-7 GB/s sequential access, enabling faster data loading for training pipelines but still creating bottlenecks for large model training. Network storage systems typically provide 1-10 GB/s depending on infrastructure configuration, with high-end configurations approaching the performance of local SSDs. RAM access delivers 50-200 GB/s typical bandwidth, explaining why in-memory data stores dramatically outperform disk-based systems for latency-sensitive applications. These bandwidth constraints explain why ETL operations frequently achieve low CPU utilization, with processors waiting on I/O operations. Training large models requires sustained read bandwidth that often exceeds storage capabilities, demonstrating why storage architecture directly impacts training feasibility.</p>
<p>Given these performance constraints, design decisions must align with specific requirements. For streaming data, consider whether you need message durability (ability to replay failed processing), ordering guarantees (maintaining event sequence), or geographic distribution. For batch processing, the key decision factors include data volume relative to memory, processing complexity, and whether computation must be distributed. Single-machine tools suffice for gigabyte-scale data, but terabyte-scale processing requires distributed frameworks that partition work across clusters. Storage decisions depend on access patterns: structured query workloads benefit from columnar storage and indexing, while unstructured data requires flexible schemas and horizontal scaling. The interactions between these layers determine the system’s overall effectiveness, with each component’s performance characteristics directly influencing the overall pipeline design and ML system capabilities.</p>
<section id="sec-data-engineering-pipeline-monitoring-debugging" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-pipeline-monitoring-debugging">Pipeline Monitoring and Debugging</h3>
<p>Once pipelines are operational, production data pipelines require comprehensive monitoring and debugging capabilities to maintain reliability and detect issues before they cascade into model failures. Experience shows that 70% of production ML failures stem from data pipeline issues that could have been caught with proper monitoring. The four-pillar framework guides monitoring strategy: Quality metrics track data accuracy and completeness, Reliability monitoring ensures consistent pipeline operation, Scalability metrics prevent resource exhaustion, and Governance monitoring maintains compliance and auditability.</p>
<p>To achieve this level of monitoring, <strong>Pipeline Observability Implementation</strong> encompasses three critical dimensions: metrics, logging, and tracing. Key metrics include data freshness (time from source update to availability), throughput rates (records processed per unit time), error rates (percentage of failed processing attempts), and data quality scores (automated assessment of accuracy and completeness). Logging captures detailed processing events for debugging, while distributed tracing tracks data lineage across pipeline stages to enable root cause analysis when issues occur.</p>
<p>Beyond establishing observability infrastructure, <strong>Common Failure Patterns and Detection</strong> in production pipelines follow predictable categories that require specific monitoring approaches. Silent failures represent the most dangerous pattern where processing continues but produces incorrect results, such as when a data transformation subtly corrupts values without throwing errors. Schema evolution failures occur when upstream data sources change field names, add new columns, or modify data types without coordination. Resource exhaustion manifests as gradually increasing processing times, memory consumption, or storage usage that eventually overwhelms system capacity. External dependency failures include database timeouts, API rate limiting, or network partitions that disrupt data ingestion.</p>
<p>Complementing failure detection capabilities, <strong>Automated Recovery and Alerting Strategies</strong> enable systems to handle failures gracefully through intelligent retry logic, circuit breakers, and escalation procedures. Exponential backoff prevents overwhelming recovering services, while dead letter queues capture failed messages for later analysis and reprocessing. Alert design must balance sensitivity with actionability: alerts should fire early enough to prevent cascading failures but include sufficient context for rapid diagnosis. Effective production monitoring implements tiered alerting where critical issues trigger immediate response while warnings accumulate for batch review.</p>
<p>These concepts become concrete when we consider a recommendation system pipeline that processes user interaction events. Monitoring would track event ingestion rates, feature computation latency, and model serving freshness. When weekend traffic patterns cause 3x normal load, monitoring detects increased processing delays and automatically scales compute resources while alerting the operations team. This proactive approach prevents user-visible degradation and maintains system reliability during demand spikes.</p>
<p>With our pipeline architecture and monitoring capabilities established, we now examine how to source data that meets our four-pillar framework requirements. Data acquisition strategy directly determines the foundational quality of our entire system.</p>
<div id="quiz-question-sec-data-engineering-pipeline-basics-31ba" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which component of a data pipeline is primarily responsible for transforming raw data into a format suitable for model training?</p>
<ol type="a">
<li>Processing Layer</li>
<li>Data Ingestion</li>
<li>Storage Layer</li>
<li>Data Sources</li>
</ol></li>
<li><p>Explain how data quality is maintained throughout a data pipeline and why it is crucial for machine learning systems.</p></li>
<li><p>Order the following stages in a typical ML data pipeline: (1) Data Ingestion, (2) Model Training, (3) Data Labeling, (4) Storage Layer.</p></li>
<li><p>In a production system handling real-time data, which data pipeline component is critical for ensuring data is processed as it arrives?</p>
<ol type="a">
<li>Batch Ingestion</li>
<li>Feature Engineering</li>
<li>Data Validation</li>
<li>Stream Processing</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-pipeline-basics-31ba" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-sources-c8d9" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-sources-c8d9">Strategic Data Acquisition</h2>
<p>The approaches we choose for sourcing training data directly determine our system’s quality foundation, reliability characteristics, scalability potential, and governance compliance. Rather than treating data sources as independent options, we examine them as strategic choices that must align with our established framework requirements.</p>
<p>For our KWS system, data source decisions have profound implications: existing datasets may provide quick prototyping but lack the linguistic diversity needed for global deployment; web scraping can increase coverage but raises privacy and quality concerns; crowdsourcing enables targeted data collection but requires careful bias management; and synthetic data offers controlled generation but may not capture real-world complexity. Each approach presents distinct trade-offs across our four pillars.</p>
<p>The strategic framework for data acquisition evaluates each source against specific criteria:</p>
<section id="sec-data-engineering-existing-datasets-4f21" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-existing-datasets-4f21">Existing Datasets</h3>
<p>Within this strategic framework, platforms like <a href="https://www.kaggle.com/">Kaggle</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and <a href="https://archive.ics.uci.edu/">UCI Machine Learning Repository</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency, as creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Kaggle</strong>: Founded in 2010 and acquired by Google in 2017, Kaggle hosts over 80,000 public datasets and serves 15+ million registered users globally. Its competition platform has driven major ML breakthroughs, including the $1 million Netflix Prize that advanced collaborative filtering algorithms.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>UCI ML Repository</strong>: Established in 1987 by UC Irvine’s Machine Learning Group, it’s one of the oldest and most cited ML dataset repositories. Contains 600+ datasets including classics like Iris (1936) and Wine (1991) that shaped decades of ML research and education.</p></div></div><p>Building on this cost efficiency, many of these datasets, such as <a href="https://www.image-net.org/">ImageNet</a>, have become standard benchmarks in the machine learning community, enabling consistent performance comparisons across different models and architectures. For ML system developers, this standardization provides clear metrics for evaluating model improvements and system performance. The immediate availability of these datasets allows teams to begin experimentation and prototyping without delays in data collection and preprocessing.</p>
<p>Despite these advantages, ML practitioners must carefully consider the quality assurance aspects of pre-existing datasets. For instance, the ImageNet dataset was found to have label errors on 6.4% of the validation set <span class="citation" data-cites="northcutt2021pervasive">(<a href="#ref-northcutt2021pervasive" role="doc-biblioref">Northcutt, Athalye, and Mueller 2021</a>)</span>. While popular datasets benefit from community scrutiny that helps identify and correct errors and biases, most datasets remain “untended gardens” where quality issues can significantly impact downstream system performance if not properly addressed. As <span class="citation" data-cites="gebru2018datasheets">(<a href="#ref-gebru2018datasheets" role="doc-biblioref">Gebru et al. 2021</a>)</span> highlighted in her paper, simply providing a dataset without documentation can lead to misuse and misinterpretation, potentially amplifying biases present in the data.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gebru2018datasheets" class="csl-entry" role="listitem">
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. <span>“Datasheets for Datasets.”</span> <em>Communications of the ACM</em> 64 (12): 86–92. <a href="https://doi.org/10.1145/3458723">https://doi.org/10.1145/3458723</a>.
</div><div id="ref-pineau2021improving" class="csl-entry" role="listitem">
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. <span>“Improving Reproducibility in Machine Learning Research (a Report from the Neurips 2019 Reproducibility Program).”</span> <em>Journal of Machine Learning Research</em> 22 (164): 1–20.
</div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>ML Reproducibility Crisis</strong>: Only 15% of ML papers include code, and fewer than 6% provide complete reproducible implementations. NeurIPS 2019 introduced mandatory reproducibility checklists, while venues like MLSys require artifact evaluation to address this systemic problem.</p></div></div><p>Beyond quality concerns, supporting documentation accompanying existing datasets is invaluable, yet is often only present in widely-used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around improving reproducibility in machine learning systems <span class="citation" data-cites="pineau2021improving">(<a href="#ref-pineau2021improving" role="doc-biblioref">Pineau et al. 2021</a>)</span><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other’s work more rapidly.</p>
<p>Even with proper documentation, understanding the context in which the data was collected becomes necessary. Researchers must avoid potential overfitting when using popular datasets such as ImageNet <span class="citation" data-cites="beyer2020we">(<a href="#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span>, which can lead to inflated performance metrics. Sometimes, these <a href="https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/">datasets do not reflect the real-world data</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. <span>“Are We Done with ImageNet?”</span> <em>arXiv Preprint arXiv:2006.07159</em>, June. <a href="http://arxiv.org/abs/2006.07159v1">http://arxiv.org/abs/2006.07159v1</a>.
</div></div><p>Central to these contextual concerns, a key consideration for ML systems is how well pre-existing datasets reflect real-world deployment conditions. Relying on standard datasets can create a concerning disconnect between training and production environments. This misalignment becomes particularly problematic when multiple ML systems are trained on the same datasets (<a href="#fig-misalignment" class="quarto-xref">Figure&nbsp;5</a>), potentially propagating biases and limitations throughout an entire ecosystem of deployed models.</p>
<div id="fig-misalignment" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-misalignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="468ba33927b8b88f56facc3c9e5a72db5d0773f0.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Dataset Convergence: Shared datasets can mask limitations and propagate biases across multiple machine learning systems, potentially leading to overoptimistic performance evaluations and reduced generalization to unseen data. Reliance on common datasets creates a false sense of progress within an ecosystem of models, hindering the development of robust and reliable AI applications."><img src="data_engineering_files/mediabag/468ba33927b8b88f56facc3c9e5a72db5d0773f0.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-misalignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Dataset Convergence</strong>: Shared datasets can mask limitations and propagate biases across multiple machine learning systems, potentially leading to overoptimistic performance evaluations and reduced generalization to unseen data. Reliance on common datasets creates a false sense of progress within an ecosystem of models, hindering the development of robust and reliable AI applications.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-web-scraping-fa9f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-web-scraping-fa9f">Web Scraping</h3>
<p>When these limitations of existing datasets become problematic, particularly in domains where pre-existing datasets are insufficient, web scraping offers a powerful approach to gathering training data at scale. This automated technique for extracting data from websites has become a powerful tool in modern ML system development. It enables teams to build custom datasets tailored to their specific needs.</p>
<p>This automated technique demonstrates its value when human-labeled data is scarce. Consider computer vision systems: major datasets like <a href="https://www.image-net.org/">ImageNet</a> and <a href="https://storage.googleapis.com/openimages/web/index.html">OpenImages</a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> were built through systematic web scraping, significantly advancing the field of computer vision.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Open Images Dataset</strong>: Google’s Open Images V7 contains 9 million images with 16 million bounding boxes across 600 object classes. Released in 2016 and continuously updated, it became the largest publicly available dataset for object detection, enabling breakthrough research in computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford’s LabelMe project demonstrated this approach’s potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.</p></div><div id="ref-groeneveld2024olmo" class="csl-entry" role="listitem">
Groeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, et al. 2024. <span>“OLMo: Accelerating the Science of Language Models.”</span> <em>arXiv Preprint arXiv:2402.00838</em>, February. <a href="http://arxiv.org/abs/2402.00838v4">http://arxiv.org/abs/2402.00838v4</a>.
</div><div id="ref-chen2021evaluating" class="csl-entry" role="listitem">
Chen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021. <span>“Evaluating Large Language Models Trained on Code.”</span> <em>arXiv Preprint arXiv:2107.03374</em>, July. <a href="http://arxiv.org/abs/2107.03374v2">http://arxiv.org/abs/2107.03374v2</a>.
</div></div><p>Expanding beyond these computer vision applications, the impact of web scraping extends well beyond computer vision systems. In natural language processing, web-scraped data has enabled the development of increasingly sophisticated ML systems. Large language models, such as ChatGPT and Claude, rely on vast amounts of text scraped from the public internet and media to learn language patterns and generate responses <span class="citation" data-cites="groeneveld2024olmo">(<a href="#ref-groeneveld2024olmo" role="doc-biblioref">Groeneveld et al. 2024</a>)</span>. Similarly, specialized ML systems like GitHub’s Copilot demonstrate how targeted web scraping, in this case of code repositories, can create powerful domain-specific assistants <span class="citation" data-cites="chen2021evaluating">(<a href="#ref-chen2021evaluating" role="doc-biblioref">Chen et al. 2021</a>)</span>.</p>
<p>Building on these foundational developments, production ML systems often require continuous data collection to maintain relevance and performance. Web scraping facilitates this by gathering structured data like stock prices, weather patterns, or product information for analytical applications. This continuous collection introduces unique challenges for ML systems. Data consistency becomes crucial, as variations in website structure or content formatting can disrupt the data pipeline and affect model performance. Proper data management through databases or warehouses becomes essential not just for storage, but for maintaining data quality and enabling model updates.</p>
<p>However, alongside these powerful capabilities, web scraping presents several challenges that ML system developers must carefully consider. Legal and ethical constraints can limit data collection, as not all websites permit scraping, and violating these restrictions can have <a href="https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/">serious consequences</a>. When building ML systems with scraped data, teams must carefully document data sources and ensure compliance with terms of service and copyright laws. Privacy considerations become important when dealing with user-generated content, often requiring systematic anonymization procedures.</p>
<p>Complementing these legal and ethical constraints, technical limitations also affect the reliability of web-scraped training data. Rate limiting by websites can slow data collection, while the dynamic nature of web content can introduce inconsistencies that impact model training. As shown in <a href="#fig-traffic-light" class="quarto-xref">Figure&nbsp;6</a>, web scraping can yield unexpected or irrelevant data, for example, historical images appearing in contemporary image searches, that can pollute training datasets and degrade model performance. These issues highlight the importance of thorough data validation and cleaning processes in ML pipelines built on web-scraped data.</p>
<div id="fig-traffic-light" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-traffic-light-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/1914_traffic.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Data Source Noise: Web scraping introduces irrelevant or outdated data into training sets, requiring systematic data validation and cleaning to maintain model performance and prevent spurious correlations. Historical images appearing in contemporary searches exemplify this noise, underscoring the need for careful filtering and quality control in web-sourced datasets. Source: Vox."><img src="images/jpg/1914_traffic.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-traffic-light-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Data Source Noise</strong>: Web scraping introduces irrelevant or outdated data into training sets, requiring systematic data validation and cleaning to maintain model performance and prevent spurious correlations. Historical images appearing in contemporary searches exemplify this noise, underscoring the need for careful filtering and quality control in web-sourced datasets. Source: Vox.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-crowdsourcing-e093" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-crowdsourcing-e093">Crowdsourcing</h3>
<p>When web scraping cannot provide the human judgment needed for complex labeling tasks, crowdsourcing offers a collaborative approach to data collection, leveraging the collective efforts of distributed individuals via the internet to tackle tasks requiring human judgment. By engaging a global pool of contributors, this method accelerates the creation of high-quality, labeled datasets for machine learning systems, especially in scenarios where pre-existing data is scarce or domain-specific. Platforms like <a href="https://www.mturk.com/">Amazon Mechanical Turk</a> exemplify how crowdsourcing facilitates this process by distributing annotation tasks to a global workforce. This enables the rapid collection of labels for complex tasks such as sentiment analysis, image recognition, and speech transcription, significantly expediting the data preparation phase.</p>
<p>To illustrate the power of this approach, one of the most impactful examples of crowdsourcing in machine learning is the creation of the <a href="https://image-net.org/">ImageNet dataset</a>. ImageNet, which revolutionized computer vision, was built by distributing image labeling tasks to contributors via Amazon Mechanical Turk. The contributors categorized millions of images into thousands of classes, enabling researchers to train and benchmark models for a wide variety of visual recognition tasks.</p>
<p>Building on this massive labeling effort, the dataset’s availability spurred advancements in deep learning, including the breakthrough AlexNet model in 2012 that demonstrated the power of large-scale neural networks and showed how large-scale, crowdsourced datasets could drive innovation. ImageNet’s success highlights how leveraging a diverse group of contributors for annotation can enable machine learning systems to achieve unprecedented performance.</p>
<p>Extending beyond academic research, another example of crowdsourcing’s potential is Google’s <a href="https://crowdsource.google.com/">Crowdsource</a><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Google Crowdsource</strong>: Launched in 2016, this gamified platform has collected over 4 billion contributions from volunteers in 120+ languages. It powers improvements to Google Translate, Maps, and other AI services while demonstrating sustainable crowdsourcing through community engagement rather than monetary incentives. By gamifying the process and engaging global participants, Google leverages diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Waze Crowdsourcing Model</strong>: Founded in Israel (2006), Waze processes 25 billion miles of driving data monthly from 140+ million users. Its real-time crowdsourced traffic model influenced Google Maps after Google’s $1.3 billion acquisition (2013), demonstrating how user-generated data creates network effects. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.</p></div></div><p>Beyond these static dataset creation efforts, crowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app <a href="https://www.waze.com/">Waze</a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting.</p>
<p>These diverse applications highlight one of the primary advantages of crowdsourcing: its scalability. By distributing microtasks to a large audience, projects can process enormous volumes of data quickly and cost-effectively. This scalability is particularly beneficial for machine learning systems that require extensive datasets to achieve high performance. The diversity of contributors introduces a wide range of perspectives, cultural insights, and linguistic variations, enriching datasets and improving models’ ability to generalize across populations.</p>
<p>Complementing this scalability advantage, flexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google’s <a href="https://www.google.com/recaptcha/about/">reCAPTCHA</a><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<strong>reCAPTCHA Evolution</strong>: Originally created at Carnegie Mellon in 2007 to digitize books, reCAPTCHA v1 helped transcribe 13 million books. After Google’s 2009 acquisition, v2 (2014) shifted to image recognition for Street View, while v3 (2018) uses behavioral analysis, processing 1+ billion CAPTCHAs weekly. Users identify objects in images, including street signs and cars, contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale effectively when embedded into everyday workflows.</p></div></div><p>However, alongside these compelling advantages, crowdsourcing presents challenges that require careful management. Quality control is a major concern, as the variability in contributors’ expertise and attention can lead to inconsistent or inaccurate annotations. Providing clear instructions and training materials helps ensure participants understand the task requirements. Techniques such as embedding known test cases, leveraging consensus algorithms, or using redundant annotations can mitigate quality issues and align the process with the problem definition discussed earlier.</p>
<p>Beyond quality control issues, ethical considerations are important in crowdsourcing, especially when datasets are built at scale using global contributors. Teams must ensure that participants are fairly compensated for their work and that they are informed about how their contributions will be used. Privacy concerns must be addressed, particularly when dealing with sensitive or personal information. Transparent sourcing practices, clear communication with contributors, and systematic auditing mechanisms are necessary for building trust and maintaining ethical standards.</p>
<p>These ethical concerns became particularly visible when the issue of fair compensation and ethical data sourcing was brought into sharp focus during the development of large-scale AI systems like OpenAI’s ChatGPT. Reports revealed that <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/">OpenAI outsourced data annotation tasks to workers in Kenya</a>, employing them to moderate content and identify harmful or inappropriate material that the model might generate. This involved reviewing and labeling distressing content, such as graphic violence and explicit material, to train the AI in recognizing and avoiding such outputs. While this approach enabled OpenAI to improve the safety and utility of ChatGPT, significant ethical concerns arose around the working conditions, the nature of the tasks, and the compensation provided to Kenyan workers.</p>
<p>Many of the contributors were reportedly paid as little as $1.32 per hour for reviewing and labeling highly traumatic material. The emotional toll of such work, coupled with low wages, raised serious questions about the fairness and transparency of the crowdsourcing process. This controversy highlights a critical gap in ethical crowdsourcing practices. The workers, often from economically disadvantaged regions, were not adequately supported to cope with the psychological impact of their tasks. The lack of mental health resources and insufficient compensation underscored the power imbalances that can emerge when outsourcing data annotation tasks to lower-income regions.</p>
<p>Unfortunately, the challenges highlighted by the ChatGPT Kenya controversy are not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation face similar issues. As machine learning systems grow more complex and require larger datasets, the demand for annotated data will continue to increase. This shows the need for industry-wide standards and best practices to ensure ethical data sourcing. This case emphasizes the importance of considering the human labor behind AI systems. While crowdsourcing offers scalability and diversity, it also brings ethical responsibilities that cannot be overlooked. Organizations must prioritize the well-being and fair treatment of contributors as they build the datasets that drive AI innovation.</p>
<p>Beyond these ethical considerations, when dealing with specialized applications like mobile ML, edge ML, or cloud ML, additional challenges may arise. These applications often require data collected from specific environments or devices, which can be difficult to gather through general crowdsourcing platforms. For example, data for mobile applications utilizing smartphone sensors may necessitate participants with specific hardware features or software versions. Similarly, edge ML systems deployed in industrial settings may require data involving proprietary processes or secure environments, introducing privacy and accessibility challenges.</p>
<p>To address these diverse challenges, hybrid approaches that combine crowdsourcing with other data collection methods can provide solutions. Organizations may engage specialized communities, partner with relevant stakeholders, or create targeted initiatives to collect domain-specific data. Synthetic data generation, as discussed in the next section, can augment real-world data when crowdsourcing falls short.</p>
</section>
<section id="sec-data-engineering-anonymization-techniques-b90b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-anonymization-techniques-b90b">Anonymization Techniques</h3>
<p>As privacy concerns become increasingly important in crowdsourced and web-scraped data collection, anonymization emerges as a critical capability. From a systems engineering perspective, anonymization represents more than regulatory compliance; it constitutes a core design constraint that affects data pipeline architecture, storage strategies, and processing efficiency. ML systems must handle sensitive data throughout their lifecycle: during collection, storage, transformation, model training, and even in error logs and debugging outputs. A single privacy breach can compromise not just individual records but entire datasets, making the system unusable for future development. This systemic risk means privacy protection must be built into the data engineering pipeline from the ground up, not retrofitted later.</p>
<p>These systemic risks demand careful consideration of the systems implications: anonymized data often requires different storage schemas, impacts join operations across tables, affects caching strategies, and changes how errors can be logged and debugged. Feature engineering becomes more complex when direct identifiers cannot be used. Model serving must handle anonymized inputs consistently with training data. These technical constraints fundamentally shape how ML systems are architected and operated.</p>
<p>Given these complex technical constraints, practitioners have developed a range of anonymization techniques to mitigate these risks. These methods transform datasets such that individual identities and sensitive attributes become difficult or nearly impossible to re-identify, while preserving, to varying extents, the overall utility of the data for analysis and the operational requirements of the ML system.</p>
<p>The most straightforward approach, masking, involves altering or obfuscating sensitive values so that they cannot be directly traced back to the original data subject. For instance, digits in financial account numbers or credit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> to protect sensitive information during display or logging.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Cryptographic Hashing</strong>: One-way mathematical functions like SHA-256 that transform input data into fixed-length strings. Critical for data anonymization because they’re computationally infeasible to reverse: even small input changes produce dramatically different outputs, ensuring original data cannot be recovered from the hash. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g.&nbsp;relationships between data points).</p></div></div><p>Building on this direct protection approach, generalization reduces the precision or granularity of data to decrease the likelihood of re-identification. Instead of revealing an exact date of birth or address, the data is aggregated into broader categories (e.g., age ranges, zip code prefixes). For example, a user’s exact age of 37 might be generalized to an age range of 30-39, while their exact address might be bucketed into a city level granularity. This technique clearly reduces the risk of identifying an individual by sharing data in aggregated form; however, we might consequently lose analytical prediction. If granularity is not chosen correctly, individuals may still be able to be identified under certain conditions.</p>
<p>While generalization reduces data precision, pseudonymization<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> takes a different approach, replacing direct identifiers (like names, Social Security numbers, or email addresses) with artificial identifiers, or “pseudonyms.” These pseudonyms must not reveal, or be easily traceable to, the original data subject.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Pseudonymization under GDPR</strong>: GDPR Article 4(5) formally defines pseudonymization as a privacy-enhancing technique. Unlike anonymization, pseudonymized data remains personal data under EU law, requiring continued protection but enabling reduced regulatory restrictions for research and analytics purposes. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the “key” linking the pseudonym to the real identifier is compromised, re-identification becomes possible.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>k-Anonymity</strong>: Introduced by Latanya Sweeney in 2002, k-anonymity ensures each record is identical to at least k-1 others on quasi-identifiers. Despite being groundbreaking for privacy research, it has critical limitations: l-diversity (2007) and t-closeness (2007) were developed to address homogeneity and background knowledge attacks that k-anonymity cannot prevent.</p></div></div><p>Moving beyond simple identifier replacement, <span class="math inline">\(k\)</span>-anonymity<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> provides a more formal approach, ensuring that each record in a dataset is indistinguishable from at least <span class="math inline">\(𝑘−1\)</span> other records. This is achieved by suppressing or generalizing quasi-identifiers, or attributes that, in combination, could be used to re-identify an individual (e.g., zip code, age, gender). For example, if <span class="math inline">\(k=5\)</span>, every record in the dataset must share the same combination of quasi-identifiers with at least four other records. Thus, an attacker cannot pinpoint a single individual simply by looking at these attributes. This approach provides a formal privacy guarantee that helps reduce chances of individual re-identification. However, it is extremely high touch and may require a significant level of data distortion and does not protect against things like <a href="https://en.wikipedia.org/wiki/K-anonymity#Attacks">homogeneity or background knowledge attacks</a>.</p>
<p>At the most sophisticated end of this spectrum, differential privacy (DP) adds carefully <a href="https://digitalprivacy.ieee.org/publications/topics/what-is-differential-privacy#:~:text=At%20its%20roots%2C%20differential%20privacy,a%20result%20of%20providing%20data.">calibrated “noise” or randomized data perturbations</a> to query results or datasets. The goal is to ensure that the inclusion or exclusion of any single individual’s data does not significantly affect the output, thereby concealing their presence. Introduced noise is controlled by the <span class="math inline">\(\epsilon\)</span> parameter in <span class="math inline">\(\epsilon\)</span>-Differential Privacy, balancing data utility and privacy guarantees. The clear advantages this approach provides are strong mathematical guarantees of privacy, and DP is widely used in academic and industrial settings (e.g., large-scale data analysis). However, the added noise can affect data accuracy and subsequent model performance; proper parameter tuning is crucial to ensure both privacy and usefulness.</p>
<p><a href="#tbl-anonymization-comparison" class="quarto-xref">Table&nbsp;1</a> summarizes the key characteristics of each anonymization approach to help practitioners select appropriate techniques based on their specific privacy requirements and data utility needs.</p>
<div id="tbl-anonymization-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-anonymization-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Anonymization Techniques Comparison
</figcaption>
<div aria-describedby="tbl-anonymization-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 13%">
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Technique</th>
<th>Data Utility</th>
<th>Privacy Level</th>
<th>Implementation</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Masking</td>
<td>High</td>
<td>Low-Medium</td>
<td>Simple</td>
<td>Displaying sensitive data</td>
</tr>
<tr class="even">
<td>Generalization</td>
<td>Medium</td>
<td>Medium</td>
<td>Moderate</td>
<td>Age ranges, location bucketing</td>
</tr>
<tr class="odd">
<td>Pseudonymization</td>
<td>High</td>
<td>Medium</td>
<td>Moderate</td>
<td>Individual tracking needed</td>
</tr>
<tr class="even">
<td>K-anonymity</td>
<td>Low-Medium</td>
<td>High</td>
<td>Complex</td>
<td>Formal privacy guarantees</td>
</tr>
<tr class="odd">
<td>Differential Privacy</td>
<td>Medium</td>
<td>Very High</td>
<td>Complex</td>
<td>Statistical guarantees</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As the comparison table illustrates, effective data anonymization is a balancing act between privacy and utility. Techniques such as masking, generalization, pseudonymization, k-anonymity, and differential privacy each target different aspects of re-identification risk. By carefully selecting and combining these methods, organizations can responsibly derive value from sensitive datasets while respecting the privacy rights and expectations of the individuals represented within them.</p>
</section>
<section id="sec-data-engineering-synthetic-data-creation-f53d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-synthetic-data-creation-f53d">Synthetic Data Creation</h3>
<p>When privacy concerns make real data collection challenging, synthetic data generation has emerged as a powerful tool for addressing limitations in data collection, particularly in machine learning applications where real-world data is scarce, expensive, or ethically challenging to obtain. This approach involves creating artificial data using algorithms, simulations, or generative models to mimic real-world datasets. The generated data can be used to supplement or replace real-world data, expanding the possibilities for training robust and accurate machine learning systems. This democratization of data access becomes particularly valuable for the social impact applications discussed in <strong><a href="../core/ai_for_good/ai_for_good.html#sec-ai-good">Chapter 18: AI for Good</a></strong>, where synthetic data can enable ML development for underserved populations and sensitive domains without compromising privacy. The theoretical foundations for how diverse synthetic data improves model performance are explored in <strong><a href="../core/frontiers/frontiers.html#sec-agi-systems">Chapter 21: AGI Systems</a></strong>. <a href="#fig-synthetic-data" class="quarto-xref">Figure&nbsp;7</a> illustrates the process of combining synthetic data with historical datasets to create larger, more diverse training sets.</p>
<div id="fig-synthetic-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9bbc195613b2889c8434fe22b8cc694ba65e2f91.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Synthetic Data Augmentation: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: anylogic."><img src="data_engineering_files/mediabag/9bbc195613b2889c8434fe22b8cc694ba65e2f91.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Synthetic Data Augmentation</strong>: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: <a href="HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/">anylogic</a>.
</figcaption>
</figure>
</div>
<p>Building on this foundation, advancements in generative modeling techniques have greatly enhanced the quality of synthetic data. Modern AI systems can produce data that closely resembles real-world distributions, making it suitable for applications ranging from computer vision to natural language processing. For example, generative models have been used to create synthetic images for object recognition tasks, producing diverse datasets that closely match real-world images. Similarly, synthetic data has been leveraged to simulate speech patterns, enhancing the robustness of voice recognition systems.</p>
<p>Beyond these quality improvements, synthetic data has become particularly valuable in domains where obtaining real-world data is either impractical or costly. The automotive industry has embraced synthetic data to train autonomous vehicle systems; there are only so many cars you can physically crash to get crash-test data that might help an ML system know how to avoid crashes in the first place. Capturing real-world scenarios, especially rare edge cases such as near-accidents or unusual road conditions, is inherently difficult. Synthetic data allows researchers to <a href="https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/">simulate these scenarios in a controlled virtual environment</a>, ensuring that models are trained to handle a wide range of conditions. This approach has proven invaluable for advancing the capabilities of self-driving cars.</p>
<p>Complementing these safety-critical applications, another important application of synthetic data lies in augmenting existing datasets. Introducing variations into datasets enhances model robustness by exposing the model to diverse conditions. For instance, in speech recognition, data augmentation techniques like SpecAugment <span class="citation" data-cites="park2019specaugment">(<a href="#ref-park2019specaugment" role="doc-biblioref">Park et al. 2019</a>)</span> introduce noise, shifts, or pitch variations, enabling models to generalize better across different environments and speaker styles. This principle extends to other domains as well, where synthetic data can fill gaps in underrepresented scenarios or edge cases.</p>
<div class="no-row-height column-margin column-container"><div id="ref-park2019specaugment" class="csl-entry" role="listitem">
Park, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. <span>“SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.”</span> <em>arXiv Preprint arXiv:1904.08779</em>, April. <a href="http://arxiv.org/abs/1904.08779v3">http://arxiv.org/abs/1904.08779v3</a>.
</div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Privacy Regulation Timeline</strong>: GDPR (2016, effective 2018) imposed maximum fines of €20 million or 4% of annual global turnover (whichever is higher) for violations, followed by California’s CCPA (2018), and now dozens of similar laws globally. These regulations significantly transformed how ML systems handle personal data, making privacy-by-design essential for any AI system.</p></div></div><p>Beyond dataset augmentation capabilities, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as <a href="https://gdpr.eu/">GDPR</a> and <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html">HIPAA</a><a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy.</p>
<p>Despite these compelling advantages, synthetic data generation faces important limitations. Poorly generated data can misrepresent underlying real-world distributions, introducing biases or inaccuracies that degrade model performance. Validating synthetic data against real-world benchmarks is essential to ensure its reliability. Models trained primarily on synthetic data must be rigorously tested in real-world scenarios to confirm their ability to generalize effectively. Another challenge is the potential amplification of biases present in the original datasets used to inform synthetic data generation. If these biases are not carefully addressed, they may be inadvertently reinforced in the resulting models. A critical consideration is maintaining proper balance between synthetic and real-world data during training - if models are overly trained on synthetic data, their outputs may become nonsensical and model performance may collapse.</p>
<p>Despite these challenges, synthetic data has revolutionized the way machine learning systems are trained, providing flexibility, diversity, and scalability in data preparation. However, as its adoption grows, practitioners must remain vigilant about its limitations and ethical implications. By combining synthetic data with rigorous validation and thoughtful application, machine learning researchers and engineers can unlock its full potential while ensuring reliability and fairness in their systems.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-1222" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-1222">Continuing the KWS Example</h3>
<p>Bringing together all these data acquisition strategies, our KWS case study demonstrates how different data collection approaches combine effectively, each with distinct trade-offs:</p>
<p>To establish the foundational data layer, pre-existing datasets like Google’s Speech Commands <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span> provide a foundation for initial development, offering carefully curated voice samples for common wake words. However, these datasets often lack diversity in accents, environments, and languages, necessitating additional data collection strategies.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.”</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="http://arxiv.org/abs/1804.03209v1">http://arxiv.org/abs/1804.03209v1</a>.
</div></div><p>To address these coverage gaps, web scraping can supplement these baseline datasets by gathering diverse voice samples from video platforms, podcast repositories, and speech databases. This helps capture natural speech patterns and wake word variations, though careful attention must be paid to audio quality and privacy considerations when scraping voice data.</p>
<p>For targeted data collection needs, crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Mechanical Turk Origins</strong>: Named after the 18th-century chess-playing “automaton” (actually a human chess master hidden inside), Amazon’s MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale, ironically reversing the original Turk’s deception of AI capabilities.</p></div><div id="ref-werchniak2021exploring" class="csl-entry" role="listitem">
Werchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha Droppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. <span>“Exploring the Application of Synthetic Audio in Training Keyword Spotters.”</span> In <em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 7993–96. IEEE; IEEE. <a href="https://doi.org/10.1109/icassp39728.2021.9413448">https://doi.org/10.1109/icassp39728.2021.9413448</a>.
</div></div><p>Finally, synthetic data generation helps fill remaining gaps by creating unlimited variations of wake word utterances. Using speech synthesis <span class="citation" data-cites="werchniak2021exploring">(<a href="#ref-werchniak2021exploring" role="doc-biblioref">Werchniak et al. 2021</a>)</span> and audio augmentation techniques, developers can generate training data that captures different acoustic environments (busy streets, quiet rooms, moving vehicles), speaker characteristics (age, accent, gender), and background noise conditions. This synthetic data approach becomes particularly valuable when developing efficient models discussed in <strong><a href="../core/efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 10: Efficient AI</a></strong>, where diverse training data can enable better model performance.</p>
<p>This comprehensive, multi-faceted approach to data collection enables the development of KWS systems that perform robustly across diverse real-world conditions. The combination of methods helps address the unique challenges of wake word detection, from handling various accents and background noise to maintaining consistent performance across different devices and environments. The theoretical foundations for how diverse, high-quality training data improves model performance are detailed in <strong><a href="../core/dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong> and <strong><a href="../core/frontiers/frontiers.html#sec-agi-systems">Chapter 21: AGI Systems</a></strong>.</p>
<div id="quiz-question-sec-data-engineering-data-sources-c8d9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary advantage of using existing datasets like ImageNet for training ML systems?</p>
<ol type="a">
<li>They are always perfectly labeled and error-free.</li>
<li>They provide immediate access to large volumes of data with established benchmarks.</li>
<li>They guarantee the elimination of biases in training data.</li>
<li>They are always cheaper than collecting new data.</li>
</ol></li>
<li><p>True or False: Web scraping is a universally accepted method for gathering training data without any legal or ethical concerns.</p></li>
<li><p>Explain how the use of crowdsourcing for data collection can impact the quality and diversity of datasets in ML systems.</p></li>
<li><p>What is a potential drawback of relying heavily on synthetic data for training ML models?</p>
<ol type="a">
<li>Synthetic data can introduce biases if not properly validated against real-world benchmarks.</li>
<li>Synthetic data is always more accurate than real-world data.</li>
<li>Synthetic data is always easier to generate than real-world data.</li>
<li>Synthetic data eliminates the need for any real-world data.</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-sources-c8d9" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-ingestion-5dfc" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-ingestion-5dfc">Data Ingestion</h2>
<p>Once our data acquisition strategy is established, the collected data must be reliably and efficiently ingested into our ML systems through well-designed data pipelines. This transformation presents several challenges that ML engineers must address.</p>
<section id="sec-data-engineering-ingestion-patterns-b977" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-ingestion-patterns-b977">Ingestion Patterns</h3>
<p>To address these ingestion challenges systematically, ML systems typically follow two primary patterns: batch ingestion and stream ingestion. Each pattern has distinct characteristics and use cases that students should understand to design effective ML systems.</p>
<p>Batch ingestion<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It’s also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning <span class="citation" data-cites="akidau2015dataflow">(<a href="#ref-akidau2015dataflow" role="doc-biblioref">Akidau et al. 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Batch Processing Evolution</strong>: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google’s MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made “big data” analytics economically feasible for the first time.</p></div><div id="ref-akidau2015dataflow" class="csl-entry" role="listitem">
Akidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. <span>“The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.”</span> <em>Proceedings of the VLDB Endowment</em> 8 (12): 1792–1803. <a href="https://doi.org/10.14778/2824032.2824076">https://doi.org/10.14778/2824032.2824076</a>.
</div></div><p>In contrast to this scheduled approach, stream ingestion processes data in real-time as it arrives. This pattern is crucial for applications requiring immediate data processing, scenarios where data loses value quickly, and systems that need to respond to events as they occur. A financial institution, for instance, might use stream ingestion for real-time fraud detection, processing each transaction as it occurs to flag suspicious activity immediately <span class="citation" data-cites="kleppmann2017designing">(<a href="#ref-kleppmann2017designing" role="doc-biblioref">Kleppmann 2016</a>)</span>. However, stream processing must handle backpressure when downstream systems cannot keep pace by implementing buffer limits, sampling strategies, or graceful degradation to prevent system overload. Data freshness Service Level Agreements (SLAs) formalize these requirements, specifying maximum acceptable delays between data generation and availability for processing.</p>
<p>Recognizing the limitations of either approach alone, many modern ML systems employ a hybrid approach, combining both batch and stream ingestion to handle different data velocities and use cases. This flexibility allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape. Production systems must balance cost versus latency trade-offs: real-time processing can cost 10-100x more than batch processing. This cost differential arises from several factors: streaming systems require always-on infrastructure rather than schedulable resources, maintain redundant processing for fault tolerance, need low-latency networking and storage, and cannot benefit from the economies of scale that batch processing achieves by amortizing startup costs across large data volumes. A batch job processing 1TB might use 100 machines for 10 minutes, while a streaming system processing the same data over 24 hours needs dedicated resources continuously available. Techniques for managing streaming systems at scale, including backpressure handling and cost optimization, are detailed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
</section>
<section id="sec-data-engineering-etl-elt-comparison-bbb7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-etl-elt-comparison-bbb7">ETL and ELT Comparison</h3>
<p>Beyond choosing ingestion patterns, designing effective data ingestion pipelines requires understanding the differences between Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) approaches, as illustrated in <a href="#fig-etl-vs-elt" class="quarto-xref">Figure&nbsp;8</a>. These paradigms determine when data transformations occur relative to the loading phase, significantly impacting the flexibility and efficiency of your ML pipeline.</p>
<div id="fig-etl-vs-elt" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-etl-vs-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="ecff59fc5f0797ded4b8d4637d9036cc3e85ac9f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Data Pipeline Architectures: ETL pipelines transform data before loading it into a data warehouse, while ELT pipelines load raw data first and transform it within the warehouse, impacting system flexibility and resource allocation for machine learning workflows. Choosing between ETL and ELT depends on data volume, transformation complexity, and the capabilities of the target data storage system."><img src="data_engineering_files/mediabag/ecff59fc5f0797ded4b8d4637d9036cc3e85ac9f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-etl-vs-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Data Pipeline Architectures</strong>: ETL pipelines transform data <em>before</em> loading it into a data warehouse, while ELT pipelines load raw data first and transform it within the warehouse, impacting system flexibility and resource allocation for machine learning workflows. Choosing between ETL and ELT depends on data volume, transformation complexity, and the capabilities of the target data storage system.
</figcaption>
</figure>
</div>
<p>ETL<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training <span class="citation" data-cites="inmon2005building">(<a href="#ref-inmon2005building" role="doc-biblioref">Inmon 2005</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>ETL Evolution</strong>: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark’s in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches.</p></div><div id="ref-inmon2005building" class="csl-entry" role="listitem">
Inmon, W. H. 2005. <em>Building the Data Warehouse</em>. John Wiley Sons.
</div></div><p>However, ETL can be less flexible when schemas or requirements change frequently, a common occurrence in evolving ML projects. This is where the ELT approach comes into play. ELT reverses the order by first loading raw data and then applying transformations as needed. This method is often seen in modern data lake or schema-on-read environments, allowing for a more agile approach when addressing evolving analytical needs in ML systems.</p>
<p>By deferring transformations, ELT can accommodate varying uses of the same dataset, which is particularly useful in exploratory data analysis phases of ML projects or when multiple models with different data requirements are being developed simultaneously. However, ELT places greater demands on storage systems and query engines, which must handle large amounts of unprocessed information.</p>
<p>In practice, many ML systems employ a hybrid approach, selecting ETL or ELT on a case-by-case basis depending on the specific requirements of each data source or ML model. For example, a system might use ETL for structured data from relational databases where schemas are well-defined and stable, while employing ELT for unstructured data like text or images where transformation requirements may evolve as the ML models are refined.</p>
<p>When implementing streaming components within ETL/ELT architectures, distributed systems principles become critical. The CAP theorem<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> fundamentally constrains streaming system design choices. Apache Kafka prioritizes consistency and partition tolerance, making it ideal for reliable event ordering but potentially experiencing availability issues during network partitions. Apache Pulsar emphasizes availability and partition tolerance, providing better fault tolerance but with relaxed consistency guarantees. Amazon Kinesis balances all three properties through careful configuration but requires understanding these trade-offs for proper deployment. These choices directly impact ML pipeline reliability: a fraud detection system might choose Kafka’s strong consistency to ensure transaction ordering, while a recommendation system might prefer Pulsar’s availability to maintain service during infrastructure failures.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>CAP Theorem</strong>: Formulated by Eric Brewer in 2000, states that distributed systems can guarantee at most two of three properties: Consistency (all nodes see the same data simultaneously), Availability (system remains operational), and Partition tolerance (system continues despite network failures). This fundamental constraint shapes all distributed ML system design decisions, from database choices to streaming architectures.</p></div></div></section>
<section id="sec-data-engineering-data-source-integration-53f9" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-source-integration-53f9">Data Source Integration</h3>
<p>Regardless of whether ETL or ELT approaches are used, integrating diverse data sources is a key challenge in data ingestion for ML systems. Data may come from various origins, including databases, APIs, file systems, and IoT devices. Each source may have its own data format, access protocol, and update frequency.</p>
<p>Given this source diversity, ML engineers must develop robust connectors or adapters for each data source to effectively integrate these sources. These connectors handle the specifics of data extraction, including authentication, rate limiting, and error handling. For example, when integrating with a REST API, the connector would manage API keys, respect rate limits, and handle HTTP status codes appropriately.</p>
<p>Beyond basic connectivity, source integration often involves data transformation at the ingestion point. This might include parsing JSON or XML responses, converting timestamps to a standard format, or performing basic data cleaning operations. The goal is to standardize the data format as it enters the ML pipeline, simplifying downstream processing.</p>
<p>In addition to data format standardization, it’s essential to consider the reliability and availability of data sources. Some sources may experience downtime or have inconsistent data quality. Implementing retry mechanisms, data quality checks, and fallback procedures can help ensure a steady flow of reliable data into the ML system.</p>
</section>
<section id="sec-data-engineering-validation-techniques-5b83" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-validation-techniques-5b83">Validation Techniques</h3>
<p>Once data sources are integrated, validation becomes an important step in the ingestion process, ensuring that incoming data meets quality standards and conforms to expected schemas. This step helps prevent downstream issues in ML pipelines caused by data anomalies or inconsistencies.</p>
<p>To achieve these quality objectives, validation at the ingestion stage typically encompasses several key aspects. Schema conformity checks ensure that incoming data adheres to the expected structure, including data types and field names. However, production systems face the reality of schema evolution<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>: fields get renamed (e.g., ‘user_id’ becoming ‘userId’), types change (integers becoming floats), and new fields appear while old ones deprecate. Managing these changes requires versioning strategies where schemas coexist during transition periods, with validation logic that can handle both old and new formats gracefully. Backward compatibility must be maintained to process historical data, while forward compatibility prepares for upcoming changes.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Schema Evolution</strong>: The process of adapting data schemas over time as business requirements change. Critical in ML systems because breaking changes can cause pipeline failures. Modern solutions include schema registries (Confluent Schema Registry), backward/forward compatibility rules, and versioning strategies. Production incidents increase 27% for each percentage point increase in schema drift.</p></div></div><p>Building on these fundamental checks, <strong>Statistical Data Validation</strong> goes beyond basic schema checks to detect subtle data quality degradation. Distribution comparison validates that incoming data maintains statistical properties consistent with training data, such as detecting when user age distributions shift unexpectedly or when categorical feature frequencies change dramatically. Outlier detection using statistical methods like z-score analysis or isolation forests can identify anomalous values that might indicate upstream processing errors or data corruption. For high-volume streams, statistical validation often relies on sampling strategies to balance validation thoroughness with system performance, typically validating 1-10% of records while maintaining detection sensitivity.</p>
<p>Extending these statistical approaches, <strong>Cross-validation between Training and Serving Data</strong> represents a critical production concern where feature distributions in real-time serving data gradually diverge from training distributions. This drift detection requires continuous monitoring of feature statistics, automated alerts when distributions exceed defined thresholds, and decision frameworks for when models require retraining. Modern validation systems implement automated schema evolution detection that flags when new fields appear, existing fields change types, or required fields become optional, enabling graceful handling of upstream changes without pipeline failures.</p>
<p>While these validation approaches provide comprehensive coverage, <strong>Performance Implications of Validation at Scale</strong> require careful trade-off analysis between validation thoroughness and system throughput. Full validation of every record may introduce unacceptable latency for real-time systems, while sampling-based validation reduces overhead but may miss edge cases. Production systems often implement tiered validation where critical fields receive full validation while optional features use statistical sampling. Asynchronous validation patterns enable immediate data processing while background validation detects issues for future remediation.</p>
<p>For example, in a healthcare ML system ingesting patient data, validation might include checking that age values are positive integers, diagnosis codes are from a predefined set, and admission dates are not in the future. Statistical validation would monitor whether the distribution of patient ages remains consistent with training data and alert if emergency department visits spike unexpectedly. This comprehensive approach enables early detection of data quality issues before they impact model accuracy. Comprehensive schema evolution and production version management are covered in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong> and <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
</section>
<section id="sec-data-engineering-error-management-e2e9" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-error-management-e2e9">Error Management</h3>
<p>Complementing validation techniques, effective error handling in data ingestion enables building resilient ML systems. Errors can occur at various points in the ingestion process, from source connection issues to data validation failures. Effective error handling strategies ensure that the ML pipeline can continue to operate even when faced with data ingestion challenges.</p>
<p>Central to resilient system design, a key concept in error handling is graceful degradation. This involves designing systems to continue functioning, possibly with reduced capabilities, when faced with partial data loss or temporary source unavailability. Production systems must handle partial failures where some data sources succeed while others fail, such as when real-time streams continue but batch updates lag, or when certain geographic regions experience outages while others operate normally. Late-arriving data presents another common challenge, requiring systems to decide whether to wait for completeness or proceed with available data based on freshness requirements.</p>
<p>To handle these various failure scenarios, implementing intelligent retry logic for transient errors, such as network interruptions or temporary service outages, requires exponential backoff<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> strategies to avoid overwhelming recovering services. Many ML systems employ the concept of dead letter queues<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>, using separate storage for data that fails processing. This allows for later analysis and potential reprocessing of problematic data <span class="citation" data-cites="kleppmann2017designing">(<a href="#ref-kleppmann2017designing" role="doc-biblioref">Kleppmann 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Exponential Backoff</strong>: Reattempt strategy where wait time doubles after each failure (1s, 2s, 4s, 8s, etc.) plus random jitter to prevent thundering herd problems. Originally developed for Ethernet collision detection (1980s), now essential for ML systems where thousands of workers might simultaneously reattempt failed operations, potentially overwhelming recovering services.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Dead Letter Queue</strong>: Storage mechanism for messages that cannot be processed successfully after multiple attempts. Coined in postal services for undeliverable mail, the pattern became critical in distributed ML systems for preserving failed training examples or corrupted feature computations for later analysis, enabling debugging without data loss.</p></div><div id="ref-kleppmann2017designing" class="csl-entry" role="listitem">
Kleppmann, Martin. 2016. <em>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems</em>. O’Reilly Media. <a href="http://shop.oreilly.com/product/0636920032175.do">http://shop.oreilly.com/product/0636920032175.do</a>.
</div></div><p>Moving beyond ad-hoc error handling, systematic failure mode analysis for ML data pipelines reveals predictable patterns that require specific engineering countermeasures. Data corruption failures occur when upstream systems introduce subtle format changes, encoding issues, or field value modifications that pass basic validation but corrupt model inputs. Schema evolution failures happen when source systems add fields, rename columns, or change data types without coordination, breaking downstream processing assumptions. Resource exhaustion manifests as gradually degrading performance when data volume growth outpaces capacity planning, eventually causing pipeline failures during peak load periods.</p>
<p>Building on this failure analysis, cascade failure prevention requires circuit breaker<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> patterns and bulkhead isolation to prevent single component failures from propagating throughout the system. When a feature computation service fails, the system should isolate the failure to prevent it from affecting other feature computations or downstream model serving. Checkpoint-restart mechanisms enable recovery from the last successful processing state rather than full pipeline recomputation, critical for long-running data processing jobs operating on terabyte-scale datasets.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Circuit Breaker Pattern</strong>: Named after electrical circuit breakers, this pattern automatically stops calling a failing service to prevent cascade failures. Popularized by Martin Fowler (2007) and Netflix’s Hystrix library, it’s critical in ML systems where feature computation failures could bring down entire recommendation pipelines serving millions of users. Has three states: Closed (normal operation), Open (calls blocked), Half-Open (testing recovery).</p></div></div><p>To complete the resilience picture, automated recovery engineering implements sophisticated strategies beyond simple retry logic. Progressive timeout increases prevent overwhelming struggling services while maintaining rapid recovery for transient issues. Multi-tier fallback systems provide degraded service when primary data sources fail—such as serving slightly stale cached features when real-time computation fails, or using approximate features when exact computation times out. Comprehensive alerting and escalation procedures ensure human intervention occurs when automated recovery fails, with sufficient diagnostic information to enable rapid debugging. Production patterns for handling distributed failures, circuit breakers, and comprehensive recovery strategies are detailed in <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
<p>These concepts become concrete when we consider, for instance, a financial ML system ingesting market data, where error handling might involve falling back to slightly delayed data sources if real-time feeds fail, while simultaneously alerting the operations team to the issue. This approach ensures that the system continues to function and that responsible parties are aware of and can address the problem.</p>
<p>This comprehensive approach to error management ensures that downstream processes have access to reliable, high-quality data for training and inference tasks, even in the face of ingestion challenges. Understanding these concepts of data validation and error handling is essential for students and practitioners aiming to build robust, production-ready ML systems.</p>
<p>With robust error handling in place, once ingestion is complete and data is validated, it is typically loaded into a storage environment suited to the organization’s analytical or machine learning needs. Some datasets flow into data warehouses for structured queries, whereas others are retained in data lakes for exploratory or large-scale analyses. Advanced systems may also employ feature stores to provide standardized features for machine learning.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-698c" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-698c">Continuing the KWS Example</h3>
<p>Bringing these ingestion concepts together, our KWS example demonstrates how production systems employ both streaming and batch ingestion patterns. The streaming pattern handles real-time audio data from active devices, where wake words must be detected with minimal latency. This requires careful implementation of pub/sub mechanisms—such as using Apache Kafka-like streams to buffer incoming audio data and enable parallel processing across multiple inference servers.</p>
<p>Parallel to this real-time processing, the system processes batch data for model training and updates. This includes ingesting new wake word recordings from crowdsourcing efforts, synthetic data from voice generation systems, and validated user interactions. The batch processing typically follows an ETL pattern, where audio data is preprocessed (normalized, filtered, segmented) before being stored in a format optimized for model training.</p>
<p>Beyond basic ingestion patterns, KWS systems must integrate data from diverse sources, such as real-time audio streams from deployed devices, crowdsourced recordings from data collection platforms etc. Each source presents unique challenges. Real-time audio streams require rate limiting to prevent system overload during usage spikes. Crowdsourced data needs systematic validation to ensure recording quality and correct labeling. Synthetic data must be verified for realistic representation of wake word variations.</p>
<p>Given these diverse data requirements, KWS systems employ sophisticated error handling mechanisms due to the nature of voice interaction. When processing real-time audio, dead letter queues store failed recognition attempts for analysis, helping identify patterns in false negatives or system failures. Data validation becomes important for maintaining wake word detection accuracy—incoming audio must be checked for quality issues such as clipping, noise levels, and appropriate sampling rates.</p>
<p>To illustrate these validation requirements, consider a smart home device processing the wake word “Alexa.” The ingestion pipeline must validate:</p>
<ul>
<li>Audio quality metrics (signal-to-noise ratio, sample rate, bit depth)</li>
<li>Recording duration (typically 1-2 seconds for wake words)</li>
<li>Background noise levels</li>
<li>Speaker proximity indicators</li>
</ul>
<p>Invalid samples are routed to dead letter queues for analysis, while valid samples are processed in real-time for wake word detection.</p>
<p>This comprehensive KWS case study illustrates how real-world ML systems must carefully balance different ingestion patterns, handle multiple data sources, and maintain robust error handling—while meeting strict latency and reliability requirements. The lessons from KWS systems apply broadly to other ML applications requiring real-time processing capabilities alongside continuous model improvement.</p>
<p>With reliable data ingestion established, we now focus on systematic data transformation that implements our four-pillar framework at the processing level. Data processing decisions must maintain quality standards while ensuring reliability, supporting scalability requirements, and adhering to governance principles throughout the transformation pipeline.</p>
<div id="quiz-question-sec-data-engineering-data-ingestion-5dfc" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the primary difference between batch ingestion and stream ingestion in ML systems?</p>
<ol type="a">
<li>Batch ingestion processes data in real-time, while stream ingestion processes data in batches.</li>
<li>Batch ingestion is suitable for real-time applications, whereas stream ingestion is used for historical data.</li>
<li>Batch ingestion requires more computational resources than stream ingestion.</li>
<li>Batch ingestion processes data at scheduled intervals, while stream ingestion processes data as it arrives.</li>
</ol></li>
<li><p>Explain how the choice between ETL and ELT approaches can impact the flexibility and efficiency of an ML pipeline.</p></li>
<li><p>True or False: In a hybrid data ingestion system, combining both batch and stream ingestion patterns is beneficial for handling different data velocities and use cases.</p></li>
<li><p>In a production ML system, what error management strategies would you implement to ensure reliable data ingestion?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-ingestion-5dfc" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-processing-c336" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-processing-c336">Systematic Data Processing</h2>
<p>With reliable data ingestion established, data processing represents the systematic application of our quality pillar, transforming raw data into ML-ready formats while maintaining reliability and scalability standards. This stage implements the data quality requirements defined in our problem definition phase, ensuring that every transformation preserves data integrity while improving model readiness.</p>
<p>For our KWS system, processing decisions directly impact all four pillars: quality transformations must preserve acoustic characteristics essential for wake word detection; reliability requires consistent processing despite varying audio formats; scalability demands efficient algorithms that handle millions of audio streams; and governance ensures privacy-preserving transformations that protect user voice data.</p>
<p>Data processing implementation follows the architectural patterns (ETL or ELT) established in <a href="#sec-data-engineering-pipeline-basics-31ba" class="quarto-xref">Section&nbsp;1.3</a> during pipeline design<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>. As detailed in the pipeline architecture section, the choice between ETL and ELT directly impacts when and where transformations occur, influencing resource allocation, system flexibility, and operational complexity.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>ETL vs ELT Performance</strong>: ETL processes 1-10GB/hour on traditional systems but scales poorly; ELT leverages cloud warehouses like Snowflake (100GB/hour+) and BigQuery (1TB/hour+) by utilizing distributed compute for transformations. This 10-100x performance difference drives modern data architecture decisions.</p></div></div><p>Regardless of architectural approach, the following data processing steps ensure that data becomes clean, relevant, and optimally formatted for machine learning algorithms. These transformations implement the quality requirements defined during problem definition while maintaining reliability, scalability, and governance standards across the processing pipeline.</p>
<section id="sec-data-engineering-cleaning-techniques-e81b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-cleaning-techniques-e81b">Cleaning Techniques</h3>
<p>At the foundation of data processing, data cleaning involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Raw data frequently contains issues such as missing values, duplicates, or outliers that can significantly impact model performance if left unaddressed.</p>
<p>To address these data quality issues, data cleaning might involve removing duplicate records, handling missing values through imputation or deletion, and correcting formatting inconsistencies. For instance, in a customer database, names might be inconsistently capitalized or formatted. A data cleaning process would standardize these entries, ensuring that “John Doe,” “john doe,” and “DOE, John” are all treated as the same entity.</p>
<p>Beyond these basic cleaning operations, ensuring consistency between training and serving pipelines represents the most critical challenge in data cleaning. Studies show that training-serving skew causes approximately 70% of production ML failures when cleaning logic differs between environments <span class="citation" data-cites="sculley2015hidden">(<a href="#ref-sculley2015hidden" role="doc-biblioref">Sculley et al. 2021</a>)</span>. Consider a simple example: normalizing transaction amounts during training by removing currency symbols and converting to floats, but forgetting to apply identical preprocessing during serving. This seemingly minor inconsistency can degrade model accuracy by 20-40%, as the model receives differently formatted inputs than it was trained on.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sculley2015hidden" class="csl-entry" role="listitem">
Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2021. <span>“Technical Debt in Machine Learning Systems.”</span> In <em>Technical Debt in Practice</em>, 177–92. The MIT Press. <a href="https://doi.org/10.7551/mitpress/12440.003.0011">https://doi.org/10.7551/mitpress/12440.003.0011</a>.
</div></div><p>Complementing consistency management, outlier detection and treatment is another important aspect of data cleaning. Outliers can sometimes represent valuable information about rare events, but they can also be the result of measurement errors or data corruption. ML practitioners must carefully consider the nature of their data and the requirements of their models when deciding how to handle outliers.</p>
</section>
<section id="sec-data-engineering-data-quality-assessment-b8b7" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-quality-assessment-b8b7">Data Quality Assessment</h3>
<p>Building on these cleaning techniques, quality assessment goes hand in hand with data cleaning, providing a systematic approach to evaluating the reliability and usefulness of data. This process involves examining various aspects of data quality, including accuracy, completeness, consistency, and timeliness. In production systems, data quality degrades in subtle ways that basic metrics miss: fields that never contain nulls suddenly show sparse patterns, numeric distributions drift from their training ranges, or categorical values appear that weren’t present during model development.</p>
<p>To address these subtle degradation patterns, production quality monitoring requires specific metrics beyond simple missing value counts. Critical indicators include null value patterns by feature (sudden increases suggest upstream failures), count anomalies (10x increases often indicate data duplication or pipeline errors), value range violations (prices becoming negative, ages exceeding realistic bounds), and join failure rates between data sources. Statistical drift detection becomes essential by monitoring means, variances, and quantiles of features over time to catch gradual degradation before it impacts model performance. For example, in an e-commerce recommendation system, the average user session length might gradually increase from 8 minutes to 12 minutes over six months due to improved site design, but a sudden drop to 3 minutes suggests a data collection bug. Similarly, if product price features trained on a range of $10-$500 suddenly show values of $50,000, this indicates either new luxury items (requiring model retraining) or data corruption (requiring investigation). Comprehensive production monitoring strategies and drift detection techniques are covered in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong> and <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
<p>Supporting these monitoring requirements, tools and techniques for quality assessment range from simple statistical measures to more complex machine learning-based approaches. Data profiling tools provide summary statistics and visualizations that help identify potential quality issues, while more advanced techniques employ unsupervised learning algorithms to detect anomalies or inconsistencies in large datasets. Establishing clear quality metrics and thresholds is essential for maintaining data quality over time, with regular assessments ensuring that data entering the ML pipeline meets the necessary standards for reliable model training and inference.</p>
</section>
<section id="sec-data-engineering-transformation-techniques-2d54" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-transformation-techniques-2d54">Transformation Techniques</h3>
<p>Once data quality is assured through cleaning and assessment, transformation converts the data from its raw form into a format more suitable for analysis and modeling. This process can include a wide range of operations, from simple conversions to complex mathematical transformations.</p>
<p>Central to effective transformation, common transformation tasks include normalization and standardization<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>, which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model’s predictions <span class="citation" data-cites="bishop2006pattern">(<a href="#ref-bishop2006pattern" role="doc-biblioref">Bishop 2006</a>)</span>. However, transformation consistency between training and serving becomes critical here: if training data is normalized using the global dataset mean and standard deviation, the serving pipeline must store and apply these exact same statistics. Computing normalization parameters on serving data batches leads to distribution shift and degraded model performance.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>Normalization in ML</strong>: Min-max scaling (range 0-1) vs z-score standardization (mean=0, std=1) can dramatically affect model performance. Gradient descent converges 10-100x faster on normalized features <span class="citation" data-cites="goodfellow2016deep">(<a href="#ref-goodfellow2016deep" role="doc-biblioref">Goodfellow, Courville, and Bengio 2013</a>)</span>, while tree-based models (Random Forest, XGBoost) are largely unaffected by scaling differences.</p><div id="ref-goodfellow2016deep" class="csl-entry" role="listitem">
Goodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013. <span>“Scaling up Spike-and-Slab Models for Unsupervised Feature Learning.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 35 (8): 1902–14. <a href="https://doi.org/10.1109/tpami.2012.273">https://doi.org/10.1109/tpami.2012.273</a>.
</div></div><div id="ref-bishop2006pattern" class="csl-entry" role="listitem">
Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
</div><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Categorical Encoding Impact</strong>: One-hot encoding can explode feature dimensions—a single categorical variable with 1000 categories creates 1000 binary features. High-cardinality encoding techniques like target encoding or embeddings (used in deep learning) can reduce dimensions by 10-100x while preserving predictive power.</p></div></div><p>Beyond numerical scaling, other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms. Training-serving consistency requires careful handling of categorical encodings—if the training dataset encounters categories A, B, and C, the serving pipeline must handle the same set, including unknown categories that weren’t present during training.</p>
</section>
<section id="sec-data-engineering-feature-engineering-ea20" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-feature-engineering-ea20">Feature Engineering</h3>
<p>Building on these foundational transformations, feature engineering is the process of using domain knowledge to create new features that make machine learning algorithms work more effectively. This step is often considered more of an art than a science, requiring creativity and deep understanding of both the data and the problem at hand.</p>
<p>To leverage domain expertise effectively, feature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> <span class="citation" data-cites="kuhn2013applied">(<a href="#ref-kuhn2013applied" role="doc-biblioref">Kuhn and Johnson 2013</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>RFM Analysis Origins</strong>: Developed by direct marketers in the 1960s, RFM (Recency, Frequency, Monetary) analysis segments customers by purchase behavior. Modern ML systems extend this to 100+ behavioral features, but the core RFM triplet remains among the most predictive features for customer lifetime value models.</p></div><div id="ref-kuhn2013applied" class="csl-entry" role="listitem">
Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4614-6849-3">https://doi.org/10.1007/978-1-4614-6849-3</a>.
</div></div><p>Given these creative possibilities, the importance of feature engineering cannot be overstated. Well-engineered features can often lead to significant improvements in model performance, sometimes outweighing the impact of algorithm selection or hyperparameter tuning.</p>
</section>
<section id="sec-data-engineering-processing-pipeline-design-48d4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-processing-pipeline-design-48d4">Processing Pipeline Design</h3>
<p>Integrating these cleaning, assessment, transformation, and feature engineering steps, processing pipelines bring together the various data processing steps into a coherent, reproducible workflow. These pipelines ensure that data is consistently prepared across training and inference stages, reducing the risk of data leakage and improving the reliability of ML systems.</p>
<p>To support these workflow requirements, modern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving. The choice of data processing framework must align with the broader ML framework ecosystem discussed in <strong><a href="../core/frameworks/frameworks.html#sec-ai-frameworks">Chapter 5: AI Frameworks</a></strong>, where framework-specific data loaders and preprocessing utilities can significantly impact development velocity and system performance.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Apache Beam Architecture</strong>: Google’s unified programming model (2016) abstracts batch and streaming data processing across multiple execution engines (Dataflow, Spark, Flink). Its key innovation: write once, run anywhere with automatic scaling from single machines to thousands of workers processing petabyte-scale data.</p></div></div><p>Beyond tool selection, effective pipeline design involves considerations such as modularity, scalability, and version control. Modular pipelines allow for easy updates and maintenance of individual processing steps. Version control for pipelines is crucial, ensuring that changes in data processing can be tracked and correlated with changes in model performance. This modular breakdown of pipeline components is well illustrated by TensorFlow Extended in <a href="#fig-tfx-pipeline-example" class="quarto-xref">Figure&nbsp;9</a>, which shows the complete flow from initial data ingestion through to final model deployment.</p>
<div id="fig-tfx-pipeline-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tfx-pipeline-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9daff432776d54e821a3a38d383ac78f17ea1f27.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Data Processing Pipeline: A modular end-to-end ML pipeline, as implemented in TensorFlow extended, highlighting key stages from raw data ingestion to trained model deployment and serving. this decomposition enables independent development, versioning, and scaling of each component, improving maintainability and reproducibility of ML systems."><img src="data_engineering_files/mediabag/9daff432776d54e821a3a38d383ac78f17ea1f27.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tfx-pipeline-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Data Processing Pipeline</strong>: A modular end-to-end ML pipeline, as implemented in TensorFlow extended, highlighting key stages from raw data ingestion to trained model deployment and serving. this decomposition enables independent development, versioning, and scaling of each component, improving maintainability and reproducibility of ML systems.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-scalability-considerations-1083" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-scalability-considerations-1083">Scalability Considerations</h3>
<p>While modular pipeline design enables maintainable systems, as datasets grow larger and ML systems become more complex, the scalability of data processing becomes increasingly important. Consider the data processing stages we’ve discussed—cleaning, quality assessment, transformation, and feature engineering. When these operations must handle terabytes of data, a single machine becomes insufficient. The cleaning techniques that work on gigabytes of data in memory must be redesigned to work across distributed systems.</p>
<p>These challenges become concrete in several ways. Quality assessment that scans for null patterns or value range violations must process data faster than it arrives. Feature engineering operations like normalization require computing statistics across the entire dataset before transforming individual records. Transformation pipelines that work sequentially on small datasets create bottlenecks when processing massive volumes.</p>
<p>To address these scaling bottlenecks, the solution lies in partitioning data across multiple computing resources, but introduces coordination challenges rooted in physical constraints. Distributed coordination is fundamentally limited by the fact that achieving consistency requires synchronization, which is bounded by network round-trip times. When local operations complete in microseconds but network coordination requires milliseconds, the 1000x latency difference creates unavoidable bottlenecks. This constraint explains why some operations are inherently harder to parallelize than others.</p>
<p>These physical constraints become evident when we consider computing global statistics for normalization when data is split across 100 machines. Each partition can compute local statistics quickly, but combining these into global statistics requires information from all partitions. This coordination becomes exponentially more complex as partition count increases and geographic distribution expands. Ensuring consistent quality checks when different machines see different data subsets requires sophisticated synchronization protocols that balance consistency guarantees with system performance.</p>
<p>Given these coordination complexities, data locality becomes critical at this scale. Moving 1TB of training data across the network takes 100+ seconds at 10GB/s, while local SSD access requires only 200 seconds at 5GB/s. This bandwidth hierarchy drives ML system design toward compute-follows-data architectures rather than data-follows-compute patterns. Geographic distribution amplifies these challenges significantly. Cross-datacenter coordination must handle network latency (often 50-200ms between regions), partial failures where some datacenters become unreachable, and regulatory constraints that prevent data from crossing national boundaries.</p>
<p>To address these bandwidth hierarchies, distributed frameworks like Apache Spark implement map-reduce style partitioning, but the fundamental challenge remains: operations requiring global coordination (like computing dataset statistics for normalization) create bottlenecks because they require information from all partitions. This is why techniques like approximate algorithms and hierarchical aggregation become essential at scale. Understanding which operations can be parallelized easily versus those that require expensive coordination determines system architecture and performance characteristics. When processing nodes can access local data at SSD speeds (1-7 GB/s) or RAM speeds (50-200 GB/s) but must coordinate over networks limited to 1-10 GB/s, the bandwidth mismatch creates fundamental coordination bottlenecks. Geographic distribution amplifies these challenges significantly. Cross-datacenter coordination must handle network latency (often 50-200ms between regions), partial failures where some datacenters become unreachable, and regulatory constraints that prevent data from crossing national boundaries. Consider a global ML system processing user interactions: European data must remain in EU datacenters under GDPR, while Chinese data requires local processing. These constraints force complex partitioning strategies and sophisticated coordination protocols to maintain data consistency while respecting sovereignty requirements. Understanding these trade-offs becomes essential for designing efficient data pipelines, particularly when implementing the comprehensive distributed architectures covered in <strong><a href="../core/workflow/workflow.html#sec-ai-workflow">Chapter 19: AI Workflow</a></strong> and the advanced partitioning strategies detailed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
<p>Beyond coordination challenges, another important consideration is the balance between preprocessing and on-the-fly computation. While extensive preprocessing can speed up model training and inference, it can also lead to increased storage requirements and potential data staleness. Production systems often implement hybrid approaches, preprocessing computationally expensive features while computing rapidly changing features on-the-fly. This balance depends on storage costs, computation resources, and freshness requirements specific to each use case.</p>
<p>Effective data processing forms the cornerstone of successful ML systems. By carefully cleaning, transforming, and engineering data, practitioners can significantly improve the performance and reliability of their models. As the field of machine learning continues to evolve, so too do the techniques and tools for data processing, making this an exciting and dynamic area of study and practice.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-8ed1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-8ed1">Continuing the KWS Example</h3>
<p>Applying these data processing concepts to our running example, the KWS system requires careful cleaning of audio recordings to ensure reliable wake word detection. Raw audio data often contains various imperfections—background noise, clipped signals, varying volumes, and inconsistent sampling rates. For example, when processing the wake word “Alexa,” the system must clean recordings to standardize volume levels, remove ambient noise, and ensure consistent audio quality across different recording environments, all while preserving the essential characteristics that make the wake word recognizable.</p>
<p>Once cleaning is established, quality assessment becomes important for KWS systems. Quality metrics for KWS data are uniquely focused on audio characteristics, including background noise levels, audio clarity scores, and speaking rate consistency. For instance, a KWS quality assessment pipeline might automatically flag recordings where background noise exceeds acceptable thresholds or where the wake word is spoken too quickly or unclearly, ensuring only high-quality samples are used for model development.</p>
<p>To ensure practical relevance, these quality metrics must be carefully calibrated to reflect real-world operating conditions. A robust training dataset incorporates both pristine recordings and samples containing controlled levels of environmental variations. For instance, while recordings with signal-masking interference are excluded, the dataset should include samples with measured background acoustics, variable speaker distances, and concurrent speech or other forms of audio signals. This approach to data diversity ensures the system maintains wake word detection reliability across the full spectrum of deployment environments and acoustic conditions.</p>
<p>With quality controls in place, transforming audio data for KWS involves converting raw waveforms into formats suitable for ML models. The typical transformation pipeline converts audio signals into standardized feature representations that emphasize speech-relevant characteristics while reducing noise and variability across different recording conditions. This transformation must be consistently applied across both training and inference, often with additional considerations for real-time processing on edge devices.</p>
<p><a href="#fig-spectrogram-example" class="quarto-xref">Figure&nbsp;10</a> illustrates this transformation process. The top panel shows a raw waveform of a simulated audio signal, which consists of a sine wave mixed with noise. This time-domain representation highlights the challenges posed by real-world recordings, where noise and variability must be addressed. The middle panel shows a frequency-based representation of the signal, which maps its frequency content over time, providing a detailed view of how energy is distributed across frequencies. The bottom panel shows compressed feature coefficients that emphasize speech-related characteristics, making them well-suited for KWS tasks.</p>
<div id="fig-spectrogram-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t!">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spectrogram-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/kws_spectrogram.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Audio Feature Transformation: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting."><img src="images/png/kws_spectrogram.png" class="img-fluid figure-img" data-fig-pos="t!"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spectrogram-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Audio Feature Transformation</strong>: Advanced audio features compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.
</figcaption>
</figure>
</div>
<p>Building on these transformed representations, feature engineering for KWS focuses on extracting characteristics that help distinguish wake words from background speech. Engineers might create features capturing tonal variations, speech energy patterns, or temporal characteristics. For the wake word “Alexa,” features might include energy distribution across frequency bands, pitch contours, and duration patterns that characterize typical pronunciations. While hand-engineered speech features have seen much success, learned features <span class="citation" data-cites="zeghidour2021leaf">(<a href="#ref-zeghidour2021leaf" role="doc-biblioref">Zeghidour et al. 2021</a>)</span> are increasingly common.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zeghidour2021leaf" class="csl-entry" role="listitem">
Zeghidour, Neil, Olivier Teboul, Félix de Chaumont Quitry, and Marco Tagliasacchi. 2021. <span>“LEAF: A Learnable Frontend for Audio Classification.”</span> <em>arXiv Preprint arXiv:2101.08596</em>, January. <a href="http://arxiv.org/abs/2101.08596v1">http://arxiv.org/abs/2101.08596v1</a>.
</div></div><p>Integrating all these processing components, KWS processing pipelines must handle both batch processing for training and real-time processing for inference. The pipeline typically includes stages for audio preprocessing, feature extraction, and quality filtering. These pipelines must be designed to operate efficiently on edge devices while maintaining consistent processing steps between training and deployment.</p>
<div id="quiz-question-sec-data-engineering-data-processing-c336" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the primary difference between ETL and ELT workflows in data processing?</p>
<ol type="a">
<li>ETL processes data after it is loaded into the target system, while ELT processes it before loading.</li>
<li>ETL is used for structured data, while ELT is used for unstructured data.</li>
<li>ETL is more flexible than ELT in handling unknown transformations.</li>
<li>ETL involves transforming data before loading into the target system, whereas ELT transforms data after loading.</li>
</ol></li>
<li><p>Explain how data cleaning can impact the performance of machine learning models.</p></li>
<li><p>True or False: In a data processing pipeline, feature engineering is considered more of an art than a science.</p></li>
<li><p>Order the following data processing steps in a typical ETL workflow: (1) Load data into the target system, (2) Transform data, (3) Extract data from source.</p></li>
<li><p>In a production system handling large datasets, what scalability considerations should be taken into account during data processing?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-processing-c336" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-labeling-95e7" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-labeling-95e7">Data Labeling</h2>
<p>With systematic data processing established, data labeling emerges as a particularly complex systems challenge within the broader data engineering landscape. As training datasets grow to millions or billions of examples, the infrastructure supporting labeling operations becomes increasingly critical to system performance.</p>
<p>Building on our processing foundations, modern machine learning systems must efficiently handle the creation, storage, and management of labels across their data pipeline. The systems architecture must support various labeling workflows while maintaining data consistency, ensuring quality, and managing computational resources effectively. These requirements compound when dealing with large-scale datasets or real-time labeling needs.</p>
<p>These storage and management requirements represent only part of the challenge. The systematic challenges extend beyond just storing and managing labels. Production ML systems need robust pipelines that integrate labeling workflows with data ingestion, preprocessing, and training components. These pipelines must maintain high throughput while ensuring label quality and adapting to changing requirements. For instance, a speech recognition system might need to continuously update its training data with new audio samples and corresponding transcription labels, requiring careful coordination between data collection, labeling, and training subsystems.</p>
<p>Given these diverse workflow requirements, infrastructure requirements vary significantly based on labeling approach and scale. Manual expert labeling may require specialized interfaces and security controls, while automated labeling systems need substantial compute resources for inference. Organizations must carefully balance these requirements against performance needs and resource constraints.</p>
<p>Understanding these infrastructure trade-offs, we explore how data labeling significantly shapes machine learning system design. From storage architectures to quality control pipelines, each aspect of the labeling process introduces unique technical challenges that ripple throughout the ML infrastructure. Understanding these systems-level implications enables building reliable, scalable labeling solutions which are an integral part of data engineering.</p>
<section id="sec-data-engineering-types-labels-76ae" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-types-labels-76ae">Types of Labels</h3>
<p>To build effective labeling systems, we must first understand how different types of labels affect our system architecture and resource requirements. Consider a practical example: building a smart city system that needs to detect and track various objects like vehicles, pedestrians, and traffic signs from video feeds. Labels capture information about key tasks or concepts.</p>
<ul>
<li><p><strong>Classification labels</strong> represent the simplest form, categorizing images with a specific tag or (in multi-label classification) tags (e.g., labeling an image as “car” or “pedestrian”). While conceptually straightforward, a production system processing millions of video frames must efficiently store and retrieve these labels.</p></li>
<li><p><strong>Bounding boxes</strong> extend beyond simple classification by identifying object locations, drawing a box around each object of interest. Our system now needs to track not just what objects exist, but where they are in each frame. This spatial information introduces new storage and processing challenges, especially when tracking moving objects across video frames.</p></li>
<li><p><strong>Segmentation maps</strong> provide the most comprehensive information by classifying objects at the pixel level, highlighting each object in a distinct color. For our traffic monitoring system, this might mean precisely outlining each vehicle, pedestrian, and road sign. These detailed annotations significantly increase our storage and processing requirements.</p></li>
</ul>
<p><a href="#fig-labels" class="quarto-xref">Figure&nbsp;11</a> illustrates the common label types:</p>
<div id="fig-labels" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/cs249r_labels_new.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Data Annotation Granularity: Increasing levels of detail in data labeling—from bounding boxes to pixel-level segmentation—impact both annotation cost and potential model accuracy. Fine-grained segmentation provides richer information for training but demands significantly more labeling effort and storage capacity than coarser annotations."><img src="images/png/cs249r_labels_new.png" class="img-fluid figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Data Annotation Granularity</strong>: Increasing levels of detail in data labeling—from bounding boxes to pixel-level segmentation—impact both annotation cost and potential model accuracy. Fine-grained segmentation provides richer information for training but demands significantly more labeling effort and storage capacity than coarser annotations.
</figcaption>
</figure>
</div>
<p>Given these increasing complexity levels, the choice of label format depends heavily on our system requirements and resource constraints <span class="citation" data-cites="10.1109/ICRA.2017.7989092">(<a href="#ref-10.1109/ICRA.2017.7989092" role="doc-biblioref">Johnson-Roberson et al. 2017</a>)</span>. While classification labels might suffice for simple traffic counting, autonomous vehicles need detailed segmentation maps to make precise navigation decisions. Leading autonomous vehicle companies often maintain hybrid systems that store multiple label types for the same data, allowing flexible use across different applications.</p>
<div class="no-row-height column-margin column-container"><div id="ref-10.1109/ICRA.2017.7989092" class="csl-entry" role="listitem">
Johnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. <span>“Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?”</span> In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 746–53. Singapore, Singapore: IEEE. <a href="https://doi.org/10.1109/icra.2017.7989092">https://doi.org/10.1109/icra.2017.7989092</a>.
</div><div id="ref-ardila2020common" class="csl-entry" role="listitem">
Ardila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. <span>“Common Voice: A Massively-Multilingual Speech Corpus.”</span> In <em>Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, 4218–22. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2020.lrec-1.520">https://aclanthology.org/2020.lrec-1.520</a>.
</div></div><p>Extending beyond these basic label types, production systems must also handle rich metadata. The Common Voice dataset <span class="citation" data-cites="ardila2020common">(<a href="#ref-ardila2020common" role="doc-biblioref">Ardila et al. 2020</a>)</span>, for instance, exemplifies this in its management of audio data for speech recognition. The system tracks speaker demographics for model fairness, recording quality metrics for data filtering, validation status for label reliability, and language information for multilingual support.</p>
<p>To address these metadata challenges, modern labeling platforms have built sophisticated metadata management systems to handle these complex relationships. This metadata becomes important for maintaining and managing data quality and debugging model behavior. If our traffic monitoring system performs poorly in rainy conditions, having metadata about weather conditions during data collection helps identify and address the issue. The infrastructure must efficiently index and query this metadata alongside the primary labels.</p>
<p>These metadata requirements demonstrate how the choice of label type cascades through our entire system design. A system built for simple classification labels would need significant modifications to handle segmentation maps efficiently. The infrastructure must optimize storage systems for the chosen label format, implement efficient data retrieval patterns for training, maintain quality control pipelines for validation, and manage version control for label updates. Resource allocation becomes particularly critical as data volume grows, requiring careful capacity planning across storage, compute, and networking components.</p>
</section>
<section id="sec-data-engineering-annotation-techniques-6ebe" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-annotation-techniques-6ebe">Annotation Techniques</h3>
<p>With label types and their system implications established, we examine how different annotation techniques address these requirements. Manual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>, experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>Medical Imaging AI Revolution</strong>: The 2012 AlexNet breakthrough began with ImageNet’s 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.</p></div><div id="ref-victor2019machine" class="csl-entry" role="listitem">
Sheng, Victor S., and Jing Zhang. 2019. <span>“Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 33 (01): 9837–43. <a href="https://doi.org/10.1609/aaai.v33i01.33019837">https://doi.org/10.1609/aaai.v33i01.33019837</a>.
</div></div><p>To address these scalability limitations, crowdsourcing offers a path to greater scalability by distributing annotation tasks across many annotators <span class="citation" data-cites="victor2019machine">(<a href="#ref-victor2019machine" role="doc-biblioref">Sheng and Zhang 2019</a>)</span>. Crowdsourcing enables non-experts to distribute annotation tasks, often through dedicated platforms <span class="citation" data-cites="victor2019machine">(<a href="#ref-victor2019machine" role="doc-biblioref">Sheng and Zhang 2019</a>)</span>. Several companies have emerged as leaders in this space, building sophisticated platforms for large-scale annotation. For instance, companies such as <a href="https://scale.com/">Scale AI</a> specialize in managing thousands of concurrent annotators through their platform. <a href="https://www.appen.com/">Appen</a> focuses on linguistic annotation and text data, while <a href="https://labelbox.com/">Labelbox</a> has developed specialized tools for computer vision tasks. These platforms allow dataset creators to access a large pool of annotators, making it possible to label vast amounts of data relatively quickly.</p>
<p>Beyond human-powered approaches, weakly supervised and programmatic methods represent a third approach, using automation to reduce manual effort <span class="citation" data-cites="ratner2018snorkel">(<a href="#ref-ratner2018snorkel" role="doc-biblioref">Ratner et al. 2018</a>)</span>. These systems leverage existing knowledge bases and heuristics to automatically generate labels. For example, distant supervision techniques might use a knowledge base to label mentions of companies in text data. While these methods can rapidly label large datasets, they require substantial compute resources for inference, sophisticated caching systems to avoid redundant computation, and careful monitoring to manage potential noise and bias.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ratner2018snorkel" class="csl-entry" role="listitem">
Ratner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré. 2018. <span>“Snorkel MeTaL: Weak Supervision for Multi-Task Learning.”</span> In <em>Proceedings of the Second Workshop on Data Management for End-to-End Machine Learning</em>, 1–4. ACM. <a href="https://doi.org/10.1145/3209889.3209898">https://doi.org/10.1145/3209889.3209898</a>.
</div></div><p>Recognizing the limitations of any single approach, most production systems combine multiple annotation approaches to balance speed, cost, and quality. A common pattern employs programmatic labeling for initial coverage, followed by crowdsourced verification and expert review of uncertain cases. This hybrid approach requires careful system design to manage the flow of data between different annotation stages. The infrastructure must track label provenance, manage quality control at each stage, and ensure consistent data access patterns across different annotator types.</p>
<p>These hybrid workflows demonstrate how the choice of annotation method significantly impacts system architecture. Expert-only systems might employ centralized architectures with high-speed access to a single data store. Crowdsourcing demands distributed architectures to handle concurrent annotators. Automated systems need substantial compute resources and caching infrastructure. Many organizations implement tiered architectures where different annotation methods operate on different subsets of data based on complexity and criticality.</p>
<p>Across all these architectural variations, clear guidelines and thorough training remain essential regardless of the chosen architecture. The system must provide consistent interfaces, documentation, and quality metrics across all annotation methods. This becomes particularly challenging when managing diverse annotator pools with varying levels of expertise. Some platforms address this by offering access to specialized annotators. For instance, providing medical professionals for healthcare datasets or domain experts for technical content.</p>
</section>
<section id="sec-data-engineering-label-quality-assessment-9c43" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-label-quality-assessment-9c43">Label Quality Assessment</h3>
<p>Regardless of the annotation technique chosen, label quality determines machine learning system performance. A model can only be as good as its training data. However, ensuring quality at scale presents significant systems challenges. The primary challenge stems from label uncertainty.</p>
<p><a href="#fig-hard-labels" class="quarto-xref">Figure&nbsp;12</a> illustrates common failure modes in labeling systems: some errors arise from data quality issues (like the blurred frog image), while others require deep domain expertise (as with the black stork identification). Even with clear instructions and careful system design, some fraction of labels will inevitably be incorrect <span class="citation" data-cites="northcutt2021pervasive">Thyagarajan et al. (<a href="#ref-thyagarajan2023multilabel" role="doc-biblioref">2022</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-thyagarajan2023multilabel" class="csl-entry" role="listitem">
Thyagarajan, Aditya, Elías Snorrason, Curtis G. Northcutt, and Jonas Mueller 0001. 2022. <span>“Identifying Incorrect Annotations in Multi-Label Classification Data.”</span> <em>CoRR</em>. <a href="https://doi.org/10.48550/ARXIV.2211.13895">https://doi.org/10.48550/ARXIV.2211.13895</a>.
</div></div><div id="fig-hard-labels" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hard-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/label-errors-examples_new.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Labeling Ambiguity: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive]."><img src="images/png/label-errors-examples_new.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hard-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Labeling Ambiguity</strong>: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: <span class="citation" data-cites="northcutt2021pervasive">(<a href="#ref-northcutt2021pervasive" role="doc-biblioref">Northcutt, Athalye, and Mueller 2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-northcutt2021pervasive" class="csl-entry" role="listitem">
Northcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021. <span>“Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.”</span> <em>arXiv</em>. https://doi.org/<a href="https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite">https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite</a>.
</div></div></figure>
</div>
<p>Given these fundamental quality challenges, production ML systems implement multiple layers of quality control to address these challenges. Typically, systematic quality checks continuously monitor the labeling pipeline. These systems randomly sample labeled data for expert review and employ statistical methods to flag potential errors. The infrastructure must efficiently process these checks across millions of examples without creating bottlenecks in the labeling pipeline.</p>
<p>Beyond random sampling approaches, collecting multiple labels per data point, often referred to as “consensus labeling,” can help identify controversial or ambiguous cases. Professional labeling companies have developed sophisticated infrastructure for this process. For example, <a href="https://labelbox.com/">Labelbox</a> has consensus tools that track inter-annotator agreement rates and automatically route controversial cases for expert review. <a href="https://scale.com">Scale AI</a> implements tiered quality control, where experienced annotators verify the work of newer team members.</p>
<p>While technical infrastructure provides the foundation, successful labeling systems must also consider human factors. When working with annotators, organizations need robust systems for training and guidance. This includes good documentation, clear examples of correct labeling, and regular feedback mechanisms. For complex or domain-specific tasks, the system might implement tiered access levels, routing challenging cases to annotators with appropriate expertise.</p>
<p>Extending beyond training and guidance, ethical considerations also significantly impact system design. For datasets containing potentially disturbing content, systems should implement protective features like grayscale viewing options <span class="citation" data-cites="googleinformation">(<a href="#ref-googleinformation" role="doc-biblioref">Blackwood et al. 2019</a>)</span>. This requires additional image processing pipelines and careful interface design. We need to develop workload management systems that track annotator exposure to sensitive content and enforce appropriate limits.</p>
<div class="no-row-height column-margin column-container"><div id="ref-googleinformation" class="csl-entry" role="listitem">
Blackwood, Jayden, Frances C. Wright, Nicole J. Look Hong, and Anna R. Gagliardi. 2019. <span>“Quality of DCIS Information on the Internet: A Content Analysis.”</span> <em>Breast Cancer Research and Treatment</em> 177 (2): 295–305. <a href="https://doi.org/10.1007/s10549-019-05315-8">https://doi.org/10.1007/s10549-019-05315-8</a>.
</div></div><p>These protective measures demonstrate how the quality control system itself generates substantial data that must be efficiently processed and monitored. Organizations typically track inter-annotator agreement rates, label confidence scores, time spent per annotation, error patterns and types, annotator performance metrics, and bias indicators. These metrics must be computed and updated efficiently across millions of examples, often requiring dedicated analytics pipelines.</p>
<p>Complementing these performance metrics, regular bias audits are another critical component of quality control. Systems must monitor for cultural, personal, or professional biases that could skew the labeled dataset. This requires infrastructure for collecting and analyzing demographic information, measuring label distributions across different annotator groups, identifying systematic biases in the labeling process, and implementing corrective measures when biases are detected.</p>
<p>Underlying all these quality control mechanisms, perhaps the most important aspect is that the process must remain iterative. As new challenges emerge, quality control systems must adapt and evolve. Through careful system design and implementation of these quality control mechanisms, organizations can maintain high label quality even at a massive scale.</p>
</section>
<section id="sec-data-engineering-ai-annotation-41b4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-ai-annotation-41b4">AI in Annotation</h3>
<p>Building on these quality control foundations, organizations increasingly leverage AI to accelerate and enhance their labeling pipelines as machine learning systems grow in scale and complexity. This approach introduces new system design considerations around model deployment, resource management, and human-AI collaboration. At the core of these scaling challenges lies data volume. Manual annotation alone cannot keep pace with modern ML systems’ data needs. As illustrated in <a href="#fig-weak-supervision" class="quarto-xref">Figure&nbsp;13</a>, AI assistance offers several paths to scale labeling operations, each requiring careful system design to balance speed, quality, and resource usage.</p>
<div id="fig-weak-supervision" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weak-supervision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="a271bd786f6723a93e5e1b3dff59701d8362b9f2.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: AI-Augmented Labeling: Programmatic labeling, distant supervision, and active learning scale data annotation by trading potential labeling errors for increased throughput, necessitating careful system design to balance labeling speed, cost, and model quality. These strategies enable machine learning systems to overcome limitations imposed by manual annotation alone, facilitating deployment in data-scarce environments. Source: Stanford AI Lab."><img src="data_engineering_files/mediabag/a271bd786f6723a93e5e1b3dff59701d8362b9f2.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weak-supervision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>AI-Augmented Labeling</strong>: Programmatic labeling, distant supervision, and active learning scale data annotation by trading potential labeling errors for increased throughput, necessitating careful system design to balance labeling speed, cost, and model quality. These strategies enable machine learning systems to overcome limitations imposed by manual annotation alone, facilitating deployment in data-scarce environments. Source: Stanford AI Lab.
</figcaption>
</figure>
</div>
<p>Modern AI-assisted labeling typically employs a combination of approaches. Pre-annotation involves using AI models to generate preliminary labels for a dataset, which humans can then review and correct. Major labeling platforms have made significant investments in this technology. <a href="https://snorkel.ai/">Snorkel AI</a> uses programmatic labeling to automatically generate initial labels at scale. Scale AI deploys pre-trained models to accelerate annotation in specific domains like autonomous driving, while manycompanies like <a href="https://www.superannotate.com/">SuperAnnotate</a> provide automated pre-labeling tools that can reduce manual effort drastically. This method, which often employs semi-supervised learning techniques <span class="citation" data-cites="chapelle2009semisupervised">(<a href="#ref-chapelle2009semisupervised" role="doc-biblioref">Chapelle, Scholkopf, and Zien 2009</a>)</span>, can save a significant amount of time, especially for extremely large datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chapelle2009semisupervised" class="csl-entry" role="listitem">
Chapelle, O., B. Scholkopf, and A. Zien Eds. 2009. <span>“Semi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book Reviews].”</span> <em>IEEE Transactions on Neural Networks</em> 20 (3): 542–42. <a href="https://doi.org/10.1109/tnn.2009.2015974">https://doi.org/10.1109/tnn.2009.2015974</a>.
</div></div><p>The emergence of Large Language Models (LLMs) like ChatGPT has further transformed labeling pipelines. Beyond simple classification, LLMs can generate rich text descriptions, create labeling guidelines, and even explain their reasoning. For instance, content moderation systems use LLMs to perform initial content classification and generate explanations for policy violations. However, integrating LLMs introduces new system challenges around inference costs, rate limiting, and output validation. Many organizations adopt a tiered approach, using smaller specialized models for routine cases while reserving larger LLMs for complex scenarios.</p>
<p>Methods such as active learning complement these approaches by intelligently prioritizing which examples need human attention <span class="citation" data-cites="coleman2022similarity">(<a href="#ref-coleman2022similarity" role="doc-biblioref">Coleman et al. 2022</a>)</span>. These systems continuously analyze model uncertainty to identify valuable labeling candidates for humans to label. The infrastructure must efficiently compute uncertainty metrics, maintain task queues, and adapt prioritization strategies based on incoming labels. Consider a medical imaging system: active learning might identify unusual pathologies for expert review while handling routine cases automatically.</p>
<div class="no-row-height column-margin column-container"><div id="ref-coleman2022similarity" class="csl-entry" role="listitem">
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I. Zeki Yalniz. 2022. <span>“Similarity Search for Efficient Active Learning and Search of Rare Concepts.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 36 (6): 6402–10. <a href="https://doi.org/10.1609/aaai.v36i6.20591">https://doi.org/10.1609/aaai.v36i6.20591</a>.
</div></div><p>Quality control becomes increasingly crucial as these AI components interact. The system must monitor both AI and human performance, detect potential errors, and maintain clear label provenance. This requires dedicated infrastructure tracking metrics like model confidence and human-AI agreement rates. In safety-critical domains like self-driving cars, these systems must maintain particularly rigorous standards while processing massive streams of sensor data.</p>
<p>Real-world deployments demonstrate these principles at scale. Medical imaging systems <span class="citation" data-cites="krishnan2022selfsupervised">(<a href="#ref-krishnan2022selfsupervised" role="doc-biblioref">Krishnan, Rajpurkar, and Topol 2022</a>)</span> combine pre-annotation for common conditions with active learning for unusual cases, all while maintaining strict patient privacy.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krishnan2022selfsupervised" class="csl-entry" role="listitem">
Krishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022. <span>“Self-Supervised Learning in Medicine and Healthcare.”</span> <em>Nature Biomedical Engineering</em> 6 (12): 1346–52. <a href="https://doi.org/10.1038/s41551-022-00914-1">https://doi.org/10.1038/s41551-022-00914-1</a>.
</div></div><p>Self-driving vehicle systems coordinate multiple AI models to label diverse sensor data in real-time. Social media platforms process millions of items hourly, using tiered approaches where simpler models handle clear cases while complex content routes to more sophisticated models or human reviewers.</p>
<p>While AI assistance offers clear benefits, it also introduces new failure modes. Systems must guard against bias amplification, where AI models trained on biased data perpetuate those biases in new labels. The infrastructure needs robust monitoring to detect such issues and mechanisms to break problematic feedback loops. These concerns align closely with the responsible AI principles in <strong><a href="../core/responsible_ai/responsible_ai.html#sec-responsible-ai">Chapter 16: Responsible AI</a></strong>, where we examine how data quality decisions impact algorithmic fairness and accountability. Human oversight remains essential, requiring careful interface design to help annotators effectively supervise and correct AI output.</p>
</section>
<section id="sec-data-engineering-labeling-challenges-d658" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-labeling-challenges-d658">Labeling Challenges</h3>
<p>Despite the sophistication of AI-assisted annotation systems, data labeling comes with its own set of challenges and limitations that practitioners must be aware of and address. One of the primary challenges in data labeling is the inherent subjectivity in many labeling tasks. Even with clear guidelines, human annotators may interpret data differently, leading to inconsistencies in labeling. This is particularly evident in tasks involving sentiment analysis, image classification of ambiguous objects, or labeling of complex medical conditions. For instance, in a study of medical image annotation, <span class="citation" data-cites="oakden2020hidden">Oakden-Rayner et al. (<a href="#ref-oakden2020hidden" role="doc-biblioref">2020</a>)</span> found significant variability in labels assigned by different radiologists, highlighting the challenge of obtaining “ground truth” in inherently subjective tasks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-oakden2020hidden" class="csl-entry" role="listitem">
Oakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. 2020. <span>“Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging.”</span> In <em>Proceedings of the ACM Conference on Health, Inference, and Learning</em>, 151–59. ACM. <a href="https://doi.org/10.1145/3368555.3384468">https://doi.org/10.1145/3368555.3384468</a>.
</div></div><p>Beyond these subjective interpretation issues, scalability presents another significant challenge, especially as datasets grow larger and more complex. Manual labeling is time-consuming and expensive, often becoming a bottleneck in the machine learning pipeline. While crowdsourcing and AI-assisted methods can help address this issue to some extent, they introduce their own complications in terms of quality control and potential biases.</p>
<p>Compounding these scalability and quality challenges, the issue of bias in data labeling is particularly concerning. Annotators bring their own cultural, personal, and professional biases to the labeling process, which can be reflected in the resulting dataset. For example, <span class="citation" data-cites="wang2019balanced">Wang et al. (<a href="#ref-wang2019balanced" role="doc-biblioref">2019</a>)</span> found that image datasets labeled predominantly by annotators from one geographic region showed biases in object recognition tasks, performing poorly on images from other regions. This highlights the need for diverse annotator pools and careful consideration of potential biases in the labeling process.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2019balanced" class="csl-entry" role="listitem">
Wang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. 2019. <span>“Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations.”</span> In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 5309–18. IEEE. <a href="https://doi.org/10.1109/iccv.2019.00541">https://doi.org/10.1109/iccv.2019.00541</a>.
</div></div><p>Adding another layer of complexity, data privacy and ethical considerations also pose challenges in data labeling. Leading data labeling companies have developed specialized solutions for these challenges. Scale AI, for instance, maintains dedicated teams and secure infrastructure for handling sensitive data in healthcare and finance. Appen implements strict data access controls and anonymization protocols, while Labelbox offers private cloud deployments for organizations with strict security requirements. When dealing with sensitive data, such as medical records or personal communications, ensuring annotator access while maintaining data privacy can be complex. These privacy-preserving techniques connect directly to the security considerations in <strong><a href="../core/privacy_security/privacy_security.html#sec-security-privacy">Chapter 15: Security & Privacy</a></strong>, where we explore comprehensive approaches to protecting sensitive data throughout the ML lifecycle.</p>
<p>Beyond privacy and ethical concerns, the dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>, necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.</p>
<div class="no-row-height column-margin column-container"><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Concept Drift Challenge</strong>: First formalized by Schlimmer &amp; Granger (1986), concept drift became critical in ML systems deployment. Amazon’s recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics.</p></div></div><p>Finally, the limitations of current labeling approaches become apparent when dealing with edge cases or rare events. In many real-world applications, it’s the unusual or rare instances that are often most critical (e.g., rare diseases in medical diagnosis, or unusual road conditions in autonomous driving). However, these cases are, by definition, underrepresented in most datasets and may be overlooked or mislabeled in large-scale annotation efforts.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-52dc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-52dc">Continuing the KWS Example</h3>
<p>Given these labeling challenges, the complex requirements of KWS reveal the role of automated data labeling in modern machine learning. The Multilingual Spoken Words Corpus (MSWC) <span class="citation" data-cites="mazumder2021multilingual">(<a href="#ref-mazumder2021multilingual" role="doc-biblioref">Mazumder et al. 2021</a>)</span> illustrates this through its innovative approach to generating labeled wake word data at scale. MSWC is large, containing over 23.4 million one-second spoken examples across 340,000 keywords in 50 different languages.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mazumder2021multilingual" class="csl-entry" role="listitem">
Mazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. <span>“Multilingual Spoken Words Corpus.”</span> In <em>Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>.
</div></div><p>To understand this automated approach, the core of this system, as illustrated in <a href="#fig-mswc" class="quarto-xref">Figure&nbsp;14</a>, begins with paired sentence audio recordings and corresponding transcriptions, which can be sourced from projects like <a href="https://commonvoice.mozilla.org/en">Common Voice</a> or multilingual captioned content platforms such as YouTube. The system processes paired audio-text inputs through forced alignment to identify word boundaries, extracts individual keywords as one-second segments, and generates a large-scale multilingual dataset suitable for training keyword spotting models. For example, when a speaker says, “He gazed up the steep bank,” their voice generates a complex acoustic signal that conveys more than just the words themselves. This signal encapsulates subtle transitions between words, variations in pronunciation, and the natural rhythm of speech. The primary challenge lies in accurately pinpointing the exact location of each word within this continuous audio stream.</p>
<div id="fig-mswc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mswc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/data_engineering_kws2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Multilingual Data Preparation: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content."><img src="images/png/data_engineering_kws2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mswc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <strong>Multilingual Data Preparation</strong>: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.
</figcaption>
</figure>
</div>
<p>To address this fundamental challenge, automated forced alignment proves useful. Tools such as the Montreal Forced Aligner <span class="citation" data-cites="mcauliffe17_interspeech">(<a href="#ref-mcauliffe17_interspeech" role="doc-biblioref">McAuliffe et al. 2017</a>)</span> analyze both the audio and its transcription, mapping the timing relationship between written words and spoken sounds, and attempts to mark the boundaries of when each word begins and ends in a speech recording at millisecond-level precision. For high-resource languages such as English, high-quality automated alignments are available “out-of-box” while alignments for low-resource languages must be bootstrapped on the speech data and transcriptions themselves, which can negatively impact timing quality.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcauliffe17_interspeech" class="csl-entry" role="listitem">
McAuliffe, Michael, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017. <span>“Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.”</span> In <em>Interspeech 2017</em>, 498–502. ISCA. <a href="https://doi.org/10.21437/interspeech.2017-1386">https://doi.org/10.21437/interspeech.2017-1386</a>.
</div></div><p>Building on these precise timing markers, the extraction system can generate clean, one-second samples of individual keywords. This process requires careful engineering decisions. Background noise might interfere with detecting word boundaries. Speakers may stretch, compress, or mispronounce words in unexpected ways. Longer words may not fit within the default 1-second boundary. To aid ML practitioners in filtering out lower-quality samples automatically, MSWC provides an automated quality assessment system that analyzes audio characteristics to identify potential issues with recording quality, speech clarity, or background noise. This automated validation becomes particularly crucial given the scale of the dataset, which includes over 23 million samples across more than 340,000 words in 50+ languages. Traditional manual review could not maintain consistent standards across such volume without significant expense.</p>
<p>Leveraging this automated infrastructure, modern voice assistant developers often build upon this type of labeling foundation. An automated corpus like MSWC may not contain the specific keywords an application developer wishes to use for their envisioned KWS system, but the corpus can provide a starting point for KWS prototyping in many underserved languages spoken around the world. While MSWC provides automated labeling at scale, production systems may add targeted human recording and verification for challenging cases, rare words, or difficult acoustic environments. The infrastructure must gracefully coordinate between automated processing and human expertise.</p>
<p>Beyond immediate practical applications, the impact of this careful engineering extends far beyond the dataset itself. Automated labeling pipelines open new avenues to how we approach wake word detection and other ML tasks across languages or other demographic boundaries. Where manual collection and annotation might yield thousands of examples, automated dataset generation can yield millions while maintaining consistent quality. This enables voice interfaces to understand an ever-expanding vocabulary across the world’s languages.</p>
<p>Through this comprehensive approach to data labeling, MSWC demonstrates how thoughtful data engineering directly impacts production machine learning systems. The careful orchestration of forced alignment, extraction, and quality control creates a foundation for reliable voice interaction across languages. When a voice assistant responds to its wake word, it draws upon this sophisticated labeling infrastructure, which is a testament to the power of automated data processing in modern machine learning systems.</p>
<p>After establishing systematic processing pipelines that transform raw data into ML-ready formats, we must design storage architectures that support the entire ML lifecycle while maintaining our four-pillar framework. Storage decisions determine how effectively we can maintain data quality over time, ensure reliable access under varying loads, scale to handle growing data volumes, and implement governance controls.</p>
<div id="quiz-question-sec-data-engineering-data-labeling-95e7" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key challenge when integrating data labeling workflows into machine learning pipelines?</p>
<ol type="a">
<li>Maintaining data consistency and label quality</li>
<li>Ensuring the availability of large datasets</li>
<li>Implementing basic data storage solutions</li>
<li>Focusing solely on algorithmic improvements</li>
</ol></li>
<li><p>Explain how the choice of label type can impact the system architecture and resource requirements of an ML system.</p></li>
<li><p>Order the following steps in a typical data labeling workflow: (1) Data Collection, (2) Labeling, (3) Quality Control, (4) Integration with Training Pipeline.</p></li>
<li><p>True or False: Automated labeling systems require less computational resources than manual expert labeling.</p></li>
<li><p>In a production system, how might you apply a hybrid approach to data labeling to balance speed, cost, and quality?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-labeling-95e7" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-storage-6296" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-storage-6296">Strategic Storage Architecture</h2>
<p>Storage architecture represents the intersection of all four pillars in our framework: quality requirements determine how data is organized and accessed; reliability needs shape redundancy and backup strategies; scalability demands influence storage system selection; and governance requirements drive access controls and retention policies.</p>
<p>For our KWS system, storage decisions have profound implications across the ML lifecycle. Training data storage must support high-throughput reads for model development while maintaining quality through versioning and lineage tracking. Feature storage must enable low-latency access for real-time inference while ensuring privacy through access controls. Model artifacts and metadata require reliable storage with comprehensive governance for production deployment.</p>
<p>Machine learning workloads create distinctive storage requirements that differ markedly from traditional applications. While transactional systems optimize for frequent writes and row-level updates, ML pipelines require high-throughput reads, large-scale data scans, and evolving schemas that support iterative model development and feature engineering.</p>
<p>ML pipelines must accommodate real-world considerations such as evolving business requirements, new data sources, and changes in data availability. These realities push storage solutions to be both scalable and flexible, ensuring that organizations can manage data collected from diverse channels, ranging from sensor feeds to social media text, without constantly retooling the entire infrastructure. In this section, we will compare the practical use of databases, data warehouses, and data lakes for ML projects, then examine how specialized services, metadata, and governance practices unify these varied systems into a coherent strategy.</p>
<section id="sec-data-engineering-storage-system-types-f802" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-system-types-f802">Storage System Types</h3>
<p>The following sections provide a systematic comparison of storage architectures for ML systems, evaluated through our four-pillar framework. We begin with high-level architectural trade-offs before examining specific implementation considerations.</p>
<p>Storage system selection represents a critical architectural decision that affects all aspects of the ML lifecycle. Databases, data warehouses, and data lakes each provide distinct advantages for different aspects of ML workflows, and modern systems often employ multiple storage types in coordinated architectures.</p>
<p><a href="#tbl-storage" class="quarto-xref">Table&nbsp;2</a> provides a framework-oriented comparison of these storage systems: Databases usually support operational and transactional purposes. They work well for smaller, well-structured datasets, but can become cumbersome and expensive when applied to large-scale ML contexts involving unstructured data (such as images, audio, or free-form text).</p>
<div id="tbl-storage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-storage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Storage System Characteristics</strong>: Different storage systems suit distinct stages of machine learning workflows based on data structure and purpose; databases manage transactional data, data warehouses support analytical reporting, and data lakes accommodate diverse, raw data for future processing. Understanding these characteristics enables efficient data management and supports the scalability of machine learning applications.
</figcaption>
<div aria-describedby="tbl-storage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Conventional Database</th>
<th style="text-align: left;">Data Warehouse</th>
<th style="text-align: left;">Data Lake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Purpose</td>
<td style="text-align: left;">Operational and transactional</td>
<td style="text-align: left;">Analytical and reporting</td>
<td style="text-align: left;">Storage for raw and diverse data for future processing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data type</td>
<td style="text-align: left;">Structured</td>
<td style="text-align: left;">Structured</td>
<td style="text-align: left;">Structured, semi-structured, and unstructured</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scale</td>
<td style="text-align: left;">Small to medium volumes</td>
<td style="text-align: left;">Medium to large volumes</td>
<td style="text-align: left;">Large volumes of diverse data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Performance Optimization</td>
<td style="text-align: left;">Optimized for transactional queries (OLTP)</td>
<td style="text-align: left;">Optimized for analytical queries (OLAP)</td>
<td style="text-align: left;">Optimized for scalable storage and retrieval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Examples</td>
<td style="text-align: left;">MySQL, PostgreSQL, Oracle DB</td>
<td style="text-align: left;">Google BigQuery, Amazon Redshift, Microsoft Azure Synapse</td>
<td style="text-align: left;">Google Cloud Storage, AWS S3, Azure Data Lake Storage</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Data warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT)<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> sensors.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>IoT Data Explosion</strong>: IDC predicts 41.6 billion IoT devices will generate 79.4 zettabytes of data by 2025—that’s 79.4 trillion gigabytes. A single autonomous vehicle generates 4TB/day, while smart city sensors produce 2.5 quintillion bytes daily. Traditional data warehouses struggle with this velocity and variety.</p></div><div id="fn30"><p><sup>30</sup>&nbsp;<strong>Data Lake Origins</strong>: The term “data lake” was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to “a data swamp” if poorly managed. The concept emerged from Hadoop’s ability to store vast amounts of unstructured data cheaply.</p></div></div><p>Data lakes<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called <em>schema-on-read</em>). As <a href="#tbl-storage" class="quarto-xref">Table&nbsp;2</a> shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos.</p>
<p>The examples provided in <a href="#tbl-storage" class="quarto-xref">Table&nbsp;2</a> illustrate the range of technologies available for each storage system type. For instance, MySQL<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> represents a traditional database system, while solutions like Google BigQuery<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> and Amazon Redshift are examples of modern, cloud-based data warehouses.</p>
<div class="no-row-height column-margin column-container"><div id="fn31"><p><sup>31</sup>&nbsp;<strong>MySQL at Scale</strong>: Originally developed by MySQL AB in 1995, MySQL powers 39% of all websites including Facebook, Twitter, and YouTube. However, single MySQL instances typically max out at 10-50TB before requiring complex sharding strategies that make ML feature extraction significantly more complex.</p></div><div id="fn32"><p><sup>32</sup>&nbsp;<strong>BigQuery Serverless Power</strong>: Google BigQuery can scan petabytes in seconds using thousands of parallel workers. Its columnar storage and automatic query optimization enables ML feature extraction from trillion-row tables in minutes—performance impossible with traditional databases at any scale. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.</p></div></div><section id="storage-architecture-decision-matrix" class="level4">
<h4 class="anchored" data-anchor-id="storage-architecture-decision-matrix">Storage Architecture Decision Matrix</h4>
<p>Choosing appropriate storage architecture requires systematic evaluation of requirements against system characteristics. The following decision matrix guides storage selection based on concrete ML workload patterns and constraints:</p>
<p><strong>Database Systems</strong> are optimal when data volume is under 1TB, query patterns involve frequent updates and complex joins, latency requirements demand sub-second response times, and strong consistency is mandatory. Common patterns include user profile management, real-time recommendation serving, and fraud detection feature stores. Avoid databases when analytical queries span large datasets, schema evolution is frequent, or storage costs exceed $500/TB/month.</p>
<p><strong>Data Warehouses</strong> excel for data volumes from 1TB to 100TB, when analytical query patterns dominate over transactional operations, when batch processing latency (minutes to hours) is acceptable, and when structured data with stable schemas represents the primary workload. Typical use cases include model training data preparation, batch feature engineering, and historical analysis. Migration from databases becomes necessary when query complexity increases or join operations span multiple gigabytes. Warehouses become inadequate when real-time streaming ingestion is required or when unstructured data comprises more than 20% of the workload.</p>
<p><strong>Data Lakes</strong> are essential for data volumes exceeding 100TB, when schema flexibility is critical for evolving data sources, when cost optimization is paramount (often 10x cheaper than warehouses), and when diverse data types (logs, images, audio, text) must coexist. Data lakes suit exploratory machine learning, large-scale model training, and multi-modal AI systems. However, they require sophisticated catalog management and metadata governance to prevent degradation into unusable “data swamps.”</p>
<p><strong>Migration Paths and Anti-Patterns</strong> follow predictable trajectories as systems scale. Database-to-warehouse migration typically occurs when analytical query performance degrades or storage costs become prohibitive. Warehouse-to-lake migration happens when schema rigidity constrains data ingestion or when cost optimization becomes critical. Common anti-patterns include using databases for analytical workloads exceeding 100GB, implementing data lakes without proper governance frameworks, or choosing warehouses for highly variable or unstructured data patterns.</p>
</section>
</section>
<section id="sec-data-engineering-storage-considerations-5f3e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-considerations-5f3e">Storage Considerations</h3>
<p>While traditional storage systems provide a foundation for ML workflows, the unique characteristics of machine learning workloads necessitate additional considerations. These ML-specific storage needs stem from the nature of ML development, training, and deployment processes, and addressing them is necessary for building efficient and scalable ML systems.</p>
<p>One of the primary challenges in ML storage is handling large model weights. Modern ML models can have millions or billions of numerical values that need to be stored. For instance, GPT-3<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>, a large language model, requires approximately 350 GB of storage just for the model weights <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Model Scaling Explosion</strong>: From AlexNet’s 60 million parameters <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span> (2012) to GPT-3’s 175 billion <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> (2020), model size grew 3,000x in 8 years. GPT-4’s rumored 1.7 trillion parameters would require 3.5 TB of storage—equivalent to 1,000 DVDs worth of model weights alone. This requirement exceeds traditional data storage and requires high-performance computing storage solutions.</p><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div></div><p>The iterative nature of ML development introduces another critical storage consideration: versioning for both datasets and models. Unlike traditional software version control, ML versioning needs to track large binary files efficiently. As data scientists experiment with different model architectures and hyperparameters, they generate numerous versions of models and datasets. Effective storage systems for ML must provide mechanisms to track these changes, revert to previous versions, and maintain reproducibility throughout the ML lifecycle. This capability is essential not only for development efficiency but also for regulatory compliance and model auditing in production environments.</p>
<p>Distributed training, often necessary for large models or datasets, generates substantial intermediate data, including partial model updates, gradients, and checkpoints. Storage systems for ML need to handle frequent, possibly concurrent, read and write operations of these intermediate results. They should also provide low-latency access to support efficient synchronization between distributed workers. This requirement pushes storage systems to balance between high throughput for large data transfers and low latency for quick synchronization operations.</p>
<p>The diversity of data types in ML workflows presents another unique challenge. ML systems often work with a wide variety of data, ranging from structured tabular data to unstructured images, audio, and text. Storage systems need to efficiently handle this diversity, often requiring a combination of different storage technologies optimized for specific data types. For instance, a single ML project might need to store and process tabular data in a columnar format for efficient feature extraction, while also managing large volumes of image data for computer vision tasks.</p>
<p>As organizations collect more data and create more sophisticated models, storage systems need to scale effectively. This scalability should support not just growing data volumes, but also increasing concurrent access from multiple data scientists and ML models. Cloud-based object storage systems have emerged as a popular solution due to their virtually unlimited scalability, but they introduce their own challenges in terms of data access latency and cost management.</p>
<p>The tension between sequential read performance for training and random access for inference is another key consideration. While training on large datasets benefits from high-throughput sequential reads, many ML serving scenarios require fast random access to individual data points or features. Storage systems for ML need to balance these potentially conflicting requirements, often leading to tiered storage architectures where frequently accessed data is kept in high-performance storage while less frequently used data is moved to cheaper, higher-latency storage.</p>
<p>The choice and configuration of storage systems can significantly impact the performance, cost-effectiveness, and overall success of ML initiatives. As the field of machine learning continues to evolve, storage solutions will need to adapt to meet the changing demands of increasingly sophisticated ML workflows.</p>
</section>
<section id="sec-data-engineering-performance-factors-56f2" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-performance-factors-56f2">Performance Factors</h3>
<p>The performance of storage systems is critical in ML workflows, directly impacting the efficiency of model training, the responsiveness of inference, and the overall productivity of data science teams. Understanding and optimizing storage performance requires a focus on several key metrics and strategies tailored to ML workloads.</p>
<p>One of the primary performance metrics for ML storage is throughput, particularly for large-scale data processing and model training. High throughput is essential when ingesting and preprocessing vast datasets or when reading large batches of data during model training. For instance, large-scale model training on large datasets may require sustained read throughput of several gigabytes per second to keep GPU accelerators fully utilized. This requirement explains why traditional spinning disks (100-200 MB/s) are inadequate for modern ML training, while SSD storage (1-7 GB/s) provides sufficient bandwidth for single-node training but may still bottleneck distributed training scenarios that require coordinating data across network storage systems (1-10 GB/s) or accessing large datasets that exceed local storage capacity.</p>
<p>Latency is another metric, especially for online inference and interactive data exploration. Low latency access to individual data points or small batches of data is vital for maintaining responsive ML services. In recommendation systems or real-time fraud detection, for example, storage systems must be able to retrieve relevant features or model parameters within milliseconds to meet strict service level agreements (SLAs).</p>
<p><strong>Storage Bottleneck Analysis</strong>: Understanding the quantitative relationship between storage performance and training throughput requires systems thinking about bandwidth hierarchies. Modern ML training throughput is fundamentally determined by the equation:</p>
<pre><code>Training Throughput = min(Compute Capacity, Data Supply Rate)
Data Supply Rate = Storage Bandwidth × (1 - Overhead)</code></pre>
<p>Consider a high-end GPU with 312 TFLOPS computational capacity processing ResNet-50 training. If the model requires 25 million parameters (100MB weights) plus 32 images × 150KB (5MB input data) per batch, the system moves 105MB per 4 billion operations—a ratio of 26 bytes per operation. When storage can only deliver 1 GB/s but the GPU could theoretically process 10 GB/s worth of data, the 10x bandwidth mismatch creates an immediate bottleneck. This analysis explains why traditional spinning disks (100-200 MB/s) fail completely for modern ML training, while even high-end SSDs (1-7 GB/s) may constrain distributed training scenarios where multiple accelerators compete for storage bandwidth.</p>
<p>This storage-compute mismatch becomes critical as models scale. Large language model training may require processing hundreds of gigabytes of text per hour, while computer vision models processing high-resolution imagery can demand sustained data rates exceeding 50 GB/s across distributed clusters. Understanding these quantitative relationships enables engineers to make informed architectural decisions about storage hierarchies, caching strategies, and data placement that directly impact training efficiency and cost.</p>
<p>The choice of file format can significantly impact both throughput and latency. Columnar storage formats<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> such as Parquet or ORC are particularly well-suited for ML workloads, delivering 5-10x I/O reduction compared to row-based formats like CSV or JSON. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. Consider a fraud detection dataset with 100 columns where models typically use only 20 features—columnar formats read only the needed columns, achieving 80% I/O reduction. Combined with column-level compression (often 20-50x for categorical features), columnar formats can achieve total I/O reduction of 20-100x compared to uncompressed row formats. This dramatic improvement directly translates to faster training iterations and reduced infrastructure costs, important for large-scale ML systems processing terabytes of data daily.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;<strong>Columnar Format Revolution</strong>: Columnar storage was pioneered by C-Store <span class="citation" data-cites="stonebraker2005cstore">(<a href="#ref-stonebraker2005cstore" role="doc-biblioref">Stonebraker et al. 2018</a>)</span> in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction.</p><div id="ref-stonebraker2005cstore" class="csl-entry" role="listitem">
Stonebraker, Mike, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, et al. 2018. <span>“C-Store: A Column-Oriented DBMS.”</span> In <em>Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker</em>, 491–518. Association for Computing Machinery. <a href="https://doi.org/10.1145/3226595.3226638">https://doi.org/10.1145/3226595.3226638</a>.
</div></div><div id="fn35"><p><sup>35</sup>&nbsp;<strong>Snappy Compression Trade-offs</strong>: Developed by Google (2011), Snappy achieves 250MB/s compression and 500MB/s decompression speeds—roughly 3-4x faster than gzip. While compression ratios are lower (2-3x vs gzip’s 6-8x), the speed advantage makes it ideal for ML pipelines where training throughput matters more than storage costs.</p></div></div><p>Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> or LZ4 being popular choices.</p>
<p>Data partitioning strategies play a role in optimizing query performance for ML workloads. By intelligently partitioning data based on frequently used query parameters (such as date ranges or categorical variables), systems can dramatically improve the efficiency of data retrieval operations. For instance, in a recommendation system processing user interactions, partitioning data by user demographic attributes and time periods can significantly speed up the retrieval of relevant training data for personalized models.</p>
<p>To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS (Hadoop Distributed File System)</a><a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> or cloud-based object stores like <a href="https://aws.amazon.com/s3/">Amazon S3</a>, distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows.</p>
<div class="no-row-height column-margin column-container"><div id="fn36"><p><sup>36</sup>&nbsp;<strong>HDFS Origins</strong>: HDFS was inspired by Google’s MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the “big data” revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems.</p></div><div id="fn37"><p><sup>37</sup>&nbsp;<strong>Redis Performance</strong>: Redis achieves sub-millisecond latency with 1M+ operations/second on modest hardware. Its in-memory architecture makes it ideal for ML feature serving, with companies like Twitter using Redis clusters to serve 400,000+ timeline requests per second for real-time recommendation systems.</p></div></div><p>Caching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems.</p>
<p>As ML workflows increasingly span from cloud to edge devices, storage performance considerations must extend to these distributed environments. Edge caching and intelligent data synchronization strategies become needed for maintaining performance in scenarios where network connectivity may be limited or unreliable. In the end, the goal is to create a storage infrastructure that can handle the volume and velocity of data in ML workflows while providing the low-latency access needed for responsive model training and inference.</p>
</section>
<section id="sec-data-engineering-storage-ml-lifecycle-a3a7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-ml-lifecycle-a3a7">Storage in ML Lifecycle</h3>
<p>The storage needs of machine learning systems evolve significantly across different phases of the ML lifecycle. Understanding these changing requirements is important for designing effective and efficient ML data infrastructures.</p>
<section id="sec-data-engineering-development-phase-ebf9" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-data-engineering-development-phase-ebf9">Development Phase</h4>
<p>In the development phase, storage systems play a critical role in supporting exploratory data analysis and iterative model development. This stage demands flexibility and collaboration, as data scientists often work with various datasets, experiment with feature engineering techniques, and rapidly iterate on model designs to refine their approaches.</p>
<p>One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>. This gap has led to the emergence of specialized tools like <a href="https://dvc.org/">DVC (Data Version Control)</a>, which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process.</p>
<div class="no-row-height column-margin column-container"><div id="fn38"><p><sup>38</sup>&nbsp;<strong>Data Versioning Challenges</strong>: Git’s inability to handle large binary files efficiently led to the “GitHub is not a CDN” problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets.</p></div></div><p>Balancing data accessibility and security further complicates the storage requirements in this phase. Data scientists require efficient access to datasets for experimentation, but organizations must simultaneously safeguard sensitive data. This tension often results in the implementation of sophisticated access control mechanisms, ensuring that datasets remain both accessible and protected. Secure data sharing systems enhance collaboration while adhering to strict organizational and regulatory requirements, enabling teams to work productively without compromising data integrity.</p>
</section>
<section id="sec-data-engineering-training-phase-b13d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-data-engineering-training-phase-b13d">Training Phase</h4>
<p>The training phase presents unique storage challenges due to the sheer volume of data processed and the computational intensity of model training. At this stage, the interplay between storage performance and computational efficiency becomes critical, as modern ML algorithms demand efficient integration between data access and processing.</p>
<p>To meet these demands, high-performance storage systems must provide the throughput required to feed data to multiple GPU or TPU accelerators simultaneously. Distributed training scenarios amplify this need, often requiring data transfer rates in the gigabytes per second range to ensure that accelerators remain fully utilized. Modern GPUs can process data faster than traditional storage can supply it: while RAM can deliver 50-200 GB/s bandwidth, network storage systems typically provide only 1-10 GB/s, and even high-end SSDs max out at 1-7 GB/s sequential throughput. This bandwidth hierarchy explains why distributed training often implements sophisticated data loading strategies, including data prefetching, parallel I/O, and strategic caching in RAM to bridge the performance gap between compute and storage capabilities.</p>
<p><strong>Data Movement Economics Analysis</strong> reveals why modern ML systems have shifted from compute-bound to data movement-bound workloads. Consider ResNet-50 training as a concrete example: the model contains 25 million parameters requiring 100MB of weights, while a typical batch of 32 images consumes 5MB of input data. During the forward pass, approximately 4 billion mathematical operations are performed, but the system must move 105MB of data (weights plus inputs). This yields a bytes-per-operation ratio of 26 bytes per operation—extraordinarily high compared to traditional computing workloads that typically operate with ratios below 1 byte per operation.</p>
<p>This shift has profound implications for system design. When a GPU can theoretically process 10 GB/s worth of computation but storage can only supply 1 GB/s of data, the 10x bandwidth mismatch creates an immediate bottleneck that no amount of computational optimization can resolve. The situation becomes more extreme with large language models, where parameter counts exceed billions and training batches may require gigabytes of data movement per iteration. Understanding these data movement economics enables engineers to make informed architectural decisions about data placement, storage hierarchies, and caching strategies that directly impact training efficiency and cost.</p>
<p>Beyond data ingestion, managing intermediate results and checkpoints is another critical challenge in the training phase. Long-running training jobs frequently save intermediate model states to allow for resumption in case of interruptions. These checkpoints can grow significantly in size, especially for large-scale models, necessitating storage solutions that enable efficient saving and retrieval without impacting overall performance.</p>
<p>Complementing these systems is the concept of burst buffers<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>, borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations.</p>
<div class="no-row-height column-margin column-container"><div id="fn39"><p><sup>39</sup>&nbsp;<strong>Burst Buffers</strong>: High-speed SSD-based storage layers that buffer data between slower traditional storage and fast compute. Originally developed for supercomputers, they’re now critical for ML training where GPUs can demand 100GB/s+ data rates—far exceeding traditional storage capabilities. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.</p></div></div></section>
<section id="sec-data-engineering-deployment-phase-a4c1" class="level4">
<h4 class="anchored" data-anchor-id="sec-data-engineering-deployment-phase-a4c1">Deployment Phase</h4>
<p>In the deployment and serving phase, the focus shifts from high-throughput batch operations during training to low-latency, often real-time, data access. This transition highlights the need to balance conflicting requirements, where storage systems must simultaneously support responsive model serving and enable continued learning in dynamic environments.</p>
<p>Real-time inference demands storage solutions capable of extremely fast access to model parameters and relevant features. To achieve this, systems often rely on in-memory databases or sophisticated caching strategies, ensuring that predictions can be made within milliseconds. These requirements become even more challenging in edge deployment scenarios, where devices operate with limited storage resources and intermittent connectivity to central data stores.</p>
<p>Adding to this complexity is the need to manage model updates in production environments. Storage systems must facilitate smooth transitions between model versions, ensuring minimal disruption to ongoing services. Techniques like shadow deployment, where new models run alongside existing ones for validation, allow organizations to iteratively roll out updates while monitoring their performance in real-world conditions.</p>
</section>
<section id="sec-data-engineering-maintenance-phase-86d3" class="level4">
<h4 class="anchored" data-anchor-id="sec-data-engineering-maintenance-phase-86d3">Maintenance Phase</h4>
<p>The monitoring and maintenance phase brings its own set of storage challenges, centered on ensuring the long-term reliability and performance of ML systems. At this stage, the focus shifts to capturing and analyzing data to monitor model behavior, detect issues, and maintain compliance with regulatory requirements.</p>
<p>A critical aspect of this phase is managing data drift, where the characteristics of incoming data change over time. Storage systems must efficiently capture and store incoming data along with prediction results, enabling ongoing analysis to detect and address shifts in data distributions. This ensures that models remain accurate and aligned with their intended use cases. For edge and mobile deployments discussed in <strong><a href="../core/ondevice_learning/ondevice_learning.html#sec-ondevice-learning">Chapter 13: On-Device Learning</a></strong>, data drift detection becomes particularly challenging due to storage constraints and intermittent connectivity, requiring specialized approaches for local adaptation and selective data transmission.</p>
<p>The sheer volume of logging and monitoring data generated by high-traffic ML services introduces questions of data retention and accessibility. Organizations must balance the need to retain historical data for analysis against the cost and complexity of storing it. Strategies such as tiered storage and compression can help manage costs while ensuring that critical data remains accessible when needed.</p>
<p>Regulated industries often require immutable storage to support auditing and compliance efforts. Storage systems designed for this purpose guarantee data integrity and non-repudiability, ensuring that stored data cannot be altered or deleted. Blockchain-inspired solutions and write-once-read-many (WORM) technologies are commonly employed to meet these stringent requirements.</p>
</section>
</section>
<section id="sec-data-engineering-feature-storage-3423" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-feature-storage-3423">Feature Storage</h3>
<p>Feature stores<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations.</p>
<div class="no-row-height column-margin column-container"><div id="fn40"><p><sup>40</sup>&nbsp;<strong>Feature Store Evolution</strong>: Feature stores were pioneered by Uber’s Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton.</p></div></div><p>What makes feature stores particularly interesting is their role in solving several key challenges in ML pipelines. First, they address the problem of feature consistency between training and serving environments. In traditional ML workflows, features are often computed differently in offline (training) and online (serving) environments, leading to discrepancies that can degrade model performance. Feature stores provide a single source of truth for feature definitions, ensuring consistency across all stages of the ML lifecycle.</p>
<p>Another fascinating aspect of feature stores is their ability to promote feature reuse across different models and teams within an organization. By centralizing feature computation and storage, feature stores can significantly reduce redundant work. For instance, if multiple teams are working on different models that require similar features (e.g., customer lifetime value in a retail context), these features can be computed once and reused across projects, improving efficiency and consistency.</p>
<p>Feature stores also play a role in managing the temporal aspects of features. Many ML use cases require correct point-in-time feature values, especially in scenarios involving time-series data or where historical context is important. Feature stores typically offer time-travel capabilities, allowing data scientists to retrieve feature values as they were at any point in the past. This is crucial for training models on historical data and for ensuring consistency between training and serving environments, as illustrated in <a href="#fig-feature-store-overview" class="quarto-xref">Figure&nbsp;15</a> which shows how data flows through these systems to eventually yield a model.</p>
<div id="fig-feature-store-overview" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-store-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="6c07b7c41e5d48f52b66da7790e17275138a59b4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Feature Store Architecture: Centralizing feature engineering and storage enables consistent feature access across model training and serving, resolving data inconsistencies and reducing redundant computation. Time-travel capabilities ensure point-in-time correctness, critical for historical analysis and maintaining model performance in dynamic environments."><img src="data_engineering_files/mediabag/6c07b7c41e5d48f52b66da7790e17275138a59b4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-store-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Feature Store Architecture</strong>: Centralizing feature engineering and storage enables consistent feature access across model training and serving, resolving data inconsistencies and reducing redundant computation. Time-travel capabilities ensure point-in-time correctness, critical for historical analysis and maintaining model performance in dynamic environments.
</figcaption>
</figure>
</div>
<p>The performance characteristics of feature stores are particularly intriguing from a storage perspective. They need to support both high-throughput batch retrieval for model training and low-latency lookups for online inference. This often leads to hybrid architectures where feature stores maintain both an offline store (optimized for batch operations) and an online store (optimized for real-time serving). Synchronization between these stores becomes a critical consideration.</p>
<p>Feature stores also introduce interesting challenges in terms of data freshness and update strategies. Some features may need to be updated in real-time (e.g., current user session information), while others might be updated on a daily or weekly basis (e.g., aggregated customer behavior metrics). Managing these different update frequencies and ensuring that the most up-to-date features are always available for inference can be complex.</p>
<p>From a storage perspective, feature stores often leverage a combination of different storage technologies to meet their diverse requirements. This might include columnar storage formats like Parquet for the offline store, in-memory databases or key-value stores for the online store, and streaming platforms like Apache Kafka for real-time feature updates.</p>
<p><strong>Production Operational Complexity</strong> emerges as feature stores scale to handle enterprise workloads. Feature freshness guarantees become critical when serving real-time models—late features can cause prediction accuracy to degrade, but implementing strict SLAs requires sophisticated monitoring and fallback strategies. When feature computation fails or produces partial data, systems must decide whether to serve stale features, computed defaults, or fail requests entirely. Each choice affects model performance differently depending on feature importance and business requirements.</p>
<p><strong>Cost Management for Real-Time Serving</strong> represents a significant operational challenge. In-memory feature stores can consume hundreds of gigabytes of RAM for large feature sets, making cost optimization critical. Intelligent caching strategies must balance feature access patterns against memory costs—frequently accessed features justify RAM costs while rarely used features can tolerate higher latency storage. Real-time feature computation for streaming data often requires dedicated compute resources that must be sized for peak load but operate efficiently during normal periods.</p>
<p><strong>Feature Store Migration and Vendor Strategy</strong> requires careful planning as organizations mature their ML infrastructure. Migrating from homegrown feature systems to commercial solutions involves complex data migration, schema evolution, and application integration challenges. Vendor changes become particularly complex when feature definitions are tightly coupled to specific feature store APIs or when large volumes of historical features must be migrated without disrupting production services. Successful feature store implementations maintain abstraction layers that enable vendor-agnostic feature access patterns.</p>
</section>
<section id="sec-data-engineering-caching-techniques-ac59" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-caching-techniques-ac59">Caching Techniques</h3>
<p>Caching plays a role in optimizing the performance of ML systems, particularly in scenarios involving frequent data access or computation-intensive operations. In the context of machine learning, caching strategies extend beyond traditional web or database caching, addressing unique challenges posed by ML workflows.</p>
<p>One of the primary applications of caching in ML systems is in feature computation and serving. Many features used in ML models are computationally expensive to calculate, especially those involving complex aggregations or time-window operations. By caching these computed features, systems can significantly reduce latency in both training and inference scenarios. For instance, in a recommendation system, caching user embedding vectors can dramatically speed up the generation of personalized recommendations.</p>
<p>Caching strategies in ML systems often need to balance between memory usage and computation time. This trade-off is particularly evident in large-scale distributed training scenarios. Caching frequently accessed data shards or mini-batches in memory can significantly reduce I/O overhead, but it requires careful memory management to avoid out-of-memory errors, especially when working with large datasets or models. These memory optimization decisions become increasingly important when considering the environmental impact discussed in <strong><a href="../core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 17: Sustainable AI</a></strong>, where energy-efficient data access patterns can significantly reduce the carbon footprint of large-scale ML training.</p>
<p>Another interesting application of caching in ML systems is model caching. In scenarios where multiple versions of a model are deployed (e.g., for A/B testing or gradual rollout), caching the most frequently used model versions in memory can significantly reduce inference latency. This becomes especially important in edge computing scenarios, where storage and computation resources are limited.</p>
<p>Caching also plays a vital role in managing intermediate results in ML pipelines. For instance, in feature engineering pipelines that involve multiple transformation steps, caching intermediate results can prevent redundant computations when rerunning pipelines with minor changes. This is particularly useful during the iterative process of model development and experimentation.</p>
<p>One of the challenges in implementing effective caching strategies for ML is managing cache invalidation and updates. ML systems often deal with dynamic data where feature values or model parameters may change over time. Implementing efficient cache update mechanisms that balance between data freshness and system performance is an ongoing area of research and development.</p>
<p>Distributed caching becomes particularly important in large-scale ML systems. Technologies like Redis or Memcached are often employed to create distributed caching layers that can serve multiple training or inference nodes. These distributed caches need to handle challenges like maintaining consistency across nodes and managing failover scenarios.</p>
<p>Edge caching is another fascinating area in ML systems, especially with the growing trend of edge AI. In these scenarios, caching strategies need to account for limited storage and computational resources on edge devices, as well as potentially intermittent network connectivity. Intelligent caching strategies that prioritize the most relevant data or model components for each edge device can significantly improve the performance and reliability of edge ML systems.</p>
<p>Lastly, the concept of semantic caching is gaining traction in ML systems. Unlike traditional caching that operates on exact matches, semantic caching attempts to reuse cached results for semantically similar queries. This can be particularly useful in ML systems where slight variations in input may not significantly change the output, potentially leading to substantial performance improvements.</p>
</section>
<section id="sec-data-engineering-data-access-patterns-f6e7" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-access-patterns-f6e7">Data Access Patterns</h3>
<p>Understanding the access patterns in ML systems is useful for designing efficient storage solutions and optimizing the overall system performance. ML workloads exhibit distinct data access patterns that often differ significantly from traditional database or analytics workloads.</p>
<p>One of the most prominent access patterns in ML systems is sequential reading of large datasets during model training. Unlike transactional systems that typically access small amounts of data randomly, ML training often involves reading entire datasets multiple times (epochs) in a sequential manner. This pattern is particularly evident in ML training tasks, where large volumes of data are processed repeatedly to improve model performance. Storage systems optimized for high-throughput sequential reads, such as distributed file systems or object stores, are well-suited for this access pattern.</p>
<p>However, the sequential read pattern is often combined with random shuffling between epochs to prevent overfitting and improve model generalization. This introduces an interesting challenge for storage systems, as they need to efficiently support both sequential and random access patterns, often within the same training job.</p>
<p>In contrast to the bulk sequential reads common in training, inference workloads often require fast random access to specific data points or features. For example, a recommendation system might need to quickly retrieve user and item features for real-time personalization. This necessitates storage solutions with low-latency random read capabilities, often leading to the use of in-memory databases or caching layers.</p>
<p>Feature stores, which we discussed earlier, introduce their own unique access patterns. They typically need to support both high-throughput batch reads for offline training and low-latency point lookups for online inference. This dual-nature access pattern often leads to the implementation of separate offline and online storage layers, each optimized for its specific access pattern.</p>
<p>Time-series data, common in many ML applications such as financial forecasting or IoT analytics, presents another interesting access pattern. These workloads often involve reading contiguous blocks of time-ordered data, but may also require efficient retrieval of specific time ranges or periodic patterns. Specialized time-series databases or carefully designed partitioning schemes in general-purpose databases are often employed to optimize these access patterns.</p>
<p>Another important consideration is the write access pattern in ML systems. While training workloads are often read-heavy, there are scenarios that involve significant write operations. For instance, continual learning systems may frequently update model parameters, and online learning systems may need to efficiently append new training examples to existing datasets.</p>
<p>Understanding these diverse access patterns is helpful in designing and optimizing storage systems for ML workloads. It often leads to hybrid storage architectures that combine different technologies to address various access patterns efficiently. For example, a system might use object storage for large-scale sequential reads during training, in-memory databases for low-latency random access during inference, and specialized time-series storage for temporal data analysis.</p>
<p>As ML systems continue to evolve, new access patterns are likely to emerge, driving further innovation in storage technologies and architectures. The challenge lies in creating flexible, scalable storage solutions that can efficiently support the diverse and often unpredictable access patterns of modern ML workloads.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-11a6" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-11a6">Continuing the KWS Example</h3>
<p>During development and training, KWS systems must efficiently store and manage large collections of audio data. This includes raw audio recordings from various sources (crowd-sourced, synthetic, and real-world captures), processed features (like spectrograms or MFCCs), and model checkpoints. A typical architecture might use a data lake for raw audio files, allowing flexible storage of diverse audio formats, while processed features are stored in a more structured data warehouse for efficient access during training.</p>
<p>KWS systems benefit significantly from feature stores, particularly for managing pre-computed audio features. For example, commonly used audio representations can be computed once and stored for reuse across different experiments or model versions. The feature store must handle both batch access for training and real-time access for inference, often implementing a dual storage architecture, which includes an offline store for training data and an online store for low-latency inference.</p>
<p>In production, KWS systems require careful consideration of edge storage requirements. The models must be compact enough to fit on resource-constrained devices while maintaining quick access to necessary parameters for real-time wake word detection. This often involves optimized storage formats and careful caching strategies to balance between memory usage and inference speed.</p>
<div id="quiz-question-sec-data-engineering-data-storage-6296" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which storage system is best suited for handling large volumes of unstructured data in ML workflows?</p>
<ol type="a">
<li>Conventional Database</li>
<li>Data Warehouse</li>
<li>In-memory Database</li>
<li>Data Lake</li>
</ol></li>
<li><p>Explain the trade-offs between using a data warehouse and a data lake for model training in a machine learning system.</p></li>
<li><p>What is a key advantage of using a data lake in the early stages of the ML lifecycle?</p>
<ol type="a">
<li>High performance for transactional queries</li>
<li>Flexibility in handling diverse data types</li>
<li>Optimized for structured data analysis</li>
<li>Low latency for real-time inference</li>
</ol></li>
<li><p>In a production ML system, how would you address the challenge of balancing high-throughput data access for training and low-latency access for inference?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-storage-6296" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-governance-f561" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-governance-f561">Data Governance</h2>
<p>Data governance is a significant component in the development and deployment of ML systems. It encompasses a set of practices and policies that ensure data is accurate, secure, compliant, and ethically used throughout the ML lifecycle. As ML systems become increasingly integral to decision-making processes across various domains, the importance of robust data governance has grown significantly.</p>
<p>One of the central challenges of data governance is addressing the unique complexities posed by ML workflows. These workflows often involve opaque processes, such as feature engineering and model training, which can obscure how data is being used. As shown in <a href="#fig-data-governance-pillars" class="quarto-xref">Figure&nbsp;16</a>, governance practices aim to tackle these issues by focusing on maintaining data privacy, ensuring fairness, and providing transparency in decision-making processes. These practices go beyond traditional data management to address the evolving needs of ML systems.</p>
<div id="fig-data-governance-pillars" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-governance-pillars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9317061a5de6846f3ef7bfa46e760b22f51b49e4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;16: Data Governance Pillars: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes."><img src="data_engineering_files/mediabag/9317061a5de6846f3ef7bfa46e760b22f51b49e4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-governance-pillars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: <strong>Data Governance Pillars</strong>: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes.
</figcaption>
</figure>
</div>
<p>Security and access control form an essential aspect of data governance. Implementing measures to protect data from unauthorized access or breaches is critical in ML systems, which often deal with sensitive or proprietary information. For instance, a healthcare application may require granular access controls to ensure that only authorized personnel can view patient data. Encrypting data both at rest and in transit is another common approach to safeguarding information while enabling secure collaboration among ML teams.</p>
<p>Privacy protection is another key pillar of data governance. As ML models often rely on large-scale datasets, there is a risk of infringing on individual privacy rights. Techniques such as differential privacy can address this concern by adding carefully calibrated noise to the data. This ensures that individual identities are protected while preserving the statistical patterns necessary for model training. These techniques allow ML systems to benefit from data-driven insights without compromising ethical considerations <span class="citation" data-cites="dwork2008differential">(<a href="#ref-dwork2008differential" role="doc-biblioref">Dwork, n.d.</a>)</span>, which we will learn more about in the Responsible AI chapter.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dwork2008differential" class="csl-entry" role="listitem">
Dwork, Cynthia. n.d. <span>“Differential Privacy: A Survey of Results.”</span> In <em>Theory and Applications of Models of Computation</em>, 1–19. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-540-79228-4\_1">https://doi.org/10.1007/978-3-540-79228-4\_1</a>.
</div><div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.”</span> <em>SSRN Electronic Journal</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div></div><p>Regulatory compliance is a critical area where data governance plays a central role. Laws such as the GDPR in Europe and the HIPAA in the United States impose strict requirements on data handling. Compliance with these regulations often involves implementing features like the ability to delete data upon request or providing individuals with copies of their data, and a “right to explanation” on decisions made by algorithms <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>. Standardized documentation frameworks like data cards (<a href="#fig-data-card" class="quarto-xref">Figure&nbsp;17</a>) provide structured approaches to document dataset characteristics, limitations, and compliance requirements. These measures not only protect individuals but also ensure organizations avoid legal and reputational risks.</p>
<div id="fig-data-card" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t!" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="aada1b08b7a12478c49c8c470f70a988a3c68f5f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;17: Data Governance Documentation: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights."><img src="data_engineering_files/mediabag/aada1b08b7a12478c49c8c470f70a988a3c68f5f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: <strong>Data Governance Documentation</strong>: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights.
</figcaption>
</figure>
</div>
<p>Documentation and metadata management, which are often less discussed, are just as important for transparency and reproducibility in ML systems. Clear records of data lineage<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>, including how data flows and transforms throughout the ML pipeline, are essential for accountability.</p>
<div class="no-row-height column-margin column-container"><div id="fn41"><p><sup>41</sup>&nbsp;<strong>Data Lineage Systems</strong>: Track data from source to consumption across complex ML pipelines. Apache Atlas (originally Hortonworks, now Apache, 2015) and DataHub (LinkedIn, 2020) enable lineage tracking at enterprise scale. Critical for regulatory compliance—GDPR Article 30 requires detailed records of data processing activities, making lineage essential for demonstrating compliance. Standardized documentation frameworks, such as Data Cards proposed by <span class="citation" data-cites="pushkarna2022data">Pushkarna, Zaldivar, and Kjartansson (<a href="#ref-pushkarna2022data" role="doc-biblioref">2022</a>)</span>, offer a structured way to document the characteristics, limitations, and potential biases of datasets.</p><div id="ref-pushkarna2022data" class="csl-entry" role="listitem">
Pushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. <span>“Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI.”</span> In <em>2022 ACM Conference on Fairness Accountability and Transparency</em>, 1776–826. ACM. <a href="https://doi.org/10.1145/3531146.3533231">https://doi.org/10.1145/3531146.3533231</a>.
</div></div><div id="fn42"><p><sup>42</sup>&nbsp;<strong>ML Audit Requirements</strong>: SOX compliance requires immutable audit logs for financial ML models, while HIPAA mandates detailed access logs for healthcare AI. Modern systems generate terabytes of audit data—Uber’s ML platform logs 50+ billion events daily for compliance and debugging purposes. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance.</p></div></div><p>Audit trails<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> are another important component of data governance. These detailed logs track data access and usage throughout the lifecycle of ML models, from collection to deployment.</p>
<p>Consider a hypothetical ML system designed to predict patient outcomes in a hospital. Such a system would need to address several governance challenges. It would need to ensure that patient data is securely stored and accessed only by authorized personnel, with privacy-preserving techniques in place to protect individual identities. The system would also need to comply with healthcare regulations governing the use of patient data, including detailed documentation of how data is processed and transformed. Comprehensive audit logs would be necessary to track data usage and ensure accountability.</p>
<p>As ML systems grow more complex and influential, the challenges of data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> for tamper-evident logs and automated governance tools, offer promising solutions for real-time monitoring and issue detection.</p>
<div class="no-row-height column-margin column-container"><div id="fn43"><p><sup>43</sup>&nbsp;<strong>Blockchain for ML Governance</strong>: Immutable ledgers provide tamper-proof audit trails for ML model decisions. Ocean Protocol and other projects use blockchain to track data provenance and usage rights. While promising for high-stakes applications like healthcare AI, blockchain’s energy costs and complexity limit widespread adoption. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy.</p></div></div><div id="quiz-question-sec-data-engineering-data-governance-f561" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following is NOT a pillar of data governance in machine learning systems?</p>
<ol type="a">
<li>Model Accuracy</li>
<li>Data Privacy</li>
<li>Transparency</li>
<li>Accountability</li>
</ol></li>
<li><p>Explain how differential privacy can be used to protect individual identities in machine learning datasets.</p></li>
<li><p>True or False: Data governance in ML systems only concerns data security and does not involve compliance with regulations.</p></li>
<li><p>A structured way to document dataset characteristics, intended uses, and potential risks is known as a ____.</p></li>
<li><p>In a production ML system designed to predict patient outcomes, what data governance challenges must be addressed?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-governance-f561" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="fallacies-and-pitfalls" class="level2">
<h2 class="anchored" data-anchor-id="fallacies-and-pitfalls">Fallacies and Pitfalls</h2>
<p>Data engineering forms the foundation of every ML system, yet it remains one of the most underestimated aspects of ML development. The complexity of managing data pipelines, ensuring quality, and maintaining governance creates numerous opportunities for costly mistakes that can undermine even the most sophisticated models.</p>
<p><strong>Fallacy:</strong> <em>More data always leads to better model performance.</em></p>
<p>This widespread belief drives teams to collect massive datasets without considering data quality or relevance. While more data can improve performance when properly curated, raw quantity often introduces noise, inconsistencies, and irrelevant examples that degrade model performance. A smaller, high-quality dataset with proper labeling and representative coverage typically outperforms a larger dataset with quality issues. The computational costs and storage requirements of massive datasets also create practical constraints that limit experimentation and deployment options. Effective data engineering prioritizes data quality and representativeness over sheer volume.</p>
<p><strong>Pitfall:</strong> <em>Treating data labeling as a simple mechanical task that can be outsourced without oversight.</em></p>
<p>Organizations often view data labeling as low-skill work that can be completed quickly by external teams or crowdsourcing platforms. This approach ignores the domain expertise, consistency requirements, and quality control necessary for reliable labels. Poor labeling guidelines, inadequate worker training, and insufficient quality validation lead to noisy labels that fundamentally limit model performance. The cost of correcting labeling errors after they affect model training far exceeds the investment in proper labeling infrastructure and oversight.</p>
<p><strong>Fallacy:</strong> <em>Data engineering is a one-time setup that can be completed before model development begins.</em></p>
<p>This misconception treats data pipelines as static infrastructure rather than evolving systems that require continuous maintenance and adaptation. Real-world data sources change over time through schema evolution, quality degradation, and distribution shifts. Models deployed in production encounter new data patterns that require pipeline updates and quality checks. Teams that view data engineering as completed infrastructure rather than ongoing engineering practice often experience system failures when their pipelines cannot adapt to changing requirements.</p>
<p><strong>Fallacy:</strong> <em>Training and test data splitting is sufficient to ensure model generalization.</em></p>
<p>While proper train/test splitting prevents overfitting to training data, it doesn’t guarantee real-world performance. Production data often differs significantly from development datasets due to temporal shifts, geographic variations, or demographic changes. A model achieving 95% accuracy on a carefully curated test set may fail catastrophically when deployed to new regions or time periods. Robust evaluation requires understanding data collection biases, implementing continuous monitoring, and maintaining representative validation sets that reflect actual deployment conditions.</p>
<p><strong>Pitfall:</strong> <em>Building data pipelines without considering failure modes and recovery mechanisms.</em></p>
<p>Data pipelines are often designed for the happy path where everything works correctly, ignoring the reality that data sources fail, formats change, and quality degrades. Teams discover these issues only when production systems crash or silently produce incorrect results. A pipeline processing financial transactions that lacks proper error handling for malformed data could lose critical records or duplicate transactions. Robust data engineering requires explicit handling of failures including data validation, checkpointing, rollback capabilities, and alerting mechanisms that detect anomalies before they impact downstream systems.</p>
</section>
<section id="sec-data-engineering-summary-9702" class="level2">
<h2 class="anchored" data-anchor-id="sec-data-engineering-summary-9702">Summary</h2>
<p>Data engineering serves as the foundational infrastructure that transforms raw information into the foundation of machine learning systems, determining not just model performance but also system reliability, ethical compliance, and long-term maintainability. This chapter revealed how every stage of the data pipeline, from initial problem definition through acquisition, storage, and governance, requires careful engineering decisions that cascade through the entire ML lifecycle. The seemingly straightforward task of “getting data ready” actually encompasses complex trade-offs between data quality and acquisition cost, real-time processing and batch efficiency, storage flexibility and query performance, and privacy protection and data utility.</p>
<p>The technical architecture of data systems demonstrates how engineering decisions compound across the pipeline to create either robust, scalable foundations or brittle, maintenance-heavy technical debt. Data acquisition strategies must navigate the reality that perfect datasets rarely exist in nature, requiring sophisticated approaches ranging from crowdsourcing and synthetic generation to careful curation and active learning. Storage architectures from traditional databases to modern data lakes and feature stores represent fundamental choices about how data flows through the system, affecting everything from training speed to serving latency. The emergence of streaming data processing and real-time feature stores reflects the growing demand for ML systems that can adapt continuously to changing environments while maintaining consistency and reliability.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Data quality decisions made early in the pipeline have multiplicative effects on downstream model performance and system reliability</li>
<li>The choice between batch and streaming processing architectures fundamentally shapes system capabilities and operational complexity</li>
<li>Feature stores and data governance frameworks are becoming essential infrastructure for production ML systems at scale</li>
<li>Effective data labeling requires both technical systems and human process design to ensure quality and ethical compliance</li>
</ul>
</div>
</div>
<p>The integration of robust data governance practices throughout the pipeline ensures that ML systems remain trustworthy, compliant, and transparent as they scale in complexity and impact. Data cards, lineage tracking, and automated monitoring create the observability needed to detect data drift, privacy violations, and quality degradation before they affect model behavior. These engineering foundations enable the distributed training strategies in <strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>, model optimization techniques in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>, and MLOps practices in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, where reliable data infrastructure becomes the prerequisite for scaling ML systems effectively.</p>


<div id="quiz-question-sec-data-engineering-summary-9702" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which of the following strategies is NOT typically considered when acquiring data for ML systems?</p>
<ol type="a">
<li>Utilizing existing datasets</li>
<li>Manual data entry by developers</li>
<li>Crowdsourcing</li>
<li>Web scraping</li>
</ol></li>
<li><p>True or False: Data governance in ML systems only concerns the security of data.</p></li>
<li><p>Explain how the choice between ETL and ELT can impact the cost and throughput of a data pipeline.</p></li>
<li><p>Order the following data engineering processes as they typically occur: (1) Data Labeling, (2) Data Acquisition, (3) Data Transformation.</p></li>
<li><p>In a production ML system, how might effective data governance influence the system’s ethical standing?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-summary-9702" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-data-engineering-problem-definition-f820" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>What is a ‘Data Cascade’ in the context of machine learning systems?</strong></p>
<ol type="a">
<li>A process where data errors compound, affecting downstream stages</li>
<li>A series of algorithmic improvements</li>
<li>A method of data augmentation</li>
<li>A type of neural network architecture</li>
</ol>
<p><em>Answer</em>: The correct answer is A. A process where data errors compound, affecting downstream stages. This is correct because data cascades refer to the amplification of data quality issues through the stages of an ML pipeline, leading to negative outcomes.</p>
<p><em>Learning Objective</em>: Understand the concept of data cascades and their impact on ML systems.</p></li>
<li><p><strong>Why is it crucial to define the problem clearly before collecting data in an ML project?</strong></p>
<p><em>Answer</em>: Defining the problem clearly ensures that data collection efforts are aligned with the project’s objectives, preventing wasted resources on irrelevant data. For example, identifying the specific keywords for a KWS system ensures the data collected is relevant and useful. This is important because it sets a solid foundation for the entire ML workflow, reducing the risk of data cascades.</p>
<p><em>Learning Objective</em>: Explain the importance of clear problem definition in guiding data collection and avoiding data quality issues.</p></li>
<li><p><strong>Which of the following is NOT a benchmark for success in a Keyword Spotting (KWS) system?</strong></p>
<ol type="a">
<li>True Positive Rate</li>
<li>False Positive Rate</li>
<li>Model Training Time</li>
<li>Power Consumption</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Model Training Time. This is correct because while true positive rate, false positive rate, and power consumption are direct performance metrics for evaluating a KWS system, model training time is not typically a benchmark for its success.</p>
<p><em>Learning Objective</em>: Identify relevant benchmarks for evaluating the success of ML systems, specifically KWS.</p></li>
<li><p><strong>In a production system, how might poor data quality at the data collection stage affect the deployment phase?</strong></p>
<p><em>Answer</em>: Poor data quality at the collection stage can lead to inaccurate models that perform poorly in real-world scenarios, necessitating costly redeployment or model retraining. For instance, if a KWS system is trained on poor quality audio data, it may fail to recognize keywords accurately, leading to user dissatisfaction. This is important because it underscores the need for robust data quality checks early in the ML pipeline to prevent cascading failures.</p>
<p><em>Learning Objective</em>: Analyze the implications of data quality issues on the deployment phase of ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-problem-definition-f820" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-pipeline-basics-31ba" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which component of a data pipeline is primarily responsible for transforming raw data into a format suitable for model training?</strong></p>
<ol type="a">
<li>Processing Layer</li>
<li>Data Ingestion</li>
<li>Storage Layer</li>
<li>Data Sources</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Processing Layer. This is correct because the processing layer handles data transformations, quality checks, and feature engineering, which are crucial for preparing data for model training. Other options focus on different aspects like data collection or storage.</p>
<p><em>Learning Objective</em>: Understand the role of the processing layer in data pipelines.</p></li>
<li><p><strong>Explain how data quality is maintained throughout a data pipeline and why it is crucial for machine learning systems.</strong></p>
<p><em>Answer</em>: Data quality is maintained through validation checks, transformations, and feature engineering in the processing layer. It is crucial because it ensures that the data used for training is accurate and reliable, which directly impacts model performance. For example, poor data quality can lead to inaccurate predictions and unreliable models. This is important because reliable data is foundational for effective ML systems.</p>
<p><em>Learning Objective</em>: Analyze the importance of data quality in the context of ML data pipelines.</p></li>
<li><p><strong>Order the following stages in a typical ML data pipeline: (1) Data Ingestion, (2) Model Training, (3) Data Labeling, (4) Storage Layer.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Data Ingestion, (4) Storage Layer, (3) Data Labeling, (2) Model Training. Data ingestion collects data, which is then stored. Data labeling is often required before training, and finally, the labeled data is used for model training.</p>
<p><em>Learning Objective</em>: Understand the sequential steps involved in an ML data pipeline.</p></li>
<li><p><strong>In a production system handling real-time data, which data pipeline component is critical for ensuring data is processed as it arrives?</strong></p>
<ol type="a">
<li>Batch Ingestion</li>
<li>Feature Engineering</li>
<li>Data Validation</li>
<li>Stream Processing</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Stream Processing. This is because stream processing is designed to handle data in real-time, ensuring that data is processed as it arrives. Batch ingestion and other components do not provide real-time processing capabilities.</p>
<p><em>Learning Objective</em>: Identify components critical for real-time data processing in ML pipelines.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-pipeline-basics-31ba" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-sources-c8d9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary advantage of using existing datasets like ImageNet for training ML systems?</strong></p>
<ol type="a">
<li>They are always perfectly labeled and error-free.</li>
<li>They provide immediate access to large volumes of data with established benchmarks.</li>
<li>They guarantee the elimination of biases in training data.</li>
<li>They are always cheaper than collecting new data.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. They provide immediate access to large volumes of data with established benchmarks. This is correct because existing datasets allow for quick start in ML projects and enable performance comparisons. Options A, C, and D are incorrect as datasets may have errors, biases, and costs can vary.</p>
<p><em>Learning Objective</em>: Understand the benefits and limitations of using existing datasets in ML systems.</p></li>
<li><p><strong>True or False: Web scraping is a universally accepted method for gathering training data without any legal or ethical concerns.</strong></p>
<p><em>Answer</em>: False. This is false because web scraping can involve legal and ethical issues, such as violating website terms of service or privacy laws.</p>
<p><em>Learning Objective</em>: Recognize the legal and ethical considerations involved in web scraping for data collection.</p></li>
<li><p><strong>Explain how the use of crowdsourcing for data collection can impact the quality and diversity of datasets in ML systems.</strong></p>
<p><em>Answer</em>: Crowdsourcing can enhance dataset quality and diversity by leveraging a wide range of contributors to provide varied perspectives and data points. For example, platforms like Amazon Mechanical Turk enable the collection of diverse linguistic and cultural data, improving model generalization. However, quality control measures are essential to ensure accuracy and consistency. This is important because diverse datasets help models generalize better across different populations and conditions.</p>
<p><em>Learning Objective</em>: Analyze the impact of crowdsourcing on dataset quality and diversity in ML systems.</p></li>
<li><p><strong>What is a potential drawback of relying heavily on synthetic data for training ML models?</strong></p>
<ol type="a">
<li>Synthetic data can introduce biases if not properly validated against real-world benchmarks.</li>
<li>Synthetic data is always more accurate than real-world data.</li>
<li>Synthetic data is always easier to generate than real-world data.</li>
<li>Synthetic data eliminates the need for any real-world data.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Synthetic data can introduce biases if not properly validated against real-world benchmarks. This is correct because synthetic data may not perfectly represent real-world distributions, leading to biased models. Options A, C, and D are incorrect as synthetic data is not inherently more accurate, easier to generate, or a complete replacement for real data.</p>
<p><em>Learning Objective</em>: Evaluate the limitations and considerations of using synthetic data in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-sources-c8d9" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-ingestion-5dfc" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the primary difference between batch ingestion and stream ingestion in ML systems?</strong></p>
<ol type="a">
<li>Batch ingestion processes data in real-time, while stream ingestion processes data in batches.</li>
<li>Batch ingestion is suitable for real-time applications, whereas stream ingestion is used for historical data.</li>
<li>Batch ingestion requires more computational resources than stream ingestion.</li>
<li>Batch ingestion processes data at scheduled intervals, while stream ingestion processes data as it arrives.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Batch ingestion processes data at scheduled intervals, while stream ingestion processes data as it arrives. This distinction is important for choosing the right pattern based on the application’s real-time processing needs.</p>
<p><em>Learning Objective</em>: Understand the fundamental differences between batch and stream ingestion patterns.</p></li>
<li><p><strong>Explain how the choice between ETL and ELT approaches can impact the flexibility and efficiency of an ML pipeline.</strong></p>
<p><em>Answer</em>: ETL involves transforming data before loading, resulting in data that’s ready-to-query, which is efficient for stable schemas but less flexible for changing requirements. ELT loads raw data first, allowing transformations as needed, offering flexibility for evolving schemas but requiring robust storage and query capabilities. This choice affects how quickly an ML system can adapt to new data requirements and the resources needed for data processing.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between ETL and ELT in terms of flexibility and efficiency for ML pipelines.</p></li>
<li><p><strong>True or False: In a hybrid data ingestion system, combining both batch and stream ingestion patterns is beneficial for handling different data velocities and use cases.</strong></p>
<p><em>Answer</em>: True. Combining batch and stream ingestion allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape. This hybrid approach offers flexibility to handle varying data velocities and use cases.</p>
<p><em>Learning Objective</em>: Evaluate the benefits of using a hybrid approach in data ingestion systems.</p></li>
<li><p><strong>In a production ML system, what error management strategies would you implement to ensure reliable data ingestion?</strong></p>
<p><em>Answer</em>: Implement graceful degradation to maintain system functionality during partial data loss, use intelligent retry logic for transient errors, and employ dead letter queues for storing failed data for later analysis. These strategies ensure that the system can continue operating and that data quality issues are addressed promptly, maintaining the reliability of the ML pipeline.</p>
<p><em>Learning Objective</em>: Design error management strategies for robust data ingestion in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-ingestion-5dfc" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-processing-c336" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the primary difference between ETL and ELT workflows in data processing?</strong></p>
<ol type="a">
<li>ETL processes data after it is loaded into the target system, while ELT processes it before loading.</li>
<li>ETL is used for structured data, while ELT is used for unstructured data.</li>
<li>ETL is more flexible than ELT in handling unknown transformations.</li>
<li>ETL involves transforming data before loading into the target system, whereas ELT transforms data after loading.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. ETL involves transforming data before loading into the target system, whereas ELT transforms data after loading. This distinction impacts when and how data processing occurs in ML systems.</p>
<p><em>Learning Objective</em>: Understand the fundamental differences between ETL and ELT workflows and their implications for data processing.</p></li>
<li><p><strong>Explain how data cleaning can impact the performance of machine learning models.</strong></p>
<p><em>Answer</em>: Data cleaning improves model performance by removing errors, inconsistencies, and inaccuracies that could lead to incorrect predictions. For example, handling missing values and standardizing formats ensures that the model learns from accurate and consistent data, reducing bias and variance in predictions. This is important because cleaner data leads to more reliable and generalizable models.</p>
<p><em>Learning Objective</em>: Analyze the role of data cleaning in enhancing the reliability and accuracy of machine learning models.</p></li>
<li><p><strong>True or False: In a data processing pipeline, feature engineering is considered more of an art than a science.</strong></p>
<p><em>Answer</em>: True. Feature engineering often relies on domain knowledge and creativity to create new features that improve model performance, making it more of an art than a science.</p>
<p><em>Learning Objective</em>: Recognize the subjective and creative aspects of feature engineering in data processing.</p></li>
<li><p><strong>Order the following data processing steps in a typical ETL workflow: (1) Load data into the target system, (2) Transform data, (3) Extract data from source.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Extract data from source, (2) Transform data, (1) Load data into the target system. In ETL, data is first extracted, then transformed, and finally loaded into the target system.</p>
<p><em>Learning Objective</em>: Understand the sequential steps involved in an ETL data processing workflow.</p></li>
<li><p><strong>In a production system handling large datasets, what scalability considerations should be taken into account during data processing?</strong></p>
<p><em>Answer</em>: Scalability considerations include using distributed computing frameworks like Apache Spark to process data across multiple machines, balancing preprocessing with on-the-fly computation to manage storage and data freshness, and optimizing resource allocation to handle large data volumes efficiently. This is important because scalable data processing ensures timely and cost-effective handling of data as system demands grow.</p>
<p><em>Learning Objective</em>: Evaluate scalability strategies for efficient data processing in large-scale ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-processing-c336" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-labeling-95e7" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key challenge when integrating data labeling workflows into machine learning pipelines?</strong></p>
<ol type="a">
<li>Maintaining data consistency and label quality</li>
<li>Ensuring the availability of large datasets</li>
<li>Implementing basic data storage solutions</li>
<li>Focusing solely on algorithmic improvements</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Maintaining data consistency and label quality is a key challenge because it directly impacts the performance and reliability of ML systems. Other options are either less critical or unrelated to the specific challenges of labeling integration.</p>
<p><em>Learning Objective</em>: Understand the challenges of integrating data labeling workflows into ML pipelines.</p></li>
<li><p><strong>Explain how the choice of label type can impact the system architecture and resource requirements of an ML system.</strong></p>
<p><em>Answer</em>: The choice of label type, such as classification labels, bounding boxes, or segmentation maps, affects system architecture by dictating storage needs and processing power. For example, segmentation maps require more storage and processing resources than classification labels due to their detailed pixel-level information. This impacts how data is stored, retrieved, and processed, influencing overall system design and resource allocation.</p>
<p><em>Learning Objective</em>: Analyze how different label types influence ML system architecture and resource planning.</p></li>
<li><p><strong>Order the following steps in a typical data labeling workflow: (1) Data Collection, (2) Labeling, (3) Quality Control, (4) Integration with Training Pipeline.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Data Collection, (2) Labeling, (3) Quality Control, (4) Integration with Training Pipeline. This sequence ensures data is first collected, then labeled, followed by quality checks, and finally integrated into the training pipeline for model development.</p>
<p><em>Learning Objective</em>: Understand the sequential steps in a data labeling workflow within ML systems.</p></li>
<li><p><strong>True or False: Automated labeling systems require less computational resources than manual expert labeling.</strong></p>
<p><em>Answer</em>: False. Automated labeling systems often require substantial computational resources for inference and processing, especially when handling large datasets or employing complex models.</p>
<p><em>Learning Objective</em>: Evaluate the resource requirements of different labeling approaches.</p></li>
<li><p><strong>In a production system, how might you apply a hybrid approach to data labeling to balance speed, cost, and quality?</strong></p>
<p><em>Answer</em>: A hybrid approach can start with programmatic labeling for broad coverage, followed by crowdsourced verification to enhance quality, and expert review for uncertain cases. This balances speed and cost by leveraging automation and crowdsourcing while ensuring high-quality labels through expert oversight. Such a system requires careful design to manage data flow and maintain quality across stages.</p>
<p><em>Learning Objective</em>: Apply hybrid data labeling strategies in production ML systems to optimize speed, cost, and quality.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-labeling-95e7" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-storage-6296" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which storage system is best suited for handling large volumes of unstructured data in ML workflows?</strong></p>
<ol type="a">
<li>Conventional Database</li>
<li>Data Warehouse</li>
<li>In-memory Database</li>
<li>Data Lake</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Data Lake. Data lakes are designed to handle large volumes of structured, semi-structured, and unstructured data, making them ideal for diverse data types in ML workflows. Conventional databases and data warehouses are optimized for structured data and may not efficiently handle unstructured data.</p>
<p><em>Learning Objective</em>: Understand the suitability of different storage systems for various data types in ML workflows.</p></li>
<li><p><strong>Explain the trade-offs between using a data warehouse and a data lake for model training in a machine learning system.</strong></p>
<p><em>Answer</em>: Data warehouses offer structured environments optimized for analytical queries, facilitating data exploration and feature engineering, but may struggle with unstructured data. Data lakes handle diverse data types and evolving schemas, offering flexibility but requiring careful metadata management to avoid becoming disorganized. The choice depends on the data’s structure and the need for flexibility versus structured analysis.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between data warehouses and data lakes in the context of ML model training.</p></li>
<li><p><strong>What is a key advantage of using a data lake in the early stages of the ML lifecycle?</strong></p>
<ol type="a">
<li>High performance for transactional queries</li>
<li>Flexibility in handling diverse data types</li>
<li>Optimized for structured data analysis</li>
<li>Low latency for real-time inference</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Flexibility in handling diverse data types. Data lakes allow for storage of structured, semi-structured, and unstructured data in its native format, making them ideal for the exploratory and experimental nature of early ML development.</p>
<p><em>Learning Objective</em>: Identify the advantages of data lakes in supporting the flexibility required during the early stages of the ML lifecycle.</p></li>
<li><p><strong>In a production ML system, how would you address the challenge of balancing high-throughput data access for training and low-latency access for inference?</strong></p>
<p><em>Answer</em>: Implementing a tiered storage architecture can address this challenge. High-throughput storage systems, like distributed file systems, can be used for training data, while low-latency systems, such as in-memory databases, support inference. This separation allows optimization for each access pattern, ensuring efficient data handling across the ML lifecycle.</p>
<p><em>Learning Objective</em>: Design a storage strategy that balances the different data access requirements of training and inference in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-storage-6296" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-governance-f561" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is NOT a pillar of data governance in machine learning systems?</strong></p>
<ol type="a">
<li>Model Accuracy</li>
<li>Data Privacy</li>
<li>Transparency</li>
<li>Accountability</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Model Accuracy. Data governance focuses on privacy, transparency, and accountability, rather than directly on model accuracy. Model accuracy is more related to algorithmic performance, not governance.</p>
<p><em>Learning Objective</em>: Understand the key pillars of data governance in ML systems.</p></li>
<li><p><strong>Explain how differential privacy can be used to protect individual identities in machine learning datasets.</strong></p>
<p><em>Answer</em>: Differential privacy protects individual identities by adding noise to datasets, ensuring that statistical patterns are preserved without revealing personal information. This technique allows ML models to learn from data without compromising privacy, which is crucial for ethical data governance.</p>
<p><em>Learning Objective</em>: Understand the application of differential privacy in data governance.</p></li>
<li><p><strong>True or False: Data governance in ML systems only concerns data security and does not involve compliance with regulations.</strong></p>
<p><em>Answer</em>: False. Data governance encompasses both data security and compliance with regulations like GDPR and HIPAA, ensuring ethical and legal data handling.</p>
<p><em>Learning Objective</em>: Recognize the comprehensive role of data governance, including regulatory compliance.</p></li>
<li><p><strong>A structured way to document dataset characteristics, intended uses, and potential risks is known as a ____.</strong></p>
<p><em>Answer</em>: Data Card. Data Cards provide a standardized overview, facilitating transparency and accountability in ML systems.</p>
<p><em>Learning Objective</em>: Recall the purpose and function of Data Cards in data governance.</p></li>
<li><p><strong>In a production ML system designed to predict patient outcomes, what data governance challenges must be addressed?</strong></p>
<p><em>Answer</em>: Such a system must ensure secure storage and access control of patient data, comply with healthcare regulations, and maintain detailed audit logs for accountability. Privacy-preserving techniques and comprehensive documentation are also essential to protect individual identities and ensure transparency.</p>
<p><em>Learning Objective</em>: Apply data governance principles to a real-world ML system scenario.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-governance-f561" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-summary-9702" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following strategies is NOT typically considered when acquiring data for ML systems?</strong></p>
<ol type="a">
<li>Utilizing existing datasets</li>
<li>Manual data entry by developers</li>
<li>Crowdsourcing</li>
<li>Web scraping</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Manual data entry by developers. This is not a scalable or efficient strategy for data acquisition in ML systems, unlike the other options which are more commonly used.</p>
<p><em>Learning Objective</em>: Identify common data acquisition strategies used in ML systems.</p></li>
<li><p><strong>True or False: Data governance in ML systems only concerns the security of data.</strong></p>
<p><em>Answer</em>: False. Data governance also involves legal compliance, privacy protection, and maintaining stakeholder trust, beyond just security.</p>
<p><em>Learning Objective</em>: Understand the comprehensive role of data governance in ML systems.</p></li>
<li><p><strong>Explain how the choice between ETL and ELT can impact the cost and throughput of a data pipeline.</strong></p>
<p><em>Answer</em>: ETL (Extract, Transform, Load) processes data before loading, which can reduce storage costs but may slow down the pipeline. ELT (Extract, Load, Transform) loads data before processing, allowing for faster ingestion and flexibility, but may increase storage costs due to larger raw data volumes. This choice impacts how quickly data can be processed and the associated costs.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between ETL and ELT approaches in data pipelines.</p></li>
<li><p><strong>Order the following data engineering processes as they typically occur: (1) Data Labeling, (2) Data Acquisition, (3) Data Transformation.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Data Acquisition, (3) Data Transformation, (1) Data Labeling. Data is first acquired, then transformed into a usable format, and finally labeled for model training.</p>
<p><em>Learning Objective</em>: Understand the sequence of processes in data engineering for ML systems.</p></li>
<li><p><strong>In a production ML system, how might effective data governance influence the system’s ethical standing?</strong></p>
<p><em>Answer</em>: Effective data governance ensures compliance with legal regulations, protects user privacy, and maintains transparency, all of which contribute to the ethical standing of an ML system. For example, by implementing differential privacy, a system can protect individual data while still providing useful insights. This is important because it builds trust with users and stakeholders.</p>
<p><em>Learning Objective</em>: Evaluate the impact of data governance on the ethical considerations of ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-summary-9702" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">AI Workflow</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/frameworks/frameworks.html" class="pagination-link" aria-label="AI Frameworks">
        <span class="nav-page-text">AI Frameworks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>