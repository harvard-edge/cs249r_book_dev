<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/data_engineering/data_engineering.html" rel="next">
<link href="../../../contents/core/dnn_architectures/dnn_architectures.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">AI Workflow</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>🚧 DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">6fb63725</code> • 2025-10-03 00:17 UTC • <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version →</em></a></p>
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-ai-workflow" id="toc-sec-ai-workflow" class="nav-link active" data-scroll-target="#sec-ai-workflow">AI Workflow</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ai-workflow-overview-97fb" id="toc-sec-ai-workflow-overview-97fb" class="nav-link" data-scroll-target="#sec-ai-workflow-overview-97fb">Overview</a>
  <ul class="collapse">
  <li><a href="#lifecycle-integration-principles" id="toc-lifecycle-integration-principles" class="nav-link" data-scroll-target="#lifecycle-integration-principles">Lifecycle Integration Principles</a></li>
  <li><a href="#sec-ai-workflow-definition-d707" id="toc-sec-ai-workflow-definition-d707" class="nav-link" data-scroll-target="#sec-ai-workflow-definition-d707">Defining the ML Lifecycle</a></li>
  <li><a href="#sec-ai-workflow-traditional-vs-ai-lifecycles-4f66" id="toc-sec-ai-workflow-traditional-vs-ai-lifecycles-4f66" class="nav-link" data-scroll-target="#sec-ai-workflow-traditional-vs-ai-lifecycles-4f66">Traditional vs.&nbsp;AI Lifecycles</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-lifecycle-stages-3032" id="toc-sec-ai-workflow-lifecycle-stages-3032" class="nav-link" data-scroll-target="#sec-ai-workflow-lifecycle-stages-3032">Lifecycle Stages</a>
  <ul class="collapse">
  <li><a href="#our-journey-through-the-ai-lifecycle-the-dr-screening-system" id="toc-our-journey-through-the-ai-lifecycle-the-dr-screening-system" class="nav-link" data-scroll-target="#our-journey-through-the-ai-lifecycle-the-dr-screening-system">Our Journey Through the AI Lifecycle: The DR Screening System</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-problem-definition-87d9" id="toc-sec-ai-workflow-problem-definition-87d9" class="nav-link" data-scroll-target="#sec-ai-workflow-problem-definition-87d9">Problem Definition</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-requirements-system-impact-db7f" id="toc-sec-ai-workflow-requirements-system-impact-db7f" class="nav-link" data-scroll-target="#sec-ai-workflow-requirements-system-impact-db7f">Requirements and System Impact</a></li>
  <li><a href="#sec-ai-workflow-definition-workflow-baa6" id="toc-sec-ai-workflow-definition-workflow-baa6" class="nav-link" data-scroll-target="#sec-ai-workflow-definition-workflow-baa6">Definition Workflow</a></li>
  <li><a href="#sec-ai-workflow-scale-distribution-e38e" id="toc-sec-ai-workflow-scale-distribution-e38e" class="nav-link" data-scroll-target="#sec-ai-workflow-scale-distribution-e38e">Scale and Distribution</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-data-collection-ab2c" id="toc-sec-ai-workflow-data-collection-ab2c" class="nav-link" data-scroll-target="#sec-ai-workflow-data-collection-ab2c">Data Collection</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-data-requirements-impact-6975" id="toc-sec-ai-workflow-data-requirements-impact-6975" class="nav-link" data-scroll-target="#sec-ai-workflow-data-requirements-impact-6975">From Laboratory to Clinic: Data Reality Gaps</a></li>
  <li><a href="#sec-ai-workflow-data-infrastructure-5088" id="toc-sec-ai-workflow-data-infrastructure-5088" class="nav-link" data-scroll-target="#sec-ai-workflow-data-infrastructure-5088">Infrastructure Design Principles</a></li>
  <li><a href="#sec-ai-workflow-scale-distribution-7fc8" id="toc-sec-ai-workflow-scale-distribution-7fc8" class="nav-link" data-scroll-target="#sec-ai-workflow-scale-distribution-7fc8">Scale and Distribution</a></li>
  <li><a href="#sec-ai-workflow-data-validation-5359" id="toc-sec-ai-workflow-data-validation-5359" class="nav-link" data-scroll-target="#sec-ai-workflow-data-validation-5359">Data Validation</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-model-development-dfdc" id="toc-sec-ai-workflow-model-development-dfdc" class="nav-link" data-scroll-target="#sec-ai-workflow-model-development-dfdc">Model Development</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-model-requirements-impact-6470" id="toc-sec-ai-workflow-model-requirements-impact-6470" class="nav-link" data-scroll-target="#sec-ai-workflow-model-requirements-impact-6470">Balancing Clinical Performance with Deployment Reality</a></li>
  <li><a href="#sec-ai-workflow-development-workflow-b547" id="toc-sec-ai-workflow-development-workflow-b547" class="nav-link" data-scroll-target="#sec-ai-workflow-development-workflow-b547">Systematic Experimentation Under Constraints</a></li>
  <li><a href="#sec-ai-workflow-scale-distribution-56d9" id="toc-sec-ai-workflow-scale-distribution-56d9" class="nav-link" data-scroll-target="#sec-ai-workflow-scale-distribution-56d9">Scaling Model Development</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-deployment-2839" id="toc-sec-ai-workflow-deployment-2839" class="nav-link" data-scroll-target="#sec-ai-workflow-deployment-2839">Deployment</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-deployment-requirements-impact-2ef2" id="toc-sec-ai-workflow-deployment-requirements-impact-2ef2" class="nav-link" data-scroll-target="#sec-ai-workflow-deployment-requirements-impact-2ef2">Deployment Requirements and Impact</a></li>
  <li><a href="#sec-ai-workflow-deployment-workflow-9bd5" id="toc-sec-ai-workflow-deployment-workflow-9bd5" class="nav-link" data-scroll-target="#sec-ai-workflow-deployment-workflow-9bd5">Deployment Workflow</a></li>
  <li><a href="#sec-ai-workflow-scale-distribution-86c2" id="toc-sec-ai-workflow-scale-distribution-86c2" class="nav-link" data-scroll-target="#sec-ai-workflow-scale-distribution-86c2">Scaling Deployment Across Diverse Environments</a></li>
  <li><a href="#sec-ai-workflow-robustness-reliability-cce1" id="toc-sec-ai-workflow-robustness-reliability-cce1" class="nav-link" data-scroll-target="#sec-ai-workflow-robustness-reliability-cce1">Robustness and Reliability</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-maintenance-e184" id="toc-sec-ai-workflow-maintenance-e184" class="nav-link" data-scroll-target="#sec-ai-workflow-maintenance-e184">Maintenance</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-monitoring-requirements-impact-104b" id="toc-sec-ai-workflow-monitoring-requirements-impact-104b" class="nav-link" data-scroll-target="#sec-ai-workflow-monitoring-requirements-impact-104b">Monitoring Requirements and Impact</a></li>
  <li><a href="#sec-ai-workflow-maintenance-workflow-ed45" id="toc-sec-ai-workflow-maintenance-workflow-ed45" class="nav-link" data-scroll-target="#sec-ai-workflow-maintenance-workflow-ed45">Maintenance Workflow</a></li>
  <li><a href="#sec-ai-workflow-scale-distribution-0f2e" id="toc-sec-ai-workflow-scale-distribution-0f2e" class="nav-link" data-scroll-target="#sec-ai-workflow-scale-distribution-0f2e">Scale and Distribution</a></li>
  <li><a href="#sec-ai-workflow-proactive-maintenance-2b10" id="toc-sec-ai-workflow-proactive-maintenance-2b10" class="nav-link" data-scroll-target="#sec-ai-workflow-proactive-maintenance-2b10">Proactive Maintenance</a></li>
  </ul></li>
  <li><a href="#systems-thinking-in-ai-development" id="toc-systems-thinking-in-ai-development" class="nav-link" data-scroll-target="#systems-thinking-in-ai-development">Systems Thinking in AI Development</a>
  <ul class="collapse">
  <li><a href="#interdependence-and-constraint-propagation" id="toc-interdependence-and-constraint-propagation" class="nav-link" data-scroll-target="#interdependence-and-constraint-propagation">Interdependence and Constraint Propagation</a></li>
  <li><a href="#feedback-loops-and-continuous-learning" id="toc-feedback-loops-and-continuous-learning" class="nav-link" data-scroll-target="#feedback-loops-and-continuous-learning">Feedback Loops and Continuous Learning</a></li>
  <li><a href="#emergent-system-behaviors" id="toc-emergent-system-behaviors" class="nav-link" data-scroll-target="#emergent-system-behaviors">Emergent System Behaviors</a></li>
  <li><a href="#resource-dependencies-and-trade-offs" id="toc-resource-dependencies-and-trade-offs" class="nav-link" data-scroll-target="#resource-dependencies-and-trade-offs">Resource Dependencies and Trade-offs</a></li>
  <li><a href="#lifecycle-integration-strategies" id="toc-lifecycle-integration-strategies" class="nav-link" data-scroll-target="#lifecycle-integration-strategies">Lifecycle Integration Strategies</a></li>
  </ul></li>
  <li><a href="#sec-ai-workflow-ai-lifecycle-roles-8d83" id="toc-sec-ai-workflow-ai-lifecycle-roles-8d83" class="nav-link" data-scroll-target="#sec-ai-workflow-ai-lifecycle-roles-8d83">AI Lifecycle Roles</a>
  <ul class="collapse">
  <li><a href="#sec-ai-workflow-collaboration-ai-2ccb" id="toc-sec-ai-workflow-collaboration-ai-2ccb" class="nav-link" data-scroll-target="#sec-ai-workflow-collaboration-ai-2ccb">Collaboration in AI</a></li>
  <li><a href="#sec-ai-workflow-role-interplay-7f0a" id="toc-sec-ai-workflow-role-interplay-7f0a" class="nav-link" data-scroll-target="#sec-ai-workflow-role-interplay-7f0a">Role Interplay</a></li>
  </ul></li>
  <li><a href="#fallacies-and-pitfalls" id="toc-fallacies-and-pitfalls" class="nav-link" data-scroll-target="#fallacies-and-pitfalls">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-ai-workflow-summary-84ad" id="toc-sec-ai-workflow-summary-84ad" class="nav-link" data-scroll-target="#sec-ai-workflow-summary-84ad">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">AI Workflow</a></li></ol></nav></header>




<section id="sec-ai-workflow" class="level1 page-columns page-full">
<h1>AI Workflow</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Create a rectangular illustration of a stylized flowchart representing the AI workflow/pipeline. From left to right, depict the stages as follows: ‘Data Collection’ with a database icon, ‘Data Preprocessing’ with a filter icon, ‘Model Design’ with a brain icon, ‘Training’ with a weight icon, ‘Evaluation’ with a checkmark, and ‘Deployment’ with a rocket. Connect each stage with arrows to guide the viewer horizontally through the AI processes, emphasizing these steps’ sequential and interconnected nature.</em></p>
</div></div><p> <img src="images/png/cover_ai_workflow.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How do structured workflows transform machine learning development from ad-hoc experimentation into reliable, reproducible engineering processes?</em></p>
<p>Machine learning development begins with exploratory data analysis and experimental model training, yet production systems require systematic, repeatable processes. Structured workflows establish standardized stages for data collection, model development, validation, and deployment. These workflows address critical engineering challenges: ensuring data quality and consistency, managing model versioning and experimentation, automating testing and validation, and coordinating deployment across different environments. Systematic workflows enable teams to build reproducible systems, reduce development cycles, and maintain quality standards. This transformation from experimental prototyping to engineering discipline creates the operational foundation that supports reliable production deployments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the ML lifecycle’s structured approach and stages for developing, deploying, and maintaining machine learning models.</p></li>
<li><p>Identify the unique challenges and distinctions between traditional software and machine learning lifecycles.</p></li>
<li><p>Analyze the roles and collaboration patterns essential for successful ML projects.</p></li>
<li><p>Examine system-level considerations, including resource constraints, infrastructure, and deployment environments.</p></li>
<li><p>Recognize the iterative nature of ML lifecycles and how feedback loops drive continuous improvement in real-world applications.</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-ai-workflow-overview-97fb" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-overview-97fb">Overview</h2>
<p>The machine learning lifecycle represents a fundamental shift from traditional software development, where success depends not only on writing correct code, but on orchestrating complex interactions between data, models, and production systems. Understanding this lifecycle as an integrated system, rather than a sequence of independent tasks, is essential to building reliable AI systems that perform well beyond the laboratory.</p>
<p>This chapter examines the AI lifecycle through Google’s diabetic retinopathy (DR) screening system <span class="citation" data-cites="gulshan2016deep">(<a href="#ref-gulshan2016deep" role="doc-biblioref">Gulshan et al. 2016</a>)</span>. This case study demonstrates how seemingly straightforward AI problems become complex systems challenges when moving from research to clinical practice, illustrating the interconnected nature of lifecycle stages and the importance of systems thinking in AI development.</p>
<div class="no-row-height column-margin column-container"></div><p>Using this real-world deployment experience, we explore how the ML lifecycle differs from traditional software development. <a href="#fig-ml-lifecycle" class="quarto-xref">Figure&nbsp;1</a> illustrates the complete lifecycle, showing both the primary flow of data and model development as well as the crucial feedback loops that enable continuous improvement. Unlike traditional software where stages typically flow in one direction, machine learning systems require continuous feedback between all stages. Insights from deployment often necessitate changes to data collection, model architecture, and validation approaches.</p>
<p>These feedback relationships become particularly evident when examining the complete system architecture.</p>
<div id="fig-ml-lifecycle" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03bbeaedbd5366f135a9b5e0ac5ecbb9aab154a3.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: ML Lifecycle Stages: Iterative data processing and model refinement drive the development of machine learning systems, with continuous feedback loops enabling improvement across each stage, from initial data collection to final model deployment and monitoring. This cyclical process ensures models adapt to changing data and maintain performance in real-world applications."><img src="workflow_files/mediabag/03bbeaedbd5366f135a9b5e0ac5ecbb9aab154a3.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>ML Lifecycle Stages</strong>: Iterative data processing and model refinement drive the development of machine learning systems, with continuous feedback loops enabling improvement across each stage, from initial data collection to final model deployment and monitoring. This cyclical process ensures models adapt to changing data and maintain performance in real-world applications.
</figcaption>
</figure>
</div>
<p>The lifecycle operates through five interconnected phases that build progressively in complexity:</p>
<p>Phase 1 involves foundation building, which encompasses problem definition and initial data collection, establishing the fundamental requirements and constraints that will shape all subsequent decisions.</p>
<p>The next phase focuses on creating robust data infrastructure.</p>
<p>Phase 2 focuses on data ecosystem development, which involves building robust data pipelines, validation systems, and preprocessing workflows that can handle the scale and variability of real-world data.</p>
<p>Development then shifts to creating and optimizing the models themselves.</p>
<p><strong>Phase 3: Model Development and Optimization</strong> focuses on creating, training, and refining models while balancing performance with operational constraints.</p>
<p>The next phase addresses the transition from development to production environments.</p>
<p><strong>Phase 4: Production Integration</strong> addresses the complex challenge of deploying models into real-world systems while maintaining reliability and performance.</p>
<p>Successful deployment establishes the foundation for ongoing system evolution and improvement.</p>
<p><strong>Phase 5: Continuous Evolution</strong> establishes monitoring, maintenance, and improvement processes that ensure long-term system effectiveness.</p>
<p>Each phase introduces new complexity while building upon foundations established in previous phases. This progressive approach reflects the reality that ML systems cannot be built successfully by jumping directly to advanced techniques without first establishing solid foundations<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>MLOps Maturity Models</strong>: Organizations typically progress through 5 maturity levels, from manual processes (Level 0) to fully automated ML pipelines (Level 4). Google’s MLOps maturity model <span class="citation" data-cites="kreuzberger2023machine">(<a href="#ref-kreuzberger2023machine" role="doc-biblioref">Kreuzberger, Kühl, and Hirschl 2023</a>)</span>, published in 2021, shows that approximately 20-25% of organizations reach Level 3+ automation, while 75-80% remain in manual or semi-automated processes. Companies at higher maturity levels report 35% faster time-to-market and 50% fewer production incidents, but require 18-24 months and significant cultural changes to advance between levels.</p></div></div><p>This systematic progression is governed by fundamental principles that distinguish AI development from conventional software engineering.</p>
<section id="lifecycle-integration-principles" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="lifecycle-integration-principles">Lifecycle Integration Principles</h3>
<p>The systematic integration of these lifecycle stages follows three key principles that are explored in detail in the Systems Thinking section: Interdependence (how decisions cascade across stages), Continuous Feedback (how insights flow between phases), and Emergent Complexity (how system behaviors arise from component interactions).</p>
<p>These principles explain why successful AI systems require different development approaches than traditional software, and why understanding the complete lifecycle is essential before examining implementation details.</p>
<p>This integrated perspective shapes how we approach each phase of development. Rather than optimizing individual components in isolation, successful AI systems require continuous coordination between data teams managing pipeline development, model teams handling training infrastructure, and infrastructure teams providing deployment platforms. The operational practices for managing these complex integrations in production environments are comprehensively addressed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>. Understanding these integration challenges is essential before examining the specific techniques and tools used within each lifecycle stage.</p>
<p>As these coordination requirements illustrate, the AI lifecycle operates within a complex web of dependencies and feedback loops. The following sections examine each phase in detail, showing how the DR case study navigated the complexity of real-world AI deployment while maintaining the integration principles that ensure long-term success. These challenges are compounded in production ML systems, where continuous integration and deployment practices<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> must account for both code changes and data evolution<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>CI/CD for Machine Learning</strong>: Traditional continuous integration is designed for deterministic builds where code changes produce predictable outputs. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google’s TFX and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like “model validation” and “data validation” that have no equivalent in traditional software.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Data Evolution in Production</strong>: Unlike traditional software where inputs are static, ML system inputs evolve continuously: user behavior changes, market conditions shift, and sensor data drifts. Netflix reports that their recommendation models see approximately 10-15% of features become stale monthly <span class="citation" data-cites="netflix2012recommendation">(<a href="#ref-netflix2012recommendation" role="doc-biblioref">Gomez-Uribe and Hunt 2015</a>)</span>, while financial fraud detection models experience 30-40% feature drift quarterly <span class="citation" data-cites="stripe2019machine">(<a href="#ref-stripe2019machine" role="doc-biblioref">Arsene, Dumitrache, and Mihu 2015</a>)</span>. This constant evolution means ML systems require “data testing” pipelines that validate 200+ statistical properties of incoming data, a complexity absent in traditional software where input validation involves simple type checking <span class="citation" data-cites="breck2017ml">(<a href="#ref-breck2017ml" role="doc-biblioref">Breck et al. 2017</a>)</span>.</p><div id="ref-netflix2012recommendation" class="csl-entry" role="listitem">
Gomez-Uribe, Carlos A., and Neil Hunt. 2015. <span>“The Netflix Recommender System: Algorithms, Business Value, and Innovation.”</span> <em>ACM Transactions on Management Information Systems</em> 6 (4): 1–19. <a href="https://doi.org/10.1145/2843948">https://doi.org/10.1145/2843948</a>.
</div></div></div><p>Before examining specific lifecycle stages, we establish a precise understanding of what distinguishes the ML lifecycle from traditional software development approaches. This foundation guides our examination of how each stage contributes to successful AI system development.</p>
<p>To build this foundation, we begin by defining what we mean by the machine learning lifecycle and how it differs from conventional development processes.</p>
</section>
<section id="sec-ai-workflow-definition-d707" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-definition-d707">Defining the ML Lifecycle</h3>
<p>To understand why AI systems require specialized development approaches, we first establish what we mean by the “machine learning lifecycle” and how it differs from traditional software development processes.</p>
<p>The machine learning (ML) lifecycle is a structured, iterative process that guides the development, evaluation, and improvement of machine learning systems. Integrating ML into broader software engineering practices introduces unique challenges that require systematic approaches to experimentation, evaluation, and adaptation over time <span class="citation" data-cites="amershi2019software">(<a href="#ref-amershi2019software" role="doc-biblioref">Amershi et al. 2019</a>)</span>. This systematic approach builds upon decades of structured development methodologies <span class="citation" data-cites="chapman2000crisp">(<a href="#ref-chapman2000crisp" role="doc-biblioref">Chapman et al. 2000</a>)</span> that have evolved to address the unique challenges of data-driven systems, creating new challenges distinct from traditional software engineering approaches.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chapman2000crisp" class="csl-entry" role="listitem">
Chapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin Shearer, and Rudiger Wirth. 2000. <span>“CRISP-DM 1.0: Step-by-Step Data Mining Guide.”</span> <em>SPSS Inc</em>, 78. <a href="https://www.the-modeling-agency.com/crisp-dm.pdf">https://www.the-modeling-agency.com/crisp-dm.pdf</a>.
</div></div><div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of the Machine Learning Lifecycle">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of the Machine Learning Lifecycle</summary><div><strong>The Machine Learning (ML) Lifecycle</strong> is a <em>structured, iterative process</em> that defines the <em>key stages</em> involved in the <em>development, deployment, and refinement</em> of ML systems. It encompasses <em>interconnected steps</em> such as <em>problem formulation, data collection, model training, evaluation, deployment, and monitoring</em>. The lifecycle focuses on <em>feedback loops and continuous improvement</em>, ensuring that systems remain <em>robust, scalable, and responsive</em> to <em>changing requirements and real-world conditions</em>.<p></p>
</div></details>
</div>
<p>Rather than prescribing a fixed methodology, the ML lifecycle focuses on achieving specific objectives at each stage. This flexibility allows practitioners to adapt the process to the unique constraints and goals of individual projects. Typical stages include problem formulation, data acquisition and preprocessing, model development and training, evaluation, deployment, and ongoing optimization. Modern practitioners often use interactive development environments like Jupyter <span class="citation" data-cites="kluyver2016jupyter">(<a href="#ref-kluyver2016jupyter" role="doc-biblioref">Thomas et al. 2016</a>)</span> that support this iterative, experimental approach to ML system development, often integrated within the broader framework ecosystems detailed in <strong><a href="../core/frameworks/frameworks.html#sec-ai-frameworks">Chapter 5: AI Frameworks</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kluyver2016jupyter" class="csl-entry" role="listitem">
Thomas, Kluyver, Ragan-Kelley Benjamin, P&amp;eacute;rez Fernando, Granger Brian, Bussonnier Matthias, Frederic Jonathan, Kelley Kyle, et al. 2016. <span>“Jupyter Notebooks &amp;Amp;ndash; a Publishing Format for Reproducible Computational Workflows.”</span> In <em>Positioning and Power in Academic Publishing: Players, Agents and Agendas</em>, 87–90. IOS Press. <a href="https://doi.org/10.3233/978-1-61499-649-1-87">https://doi.org/10.3233/978-1-61499-649-1-87</a>.
</div></div><p>Although these stages appear sequential, they are frequently revisited, creating a dynamic and interconnected process. The iterative nature of the lifecycle creates feedback loops, where insights from later stages, including deployment, can inform earlier phases, including data preparation or model architecture design. This adaptability is essential for managing the uncertainties and complexities inherent in real-world ML applications. The systematic approaches to managing these feedback loops in production systems, including data versioning and lineage tracking, are detailed in <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong>.</p>
<p>This iterative, feedback-driven structure provides an organizing framework for both practitioners and students. The ML lifecycle provides a clear framework for organizing the study of machine learning systems. By decomposing the field into well-defined stages, students can engage systematically with its core components. This structure mirrors industrial practice while supporting conceptual understanding.</p>
<p>Having established this conceptual foundation, we can now distinguish between the lifecycle framework and related operational concepts. We distinguish between the ML lifecycle and machine learning operations (MLOps), as the two are often conflated. The ML lifecycle, as presented in this chapter, addresses the stages and evolution of ML systems: the “what” and “why” of system development. In contrast, MLOps, discussed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>, addresses the “how,” focusing on tools, processes, and automation that support efficient implementation and maintenance. This includes the critical production concerns of incident response, debugging complex ML system failures, and maintaining operational excellence at scale. Understanding the lifecycle first provides a conceptual foundation for the operational aspects that follow.</p>
</section>
<section id="sec-ai-workflow-traditional-vs-ai-lifecycles-4f66" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-traditional-vs-ai-lifecycles-4f66">Traditional vs.&nbsp;AI Lifecycles</h3>
<p>Software development lifecycles have evolved through decades of engineering practice, establishing well-defined patterns for system development. Traditional lifecycles consist of sequential phases: requirements gathering, system design, implementation, testing, and deployment <span class="citation" data-cites="royce1970managing">(<a href="#ref-royce1970managing" role="doc-biblioref">Royce 1987</a>)</span>. Each phase produces specific artifacts that serve as inputs to subsequent phases. In financial software development, the requirements phase produces detailed specifications for transaction processing, security protocols, and regulatory compliance. These specifications translate directly into system behavior through explicit programming, contrasting sharply with the probabilistic nature of ML systems explored throughout <strong><a href="../core/introduction/introduction.html#sec-introduction">Chapter 1: Introduction</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-royce1970managing" class="csl-entry" role="listitem">
Royce, W. W. 1987. <span>“Managing the Development of Large Software Systems: Concepts and Techniques.”</span> In <em>Proceedings of IEEE WESCON</em>, 26:328–39. IEEE. <a href="http://dl.acm.org/citation.cfm?id=41801">http://dl.acm.org/citation.cfm?id=41801</a>.
</div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>ML-Based Fraud Detection Evolution</strong>: Traditional rule-based fraud systems (developed in the 1990s) had 60-70% accuracy and generated 20-30% false positives, causing customer friction. Modern ML fraud detection, pioneered by companies like PayPal (2000s) and Stripe (2010s), achieves 95%+ accuracy with &lt;1% false positive rates by analyzing 500+ behavioral features in real-time <span class="citation" data-cites="stripe2019machine">(<a href="#ref-stripe2019machine" role="doc-biblioref">Arsene, Dumitrache, and Mihu 2015</a>)</span>. However, this improvement comes with new challenges: fraudsters adapt to ML patterns within 3-6 months, requiring continuous model retraining that rule-based systems never needed <span class="citation" data-cites="stripe2019machine">(<a href="#ref-stripe2019machine" role="doc-biblioref">Arsene, Dumitrache, and Mihu 2015</a>)</span>.</p><div id="ref-stripe2019machine" class="csl-entry" role="listitem">
Arsene, Octavian, Ioan Dumitrache, and Ioana Mihu. 2015. <span>“Expert System for Medicine Diagnosis Using Software Agents.”</span> <em>Expert Systems with Applications</em> 42 (4): 1825–34. <a href="https://doi.org/10.1016/j.eswa.2014.10.026">https://doi.org/10.1016/j.eswa.2014.10.026</a>.
</div></div></div><p>This deterministic approach differs from the uncertainty inherent in machine learning systems. Machine learning systems require a different approach to this traditional lifecycle model. The deterministic nature of conventional software, where behavior is explicitly programmed, contrasts with the probabilistic nature of ML systems. Consider financial transaction processing: traditional systems follow predetermined rules (if account balance &gt; transaction amount, then allow transaction), while ML-based fraud detection systems<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> learn to recognize suspicious patterns from historical transaction data. This shift from explicit programming to learned behavior reshapes the development lifecycle, altering how we approach system reliability and robustness as detailed in <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
<p>These fundamental differences in system behavior create cascading effects throughout the development process. The unique characteristics of machine learning systems, characterized by data dependency, probabilistic outputs <span class="citation" data-cites="domingos2012few">(<a href="#ref-domingos2012few" role="doc-biblioref">Domingos 2012</a>)</span>, and evolving performance, introduce new dynamics that alter how lifecycle stages interact. These systems require ongoing refinement, with insights from later stages frequently feeding back into earlier ones. Machine learning systems are inherently dynamic and must adapt to changing data distributions and objectives through continuous deployment<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> practices.</p>
<div class="no-row-height column-margin column-container"><div id="ref-domingos2012few" class="csl-entry" role="listitem">
Domingos, Pedro. 2012. <span>“A Few Useful Things to Know about Machine Learning.”</span> <em>Communications of the ACM</em> 55 (10): 78–87. <a href="https://doi.org/10.1145/2347736.2347755">https://doi.org/10.1145/2347736.2347755</a>.
</div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Continuous Deployment</strong>: Software engineering practice where code changes are automatically deployed to production after passing automated tests, enabling multiple deployments per day instead of monthly releases. Popularized by companies like Netflix (2008) and Etsy (2009), continuous deployment reduces deployment risk through small, frequent changes rather than large, infrequent releases. However, ML systems require specialized continuous deployment because models need statistical validation, gradual rollouts with A/B testing, and rollback mechanisms based on performance metrics rather than just functional correctness.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Data Versioning Challenges</strong>: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS and DVC.</p></div></div><p>These contrasts become clearer when we examine the specific differences across development lifecycle dimensions. The key distinctions are summarized in <a href="#tbl-sw-ml-cycles" class="quarto-xref">Table&nbsp;1</a> below. These differences reflect the core challenge of working with data as a first-class citizen in system design, something traditional software engineering methodologies were not designed to handle<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
<div id="tbl-sw-ml-cycles" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sw-ml-cycles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Traditional vs ML Development</strong>: Traditional software and machine learning systems diverge in their development processes due to the data-driven and iterative nature of ML. Machine learning lifecycles emphasize experimentation and evolving objectives, requiring feedback loops between stages, whereas traditional software follows a linear progression with predefined specifications.
</figcaption>
<div aria-describedby="tbl-sw-ml-cycles-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 36%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Traditional Software Lifecycles</th>
<th style="text-align: left;">Machine Learning Lifecycles</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Problem Definition</td>
<td style="text-align: left;">Precise functional specifications are defined upfront.</td>
<td style="text-align: left;">Performance-driven objectives evolve as the problem space is explored.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Development Process</td>
<td style="text-align: left;">Linear progression of feature implementation.</td>
<td style="text-align: left;">Iterative experimentation with data, features and models.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Testing and Validation</td>
<td style="text-align: left;">Deterministic, binary pass/fail testing criteria.</td>
<td style="text-align: left;">Statistical validation and metrics that involve uncertainty.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment</td>
<td style="text-align: left;">Behavior remains static until explicitly updated.</td>
<td style="text-align: left;">Performance may change over time due to shifts in data distributions.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maintenance</td>
<td style="text-align: left;">Maintenance involves modifying code to address bugs or add features.</td>
<td style="text-align: left;">Continuous monitoring, updating data pipelines, retraining models, and adapting to new data distributions.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Feedback Loops</td>
<td style="text-align: left;">Minimal; later stages rarely impact earlier phases.</td>
<td style="text-align: left;">Frequent; insights from deployment and monitoring often refine earlier stages like data preparation and model design.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These differences emphasize the need for a robust ML lifecycle framework that can accommodate iterative development, dynamic behavior, and data-driven decision-making. Understanding these distinctions prepares us to examine how ML projects unfold through their lifecycle stages, each presenting unique challenges that traditional software methodologies cannot adequately address.</p>
<p>With this foundation established, we can now explore the specific stages that comprise the ML lifecycle and how they work together to address these unique challenges.</p>
<div id="quiz-question-sec-ai-workflow-overview-97fb" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Order the following stages of the ML lifecycle: (1) Data Collection, (2) Model Training, (3) Data Validation, (4) Model Evaluation.</p></li>
<li><p>Which stage of the ML lifecycle involves ensuring that data is properly annotated and verified for usability?</p>
<ol type="a">
<li>Data Collection</li>
<li>Data Labeling</li>
<li>Model Training</li>
<li>Model Evaluation</li>
</ol></li>
<li><p>Explain why feedback loops are important in the ML lifecycle.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-overview-97fb" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-lifecycle-stages-3032" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-lifecycle-stages-3032">Lifecycle Stages</h2>
<p>Having established why AI systems require specialized development approaches, we now examine the specific stages that comprise the ML lifecycle. These stages operate as an integrated framework where each builds upon previous foundations while preparing for subsequent phases, an interconnected approach detailed in our earlier discussion of lifecycle integration principles.</p>
<p>The following framework illustrates how these stages work together to manage AI system complexity. <a href="#fig-lifecycle-overview" class="quarto-xref">Figure&nbsp;2</a> illustrates the six core stages that characterize successful AI system development. These stages operate through continuous feedback loops, with insights from later stages frequently informing refinements in earlier phases. This cyclical nature reflects the experimental and data-driven characteristics that distinguish ML development from conventional software engineering.</p>
<div id="fig-lifecycle-overview" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lifecycle-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9b58b3aa72de00d638019e0cdf45f488ecf3911f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: ML System Lifecycle: Iterative development defines successful machine learning systems, progressing through problem definition, data preparation, model building, evaluation, deployment, and ongoing monitoring for continuous improvement. Each stage informs subsequent iterations, enabling refinement and adaptation to changing requirements and data distributions."><img src="workflow_files/mediabag/9b58b3aa72de00d638019e0cdf45f488ecf3911f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lifecycle-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>ML System Lifecycle</strong>: Iterative development defines successful machine learning systems, progressing through problem definition, data preparation, model building, evaluation, deployment, and ongoing monitoring for continuous improvement. Each stage informs subsequent iterations, enabling refinement and adaptation to changing requirements and data distributions.
</figcaption>
</figure>
</div>
<p>The lifecycle begins with problem definition and requirements gathering, where teams clearly define the problem to be solved, establish measurable performance objectives, and identify key constraints. Precise problem definition ensures alignment between the system’s goals and the desired outcomes, setting the foundation for all subsequent work.</p>
<p>The next stage assembles the data resources needed to realize these objectives. Data collection and preparation includes gathering relevant data, cleaning it, and preparing it for model training. This process involves curating diverse datasets, ensuring high-quality labeling, and developing preprocessing pipelines to address variations in the data. The complexities of this stage are explored in <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong>.</p>
<p>Building upon prepared data, the development process creates models that can learn from these resources. Model development and training involves selecting appropriate algorithms, designing model architectures, and training models using the prepared data. Success depends on choosing techniques suited to the problem and iterating on the model design for optimal performance. Advanced training approaches and distributed training strategies are detailed in <strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>, while the underlying architectures are covered in <strong><a href="../core/dnn_architectures/dnn_architectures.html#sec-dnn-architectures">Chapter 4: DNN Architectures</a></strong>.</p>
<p>Rigorous evaluation ensures trained models meet performance requirements before deployment. This evaluation and validation stage involves rigorously testing the model’s performance against predefined metrics and validating its behavior in different scenarios, ensuring the model is accurate, reliable, and robust in real-world conditions.</p>
<p>Validated models transition from development environments to operational systems through careful deployment processes. Deployment and integration requires addressing practical challenges such as system compatibility, scalability, and operational constraints across different deployment contexts ranging from cloud to edge environments, as explored in <strong><a href="../core/ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong>.</p>
<p>Finally, deployed systems require ongoing oversight to maintain performance and adapt to changing conditions. This monitoring and maintenance stage focuses on continuously tracking the system’s performance in real-world environments and updating it as necessary. Effective monitoring ensures the system remains relevant and accurate over time, adapting to changes in data, requirements, or external conditions.</p>
<p>To illustrate how these stages work together in practice, we follow the development of a real-world medical AI system.</p>
<section id="our-journey-through-the-ai-lifecycle-the-dr-screening-system" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="our-journey-through-the-ai-lifecycle-the-dr-screening-system">Our Journey Through the AI Lifecycle: The DR Screening System</h3>
<p>To illustrate these lifecycle principles, we follow the development of Google’s diabetic retinopathy (DR) screening system from initial research to widespread clinical deployment <span class="citation" data-cites="gulshan2016deep">(<a href="#ref-gulshan2016deep" role="doc-biblioref">Gulshan et al. 2016</a>)</span>. This real-world case study reveals how each lifecycle stage presents unique challenges that cannot be solved in isolation, demonstrating the importance of integrated systems thinking.</p>
<div class="no-row-height column-margin column-container"></div><p>At first glance, the DR screening challenge appeared to be a straightforward computer vision problem: develop an AI system to analyze retinal images and detect signs of diabetic retinopathy with accuracy comparable to expert ophthalmologists. The initial research results were promising, achieving expert-level performance in controlled laboratory conditions. However, the journey from research success to clinical impact revealed the complexity of the AI lifecycle, where technical excellence must integrate with operational realities, regulatory requirements, and real-world deployment constraints.</p>
<p>The scale of this medical challenge underscores why AI-assisted screening became not just technically interesting but medically essential. Diabetic retinopathy affects over 100 million people worldwide and represents a leading cause of preventable blindness<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. <a href="#fig-eye-dr" class="quarto-xref">Figure&nbsp;3</a> shows the clinical challenge: distinguishing healthy retinas from those showing early signs of retinopathy, such as the characteristic hemorrhages visible as dark red spots. While this appears to be a straightforward image classification problem, the path from laboratory success to clinical deployment illustrates every aspect of the AI lifecycle complexity.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Diabetic Retinopathy Global Impact</strong>: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy <span class="citation" data-cites="who2019classification">(<a href="#ref-who2019classification" role="doc-biblioref">Steinmetz et al. 2024</a>)</span>. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited: rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000 <span class="citation" data-cites="who2019classification">(<a href="#ref-who2019classification" role="doc-biblioref">Steinmetz et al. 2024</a>)</span>. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions <span class="citation" data-cites="rajkomar2019machine">(<a href="#ref-rajkomar2019machine" role="doc-biblioref">Rajkomar, Dean, and Kohane 2019</a>)</span>.</p><div id="ref-who2019classification" class="csl-entry" role="listitem">
Steinmetz, Jaimie D, Katrin Maria Seeher, Nicoline Schiess, Emma Nichols, Bochen Cao, Chiara Servili, Vanessa Cavallera, et al. 2024. <span>“Global, Regional, and National Burden of Disorders Affecting the Nervous System, 1990–2021: A Systematic Analysis for the Global Burden of Disease Study 2021.”</span> <em>The Lancet Neurology</em> 23 (4): 344–81. <a href="https://doi.org/10.1016/s1474-4422(24)00038-3">https://doi.org/10.1016/s1474-4422(24)00038-3</a>.
</div></div></div><div id="fig-eye-dr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eye-dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/eye-dr.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Retinal Hemorrhages: Diabetic retinopathy causes visible hemorrhages in retinal images, providing a key visual indicator for model training and evaluation in medical image analysis. these images represent the input data used to develop algorithms that automatically detect and classify retinal diseases, ultimately assisting in early diagnosis and treatment. Source: Google."><img src="images/png/eye-dr.png" class="img-fluid figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eye-dr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Retinal Hemorrhages</strong>: Diabetic retinopathy causes visible hemorrhages in retinal images, providing a key visual indicator for model training and evaluation in medical image analysis. these images represent the input data used to develop algorithms that automatically detect and classify retinal diseases, ultimately assisting in early diagnosis and treatment. Source: Google.
</figcaption>
</figure>
</div>
<p>As we examine each lifecycle stage, we see how the DR team’s experiences illustrate fundamental AI systems principles. Their challenges with data quality led to innovations in distributed data validation. Infrastructure constraints in rural clinics drove breakthroughs in edge computing<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> optimization.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Edge Computing</strong>: Distributed computing paradigm that processes data near the source rather than in centralized cloud data centers, reducing latency from 100-500ms (cloud) to 1-10ms (edge). Originally developed for CDNs (1990s), edge computing became essential for ML when real-time applications like autonomous vehicles and medical devices required sub-20ms response times that cloud computing couldn’t achieve <span class="citation" data-cites="shi2016edge">(<a href="#ref-shi2016edge" role="doc-biblioref"><strong>shi2016edge?</strong></a>)</span>. The edge AI market grew from $590M in 2018 to $8.9B in 2023, driven by IoT devices generating 79.4 zettabytes of data annually that cannot be efficiently transmitted to cloud servers. Integration with clinical workflows revealed the importance of human-AI collaboration design. These experiences demonstrate that successful AI systems require more than accurate models; they require systematic engineering approaches that address the complexity of real-world deployment.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Healthcare AI Deployment Reality</strong>: Studies show that 75-80% of healthcare AI projects never reach clinical deployment <span class="citation" data-cites="chen2019machine">(<a href="#ref-chen2019machine" role="doc-biblioref">Chen and Asch 2017</a>)</span>, with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The “AI chasm” between research success and clinical adoption is particularly wide in healthcare: while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues <span class="citation" data-cites="kelly2019key">(<a href="#ref-kelly2019key" role="doc-biblioref">Kelly et al. 2019</a>)</span>.</p><div id="ref-chen2019machine" class="csl-entry" role="listitem">
Chen, Jonathan H., and Steven M. Asch. 2017. <span>“Machine Learning and Prediction in Medicine — Beyond the Peak of Inflated Expectations.”</span> <em>New England Journal of Medicine</em> 376 (26): 2507–9. <a href="https://doi.org/10.1056/nejmp1702071">https://doi.org/10.1056/nejmp1702071</a>.
</div><div id="ref-kelly2019key" class="csl-entry" role="listitem">
Kelly, Christopher J., Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King. 2019. <span>“Key Challenges for Delivering Clinical Impact with Artificial Intelligence.”</span> <em>BMC Medicine</em> 17 (1): 1–9. <a href="https://doi.org/10.1186/s12916-019-1426-2">https://doi.org/10.1186/s12916-019-1426-2</a>.
</div></div></div><p>This comprehensive journey through real-world deployment challenges reflects broader patterns in healthcare AI development. The DR case study serves as our guide through each lifecycle stage, showing how decisions made in early phases influence later stages, how feedback loops drive continuous improvement, and how emergent system behaviors require holistic solutions. These deployment challenges reflect broader issues in healthcare AI<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> that affect most real-world medical ML applications.</p>
<p>Through this narrative thread, we see how the AI lifecycle’s integrated nature requires systems thinking from the beginning. The DR team’s journey shows that sustainable AI systems emerge from understanding and designing for the complex interactions between all lifecycle stages, not from optimizing individual components.</p>
<p><em>Note: While this case study draws from Google’s documented experiences with diabetic retinopathy screening deployments, specific technical details have been adapted to illustrate common challenges encountered in healthcare AI systems while maintaining educational value and practical relevance.</em></p>
<p>With this framework and case study established, we can now examine each lifecycle stage in detail, beginning with problem definition.</p>
<div id="quiz-question-sec-ai-workflow-lifecycle-stages-3032" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which stage of the ML lifecycle involves integrating the trained model into production systems and addressing challenges such as scalability and operational constraints?</p>
<ol type="a">
<li>Problem Definition</li>
<li>Data Collection and Preparation</li>
<li>Deployment and Integration</li>
<li>Monitoring and Maintenance</li>
</ol></li>
<li><p>True or False: The Monitoring and Maintenance stage is only necessary if the model’s performance begins to degrade.</p></li>
<li><p>Explain how the feedback loop in the ML lifecycle contributes to the system’s continuous improvement.</p></li>
<li><p>In the context of Google’s Diabetic Retinopathy project, what was a significant challenge encountered during the deployment stage?</p>
<ol type="a">
<li>Integration with rural clinic workflows</li>
<li>Data preprocessing</li>
<li>Algorithm selection</li>
<li>Model architecture design</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-lifecycle-stages-3032" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-problem-definition-87d9" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-problem-definition-87d9">Problem Definition</h2>
<p>The development of machine learning systems begins with a challenge that differs from traditional software development: defining not just what the system should do, but how it should learn to do it. Unlike conventional software, where requirements translate directly into implementation rules, ML systems require teams to consider how the system will learn from data while operating within real-world constraints<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. This stage lays the foundation for all subsequent phases in the ML lifecycle.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>ML vs.&nbsp;Traditional Problem Definition</strong>: Traditional software problems are defined by deterministic specifications (“if input X, then output Y”), but ML problems are defined by examples and desired behaviors. This shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software <span class="citation" data-cites="standish2020chaos">(<a href="#ref-standish2020chaos" role="doc-biblioref">Maor 1987</a>)</span>. The challenge lies in translating business objectives into learning objectives—something that didn’t exist in software engineering until the rise of data-driven systems in the 2000s <span class="citation" data-cites="amershi2019software">(<a href="#ref-amershi2019software" role="doc-biblioref">Amershi et al. 2019</a>)</span>.</p><div id="ref-standish2020chaos" class="csl-entry" role="listitem">
Maor, Eli. 1987. <span>“CHAOS 2020: Beyond Infinity.”</span> In <em>To Infinity and Beyond</em>, 60–65. Birkhäuser Boston. <a href="https://doi.org/10.1007/978-1-4612-5394-5\_10">https://doi.org/10.1007/978-1-4612-5394-5\_10</a>.
</div><div id="ref-amershi2019software" class="csl-entry" role="listitem">
Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. <span>“Software Engineering for Machine Learning: A Case Study.”</span> In <em>2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</em>, 291–300. IEEE. <a href="https://doi.org/10.1109/icse-seip.2019.00042">https://doi.org/10.1109/icse-seip.2019.00042</a>.
</div></div></div><p>The DR screening system exemplifies how this complexity manifests in practice.</p>
<p><strong>DR Case Study: Defining Success Beyond Accuracy</strong></p>
<p>For the diabetic retinopathy screening system, problem definition revealed the complexity beneath an apparently straightforward medical imaging task. The initial technical goal appeared clear: achieve expert-level diagnostic accuracy on retinal images. However, the DR team discovered that clinical deployment success required defining multiple interconnected objectives that would shape every subsequent lifecycle stage.</p>
<p>The team balanced competing constraints: diagnostic accuracy for patient safety, computational efficiency for rural clinic hardware, workflow integration for clinical adoption, regulatory compliance for medical device approval, and cost-effectiveness for sustainable deployment. Each constraint influenced the others, creating a complex optimization problem that traditional software development approaches could not address. This multi-dimensional problem definition drove data collection strategies, model architecture choices, and deployment infrastructure decisions throughout the project lifecycle.</p>
<section id="sec-ai-workflow-requirements-system-impact-db7f" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-requirements-system-impact-db7f">Requirements and System Impact</h3>
<p>The DR team’s requirements analysis revealed how problem definition decisions cascade through every aspect of system design. Their initial focus on diagnostic accuracy metrics quickly expanded as they understood the deployment environment’s constraints and opportunities.</p>
<p><strong>Technical Requirements:</strong> The system required 90%+ sensitivity for detecting referable diabetic retinopathy to prevent vision loss, while maintaining 80%+ specificity to avoid overwhelming referral systems with false positives. These metrics had to be achieved across diverse patient populations, camera equipment, and image quality conditions typical in resource-limited settings.</p>
<p><strong>Operational Requirements:</strong> Rural clinics imposed strict constraints: models must run on devices with limited computational power, operate reliably with intermittent internet connectivity, and produce results within clinical workflow timeframes. The system required operation by healthcare workers with minimal technical training.</p>
<p><strong>Regulatory Requirements:</strong> Medical device regulations required extensive validation, audit trails, and performance monitoring capabilities that influenced data collection, model development, and deployment strategies.</p>
<p>These interconnected requirements demonstrated how problem definition in ML systems requires understanding the complete ecosystem in which the system will operate. The DR team’s early recognition of these constraints enabled them to make architecture decisions crucial for successful deployment, rather than discovering limitations after significant development investment.</p>
</section>
<section id="sec-ai-workflow-definition-workflow-baa6" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-definition-workflow-baa6">Definition Workflow</h3>
<p>Establishing clear and actionable problem definitions involves a workflow that bridges technical, operational, and user considerations. The process begins with identifying the core objective of the system: what tasks it must perform and what constraints it must satisfy. Teams collaborate with stakeholders to gather domain knowledge, outline requirements, and anticipate challenges that may arise in real-world deployment.</p>
<p>In the DR project, this phase involved close collaboration with clinicians to determine the diagnostic needs of rural clinics. Key decisions, such as balancing model complexity with hardware limitations and ensuring interpretability for healthcare providers, were made during this phase. The team’s approach accounted for regulatory considerations, such as patient privacy and compliance with healthcare standards. This collaborative process ensured that the problem definition aligned with both technical feasibility and clinical relevance.</p>
</section>
<section id="sec-ai-workflow-scale-distribution-e38e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-scale-distribution-e38e">Scale and Distribution</h3>
<p>As ML systems scale, their problem definitions must adapt to new operational challenges<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. For example, the DR project initially focused on a limited number of clinics with consistent imaging setups. However, as the system expanded to include clinics with varying equipment, staff expertise, and patient demographics<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, the original problem definition required adjustments to accommodate these variations.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>ML System Scaling Complexity</strong>: Scaling ML systems is exponentially more complex than traditional software due to data heterogeneity, model drift, and infrastructure requirements. Studies show that ML systems require 10x more monitoring infrastructure than traditional applications <span class="citation" data-cites="paleyes2022challenges">(<a href="#ref-paleyes2022challenges" role="doc-biblioref">Paleyes, Urma, and Lawrence 2022</a>)</span>, with companies like Uber running 1,000+ model quality checks daily across their ML platform <span class="citation" data-cites="uber2017michelangelo">(<a href="#ref-uber2017michelangelo" role="doc-biblioref">Hermann and Del Balso 2017</a>)</span>. The “scaling wall” typically hits at 100+ models in production, where manual processes break down and teams need specialized MLOps platforms—explaining why the ML platform market grew from approximately $350M in 2019 to $4.0B in 2023, with pure MLOps tools reaching $1.2B in 2023 <span class="citation" data-cites="kreuzberger2023machine">(<a href="#ref-kreuzberger2023machine" role="doc-biblioref">Kreuzberger, Kühl, and Hirschl 2023</a>)</span>.</p><div id="ref-kreuzberger2023machine" class="csl-entry" role="listitem">
Kreuzberger, Dominik, Niklas Kühl, and Sebastian Hirschl. 2023. <span>“Machine Learning Operations (MLOps): Overview, Definition, and Architecture.”</span> <em>IEEE Access</em> 11: 31866–79. <a href="https://doi.org/10.1109/access.2023.3262138">https://doi.org/10.1109/access.2023.3262138</a>.
</div></div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Algorithmic Fairness in Healthcare</strong>: Medical AI systems show significant performance disparities across demographic groups—dermatology AI trained on light-skinned patients shows 36% worse accuracy on dark-skinned patients <span class="citation" data-cites="larson2017gender">(<a href="#ref-larson2017gender" role="doc-biblioref">Chin-Purcell and Chambers 2021</a>)</span>, while diabetic retinopathy models trained primarily on European populations show 15-25% accuracy drops for Asian and African populations <span class="citation" data-cites="gulshan2016deep">(<a href="#ref-gulshan2016deep" role="doc-biblioref">Gulshan et al. 2016</a>)</span>. The FDA’s 2021 Action Plan for AI/ML-based medical devices now requires demographic performance reporting <span class="citation" data-cites="fda2021artificial">(<a href="#ref-fda2021artificial" role="doc-biblioref">Food and Administration 2021</a>)</span>, and companies like Google Health spend 20-30% of development resources on fairness testing and bias mitigation across racial, gender, and socioeconomic groups <span class="citation" data-cites="rajkomar2019machine">(<a href="#ref-rajkomar2019machine" role="doc-biblioref">Rajkomar, Dean, and Kohane 2019</a>)</span>.</p><div id="ref-larson2017gender" class="csl-entry" role="listitem">
Chin-Purcell, Lia, and America Chambers. 2021. <span>“Investigating Accuracy Disparities for Gender Classification Using Convolutional Neural Networks.”</span> In <em>2021 IEEE International Symposium on Technology and Society (ISTAS)</em>, 81:1–7. IEEE. <a href="https://doi.org/10.1109/istas52410.2021.9629153">https://doi.org/10.1109/istas52410.2021.9629153</a>.
</div><div id="ref-gulshan2016deep" class="csl-entry" role="listitem">
Gulshan, Varun, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, et al. 2016. <span>“Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.”</span> <em>JAMA</em> 316 (22): 2402. <a href="https://doi.org/10.1001/jama.2016.17216">https://doi.org/10.1001/jama.2016.17216</a>.
</div><div id="ref-fda2021artificial" class="csl-entry" role="listitem">
Food, U. S., and Drug Administration. 2021. <span>“Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan.”</span> U.S. Department of Health; Human Services. <a href="https://www.fda.gov/media/145022/download">https://www.fda.gov/media/145022/download</a>.
</div><div id="ref-rajkomar2019machine" class="csl-entry" role="listitem">
Rajkomar, Alvin, Jeffrey Dean, and Isaac Kohane. 2019. <span>“Machine Learning in Medicine.”</span> <em>New England Journal of Medicine</em> 380 (14): 1347–58. <a href="https://doi.org/10.1056/nejmra1814259">https://doi.org/10.1056/nejmra1814259</a>.
</div></div></div><p>Scaling also introduces data challenges. Larger datasets may include more diverse edge cases, which can expose weaknesses in the initial model design. In the DR project, for instance, expanding the deployment to new regions introduced variations in imaging equipment and patient populations that required further tuning of the system. Defining a problem that accommodates such diversity from the outset ensures the system can handle future expansion without requiring a complete redesign.</p>
<p><strong>Connecting to the Next Stage:</strong> The DR team’s problem definition process shaped their data collection strategy. Their requirements for multi-population validation drove the need for diverse training data, while edge deployment constraints influenced data preprocessing approaches. Their regulatory compliance needs determined annotation protocols and quality assurance standards. These interconnected requirements demonstrate how effective problem definition anticipates constraints that will emerge in subsequent lifecycle stages, establishing a foundation for integrated system development rather than sequential, isolated optimization.</p>
<p>With problem definition complete, the development process transitions to assembling the data resources needed to achieve these objectives.</p>
<div id="quiz-question-sec-ai-workflow-problem-definition-87d9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>How does problem definition in machine learning systems fundamentally differ from traditional software development?</p>
<ol type="a">
<li>ML systems require deterministic specifications.</li>
<li>Traditional software focuses on learning from data.</li>
<li>ML systems are defined by examples and desired behaviors.</li>
<li>ML systems do not need to consider real-world constraints.</li>
</ol></li>
<li><p>Explain why aligning learning objectives with system constraints is crucial in ML problem definition.</p></li>
<li><p>True or False: A well-defined ML problem only needs to focus on achieving high performance metrics.</p></li>
<li><p>The process of defining an ML problem involves identifying the core objective of the system and the constraints it must satisfy, often requiring collaboration with stakeholders to gather ____ knowledge.</p></li>
<li><p>In a production system, what are the potential consequences of a poorly defined ML problem?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-problem-definition-87d9" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-data-collection-ab2c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-data-collection-ab2c">Data Collection</h2>
<p>Data collection and preparation present unique challenges that extend beyond gathering sufficient training examples<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. These fundamental challenges form the core focus of <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong>. For medical AI systems like DR screening, data collection must balance statistical rigor with operational feasibility while meeting the highest standards for diagnostic accuracy.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>The 80/20 Rule in ML</strong>: Data scientists spend 80% of their time on data collection, cleaning, and preparation: only 20% on actual modeling. This ratio, first documented by CrowdFlower <span class="citation" data-cites="crowdflower2016data">(<a href="#ref-crowdflower2016data" role="doc-biblioref">CrowdFlower, n.d.</a>)</span> in 2016, remains consistent across industries despite advances in automated tools. The “data preparation tax” includes handling missing values (present in 90% of real-world datasets), resolving inconsistencies (affecting 60% of data fields), and ensuring legal compliance (requiring 15+ different consent mechanisms for EU data). This explains why successful ML teams invest heavily in data engineering capabilities from day one.</p><div id="ref-crowdflower2016data" class="csl-entry" role="listitem">
CrowdFlower. n.d. <span>“Supplemental Information 1: Source Code for Analysis in Matlab, Correlation Matrix, XML Code for Crowdflower Survey.”</span> <em>CrowdFlower Inc</em>. PeerJ. <a href="https://doi.org/10.7287/peerj.preprints.1069/supp-1">https://doi.org/10.7287/peerj.preprints.1069/supp-1</a>.
</div></div></div><p><strong>DR Case Study: Building Clinical-Grade Training Data</strong></p>
<p>The DR team’s data collection challenges illustrate how problem definition decisions shape data requirements. Their multi-dimensional success criteria (accuracy across diverse populations, hardware efficiency, and regulatory compliance) required a data collection strategy that went beyond typical computer vision datasets.</p>
<p><strong>Training Data Architecture:</strong> The team assembled a development dataset of 128,000 retinal fundus photographs, each reviewed by 3-7 expert ophthalmologists from a panel of 54 specialists<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. This expert consensus approach addressed the inherent subjectivity in medical diagnosis while establishing ground truth labels that could withstand regulatory scrutiny. The annotation process captured clinically relevant features like microaneurysms, hemorrhages, and hard exudates across the spectrum of disease severity.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Medical Data Annotation Costs</strong>: Expert medical annotation is extraordinarily expensive: ophthalmologists charge $200-500 per hour, meaning the DR dataset’s annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history, driving interest in active learning and synthetic data generation.</p></div></div><p><strong>Infrastructure Demands:</strong> Each high-resolution retinal scan generated files ranging from tens to hundreds of megabytes, creating infrastructure challenges that influenced model development and deployment strategies. The team implemented a multi-tier storage architecture: hot tier SSD storage for active training data (sub-100ms access), warm tier HDD storage for historical datasets, and cold tier object storage for archives. This infrastructure investment proved essential for supporting the iterative model development process that followed.</p>
<section id="sec-ai-workflow-data-requirements-impact-6975" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-data-requirements-impact-6975">From Laboratory to Clinic: Data Reality Gaps</h3>
<p>The DR team’s transition from laboratory-quality training data to real-world deployment data revealed fundamental gaps that would require innovative solutions across the entire system architecture.</p>
<p><strong>Clinical Deployment Challenges:</strong> When deployment began in rural clinics across Thailand and India, the team discovered that real-world data differed from their carefully curated training set. Images came from diverse camera equipment operated by staff with varying expertise levels, often under suboptimal lighting conditions and with inconsistent patient positioning. These variations threatened model performance and revealed the need for robust preprocessing and quality assurance systems.</p>
<p><strong>Infrastructure Constraints Drive Architecture:</strong> Rural clinics processing 50 patients daily generated 2.5-10GB of imaging data, exceeding typical bandwidth of 1-5 Mbps in these environments. This constraint drove a fundamental architectural decision: edge computing deployment rather than cloud-based inference. Local preprocessing reduced bandwidth requirements by 95% but required 10x more local computational resources, shaping both model optimization strategies and deployment hardware requirements using specialized edge devices like NVIDIA Jetson<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>NVIDIA Jetson</strong>: Series of embedded computing boards designed for AI edge computing, featuring GPU acceleration in power-efficient form factors (5-30 watts vs.&nbsp;250+ watts for desktop GPUs). First released in 2014, Jetson modules enable real-time AI inference on devices like autonomous drones, medical equipment, and industrial robots. Popular models include Jetson Nano ($99, 472 GFLOPS), Jetson Xavier ($399, 21 TOPS), and Jetson Orin ($599, 275 TOPS), making high-performance AI accessible for edge deployment scenarios where cloud connectivity is unreliable or latency-critical.</p></div></div><p><strong>Distributed System Design:</strong> The solution architecture emerged directly from data collection constraints: NVIDIA Jetson edge devices (4GB RAM, 128 CUDA cores) for local inference, clinic aggregation servers (8-core CPUs, 32GB RAM) for data management, and cloud training infrastructure using 32-GPU clusters for weekly model updates. This distributed approach achieved sub-100ms inference latency with 94% uptime across 200+ clinic deployments.</p>
<p><strong>Privacy and Compliance Integration:</strong> Patient privacy regulations required federated learning architecture, enabling model training without centralizing sensitive patient data. This approach added complexity to both data collection workflows and model training infrastructure, but proved essential for regulatory approval and clinical adoption.</p>
<p>These data collection experiences demonstrate how early lifecycle decisions create constraints and opportunities that propagate through the entire system development process.</p>
</section>
<section id="sec-ai-workflow-data-infrastructure-5088" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-data-infrastructure-5088">Infrastructure Design Principles</h3>
<p>The DR team’s infrastructure requirements illustrate how data characteristics and deployment constraints drive architectural decisions. Each retinal image followed a complex journey: capture on clinic cameras, local storage and initial processing, quality validation, secure transmission to central systems, and integration with training datasets. This flow revealed infrastructure design principles that apply broadly to AI systems.</p>
<p><strong>Multi-Tier Storage Strategy:</strong> Different data access patterns require different storage solutions. The team implemented a tiered approach balancing cost, performance, and availability: frequently accessed training data required high-speed storage for rapid model iteration, while historical datasets could tolerate slower access times in exchange for cost efficiency. Intelligent caching systems optimized data access based on usage patterns, ensuring that relevant data remained readily available.</p>
<p><strong>Connectivity Adaptation:</strong> Rural clinic deployments faced significant connectivity constraints, requiring flexible data transmission strategies. Real-time transmission worked well for clinics with reliable internet, while store-and-forward systems enabled operation in areas with intermittent connectivity. This adaptive approach ensured consistent system operation regardless of local infrastructure limitations.</p>
<p><strong>Scalable Architecture:</strong> Infrastructure design anticipated growth from pilot deployments to hundreds of clinics. The architecture accommodated varying data volumes, different hardware configurations, and diverse operational requirements while maintaining data consistency and system reliability. This scalability foundation proved essential as the system expanded to new regions.</p>
</section>
<section id="sec-ai-workflow-scale-distribution-7fc8" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-scale-distribution-7fc8">Scale and Distribution</h3>
<p>As ML systems scale, the challenges of data collection grow exponentially. In the DR project, scaling from an initial few clinics to a broader network introduced significant variability in equipment, workflows, and operating conditions. Each clinic effectively became an independent data node<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, yet the system needed to ensure consistent performance and reliability across all locations. The team discovered that distributed coordination required specialized orchestration: data teams managed pipeline development and quality monitoring, model teams handled training infrastructure and experiment tracking, while infrastructure teams provided resource provisioning and automated scaling. Cross-team coordination protocols included shared artifact repositories for centralized model and data versioning, versioned APIs for component integration, and automated testing pipelines that validated cross-team integrations. This distributed workflow orchestration enabled the system to manage the 200+ clinic network efficiently, with centralized experiment tracking systems processing metadata from thousands of daily inference requests while maintaining sub-second response times for clinic operations.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Federated Learning Architecture</strong>: Federated learning <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>, introduced by Google in 2016 for mobile keyboards, enables training across distributed data sources without centralizing data. Healthcare applications are particularly suited for federated learning due to privacy regulations: studies show federated medical models achieve 85-95% of centralized model accuracy while keeping data local. However, federated learning introduces new challenges: communication costs increase 100-1000x per training iteration, and statistical heterogeneity across sites can cause model convergence issues that centralized training doesn’t face.</p><div id="ref-mcmahan2017communication" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. <span>“Communication-Efficient Learning of Deep Networks from Decentralized Data.”</span> In <em>Artificial Intelligence and Statistics</em>, 1273–82. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div></div><p>Scaling the DR system to additional clinics also brought increasing data volumes, as higher-resolution imaging devices became standard, generating larger and more detailed images. These advances amplified the demands on storage and processing infrastructure, requiring optimizations to maintain efficiency without compromising quality. Differences in patient demographics, clinic workflows, and connectivity patterns further underscored the need for robust design to handle these variations gracefully.</p>
<p>Scaling challenges highlight how decisions made during the data collection phase ripple through the lifecycle, impacting subsequent stages like model development, deployment, and monitoring. For instance, accommodating higher-resolution data during collection directly influences computational requirements for training and inference, emphasizing the need for lifecycle thinking even at this early stage.</p>
</section>
<section id="sec-ai-workflow-data-validation-5359" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-data-validation-5359">Data Validation</h3>
<p>Quality assurance is an integral part of the data collection process, ensuring that data meets the requirements for downstream stages. In the DR project, automated checks at the point of collection flagged issues like poor focus or incorrect framing, allowing clinic staff to address problems immediately. These proactive measures ensured that low-quality data was not propagated through the pipeline.</p>
<p>Validation systems extended these efforts by verifying not just image quality but also proper labeling, patient association, and compliance with privacy regulations. Operating at both local and centralized levels, these systems ensured data reliability and robustness, safeguarding the integrity of the entire ML pipeline.</p>
<p><strong>Setting the Stage for Model Development:</strong> The DR team’s data collection experiences informed their model development approach. The infrastructure constraints discovered during data collection (limited bandwidth, diverse hardware, intermittent connectivity) established requirements for model efficiency that would drive architectural decisions. The distributed federated learning approach required by privacy constraints influenced training pipeline design. The quality variations observed across different clinic environments shaped validation strategies and robustness requirements. This coupling between data collection insights and model development strategies exemplifies how successful AI systems require integrated lifecycle planning rather than sequential stage optimization.</p>
<p><a href="#fig-ml-lifecycle-feedback" class="quarto-xref">Figure&nbsp;4</a> illustrates these critical feedback loops that enable continuous system improvement. As we transition to model development, we will see how the foundation established during data collection enables or constrains the technical approaches available for creating effective models.</p>
<div id="fig-ml-lifecycle-feedback" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-lifecycle-feedback-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5d4c6dbc2d5c776a4c56979510d7852864221bd3.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: ML Lifecycle Dependencies: Iterative feedback loops connect data collection, preparation, model training, evaluation, and monitoring, emphasizing that each stage informs and influences subsequent stages in a continuous process. Effective machine learning system development requires acknowledging these dependencies to refine data, retrain models, and maintain performance over time."><img src="workflow_files/mediabag/5d4c6dbc2d5c776a4c56979510d7852864221bd3.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-lifecycle-feedback-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>ML Lifecycle Dependencies</strong>: Iterative feedback loops connect data collection, preparation, model training, evaluation, and monitoring, emphasizing that each stage informs and influences subsequent stages in a continuous process. Effective machine learning system development requires acknowledging these dependencies to refine data, retrain models, and maintain performance over time.
</figcaption>
</figure>
</div>
<div id="quiz-question-sec-ai-workflow-data-collection-ab2c" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>What is a significant challenge faced during data collection for medical ML systems like the DR project?</p>
<ol type="a">
<li>Low cost of data annotation</li>
<li>High cost and complexity of expert data annotation</li>
<li>Uniform data quality across all sources</li>
<li>Availability of large datasets without privacy concerns</li>
</ol></li>
<li><p>True or False: Data collection strategies have no impact on the ML system’s ability to handle new inputs over time.</p></li>
<li><p>Explain how the data collection process in the DR project influenced the system’s infrastructure design.</p></li>
<li><p>Which of the following is an example of how feedback loops in data collection influence the ML lifecycle?</p>
<ol type="a">
<li>Collecting additional data to address training data gaps</li>
<li>Ignoring data quality issues during model training</li>
<li>Reducing the number of data sources to simplify the system</li>
<li>Focusing only on model deployment without data updates</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-data-collection-ab2c" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-model-development-dfdc" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-model-development-dfdc">Model Development</h2>
<p>Model development and training form the core of machine learning systems, yet this stage presents unique challenges that extend beyond selecting algorithms and tuning hyperparameters<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. The training methodologies, infrastructure requirements, and distributed training strategies are covered in <strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>. In high-stakes domains like healthcare, every design decision impacts clinical outcomes, making the integration of technical performance with operational constraints critical.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Hyperparameter Optimization Complexity</strong>: Modern deep learning models have 10-100+ hyperparameters (learning rate, batch size, architecture choices), creating search spaces with 10^20+ possible combinations. AutoML platforms like Google’s AutoML and H2O spend $10,000-100,000 in compute costs to find optimal configurations for complex models. Random search (2012) surprisingly outperforms grid search, while Bayesian optimization (2010s) and population-based training (2017) represent current state-of-the-art, reducing tuning time by 10-100x but still requiring substantial computational resources that didn’t exist in traditional software development.</p></div></div><p><strong>DR Case Study: Constraint-Driven Model Architecture</strong></p>
<p>The DR team’s model development process exemplifies how early lifecycle decisions cascade through subsequent stages. Their problem definition requirements (expert-level accuracy with edge device compatibility) created an optimization challenge that required innovative approaches to model architecture and training strategies.</p>
<p><strong>Performance Achievement:</strong> Using transfer learning from ImageNet<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> and their meticulously labeled dataset of 128,000 images, the team achieved an F-score of 0.95, slightly exceeding the median performance of consulted ophthalmologists (0.91). This result validated their approach to combining large-scale pre-training with domain-specific fine-tuning.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Transfer Learning</strong>: A technique where models pre-trained on large datasets (like ImageNet’s 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements <span class="citation" data-cites="krizhevsky2012imagenet deng2009imagenet">(<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>; <a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span>. Introduced by Yann LeCun’s team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples.</p><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“ImageNet Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE; IEEE. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.
</div></div></div><p><strong>Architecture Constraints:</strong> Achieving high accuracy was the first challenge. The team’s data collection insights about edge deployment constraints meant their model had to meet strict efficiency requirements: under 100MB model size, sub-50ms inference latency, and under 500MB RAM usage. These constraints drove architectural innovations including model optimization techniques for size reduction, inference acceleration, and efficient deployment scenarios.</p>
<p><strong>Iterative Optimization:</strong> The model development process required continuous iteration between accuracy optimization and efficiency optimization. Each architectural decision had to be validated against test set metrics and the infrastructure constraints identified during data collection. This multi-objective optimization approach differs from traditional ML development focused solely on predictive performance.</p>
<section id="sec-ai-workflow-model-requirements-impact-6470" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-model-requirements-impact-6470">Balancing Clinical Performance with Deployment Reality</h3>
<p>The DR team’s model development experience illustrates the trade-offs between clinical effectiveness and deployment feasibility that characterize real-world AI systems.</p>
<p><strong>Clinical Performance Requirements:</strong> Medical applications demand specific performance metrics<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> that differ from standard ML evaluation. The DR system required &gt;90% sensitivity (to prevent vision loss from missed cases) and &gt;80% specificity (to avoid overwhelming referral systems). These metrics had to be maintained across diverse patient populations and image quality conditions.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Medical AI Performance Metrics</strong>: Medical AI requires different metrics than general ML: sensitivity (true positive rate) and specificity (true negative rate) are often more important than overall accuracy. For diabetic retinopathy screening, &gt;90% sensitivity is crucial (missing cases causes blindness), while &gt;80% specificity prevents unnecessary referrals. Medical AI also requires metrics like positive predictive value (PPV) and negative predictive value (NPV) that vary with disease prevalence in different populations—a model with 95% accuracy in a lab setting might have only 50% PPV in a low-prevalence population, making it clinically useless despite high technical performance.</p></div></div><p><strong>Multi-Constraint Optimization:</strong> The team discovered that optimizing for clinical performance alone was insufficient. Edge deployment constraints from the data collection phase imposed additional requirements: the model had to run efficiently on resource-limited hardware while maintaining real-time inference speeds compatible with clinical workflows. This created a multi-objective optimization problem where improvements in one dimension often came at the cost of others.</p>
<p><strong>Architecture Decision Impact:</strong> The choice to use an ensemble of lightweight models rather than a single large model exemplifies how model development decisions propagate through the system lifecycle. This architectural decision reduced individual model complexity (enabling edge deployment) but increased inference pipeline complexity (affecting deployment and monitoring strategies). The team developed orchestration logic for model ensembles and created monitoring systems that could track performance across multiple model components.</p>
<p><strong>Lifecycle Integration:</strong> These model development experiences reinforced the interconnected nature of the AI lifecycle. Architecture decisions influenced data preprocessing pipelines (models required specific input formats), training infrastructure requirements (ensemble training needed coordinated resources), and deployment strategies (multiple models required serving infrastructure). The team learned that successful model development requires anticipating constraints from subsequent lifecycle stages rather than optimizing models in isolation.</p>
</section>
<section id="sec-ai-workflow-development-workflow-b547" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-development-workflow-b547">Systematic Experimentation Under Constraints</h3>
<p>The DR team’s development workflow demonstrates how real-world constraints shape the entire model development process, from initial exploration through final optimization.</p>
<p><strong>Collaborative Foundation:</strong> Development began with collaboration between data scientists and ophthalmologists to identify image characteristics indicative of retinopathy. This interdisciplinary approach ensured that model architectures could capture clinically relevant features while meeting the computational constraints identified during data collection.</p>
<p><strong>Resource-Constrained Experimentation:</strong> The team faced computational constraints that shaped their experimental approach. Production ML workflows create multiplicative costs: 10 model variants × 5 hyperparameter sweeps × 3 preprocessing approaches = 150 training runs. At $1000 per run for medical-grade models, iteration costs reached $150K per experiment cycle. This economic reality drove innovations in efficient experimentation: intelligent job scheduling for resource sharing, caching of intermediate results, early stopping techniques for unpromising experiments, and automated resource optimization.</p>
<p><strong>Scientific Rigor:</strong> ML model development exhibits emergent behaviors that make outcomes inherently uncertain. The DR team implemented scientific methodology principles: controlled variables through fixed random seeds and environment versions, systematic ablation studies to isolate component contributions, confounding factor analysis to separate architecture effects from optimization effects, and statistical significance testing across multiple runs using A/B testing<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> frameworks. This approach proved essential for distinguishing genuine performance improvements from statistical noise.</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;<strong>A/B Testing in ML</strong>: Statistical method for comparing two model versions by randomly assigning users to different groups and measuring performance differences. Originally developed for web optimization (2000s), A/B testing became crucial for ML deployment because models can perform differently in production than in development. Companies like Netflix run hundreds of concurrent experiments with users participating in multiple tests simultaneously, while Uber tests 100+ ML model improvements weekly <span class="citation" data-cites="uber2017michelangelo">(<a href="#ref-uber2017michelangelo" role="doc-biblioref">Hermann and Del Balso 2017</a>)</span>. A/B testing requires careful statistical design to avoid confounding variables and ensure sufficient sample sizes for reliable conclusions.</p></div></div><p><strong>Integration with Deployment Constraints:</strong> Throughout development, the team validated models against deployment constraints identified in earlier lifecycle stages. Each architectural innovation had to be evaluated for accuracy improvements and compatibility with edge device limitations and clinical workflow requirements. This dual validation approach ensured that development efforts aligned with deployment goals rather than optimizing for laboratory conditions that wouldn’t translate to real-world performance.</p>
</section>
<section id="sec-ai-workflow-scale-distribution-56d9" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-scale-distribution-56d9">Scaling Model Development</h3>
<p>As the DR project evolved from prototype to production system, the team encountered increases in complexity across multiple dimensions: larger datasets, more sophisticated models, concurrent experiments, and distributed training infrastructure. These scaling challenges illustrate principles that apply broadly to large-scale AI system development.</p>
<p><strong>Infrastructure Orchestration:</strong> Moving from single-machine training to distributed systems introduced coordination requirements. The team balanced training speed improvements against increased system complexity, implementing fault tolerance mechanisms and automated failure recovery systems. Orchestration frameworks enabled component-based pipeline construction with reusable stages, automatic resource scaling, and monitoring across distributed components.</p>
<p><strong>Experiment Management:</strong> Systematic tracking became critical as experiments generated artifacts including model checkpoints, training logs, and performance metrics. Without structured organization, teams risk losing institutional knowledge from their experimentation efforts. The DR team’s solution involved implementing systematic experiment identification, automated artifact versioning, and search capabilities to query experiments by performance characteristics and configuration parameters.</p>
<p><strong>Resource Optimization:</strong> Large-scale model development requires resource allocation between training computation and supporting infrastructure. The team discovered that effective experiment management requires computational overhead, but this investment pays dividends in accelerated development cycles and improved model quality through systematic performance analysis and optimization.</p>
<p><strong>Transition to Deployment:</strong> The DR team’s model development achievements created challenges for deployment. Their optimized ensemble architecture, while meeting edge device constraints, required serving infrastructure. The distributed training approach that enabled rapid iteration required model versioning and synchronization across clinic deployments. The regulatory validation requirements that guided model development informed deployment validation and monitoring strategies. These interconnections demonstrate how successful model development must anticipate deployment challenges, ensuring that technical innovations can be translated into operational systems that deliver clinical value.</p>
<div id="quiz-question-sec-ai-workflow-model-development-dfdc" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key consideration when designing ML models for deployment in resource-constrained environments?</p>
<ol type="a">
<li>Ensuring high sensitivity and specificity</li>
<li>Maximizing model complexity</li>
<li>Using the largest possible dataset</li>
<li>Focusing solely on algorithm selection</li>
</ol></li>
<li><p>Explain why interdisciplinary collaboration is critical in the development of machine learning models for healthcare applications.</p></li>
<li><p>True or False: In the DR project, the model’s architecture decisions only affected the training phase and not the deployment strategy.</p></li>
<li><p>Order the following components of the model development workflow: (1) Data Exploration, (2) Model Design, (3) Training Infrastructure Setup, (4) Experiment Tracking.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-model-development-dfdc" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-deployment-2839" class="level2">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-deployment-2839">Deployment</h2>
<p>The trained model is integrated into production systems and workflows. Deployment requires addressing practical challenges such as system compatibility, scalability, and operational constraints. Successful integration ensures that the model’s predictions are accurate and actionable in real-world settings, where resource limitations and workflow disruptions can pose barriers. The operational aspects of deployment and maintenance are covered in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
<p>In the DR project, deployment strategies were shaped by the diverse environments in which the system would operate. Edge deployment enabled local processing of retinal images in rural clinics with intermittent connectivity, while automated quality checks flagged poor-quality images for recapture, ensuring reliable predictions. These measures demonstrate how deployment must bridge technological sophistication with usability and scalability across clinical settings.</p>
<section id="sec-ai-workflow-deployment-requirements-impact-2ef2" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-deployment-requirements-impact-2ef2">Deployment Requirements and Impact</h3>
<p>The requirements for deployment stem from both the technical specifications of the model and the operational constraints of its intended environment. In the DR project, the model operated in rural clinics with limited computational resources and intermittent internet connectivity. It had to fit into the existing clinical workflow, which required rapid, interpretable results that could assist healthcare providers without causing disruption.</p>
<p>These requirements influenced deployment strategies. A cloud-based deployment, while technically simpler, was not feasible due to unreliable connectivity in many clinics. Instead, the team opted for edge deployment, where models ran locally on clinic hardware. This approach required model optimization to meet specific hardware constraints: target metrics included under 100MB model size, sub-50ms inference latency, and under 500MB RAM usage on edge devices. Achieving these targets required systematic application of optimization techniques that reduce model size and computational requirements while balancing accuracy trade-offs. The team discovered that their original 2GB model with 95.2% accuracy could be optimized to 500MB with 94.8% accuracy, meeting deployment constraints while maintaining clinical utility through careful trade-off analysis and evaluation of accuracy versus efficiency.</p>
<p>Integration with existing systems posed additional challenges. The ML system had to interface with hospital information systems (HIS) for accessing patient records and storing results. Privacy regulations mandated secure data handling at every step, shaping deployment decisions. These considerations ensured that the system adhered to clinical and legal standards while remaining practical for daily use.</p>
</section>
<section id="sec-ai-workflow-deployment-workflow-9bd5" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-deployment-workflow-9bd5">Deployment Workflow</h3>
<p>The deployment and integration workflow in the DR project highlighted the interplay between model functionality, infrastructure, and user experience. The process began with thorough testing in simulated environments that replicated the technical constraints and workflows of the target clinics. These simulations helped identify potential bottlenecks and incompatibilities early, allowing the team to refine the deployment strategy before full-scale rollout.</p>
<p>Once the deployment strategy was finalized, the team implemented a phased rollout. Initial deployments were limited to a few pilot sites, allowing for controlled testing in real-world conditions. This approach provided valuable feedback from clinicians and technical staff, helping to identify issues that hadn’t surfaced during simulations.</p>
<p>Integration efforts focused on ensuring seamless interaction between the ML system and existing tools. For example, the DR system had to pull patient information from the HIS, process retinal images from connected cameras, and return results in a format that clinicians could easily interpret. These tasks required the development of robust APIs, real-time data processing pipelines, and user-friendly interfaces tailored to the needs of healthcare providers.</p>
</section>
<section id="sec-ai-workflow-scale-distribution-86c2" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-scale-distribution-86c2">Scaling Deployment Across Diverse Environments</h3>
<p>Deploying the DR system across multiple clinic locations revealed the fundamental challenges of scaling AI systems beyond controlled laboratory environments. Each clinic presented unique constraints: different imaging equipment, varying network reliability, diverse operator expertise levels, and distinct workflow patterns.</p>
<p><strong>Laboratory-to-Production Gap:</strong> The transition from development to deployment exposed significant performance challenges. Variations in imaging equipment and operator expertise created data quality inconsistencies that the model struggled to handle. Infrastructure constraints forced emergency model optimizations, demonstrating how deployment realities can propagate backwards through the development process, influencing preprocessing strategies, architecture decisions, and validation approaches.</p>
<p><strong>Architecture Trade-offs:</strong> The team discovered that deployment architecture decisions create cascading effects throughout the system. Edge deployment minimized latency for real-time clinical workflows but imposed strict constraints on model complexity. Cloud deployment enabled model flexibility but introduced latency that proved unacceptable for time-sensitive medical applications.</p>
<p><strong>Operational Integration:</strong> Successful deployment required more than technical optimization. Clinician feedback revealed that initial system interfaces needed significant redesign to achieve widespread adoption. The team had to balance technical sophistication with clinical usability, recognizing that user trust and proficiency were as critical as algorithmic performance.</p>
<p><strong>Update Synchronization:</strong> Managing improvements across distributed deployments required sophisticated coordination mechanisms. Centralized version control systems and automated update pipelines ensured that performance improvements reached all deployment sites while minimizing disruption to clinical operations. As illustrated in <a href="#fig-ml-lifecycle-feedback" class="quarto-xref">Figure&nbsp;4</a>, deployment challenges create multiple feedback paths that drive continuous system improvement.</p>
</section>
<section id="sec-ai-workflow-robustness-reliability-cce1" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-robustness-reliability-cce1">Robustness and Reliability</h3>
<p>In a clinical context, reliability is paramount. The DR system needed to function seamlessly under a wide range of conditions, from high patient volumes to suboptimal imaging setups. To ensure robustness, the team implemented fail-safes that could detect and handle common issues, such as incomplete or poor-quality data. These mechanisms included automated image quality checks and fallback workflows for cases where the system encountered errors.</p>
<p>Testing played a central role in ensuring reliability. The team conducted extensive stress testing to simulate peak usage scenarios, validating that the system could handle high throughput without degradation in performance. Redundancy was built into critical components to minimize the risk of downtime, and all interactions with external systems, such as the HIS, were rigorously tested for compatibility and security.</p>
<p><strong>Preparing for Continuous Operation:</strong> The DR team’s deployment experiences revealed how this stage transitions from development-focused activities to operation-focused concerns. Real-world deployment feedback (from clinician usability concerns to hardware performance issues) generated insights that would inform ongoing monitoring and maintenance strategies. The distributed edge deployment architecture created new requirements for system-wide monitoring and coordinated updates. The integration challenges with hospital information systems established protocols for managing system evolution without disrupting clinical workflows.</p>
<p>Successful deployment establishes the foundation for effective monitoring and maintenance, creating the operational infrastructure, feedback mechanisms, and performance baselines that enable continuous system improvement. The deployment experience demonstrates that this stage is not an endpoint but a transition into the continuous operations phase of the AI lifecycle.</p>
<div id="quiz-question-sec-ai-workflow-deployment-2839" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which deployment strategy was chosen for the DR project due to unreliable connectivity in rural clinics?</p>
<ol type="a">
<li>Cloud-based deployment</li>
<li>Hybrid deployment</li>
<li>Edge deployment</li>
<li>Centralized deployment</li>
</ol></li>
<li><p>True or False: The deployment of ML systems in the DR project required no modifications to existing clinical workflows.</p></li>
<li><p>Explain how deployment feedback in the DR project influenced subsequent model optimizations.</p></li>
<li><p>What was a critical consideration for ensuring the reliability of the DR system in clinical settings?</p>
<ol type="a">
<li>Implementing fail-safes for common issues</li>
<li>Maximizing computational efficiency</li>
<li>Reducing deployment costs</li>
<li>Increasing the number of deployment sites</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-deployment-2839" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ai-workflow-maintenance-e184" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-maintenance-e184">Maintenance</h2>
<p>Monitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, changing usage patterns, and evolving operational requirements<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs. These operational practices form the foundation of <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Data Drift Detection</strong>: Data drift occurs when input data characteristics change over time: user behavior shifts, sensor calibration drifts, or population demographics evolve. Studies show that 78% of production ML models experience significant data drift within 12 months <span class="citation" data-cites="breck2017ml">(<a href="#ref-breck2017ml" role="doc-biblioref">Breck et al. 2017</a>)</span>, yet only 23% of organizations have automated drift detection <span class="citation" data-cites="paleyes2022challenges">(<a href="#ref-paleyes2022challenges" role="doc-biblioref">Paleyes, Urma, and Lawrence 2022</a>)</span>. Statistical tests like Kolmogorov-Smirnov and Population Stability Index can detect drift, but require setting thresholds and monitoring 100+ features continuously. Cloud providers now offer drift detection services (AWS SageMaker Model Monitor, Google AI Platform), but custom implementation remains necessary for domain-specific requirements.</p><div id="ref-breck2017ml" class="csl-entry" role="listitem">
Breck, Eric, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley. 2017. <span>“The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction.”</span> In <em>2017 IEEE International Conference on Big Data (Big Data)</em>, 1123–32. IEEE; IEEE. <a href="https://doi.org/10.1109/bigdata.2017.8258038">https://doi.org/10.1109/bigdata.2017.8258038</a>.
</div><div id="ref-paleyes2022challenges" class="csl-entry" role="listitem">
Paleyes, Andrei, Raoul-Gabriel Urma, and Neil D. Lawrence. 2022. <span>“Challenges in Deploying Machine Learning: A Survey of Case Studies.”</span> <em>ACM Computing Surveys</em> 55 (6): 1–29. <a href="https://doi.org/10.1145/3533378">https://doi.org/10.1145/3533378</a>.
</div></div><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Model Drift Phenomenon</strong>: ML models degrade over time without any code changes—a phenomenon unknown in traditional software. Studies show that 50-80% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift <span class="citation" data-cites="polyzotis2019data">(<a href="#ref-polyzotis2019data" role="doc-biblioref">Polyzotis et al. 2017</a>)</span>. This “silent failure” problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering.</p><div id="ref-polyzotis2019data" class="csl-entry" role="listitem">
Polyzotis, Neoklis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich. 2017. <span>“Data Management Challenges in Production Machine Learning.”</span> In <em>Proceedings of the 2017 ACM International Conference on Management of Data</em>, 1723–26. ACM. <a href="https://doi.org/10.1145/3035918.3054782">https://doi.org/10.1145/3035918.3054782</a>.
</div></div></div><p>As shown in <a href="#fig-ml-lifecycle-feedback" class="quarto-xref">Figure&nbsp;4</a>, monitoring serves as a central hub for system improvement, generating three critical feedback loops: “Performance Insights” flowing back to data collection to address gaps, “Data Quality Issues” triggering refinements in data preparation, and “Model Updates” initiating retraining when performance drifts. In the DR project, these feedback loops enabled continuous system improvement, from identifying underrepresented patient demographics (triggering new data collection) to detecting image quality issues (improving preprocessing) and addressing model drift (initiating retraining).</p>
<p>For DR screening, continuous monitoring tracked system performance across diverse clinics, detecting issues such as changing patient demographics or new imaging technologies that could impact accuracy. Proactive maintenance included plans to incorporate 3D imaging modalities like OCT, expanding the system’s capabilities to diagnose a wider range of conditions. This highlights the importance of designing systems that can adapt to future challenges while maintaining compliance with rigorous healthcare regulations and the responsible AI principles explored in <strong><a href="../core/responsible_ai/responsible_ai.html#sec-responsible-ai">Chapter 16: Responsible AI</a></strong>.</p>
<section id="sec-ai-workflow-monitoring-requirements-impact-104b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-monitoring-requirements-impact-104b">Monitoring Requirements and Impact</h3>
<p>The requirements for monitoring and maintenance emerged from both technical needs and operational realities. In the DR project, the technical perspective required continuous tracking of model performance, data quality, and system resource usage. However, operational constraints added layers of complexity: monitoring systems had to align with clinical workflows, detect shifts in patient demographics, and provide actionable insights to both technical teams and healthcare providers.</p>
<p>Initial deployment highlighted several areas where the system failed to meet real-world needs, such as decreased accuracy in clinics with outdated equipment or lower-quality images. Monitoring systems detected performance drops in specific subgroups, such as patients with less common retinal conditions, demonstrating that even a well-trained model could face blind spots in practice<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>. These insights informed maintenance strategies, including targeted updates to address specific challenges and expanded training datasets to cover edge cases.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>The Lab-to-Clinic Performance Gap</strong>: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the “deployment reality gap.” This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions—different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require “real-world performance studies” for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility.</p></div><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Population Stability Index (PSI)</strong>: Statistical measure that quantifies how much a dataset’s distribution has shifted compared to a baseline, with values 0-0.1 indicating minimal shift, 0.1-0.2 moderate shift requiring investigation, and &gt;0.2 significant shift requiring model retraining. Developed by credit risk analysts in the 1990s, PSI became standard for ML monitoring because distribution shifts often precede model performance degradation. PSI = Σ((actual% - expected%) × ln(actual%/expected%)), providing early warning of data drift before accuracy metrics decline, which is crucial since model retraining can take days or weeks. To prevent alert fatigue, the team limited alerts to 10 per day per team, implementing escalation hierarchies and alert suppression mechanisms. To support this, the team implemented advanced logging and analytics pipelines to process large amounts of operational data from clinics without disrupting diagnostic workflows. Secure and efficient data handling was essential to transmit data across multiple clinics while preserving patient confidentiality.</p></div></div><p>These requirements influenced system design significantly. The critical nature of the DR system’s function demanded real-time monitoring capabilities rather than periodic offline evaluations. The team established quantitative performance thresholds with clear action triggers: P95 latency exceeding 2x baseline generated immediate alerts with 5-minute response SLAs, model accuracy drops greater than 5% triggered daily alerts with automated retraining workflows, data drift Population Stability Index (PSI)<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> scores above 0.2 initiated weekly alerts with data team notifications, and resource utilization exceeding 80% activated auto-scaling mechanisms with cost monitoring.</p>
<p>Monitoring requirements also affected model design, as the team incorporated mechanisms for granular performance tracking and anomaly detection. Even the system’s user interface was influenced, needing to present monitoring data in a clear, actionable manner for clinical and technical staff alike.</p>
</section>
<section id="sec-ai-workflow-maintenance-workflow-ed45" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-maintenance-workflow-ed45">Maintenance Workflow</h3>
<p>The monitoring and maintenance workflow in the DR project revealed the intricate interplay between automated systems, human expertise, and evolving healthcare practices. The process began with defining a complete monitoring framework, establishing key performance indicators (KPIs), and implementing dashboards and alert systems. This framework had to balance depth of monitoring with system performance and privacy considerations, collecting sufficient data to detect issues without overburdening the system or violating patient confidentiality.</p>
<p>As the system matured, maintenance became an increasingly dynamic process. Model updates driven by new medical knowledge or performance improvements required careful validation and controlled rollouts. The team employed A/B testing frameworks to evaluate updates in real-world conditions and implemented rollback mechanisms<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> to address issues quickly when they arose.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Rollback Mechanisms</strong>: Automated systems that quickly revert software to a previous stable version when issues are detected, essential for maintaining service reliability during deployments. In traditional software, rollbacks take 5-30 minutes and restore deterministic behavior, but ML rollbacks are more complex because model behavior depends on current data distributions. Companies like Uber maintain shadow deployments where old and new models run simultaneously, enabling instant rollbacks within 60 seconds while preserving prediction consistency <span class="citation" data-cites="uber2017michelangelo">(<a href="#ref-uber2017michelangelo" role="doc-biblioref">Hermann and Del Balso 2017</a>)</span>. ML rollbacks require careful consideration of data compatibility and feature dependencies.</p><div id="ref-uber2017michelangelo" class="csl-entry" role="listitem">
Hermann, Jeremy, and Mike Del Balso. 2017. <span>“Michelangelo: Uber’s Machine Learning Platform.”</span> In <em>Data Engineering Bulletin</em>, 40:8–21. 4.
</div></div></div><p>Monitoring and maintenance formed an iterative cycle rather than discrete phases. Insights from monitoring informed maintenance activities, while maintenance efforts often necessitated updates to monitoring strategies. The team developed workflows to transition seamlessly from issue detection to resolution, involving collaboration across technical and clinical domains.</p>
</section>
<section id="sec-ai-workflow-scale-distribution-0f2e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-scale-distribution-0f2e">Scale and Distribution</h3>
<p>As the DR project scaled from pilot sites to widespread deployment, monitoring and maintenance complexities grew exponentially. Each additional clinic added to the volume of operational data and introduced new environmental variables, such as differing hardware configurations or demographic patterns.</p>
<p>The need to monitor both global performance metrics and site-specific behaviors required sophisticated infrastructure. The monitoring system tracked stage-level metrics including processing time, error rates, and resource utilization across the distributed workflow, maintained complete data lineage<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> tracking with source-to-prediction audit trails for regulatory compliance, correlated production issues with specific training experiments to enable rapid root cause analysis, and provided cost attribution tracking resource usage across teams and projects.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Data Lineage</strong>: Complete record of data flow from source systems through transformations to final outputs, enabling traceability, debugging, and regulatory compliance. Originally developed for financial systems (1990s) to meet audit requirements, data lineage became crucial for ML because model predictions depend on complex data pipelines with 10+ transformation steps. Regulations like GDPR “right to explanation” require organizations to trace how individual data points influence ML decisions. Companies like Netflix track lineage for 100,000+ daily data transformations, while financial firms maintain 7+ years of lineage data for regulatory compliance. While global metrics provided an overview of system health, localized issues, including a hardware malfunction at a specific clinic or unexpected patterns in patient data, needed targeted monitoring. Advanced analytics systems processed data from all clinics to identify these localized anomalies while maintaining a system-wide perspective, enabling the team to detect subtle system-wide diagnostic pattern shifts that were invisible in individual clinics but evident in aggregated data.</p></div></div><p>Continuous adaptation added further complexity. Real-world usage exposed the system to an ever-expanding range of scenarios. Capturing insights from these scenarios and using them to drive system updates required efficient mechanisms for integrating new data into training pipelines and deploying improved models without disrupting clinical workflows.</p>
</section>
<section id="sec-ai-workflow-proactive-maintenance-2b10" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-proactive-maintenance-2b10">Proactive Maintenance</h3>
<p>Reactive maintenance alone was insufficient for the DR project’s dynamic operating environment. Proactive strategies became essential to anticipate and prevent issues before they affected clinical operations.</p>
<p>The team implemented predictive maintenance models to identify potential problems based on patterns in operational data. Continuous learning pipelines allowed the system to retrain and adapt based on new data, ensuring its relevance as clinical practices or patient demographics evolved. These capabilities required careful balancing to ensure safety and reliability while maintaining system performance.</p>
<p>Metrics assessing adaptability and resilience became as important as accuracy, reflecting the system’s ability to evolve alongside its operating environment. Proactive maintenance ensured the system could handle future challenges without sacrificing reliability.</p>
<p><strong>Full Circle: Closing the AI Lifecycle Loop</strong></p>
<p>The DR team’s monitoring and maintenance experiences brought their lifecycle journey full circle, revealing how successful AI systems require continuous evolution rather than one-time deployment. Monitoring insights led to refined problem definitions for future system iterations. Data quality issues discovered in production informed improved data collection protocols. Model performance patterns guided architectural improvements in subsequent versions. Infrastructure challenges from distributed deployment influenced maintenance strategies and resource planning.</p>
<p>This continuous feedback and improvement cycle shows that the AI lifecycle is not a linear process but an ongoing system that evolves with changing requirements, technological capabilities, and real-world conditions. The DR team’s success emerged not from perfecting individual lifecycle stages in isolation, but from building systems that could learn, adapt, and improve through integrated lifecycle thinking.</p>
<div id="quiz-question-sec-ai-workflow-maintenance-e184" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>What is a primary reason that monitoring is critical in deployed ML systems?</p>
<ol type="a">
<li>To detect and address model drift over time.</li>
<li>To ensure the system meets initial training accuracy.</li>
<li>To replace traditional software debugging processes.</li>
<li>To maintain the original data distribution.</li>
</ol></li>
<li><p>Explain how feedback loops in monitoring contribute to the maintenance of ML systems.</p></li>
<li><p>Order the following steps in a typical ML maintenance workflow: (1) Define monitoring framework, (2) Detect performance issues, (3) Implement model updates, (4) Validate updates.</p></li>
<li><p>True or False: Proactive maintenance in ML systems involves only reacting to issues as they occur.</p></li>
<li><p>In the context of the DR project, why was real-time monitoring preferred over periodic evaluations?</p>
<ol type="a">
<li>To reduce system resource usage.</li>
<li>To comply with healthcare regulations.</li>
<li>To quickly detect and address performance issues.</li>
<li>To simplify the monitoring process.</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-maintenance-e184" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="systems-thinking-in-ai-development" class="level2">
<h2 class="anchored" data-anchor-id="systems-thinking-in-ai-development">Systems Thinking in AI Development</h2>
<p>Having explored each stage of the AI lifecycle through the DR case study, we can now step back to examine the systems-level principles that enable successful AI development. The interconnected nature of AI systems requires different thinking patterns than traditional software development, where stages can often be optimized in isolation.</p>
<section id="interdependence-and-constraint-propagation" class="level3">
<h3 class="anchored" data-anchor-id="interdependence-and-constraint-propagation">Interdependence and Constraint Propagation</h3>
<p>The DR project demonstrates how decisions made in any lifecycle stage create constraints and opportunities that ripple through the entire system. Problem definition requirements for regulatory compliance influenced data collection protocols, which shaped model architecture choices, which determined deployment strategies, which informed monitoring approaches. This constraint propagation means that AI system architects must think holistically from the project’s inception, anticipating how early decisions will influence later stages.</p>
<p>Constraint propagation works in reverse as well. Deployment challenges in rural clinics forced model optimization that required new data preprocessing approaches. Monitoring insights about demographic bias triggered expanded data collection efforts. This bidirectional influence between stages requires flexible architectures that can adapt to evolving requirements while maintaining system integrity.</p>
</section>
<section id="feedback-loops-and-continuous-learning" class="level3">
<h3 class="anchored" data-anchor-id="feedback-loops-and-continuous-learning">Feedback Loops and Continuous Learning</h3>
<p>AI systems operate through multiple nested feedback loops operating at different timescales. Short-term feedback loops enable rapid iteration during development: daily model training updates, weekly performance reviews, and monthly architecture assessments. Medium-term feedback loops connect development to deployment: quarterly system updates, semi-annual performance audits, and yearly architecture reviews. Long-term feedback loops enable strategic evolution: annual technology assessments and multi-year capability planning.</p>
<p>The DR team’s experience shows that feedback loop timing critically impacts development velocity. Daily feedback enabled rapid performance improvement, while monthly feedback slowed progress significantly. However, different types of insights require different timescales: algorithmic improvements benefit from rapid iteration, while architectural changes require longer assessment periods.</p>
</section>
<section id="emergent-system-behaviors" class="level3">
<h3 class="anchored" data-anchor-id="emergent-system-behaviors">Emergent System Behaviors</h3>
<p>AI systems exhibit emergent behaviors that arise from component interactions rather than individual component design. The DR system’s distributed deployment created system-wide diagnostic pattern shifts that were invisible at individual clinic levels but evident in aggregated data. These emergent behaviors can be beneficial (unexpected use cases and performance improvements) or problematic (resource bottlenecks and failure modes).</p>
<p>Managing emergent behaviors requires sophisticated monitoring and analytics capabilities that can detect system-level patterns. The DR team invested heavily in distributed monitoring infrastructure that could correlate performance across multiple clinics, enabling early detection of emergent issues and opportunities for system improvement.</p>
</section>
<section id="resource-dependencies-and-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="resource-dependencies-and-trade-offs">Resource Dependencies and Trade-offs</h3>
<p>AI systems involve complex resource trade-offs across multiple dimensions: computational resources, human expertise, time, and capital. The DR project illustrates how these trade-offs create intricate dependencies: model complexity affects deployment hardware requirements, which influences infrastructure costs, which constrains model development resources.</p>
<p>Successful AI systems require careful resource allocation strategies that balance short-term needs with long-term capabilities. The DR team’s investment in robust data infrastructure proved essential for rapid model iteration, while their focus on edge optimization enabled widespread deployment. These resource allocation decisions shaped the system’s ultimate success more than any individual algorithmic innovation.</p>
</section>
<section id="lifecycle-integration-strategies" class="level3">
<h3 class="anchored" data-anchor-id="lifecycle-integration-strategies">Lifecycle Integration Strategies</h3>
<p>Effective AI development requires integration strategies that coordinate across lifecycle stages, teams, and timescales. The DR team implemented several key integration practices:</p>
<p><strong>Shared Infrastructure:</strong> Common platforms for data storage, experiment tracking, and model deployment enable seamless transitions between lifecycle stages while maintaining system coherence.</p>
<p><strong>Cross-Functional Teams:</strong> Including representatives from each lifecycle stage in decision-making processes ensures that solutions account for system-wide constraints and opportunities.</p>
<p><strong>Iterative Planning:</strong> Regular planning cycles that reassess priorities based on learnings from all lifecycle stages enable adaptive strategies that respond to changing conditions.</p>
<p><strong>Performance Metrics:</strong> System-level metrics that span multiple lifecycle stages provide visibility into overall system health and development progress.</p>
<p>These systems thinking principles distinguish successful AI projects from those that struggle with integration challenges. The AI lifecycle’s inherent complexity requires systematic approaches that embrace interdependence, leverage feedback loops, anticipate emergent behaviors, optimize resource allocation, and implement effective integration strategies.</p>
<p>The lessons learned from monitoring and maintenance, combined with these systems thinking principles, feed directly into the next generation of AI systems, creating a virtuous cycle of improvement that characterizes mature AI development practices. This systemic approach to continuous improvement is critical for building AI systems that remain valuable and effective over time, adapting to new challenges while maintaining reliability and performance.</p>
</section>
</section>
<section id="sec-ai-workflow-ai-lifecycle-roles-8d83" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-ai-lifecycle-roles-8d83">AI Lifecycle Roles</h2>
<p>Building effective and resilient machine learning systems is far more than a solo pursuit; it’s a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>. Each role in this intricate dance brings unique skills and insights, supporting different phases of the AI development process, often coordinated through modern DevOps<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> practices adapted for ML workflows.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>ML Team Role Evolution</strong>: The “data scientist” role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while “ML engineer” became common around 2015 as companies realized that research models need production engineering. “MLOps engineer” appeared around 2018, and “AI ethics officer” became standard at major tech companies by 2020. This rapid role specialization reflects ML’s evolution from research curiosity to production necessity—modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams.</p></div><div id="fn28"><p><sup>28</sup>&nbsp;<strong>DevOps</strong>: Software engineering culture and practice that unifies development (Dev) and operations (Ops) teams to reduce deployment time from months to minutes through automation, continuous integration, and shared responsibility. Coined by Patrick Debois in 2009, DevOps emerged from the need to bridge the gap between developers who want rapid feature deployment and operations teams who prioritize system stability. Companies practicing DevOps achieve 208x more frequent deployments, 106x faster lead times, and 7x lower change failure rates compared to traditional approaches <span class="citation" data-cites="accelerate2018">(<a href="#ref-accelerate2018" role="doc-biblioref"><strong>accelerate2018?</strong></a>)</span>, making it essential for competitive software development. Understanding who these players are, what they contribute, and how they interconnect is crucial to navigating the complexities of modern AI systems.</p></div></div><section id="sec-ai-workflow-collaboration-ai-2ccb" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-collaboration-ai-2ccb">Collaboration in AI</h3>
<p>At the heart of any AI project is a team of data scientists. These innovative thinkers focus on model creation, experiment with architectures, and refine the algorithms that will become the neural networks driving insights from data. In our DR project, data scientists were instrumental in architecting neural networks capable of identifying retinal anomalies, advancing through iterations to fine-tune a balance between accuracy and computational efficiency.</p>
<p>Behind the scenes, data engineers work tirelessly to design robust data pipelines, ensuring that vast amounts of data are ingested, transformed, and stored effectively. They play a crucial role in the DR project, handling data from various clinics and automating quality checks to guarantee that the training inputs were standardized and reliable.</p>
<p>Meanwhile, machine learning engineers take the baton to integrate these models into production settings. They guarantee that models are nimble, scalable, and fit the constraints of the deployment environment. In rural clinics where computational resources can be scarce, their work in optimizing models was pivotal to enabling on-the-spot diagnosis.</p>
<p>Domain experts, such as ophthalmologists in the DR project, infuse technical progress with practical relevance. Their insights shape early problem definitions and ensure that AI tools align closely with real-world needs, offering a measure of validation that keeps the outcome aligned with clinical and operational realities.</p>
<p>MLOps engineers are the guardians of workflow automation, orchestrating the continuous integration and monitoring systems that keep AI models up and running through sophisticated orchestration platforms<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Orchestration Platforms</strong>: Software systems that automate, coordinate, and manage complex workflows across distributed computing resources, essential for ML pipelines that involve data processing, training, validation, and deployment stages. Kubernetes (2014) became the dominant orchestration platform, while ML-specific tools like Kubeflow (2018), Airflow (2015), and Prefect (2018) emerged to handle ML workflow complexities. Large organizations run 10,000+ orchestrated workflows daily: Netflix uses 150,000+ containers across 3 AWS regions, while Uber coordinates 4,000+ ML models through centralized orchestration, reducing deployment time from weeks to hours. They crafted centralized monitoring frameworks in the DR project, ensuring that updates were streamlined and model performance remained optimal across different deployment sites.</p></div></div><p>Ethicists and compliance officers remind us of the larger responsibility that accompanies AI deployment, ensuring adherence to ethical standards and legal requirements. Their oversight in the DR initiative safeguarded patient privacy amidst strict healthcare regulations.</p>
<p>Project managers weave together these diverse strands, orchestrating timelines, resources, and communication streams to maintain project momentum and alignment with objectives. They acted as linchpins within the project, harmonizing efforts between tech teams, clinical practitioners, and policy makers.</p>
</section>
<section id="sec-ai-workflow-role-interplay-7f0a" class="level3">
<h3 class="anchored" data-anchor-id="sec-ai-workflow-role-interplay-7f0a">Role Interplay</h3>
<p>The synergy between these roles drives AI projects toward successful outcomes. Data engineers establish a foundation for data scientists’ model-building efforts. As models transition into real-world applications, ML engineers ensure compatibility and efficiency. Feedback loops between MLOps engineers and data scientists foster continuous improvement, enabling quick adaptation to data-driven discoveries.</p>
<p>Ultimately, the success of the DR project underscores the value of interdisciplinary collaboration. From bridging clinical insights with technical expertise to ensuring ethical deployment through the responsible AI principles covered in <strong><a href="../core/responsible_ai/responsible_ai.html#sec-responsible-ai">Chapter 16: Responsible AI</a></strong>, this collective effort exemplifies how AI initiatives can be both technically successful and socially impactful.</p>
<p>This interconnected approach underlines why our examination in later chapters will address various aspects of AI development, including those that may be outside an individual’s primary expertise. Understanding these diverse roles will equip us to build more robust AI solutions. By comprehending the broader context and the interplay of roles, you’ll be better prepared to address challenges and collaborate effectively, enabling innovative and responsible AI systems as explored in <strong><a href="../core/responsible_ai/responsible_ai.html#sec-responsible-ai">Chapter 16: Responsible AI</a></strong>.</p>
<div id="quiz-question-sec-ai-workflow-ai-lifecycle-roles-8d83" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which role is primarily responsible for ensuring that AI models are optimized for deployment environments with limited computational resources?</p>
<ol type="a">
<li>Data Scientist</li>
<li>AI Ethics Officer</li>
<li>Data Engineer</li>
<li>Machine Learning Engineer</li>
</ol></li>
<li><p>Explain how the collaboration between data scientists and domain experts can enhance the AI development process.</p></li>
<li><p>True or False: The role of an AI ethics officer is to ensure that AI systems comply with ethical standards and legal requirements.</p></li>
<li><p>In the context of the DR project, which role was critical in maintaining the performance of AI models across different deployment sites?</p>
<ol type="a">
<li>MLOps Engineer</li>
<li>Project Manager</li>
<li>Domain Expert</li>
<li>Data Scientist</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-ai-lifecycle-roles-8d83" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="fallacies-and-pitfalls" class="level2">
<h2 class="anchored" data-anchor-id="fallacies-and-pitfalls">Fallacies and Pitfalls</h2>
<p>Machine learning development introduces unique complexities that differ from traditional software engineering, yet many teams attempt to apply familiar development patterns without recognizing these differences. The experimental nature of ML, the central role of data quality, and the probabilistic behavior of models create workflow challenges that traditional methodologies were not designed to address.</p>
<p><strong>Fallacy:</strong> <em>ML development can follow traditional software engineering workflows without modification.</em></p>
<p>This misconception leads teams to apply conventional software development practices directly to machine learning projects. As established in our comparison of Traditional vs.&nbsp;AI Lifecycles, ML systems introduce fundamental uncertainties through data variability, algorithmic randomness, and evolving model performance that traditional deterministic approaches cannot handle. Attempting to force ML projects into rigid waterfall or even standard agile methodologies often results in missed deadlines, inadequate model validation, and deployment failures. Successful ML workflows require specialized stages for data validation (<strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong>), experiment tracking (<strong><a href="../core/frameworks/frameworks.html#sec-ai-frameworks">Chapter 5: AI Frameworks</a></strong>), and iterative model refinement (<strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>).</p>
<p><strong>Pitfall:</strong> <em>Treating data preparation as a one-time preprocessing step.</em></p>
<p>Many practitioners view data collection and preprocessing as initial workflow stages that, once completed, remain static throughout the project lifecycle. This approach fails to account for the dynamic nature of real-world data, where distribution shifts, quality changes, and new data sources continuously emerge. Production systems require ongoing data validation, monitoring for drift, and adaptive preprocessing pipelines as detailed in <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong>. Teams that treat data preparation as a completed milestone often encounter unexpected model degradation when deployed systems encounter data that differs from training conditions, highlighting the robustness challenges explored in <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong>.</p>
<p><strong>Fallacy:</strong> <em>Model performance in development environments accurately predicts production performance.</em></p>
<p>This belief assumes that achieving good metrics during development ensures successful deployment. Development environments typically use clean, well-curated datasets and controlled computational resources, creating artificial conditions that rarely match production realities. Production systems face data quality issues, latency constraints, resource limitations, and adversarial inputs not present during development. Models that excel in development can fail in production due to these environmental differences, requiring workflow stages specifically designed to bridge this gap through robust deployment practices covered in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong> and system design principles from <strong><a href="../core/ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong>.</p>
<p><strong>Pitfall:</strong> <em>Skipping systematic validation stages to accelerate development timelines.</em></p>
<p>Under pressure to deliver quickly, teams often bypass validation, testing, and documentation stages. This approach treats validation as overhead rather than essential engineering discipline. Inadequate validation leads to models with hidden biases, poor generalization, or unexpected failure modes that only manifest in production. The cost of fixing these issues after deployment exceeds the time investment required for systematic validation. Robust workflows embed validation throughout the development process rather than treating it as a final checkpoint, incorporating the benchmarking and evaluation principles detailed in <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong>.</p>
</section>
<section id="sec-ai-workflow-summary-84ad" class="level2">
<h2 class="anchored" data-anchor-id="sec-ai-workflow-summary-84ad">Summary</h2>
<p>AI development follows systematic workflows that provide structured methodologies for building machine learning systems across diverse application domains. These workflows encompass problem definition, data collection and preparation, model development and training, validation and testing, deployment, and ongoing maintenance phases. While specific implementations vary across domains from healthcare to finance to autonomous systems, the core workflow stages remain consistent, providing a framework for managing the complexity inherent in AI system development.</p>
<p>The interconnected nature of AI workflows creates feedback loops where decisions in one stage impact all others. Data quality constraints influence model architecture choices, deployment environment limitations affect training strategies, and real-world performance metrics drive iterative refinement of the entire pipeline. This systems perspective recognizes that optimizing individual components in isolation often fails to achieve overall system objectives, requiring holistic approaches that consider the entire development lifecycle.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>AI workflows provide systematic frameworks applicable across diverse domains, from healthcare diagnostics to fraud detection to predictive maintenance</li>
<li>Interconnected workflow stages create feedback loops where data quality, model performance, deployment constraints, and usage patterns influence each other</li>
<li>Systems thinking becomes essential for AI development success, requiring optimization of the entire pipeline rather than individual components</li>
<li>Effective workflows balance technical performance with practical constraints including computational resources, deployment environments, and ethical considerations</li>
</ul>
</div>
</div>
<p>Modern AI development demands workflow orchestration that addresses growing system complexity, ensures adaptability to changing requirements, and maintains alignment with ethical and regulatory considerations. Success requires understanding not just the technical aspects of machine learning, but also the organizational, operational, and societal contexts in which AI systems operate. This holistic approach enables the creation of AI solutions that are technically sound, operationally viable, and aligned with real-world needs across the spectrum of AI applications.</p>


<div id="quiz-question-sec-ai-workflow-summary-84ad" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>True or False: The AI lifecycle stages and feedback loops are unique to each domain and cannot be generalized across different applications.</p></li>
<li><p>Explain how systems thinking can enhance the development of AI systems in diverse domains.</p></li>
<li><p>Which of the following best describes the role of feedback loops in the AI lifecycle?</p>
<ol type="a">
<li>Feedback loops are used to finalize the AI system design.</li>
<li>Feedback loops help refine the AI system by continuously integrating performance insights and real-world usage patterns.</li>
<li>Feedback loops are only relevant during the initial model training phase.</li>
<li>Feedback loops are primarily concerned with data collection and have no impact on model deployment.</li>
</ol></li>
<li><p>The concept of ‘____’ involves understanding and managing the complex interactions between stages of the AI lifecycle to create robust and adaptable AI systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-workflow-summary-84ad" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ai-workflow-overview-97fb" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Order the following stages of the ML lifecycle: (1) Data Collection, (2) Model Training, (3) Data Validation, (4) Model Evaluation.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Data Collection, (3) Data Validation, (2) Model Training, (4) Model Evaluation. This sequence reflects the progression from gathering raw data to validating it, training models, and finally evaluating their performance.</p>
<p><em>Learning Objective</em>: Understand the sequential order of the ML lifecycle stages.</p></li>
<li><p><strong>Which stage of the ML lifecycle involves ensuring that data is properly annotated and verified for usability?</strong></p>
<ol type="a">
<li>Data Collection</li>
<li>Data Labeling</li>
<li>Model Training</li>
<li>Model Evaluation</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Data Labeling. This is correct because data labeling involves annotating data to ensure it is usable for training models. Data Collection refers to gathering raw data, Model Training involves creating models, and Model Evaluation assesses model performance.</p>
<p><em>Learning Objective</em>: Identify the purpose of the Data Labeling stage in the ML lifecycle.</p></li>
<li><p><strong>Explain why feedback loops are important in the ML lifecycle.</strong></p>
<p><em>Answer</em>: Feedback loops are crucial because they allow insights from later stages, like deployment and monitoring, to inform earlier stages such as data preparation and model design. For example, if a deployed model’s performance degrades, feedback can lead to retraining with updated data. This is important because it ensures the system adapts to new data and maintains performance.</p>
<p><em>Learning Objective</em>: Understand the role and importance of feedback loops in the ML lifecycle.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-overview-97fb" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-lifecycle-stages-3032" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which stage of the ML lifecycle involves integrating the trained model into production systems and addressing challenges such as scalability and operational constraints?</strong></p>
<ol type="a">
<li>Problem Definition</li>
<li>Data Collection and Preparation</li>
<li>Deployment and Integration</li>
<li>Monitoring and Maintenance</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Deployment and Integration. This stage involves integrating the model into production systems, addressing scalability, and ensuring compatibility. Other stages focus on different aspects like problem definition or data preparation.</p>
<p><em>Learning Objective</em>: Understand the role of the Deployment and Integration stage in the ML lifecycle.</p></li>
<li><p><strong>True or False: The Monitoring and Maintenance stage is only necessary if the model’s performance begins to degrade.</strong></p>
<p><em>Answer</em>: False. Monitoring and Maintenance is a continuous process to ensure the model remains relevant and accurate over time, adapting to changes in data and requirements.</p>
<p><em>Learning Objective</em>: Recognize the importance of ongoing monitoring and maintenance in ML systems.</p></li>
<li><p><strong>Explain how the feedback loop in the ML lifecycle contributes to the system’s continuous improvement.</strong></p>
<p><em>Answer</em>: The feedback loop allows for iterative refinement by using insights from the Monitoring and Maintenance stage to inform Data Collection and Preparation. For example, if a model’s performance drops, new data can be collected to retrain and improve the model. This is important because it ensures the system adapts to changing conditions and maintains high performance.</p>
<p><em>Learning Objective</em>: Understand the role of feedback loops in facilitating continuous improvement in ML systems.</p></li>
<li><p><strong>In the context of Google’s Diabetic Retinopathy project, what was a significant challenge encountered during the deployment stage?</strong></p>
<ol type="a">
<li>Integration with rural clinic workflows</li>
<li>Data preprocessing</li>
<li>Algorithm selection</li>
<li>Model architecture design</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Integration with rural clinic workflows. This challenge highlights the practical difficulties of deploying ML systems in real-world settings, such as adapting to existing workflows and infrastructure.</p>
<p><em>Learning Objective</em>: Identify real-world challenges in deploying ML systems, using case studies as examples.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-lifecycle-stages-3032" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-problem-definition-87d9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>How does problem definition in machine learning systems fundamentally differ from traditional software development?</strong></p>
<ol type="a">
<li>ML systems require deterministic specifications.</li>
<li>Traditional software focuses on learning from data.</li>
<li>ML systems are defined by examples and desired behaviors.</li>
<li>ML systems do not need to consider real-world constraints.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. ML systems are defined by examples and desired behaviors. This is correct because ML systems rely on data to learn and adapt, unlike traditional software which follows explicit rules. Options A and D are incorrect as ML systems are not deterministic and must consider real-world constraints. Option C is incorrect because traditional software does not focus on learning from data.</p>
<p><em>Learning Objective</em>: Understand the fundamental differences in problem definition between ML systems and traditional software.</p></li>
<li><p><strong>Explain why aligning learning objectives with system constraints is crucial in ML problem definition.</strong></p>
<p><em>Answer</em>: Aligning learning objectives with system constraints ensures that the ML system can operate effectively in real-world conditions. For example, in the DR project, this alignment was necessary to handle diverse imaging conditions and hardware limitations. This is important because it ensures the system’s viability and effectiveness in its intended context.</p>
<p><em>Learning Objective</em>: Analyze the importance of aligning learning objectives with operational constraints in ML systems.</p></li>
<li><p><strong>True or False: A well-defined ML problem only needs to focus on achieving high performance metrics.</strong></p>
<p><em>Answer</em>: False. This is false because a well-defined ML problem must also consider operational realities, such as computational constraints and data variability, to ensure long-term viability.</p>
<p><em>Learning Objective</em>: Challenge the misconception that performance metrics are the sole focus in ML problem definition.</p></li>
<li><p><strong>The process of defining an ML problem involves identifying the core objective of the system and the constraints it must satisfy, often requiring collaboration with stakeholders to gather ____ knowledge.</strong></p>
<p><em>Answer</em>: domain. Domain knowledge is critical for understanding the specific requirements and challenges of the environment in which the ML system will operate.</p>
<p><em>Learning Objective</em>: Recall the importance of domain knowledge in defining ML problems.</p></li>
<li><p><strong>In a production system, what are the potential consequences of a poorly defined ML problem?</strong></p>
<p><em>Answer</em>: A poorly defined ML problem can lead to inefficiencies, failures, and costly redesigns. For example, if a system is not designed to handle diverse imaging conditions, it may fail in new environments. This is important because it emphasizes the need for a comprehensive problem definition to ensure system success.</p>
<p><em>Learning Objective</em>: Understand the potential impacts of inadequate problem definition in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-problem-definition-87d9" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-data-collection-ab2c" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>What is a significant challenge faced during data collection for medical ML systems like the DR project?</strong></p>
<ol type="a">
<li>Low cost of data annotation</li>
<li>High cost and complexity of expert data annotation</li>
<li>Uniform data quality across all sources</li>
<li>Availability of large datasets without privacy concerns</li>
</ol>
<p><em>Answer</em>: The correct answer is B. High cost and complexity of expert data annotation. This is correct because medical data annotation requires specialized expertise, leading to high costs and logistical challenges. Other options are incorrect as they do not reflect the realities of medical data collection.</p>
<p><em>Learning Objective</em>: Understand the challenges and costs associated with data collection in medical ML systems.</p></li>
<li><p><strong>True or False: Data collection strategies have no impact on the ML system’s ability to handle new inputs over time.</strong></p>
<p><em>Answer</em>: False. Data collection strategies significantly impact the system’s ability to handle new inputs, as they determine the quality and diversity of the training data, which affects the model’s adaptability.</p>
<p><em>Learning Objective</em>: Recognize the long-term implications of data collection strategies on system adaptability.</p></li>
<li><p><strong>Explain how the data collection process in the DR project influenced the system’s infrastructure design.</strong></p>
<p><em>Answer</em>: The DR project’s data collection process required local storage and preprocessing capabilities at clinics due to the volume and size of high-resolution images and unreliable internet access. This influenced the infrastructure design to accommodate local needs while ensuring centralized data aggregation, balancing operational realities with technical requirements.</p>
<p><em>Learning Objective</em>: Analyze how data collection processes shape infrastructure design in ML systems.</p></li>
<li><p><strong>Which of the following is an example of how feedback loops in data collection influence the ML lifecycle?</strong></p>
<ol type="a">
<li>Collecting additional data to address training data gaps</li>
<li>Ignoring data quality issues during model training</li>
<li>Reducing the number of data sources to simplify the system</li>
<li>Focusing only on model deployment without data updates</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Collecting additional data to address training data gaps. Feedback loops help identify data gaps during model evaluation, prompting targeted data collection to improve model performance.</p>
<p><em>Learning Objective</em>: Understand the role of feedback loops in improving data collection and model performance.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-data-collection-ab2c" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-model-development-dfdc" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key consideration when designing ML models for deployment in resource-constrained environments?</strong></p>
<ol type="a">
<li>Ensuring high sensitivity and specificity</li>
<li>Maximizing model complexity</li>
<li>Using the largest possible dataset</li>
<li>Focusing solely on algorithm selection</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Ensuring high sensitivity and specificity is crucial for models in resource-constrained environments, as it balances performance with operational feasibility. Options A, C, and D do not directly address the constraints and requirements of such environments.</p>
<p><em>Learning Objective</em>: Understand the importance of balancing model performance with deployability in constrained environments.</p></li>
<li><p><strong>Explain why interdisciplinary collaboration is critical in the development of machine learning models for healthcare applications.</strong></p>
<p><em>Answer</em>: Interdisciplinary collaboration is critical because it combines domain expertise with technical skills, ensuring that models are both accurate and clinically relevant. For example, data scientists and medical experts work together to refine features and interpret model outputs, which is important because it enhances the model’s applicability and trustworthiness in clinical settings.</p>
<p><em>Learning Objective</em>: Appreciate the role of interdisciplinary collaboration in developing effective ML models for specialized domains.</p></li>
<li><p><strong>True or False: In the DR project, the model’s architecture decisions only affected the training phase and not the deployment strategy.</strong></p>
<p><em>Answer</em>: False. The model’s architecture decisions influenced both the training phase and the deployment strategy, as they affected data preprocessing, training infrastructure, and deployment feasibility. This interconnectedness is crucial for ensuring the model meets operational constraints.</p>
<p><em>Learning Objective</em>: Recognize the interconnected impact of model architecture decisions across different stages of the ML lifecycle.</p></li>
<li><p><strong>Order the following components of the model development workflow: (1) Data Exploration, (2) Model Design, (3) Training Infrastructure Setup, (4) Experiment Tracking.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Data Exploration, (2) Model Design, (3) Training Infrastructure Setup, (4) Experiment Tracking. This sequence reflects the progression from understanding data to designing models, setting up infrastructure, and managing experiments.</p>
<p><em>Learning Objective</em>: Understand the sequential steps involved in the model development workflow.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-model-development-dfdc" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-deployment-2839" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which deployment strategy was chosen for the DR project due to unreliable connectivity in rural clinics?</strong></p>
<ol type="a">
<li>Cloud-based deployment</li>
<li>Hybrid deployment</li>
<li>Edge deployment</li>
<li>Centralized deployment</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Edge deployment. This strategy was chosen because it allowed models to run locally on clinic hardware, which was necessary due to unreliable internet connectivity in rural clinics. Cloud-based deployment was not feasible under these conditions.</p>
<p><em>Learning Objective</em>: Understand the rationale behind choosing specific deployment strategies based on environmental constraints.</p></li>
<li><p><strong>True or False: The deployment of ML systems in the DR project required no modifications to existing clinical workflows.</strong></p>
<p><em>Answer</em>: False. The deployment required the ML system to fit seamlessly into existing clinical workflows, which involved ensuring rapid, interpretable results that could assist healthcare providers without causing disruption.</p>
<p><em>Learning Objective</em>: Recognize the importance of integrating ML systems into existing workflows without causing disruption.</p></li>
<li><p><strong>Explain how deployment feedback in the DR project influenced subsequent model optimizations.</strong></p>
<p><em>Answer</em>: Deployment feedback revealed challenges such as inconsistencies in image quality due to variations in imaging equipment. This feedback looped back to model training, prompting optimizations to improve performance under these conditions. For example, the system struggled with images from older camera models, leading to targeted data collection and model adjustments.</p>
<p><em>Learning Objective</em>: Analyze how real-world deployment feedback can drive continuous model improvements.</p></li>
<li><p><strong>What was a critical consideration for ensuring the reliability of the DR system in clinical settings?</strong></p>
<ol type="a">
<li>Implementing fail-safes for common issues</li>
<li>Maximizing computational efficiency</li>
<li>Reducing deployment costs</li>
<li>Increasing the number of deployment sites</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Implementing fail-safes for common issues. Ensuring reliability involved implementing mechanisms to detect and handle issues like incomplete or poor-quality data, which is crucial in clinical settings.</p>
<p><em>Learning Objective</em>: Identify key strategies for ensuring the reliability of ML systems in real-world applications.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-deployment-2839" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-maintenance-e184" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary reason that monitoring is critical in deployed ML systems?</strong></p>
<ol type="a">
<li>To detect and address model drift over time.</li>
<li>To ensure the system meets initial training accuracy.</li>
<li>To replace traditional software debugging processes.</li>
<li>To maintain the original data distribution.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. To detect and address model drift over time. Monitoring is essential for identifying changes in data distributions and usage patterns that can degrade model performance.</p>
<p><em>Learning Objective</em>: Understand the role of monitoring in addressing model drift in ML systems.</p></li>
<li><p><strong>Explain how feedback loops in monitoring contribute to the maintenance of ML systems.</strong></p>
<p><em>Answer</em>: Feedback loops in monitoring provide insights that inform maintenance activities, such as retraining models or refining data collection processes. For example, detecting underrepresented demographics can trigger new data collection, ensuring the model remains accurate and relevant. This is important because it allows the system to adapt to real-world changes and maintain performance.</p>
<p><em>Learning Objective</em>: Analyze the role of feedback loops in the ongoing maintenance of ML systems.</p></li>
<li><p><strong>Order the following steps in a typical ML maintenance workflow: (1) Define monitoring framework, (2) Detect performance issues, (3) Implement model updates, (4) Validate updates.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Define monitoring framework, (2) Detect performance issues, (3) Implement model updates, (4) Validate updates. This sequence reflects the iterative nature of maintenance, where monitoring informs updates, and validation ensures changes are effective.</p>
<p><em>Learning Objective</em>: Understand the sequence of activities in the maintenance workflow of ML systems.</p></li>
<li><p><strong>True or False: Proactive maintenance in ML systems involves only reacting to issues as they occur.</strong></p>
<p><em>Answer</em>: False. Proactive maintenance involves anticipating and preventing issues before they occur, using predictive models and continuous learning pipelines to maintain system performance.</p>
<p><em>Learning Objective</em>: Differentiate between proactive and reactive maintenance strategies in ML systems.</p></li>
<li><p><strong>In the context of the DR project, why was real-time monitoring preferred over periodic evaluations?</strong></p>
<ol type="a">
<li>To reduce system resource usage.</li>
<li>To comply with healthcare regulations.</li>
<li>To quickly detect and address performance issues.</li>
<li>To simplify the monitoring process.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. To quickly detect and address performance issues. Real-time monitoring allows for immediate identification of issues, ensuring the system remains reliable and effective in clinical settings.</p>
<p><em>Learning Objective</em>: Understand the advantages of real-time monitoring in maintaining ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-maintenance-e184" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-ai-lifecycle-roles-8d83" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which role is primarily responsible for ensuring that AI models are optimized for deployment environments with limited computational resources?</strong></p>
<ol type="a">
<li>Data Scientist</li>
<li>AI Ethics Officer</li>
<li>Data Engineer</li>
<li>Machine Learning Engineer</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Machine Learning Engineer. This is correct because ML engineers focus on integrating models into production, ensuring they are scalable and efficient, especially in resource-constrained environments. Data scientists focus on model creation, while data engineers handle data pipelines.</p>
<p><em>Learning Objective</em>: Identify the role responsible for optimizing AI models for specific deployment environments.</p></li>
<li><p><strong>Explain how the collaboration between data scientists and domain experts can enhance the AI development process.</strong></p>
<p><em>Answer</em>: Collaboration between data scientists and domain experts ensures that AI models are relevant and aligned with real-world needs. Domain experts provide insights that shape problem definitions and validate model outputs, while data scientists leverage these insights to refine algorithms. For example, in a healthcare project, domain experts like doctors guide data scientists in understanding clinical significance, leading to models that are both technically sound and practically useful. This collaboration is crucial for creating impactful AI solutions.</p>
<p><em>Learning Objective</em>: Understand the importance of interdisciplinary collaboration in AI development.</p></li>
<li><p><strong>True or False: The role of an AI ethics officer is to ensure that AI systems comply with ethical standards and legal requirements.</strong></p>
<p><em>Answer</em>: True. This is true because AI ethics officers focus on the ethical implications of AI systems, ensuring they adhere to ethical standards and legal regulations, such as data privacy laws. Their oversight is crucial in maintaining public trust and preventing misuse of AI technologies.</p>
<p><em>Learning Objective</em>: Recognize the responsibilities of an AI ethics officer in the AI lifecycle.</p></li>
<li><p><strong>In the context of the DR project, which role was critical in maintaining the performance of AI models across different deployment sites?</strong></p>
<ol type="a">
<li>MLOps Engineer</li>
<li>Project Manager</li>
<li>Domain Expert</li>
<li>Data Scientist</li>
</ol>
<p><em>Answer</em>: The correct answer is A. MLOps Engineer. This is correct because MLOps engineers are responsible for workflow automation and monitoring systems, ensuring consistent model performance across deployment sites. They handle continuous integration and updates, which are essential for maintaining model efficacy.</p>
<p><em>Learning Objective</em>: Identify the role responsible for maintaining AI model performance across multiple sites.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-ai-lifecycle-roles-8d83" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-workflow-summary-84ad" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>True or False: The AI lifecycle stages and feedback loops are unique to each domain and cannot be generalized across different applications.</strong></p>
<p><em>Answer</em>: False. The AI lifecycle stages and feedback loops are consistent across different domains, even though their specific implementations may vary. This universality allows for a generalized approach to AI system development.</p>
<p><em>Learning Objective</em>: Understand the universality of AI lifecycle stages and feedback loops across various domains.</p></li>
<li><p><strong>Explain how systems thinking can enhance the development of AI systems in diverse domains.</strong></p>
<p><em>Answer</em>: Systems thinking allows developers to understand and manage the complex interactions between different stages of the AI lifecycle. By considering the broader context in which the system operates, developers can create solutions that are technically proficient, robust, adaptable, and aligned with real-world needs. For example, in a healthcare AI system, systems thinking helps balance performance with ethical considerations, ensuring the system is effective and responsible.</p>
<p><em>Learning Objective</em>: Apply systems thinking to enhance AI system development across diverse domains.</p></li>
<li><p><strong>Which of the following best describes the role of feedback loops in the AI lifecycle?</strong></p>
<ol type="a">
<li>Feedback loops are used to finalize the AI system design.</li>
<li>Feedback loops help refine the AI system by continuously integrating performance insights and real-world usage patterns.</li>
<li>Feedback loops are only relevant during the initial model training phase.</li>
<li>Feedback loops are primarily concerned with data collection and have no impact on model deployment.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Feedback loops help refine the AI system by continuously integrating performance insights and real-world usage patterns. This is correct because they allow for ongoing improvement and adaptation of the system, ensuring it remains effective and relevant.</p>
<p><em>Learning Objective</em>: Understand the role of feedback loops in refining and improving AI systems.</p></li>
<li><p><strong>The concept of ‘____’ involves understanding and managing the complex interactions between stages of the AI lifecycle to create robust and adaptable AI systems.</strong></p>
<p><em>Answer</em>: systems thinking. This concept involves understanding and managing the complex interactions between stages of the AI lifecycle to create robust and adaptable AI systems.</p>
<p><em>Learning Objective</em>: Recall the concept of systems thinking and its importance in AI development.</p></li>
</ol>
<p><a href="#quiz-question-sec-ai-workflow-summary-84ad" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="pagination-link" aria-label="DNN Architectures">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">DNN Architectures</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/data_engineering/data_engineering.html" class="pagination-link" aria-label="Data Engineering">
        <span class="nav-page-text">Data Engineering</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>