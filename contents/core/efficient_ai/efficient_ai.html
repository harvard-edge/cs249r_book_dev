<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/optimizations/optimizations.html" rel="next">
<link href="../../../contents/core/training/training.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-0769fbf68cc3e722256a1e1e51d908bf.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
</style>
<style>
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-code.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-example.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-definition.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Efficient AI</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p style="margin: 0 0 12px 0; padding: 8px 12px; background: rgba(255,193,7,0.2); border: 1px solid #ffc107; border-radius: 4px; font-weight: 600;"><i class="bi bi-exclamation-triangle-fill" style="margin-right: 6px; color: #856404;"></i><strong>🚧 DEVELOPMENT PREVIEW</strong> - Built from dev@<code style="background: rgba(0,0,0,0.1); padding: 2px 4px; border-radius: 3px; font-size: 0.9em;">38b88181</code> • 2025-09-29 23:20 UTC • <a href="https://mlsysbook.ai" style="color: #856404; text-decoration: underline;"><em>Stable version →</em></a></p>
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action" style="display: none;"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true">
 <span class="menu-text">References</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Bibliography</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-efficient-ai" id="toc-sec-efficient-ai" class="nav-link active" data-scroll-target="#sec-efficient-ai">Efficient AI</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-efficient-ai-overview-6f6a" id="toc-sec-efficient-ai-overview-6f6a" class="nav-link" data-scroll-target="#sec-efficient-ai-overview-6f6a">Overview</a></li>
  <li><a href="#sec-efficient-ai-defining-system-efficiency-8e59" id="toc-sec-efficient-ai-defining-system-efficiency-8e59" class="nav-link" data-scroll-target="#sec-efficient-ai-defining-system-efficiency-8e59">Defining System Efficiency</a></li>
  <li><a href="#sec-efficient-ai-ai-scaling-laws-a043" id="toc-sec-efficient-ai-ai-scaling-laws-a043" class="nav-link" data-scroll-target="#sec-efficient-ai-ai-scaling-laws-a043">AI Scaling Laws</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-fundamental-principles-16fa" id="toc-sec-efficient-ai-fundamental-principles-16fa" class="nav-link" data-scroll-target="#sec-efficient-ai-fundamental-principles-16fa">The Scaling Reality</a></li>
  <li><a href="#sec-efficient-ai-empirical-scaling-laws-b941" id="toc-sec-efficient-ai-empirical-scaling-laws-b941" class="nav-link" data-scroll-target="#sec-efficient-ai-empirical-scaling-laws-b941">Three Scaling Regimes</a></li>
  <li><a href="#sec-efficient-ai-optimal-resource-allocation" id="toc-sec-efficient-ai-optimal-resource-allocation" class="nav-link" data-scroll-target="#sec-efficient-ai-optimal-resource-allocation">Optimal Resource Allocation</a></li>
  <li><a href="#sec-efficient-ai-mathematical-framework" id="toc-sec-efficient-ai-mathematical-framework" class="nav-link" data-scroll-target="#sec-efficient-ai-mathematical-framework">Mathematical Framework</a></li>
  <li><a href="#sec-efficient-ai-scaling-regimes-16d7" id="toc-sec-efficient-ai-scaling-regimes-16d7" class="nav-link" data-scroll-target="#sec-efficient-ai-scaling-regimes-16d7">Scaling Regimes</a></li>
  <li><a href="#sec-efficient-ai-system-design-054c" id="toc-sec-efficient-ai-system-design-054c" class="nav-link" data-scroll-target="#sec-efficient-ai-system-design-054c">System Design</a></li>
  <li><a href="#sec-efficient-ai-scaling-vs-efficiency-f579" id="toc-sec-efficient-ai-scaling-vs-efficiency-f579" class="nav-link" data-scroll-target="#sec-efficient-ai-scaling-vs-efficiency-f579">Scaling vs.&nbsp;Efficiency</a></li>
  <li><a href="#sec-efficient-ai-scaling-breakdown-2247" id="toc-sec-efficient-ai-scaling-breakdown-2247" class="nav-link" data-scroll-target="#sec-efficient-ai-scaling-breakdown-2247">Scaling Breakdown</a></li>
  <li><a href="#sec-efficient-ai-toward-efficient-scaling-7f6d" id="toc-sec-efficient-ai-toward-efficient-scaling-7f6d" class="nav-link" data-scroll-target="#sec-efficient-ai-toward-efficient-scaling-7f6d">Toward Efficient Scaling</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-pillars-ai-efficiency-c024" id="toc-sec-efficient-ai-pillars-ai-efficiency-c024" class="nav-link" data-scroll-target="#sec-efficient-ai-pillars-ai-efficiency-c024">The Efficiency Framework</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-algorithmic-efficiency-a3ba" id="toc-sec-efficient-ai-algorithmic-efficiency-a3ba" class="nav-link" data-scroll-target="#sec-efficient-ai-algorithmic-efficiency-a3ba">Algorithmic Efficiency: Doing More with Less</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-early-efficiency-a537" id="toc-sec-efficient-ai-early-efficiency-a537" class="nav-link" data-scroll-target="#sec-efficient-ai-early-efficiency-a537">Modern Compression Techniques</a></li>
  <li><a href="#sec-efficient-ai-deep-learning-era-4647" id="toc-sec-efficient-ai-deep-learning-era-4647" class="nav-link" data-scroll-target="#sec-efficient-ai-deep-learning-era-4647">Hardware-Algorithm Co-optimization</a></li>
  <li><a href="#sec-efficient-ai-architectural-innovation" id="toc-sec-efficient-ai-architectural-innovation" class="nav-link" data-scroll-target="#sec-efficient-ai-architectural-innovation">Architectural Innovation for Efficiency</a></li>
  <li><a href="#sec-efficient-ai-modern-efficiency-7708" id="toc-sec-efficient-ai-modern-efficiency-7708" class="nav-link" data-scroll-target="#sec-efficient-ai-modern-efficiency-7708">Interconnected Efficiency: Parameter-Efficient Adaptation</a></li>
  <li><a href="#sec-efficient-ai-efficiency-design-5416" id="toc-sec-efficient-ai-efficiency-design-5416" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-design-5416">Production Efficiency Monitoring</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-compute-efficiency-e72b" id="toc-sec-efficient-ai-compute-efficiency-e72b" class="nav-link" data-scroll-target="#sec-efficient-ai-compute-efficiency-e72b">Compute Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-generalpurpose-computing-era-e3d1" id="toc-sec-efficient-ai-generalpurpose-computing-era-e3d1" class="nav-link" data-scroll-target="#sec-efficient-ai-generalpurpose-computing-era-e3d1">General-Purpose Computing Era</a></li>
  <li><a href="#sec-efficient-ai-accelerated-computing-era-0575" id="toc-sec-efficient-ai-accelerated-computing-era-0575" class="nav-link" data-scroll-target="#sec-efficient-ai-accelerated-computing-era-0575">Accelerated Computing Era</a></li>
  <li><a href="#sec-efficient-ai-sustainable-computing-era-98a6" id="toc-sec-efficient-ai-sustainable-computing-era-98a6" class="nav-link" data-scroll-target="#sec-efficient-ai-sustainable-computing-era-98a6">Sustainable Computing Era</a></li>
  <li><a href="#sec-efficient-ai-mobile-edge-deployment" id="toc-sec-efficient-ai-mobile-edge-deployment" class="nav-link" data-scroll-target="#sec-efficient-ai-mobile-edge-deployment">Mobile and Edge AI Deployment Patterns</a></li>
  <li><a href="#sec-efficient-ai-production-case-studies" id="toc-sec-efficient-ai-production-case-studies" class="nav-link" data-scroll-target="#sec-efficient-ai-production-case-studies">Production Deployment Case Studies</a></li>
  <li><a href="#sec-efficient-ai-compute-efficiencys-role-3789" id="toc-sec-efficient-ai-compute-efficiencys-role-3789" class="nav-link" data-scroll-target="#sec-efficient-ai-compute-efficiencys-role-3789">Compute Efficiency’s Role</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-data-efficiency-d30c" id="toc-sec-efficient-ai-data-efficiency-d30c" class="nav-link" data-scroll-target="#sec-efficient-ai-data-efficiency-d30c">Data Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-data-scarcity-era-85a6" id="toc-sec-efficient-ai-data-scarcity-era-85a6" class="nav-link" data-scroll-target="#sec-efficient-ai-data-scarcity-era-85a6">Data Scarcity Era</a></li>
  <li><a href="#sec-efficient-ai-big-data-era-f494" id="toc-sec-efficient-ai-big-data-era-f494" class="nav-link" data-scroll-target="#sec-efficient-ai-big-data-era-f494">Big Data Era</a></li>
  <li><a href="#sec-efficient-ai-modern-data-efficiency-era-f067" id="toc-sec-efficient-ai-modern-data-efficiency-era-f067" class="nav-link" data-scroll-target="#sec-efficient-ai-modern-data-efficiency-era-f067">Modern Data Efficiency Era</a></li>
  <li><a href="#sec-efficient-ai-data-efficiencys-role-c216" id="toc-sec-efficient-ai-data-efficiencys-role-c216" class="nav-link" data-scroll-target="#sec-efficient-ai-data-efficiencys-role-c216">Data Efficiency’s Role</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-system-efficiency-3cf1" id="toc-sec-efficient-ai-system-efficiency-3cf1" class="nav-link" data-scroll-target="#sec-efficient-ai-system-efficiency-3cf1">System Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-defining-system-efficiency-8e59" id="toc-sec-efficient-ai-defining-system-efficiency-8e59" class="nav-link" data-scroll-target="#sec-efficient-ai-defining-system-efficiency-8e59">Defining System Efficiency</a></li>
  <li><a href="#sec-efficient-ai-efficiency-interdependencies-bff1" id="toc-sec-efficient-ai-efficiency-interdependencies-bff1" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-interdependencies-bff1">Efficiency Interdependencies</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-algorithmic-efficiency-aids-compute-data-a77f" id="toc-sec-efficient-ai-algorithmic-efficiency-aids-compute-data-a77f" class="nav-link" data-scroll-target="#sec-efficient-ai-algorithmic-efficiency-aids-compute-data-a77f">Algorithmic Efficiency Aids Compute and Data</a></li>
  <li><a href="#sec-efficient-ai-compute-efficiency-supports-model-data-3501" id="toc-sec-efficient-ai-compute-efficiency-supports-model-data-3501" class="nav-link" data-scroll-target="#sec-efficient-ai-compute-efficiency-supports-model-data-3501">Compute Efficiency Supports Model and Data</a></li>
  <li><a href="#sec-efficient-ai-data-efficiency-strengthens-model-compute-f801" id="toc-sec-efficient-ai-data-efficiency-strengthens-model-compute-f801" class="nav-link" data-scroll-target="#sec-efficient-ai-data-efficiency-strengthens-model-compute-f801">Data Efficiency Strengthens Model and Compute</a></li>
  <li><a href="#sec-efficient-ai-progression-takeaways-e267" id="toc-sec-efficient-ai-progression-takeaways-e267" class="nav-link" data-scroll-target="#sec-efficient-ai-progression-takeaways-e267">Progression and Takeaways</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-scalability-sustainability-0d26" id="toc-sec-efficient-ai-scalability-sustainability-0d26" class="nav-link" data-scroll-target="#sec-efficient-ai-scalability-sustainability-0d26">Scalability and Sustainability</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-efficiencyscalability-relationship-3540" id="toc-sec-efficient-ai-efficiencyscalability-relationship-3540" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiencyscalability-relationship-3540">Efficiency-Scalability Relationship</a></li>
  <li><a href="#sec-efficient-ai-scalabilitysustainability-relationship-a3ee" id="toc-sec-efficient-ai-scalabilitysustainability-relationship-a3ee" class="nav-link" data-scroll-target="#sec-efficient-ai-scalabilitysustainability-relationship-a3ee">Scalability-Sustainability Relationship</a></li>
  <li><a href="#sec-efficient-ai-sustainabilityefficiency-relationship-8bbb" id="toc-sec-efficient-ai-sustainabilityefficiency-relationship-8bbb" class="nav-link" data-scroll-target="#sec-efficient-ai-sustainabilityefficiency-relationship-8bbb">Sustainability-Efficiency Relationship</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-concrete-example-5f2a" id="toc-sec-efficient-ai-concrete-example-5f2a" class="nav-link" data-scroll-target="#sec-efficient-ai-concrete-example-5f2a">A Concrete Example: Photo Search Application</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-efficiency-tradeoffs-challenges-946d" id="toc-sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-tradeoffs-challenges-946d">Efficiency Trade-offs and Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-tradeoffs-source-5907" id="toc-sec-efficient-ai-tradeoffs-source-5907" class="nav-link" data-scroll-target="#sec-efficient-ai-tradeoffs-source-5907">Trade-offs Source</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-efficiency-compute-requirements-a1a1" id="toc-sec-efficient-ai-efficiency-compute-requirements-a1a1" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-compute-requirements-a1a1">Efficiency and Compute Requirements</a></li>
  <li><a href="#sec-efficient-ai-efficiency-realtime-needs-bc6a" id="toc-sec-efficient-ai-efficiency-realtime-needs-bc6a" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-realtime-needs-bc6a">Efficiency and Real-Time Needs</a></li>
  <li><a href="#sec-efficient-ai-efficiency-model-generalization-2d9c" id="toc-sec-efficient-ai-efficiency-model-generalization-2d9c" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-model-generalization-2d9c">Efficiency and Model Generalization</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-common-tradeoffs-bde0" id="toc-sec-efficient-ai-common-tradeoffs-bde0" class="nav-link" data-scroll-target="#sec-efficient-ai-common-tradeoffs-bde0">Common Trade-offs</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-complexity-vs-resources-a86d" id="toc-sec-efficient-ai-complexity-vs-resources-a86d" class="nav-link" data-scroll-target="#sec-efficient-ai-complexity-vs-resources-a86d">Complexity vs.&nbsp;Resources</a></li>
  <li><a href="#sec-efficient-ai-energy-vs-performance-087b" id="toc-sec-efficient-ai-energy-vs-performance-087b" class="nav-link" data-scroll-target="#sec-efficient-ai-energy-vs-performance-087b">Energy vs.&nbsp;Performance</a></li>
  <li><a href="#sec-efficient-ai-data-size-vs-generalization-ae8b" id="toc-sec-efficient-ai-data-size-vs-generalization-ae8b" class="nav-link" data-scroll-target="#sec-efficient-ai-data-size-vs-generalization-ae8b">Data Size vs.&nbsp;Generalization</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-managing-tradeoffs-80e8" id="toc-sec-efficient-ai-managing-tradeoffs-80e8" class="nav-link" data-scroll-target="#sec-efficient-ai-managing-tradeoffs-80e8">Managing Trade-offs</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-contextual-prioritization-bff3" id="toc-sec-efficient-ai-contextual-prioritization-bff3" class="nav-link" data-scroll-target="#sec-efficient-ai-contextual-prioritization-bff3">Contextual Prioritization</a></li>
  <li><a href="#sec-efficient-ai-testtime-compute-dbb3" id="toc-sec-efficient-ai-testtime-compute-dbb3" class="nav-link" data-scroll-target="#sec-efficient-ai-testtime-compute-dbb3">Test-Time Compute</a></li>
  <li><a href="#sec-efficient-ai-codesign-5516" id="toc-sec-efficient-ai-codesign-5516" class="nav-link" data-scroll-target="#sec-efficient-ai-codesign-5516">Co-Design</a></li>
  <li><a href="#sec-efficient-ai-automation-0e1f" id="toc-sec-efficient-ai-automation-0e1f" class="nav-link" data-scroll-target="#sec-efficient-ai-automation-0e1f">Automation</a></li>
  <li><a href="#sec-efficient-ai-systematic-evaluation-8c2a" id="toc-sec-efficient-ai-systematic-evaluation-8c2a" class="nav-link" data-scroll-target="#sec-efficient-ai-systematic-evaluation-8c2a">Systematic Evaluation</a></li>
  <li><a href="#sec-efficient-ai-continuous-assessment-d74b" id="toc-sec-efficient-ai-continuous-assessment-d74b" class="nav-link" data-scroll-target="#sec-efficient-ai-continuous-assessment-d74b">Continuous Assessment</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-efficiencyfirst-mindset-39e7" id="toc-sec-efficient-ai-efficiencyfirst-mindset-39e7" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiencyfirst-mindset-39e7">Efficiency-First Mindset</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-endtoend-perspective-fc95" id="toc-sec-efficient-ai-endtoend-perspective-fc95" class="nav-link" data-scroll-target="#sec-efficient-ai-endtoend-perspective-fc95">End-to-End Perspective</a></li>
  <li><a href="#sec-efficient-ai-scenarios-15e0" id="toc-sec-efficient-ai-scenarios-15e0" class="nav-link" data-scroll-target="#sec-efficient-ai-scenarios-15e0">Scenarios</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-prototypes-vs-production-c0ea" id="toc-sec-efficient-ai-prototypes-vs-production-c0ea" class="nav-link" data-scroll-target="#sec-efficient-ai-prototypes-vs-production-c0ea">Prototypes vs.&nbsp;Production</a></li>
  <li><a href="#sec-efficient-ai-cloud-apps-vs-constrained-systems-22ae" id="toc-sec-efficient-ai-cloud-apps-vs-constrained-systems-22ae" class="nav-link" data-scroll-target="#sec-efficient-ai-cloud-apps-vs-constrained-systems-22ae">Cloud Apps vs.&nbsp;Constrained Systems</a></li>
  <li><a href="#sec-efficient-ai-frequent-retraining-vs-stability-9b56" id="toc-sec-efficient-ai-frequent-retraining-vs-stability-9b56" class="nav-link" data-scroll-target="#sec-efficient-ai-frequent-retraining-vs-stability-9b56">Frequent Retraining vs.&nbsp;Stability</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-broader-challenges-fe2d" id="toc-sec-efficient-ai-broader-challenges-fe2d" class="nav-link" data-scroll-target="#sec-efficient-ai-broader-challenges-fe2d">Broader Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-optimization-limits-a52b" id="toc-sec-efficient-ai-optimization-limits-a52b" class="nav-link" data-scroll-target="#sec-efficient-ai-optimization-limits-a52b">Optimization Limits</a></li>
  <li><a href="#sec-efficient-ai-moores-law-case-study-2e70" id="toc-sec-efficient-ai-moores-law-case-study-2e70" class="nav-link" data-scroll-target="#sec-efficient-ai-moores-law-case-study-2e70">Moore’s Law Case Study</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-ml-optimization-parallels-0fbb" id="toc-sec-efficient-ai-ml-optimization-parallels-0fbb" class="nav-link" data-scroll-target="#sec-efficient-ai-ml-optimization-parallels-0fbb">ML Optimization Parallels</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-equity-concerns-9026" id="toc-sec-efficient-ai-equity-concerns-9026" class="nav-link" data-scroll-target="#sec-efficient-ai-equity-concerns-9026">Equity Concerns</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-uneven-access-8ace" id="toc-sec-efficient-ai-uneven-access-8ace" class="nav-link" data-scroll-target="#sec-efficient-ai-uneven-access-8ace">Uneven Access</a></li>
  <li><a href="#sec-efficient-ai-lowresource-challenges-a3d1" id="toc-sec-efficient-ai-lowresource-challenges-a3d1" class="nav-link" data-scroll-target="#sec-efficient-ai-lowresource-challenges-a3d1">Low-Resource Challenges</a></li>
  <li><a href="#sec-efficient-ai-efficiency-accessibility-71c4" id="toc-sec-efficient-ai-efficiency-accessibility-71c4" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiency-accessibility-71c4">Efficiency for Accessibility</a></li>
  <li><a href="#sec-efficient-ai-democratization-pathways-1b49" id="toc-sec-efficient-ai-democratization-pathways-1b49" class="nav-link" data-scroll-target="#sec-efficient-ai-democratization-pathways-1b49">Democratization Pathways</a></li>
  </ul></li>
  <li><a href="#sec-efficient-ai-balancing-innovation-efficiency-6be4" id="toc-sec-efficient-ai-balancing-innovation-efficiency-6be4" class="nav-link" data-scroll-target="#sec-efficient-ai-balancing-innovation-efficiency-6be4">Balancing Innovation and Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-efficient-ai-stability-vs-experimentation-e472" id="toc-sec-efficient-ai-stability-vs-experimentation-e472" class="nav-link" data-scroll-target="#sec-efficient-ai-stability-vs-experimentation-e472">Stability vs.&nbsp;Experimentation</a></li>
  <li><a href="#sec-efficient-ai-resourceintensive-innovation-8a8d" id="toc-sec-efficient-ai-resourceintensive-innovation-8a8d" class="nav-link" data-scroll-target="#sec-efficient-ai-resourceintensive-innovation-8a8d">Resource-Intensive Innovation</a></li>
  <li><a href="#sec-efficient-ai-efficiencycreativity-constraint-1ae1" id="toc-sec-efficient-ai-efficiencycreativity-constraint-1ae1" class="nav-link" data-scroll-target="#sec-efficient-ai-efficiencycreativity-constraint-1ae1">Efficiency-Creativity Constraint</a></li>
  <li><a href="#sec-efficient-ai-striking-balance-d49a" id="toc-sec-efficient-ai-striking-balance-d49a" class="nav-link" data-scroll-target="#sec-efficient-ai-striking-balance-d49a">Striking a Balance</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#fallacies-and-pitfalls" id="toc-fallacies-and-pitfalls" class="nav-link" data-scroll-target="#fallacies-and-pitfalls">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-efficient-ai-summary-66bb" id="toc-sec-efficient-ai-summary-66bb" class="nav-link" data-scroll-target="#sec-efficient-ai-summary-66bb">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Efficient AI</a></li></ol></nav></header>




<section id="sec-efficient-ai" class="level1 page-columns page-full">
<h1>Efficient AI</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability.</em></p>
</div></div><p> <img src="images/png/cover_efficient_ai.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What key trade-offs shape the pursuit of efficiency in machine learning systems, and why must engineers balance competing objectives?</em></p>
<p>Machine learning system efficiency cannot be achieved by optimizing single metrics in isolation. True efficiency requires balancing trade-offs across algorithmic complexity, computational resources, and data utilization, where improvements in one dimension often degrade performance in others. Algorithmic efficiency reduces computational requirements but may increase development complexity. Compute efficiency maximizes hardware utilization but may limit model expressiveness. Data efficiency enables learning with fewer examples but may require more sophisticated training procedures. These interdependent relationships demand systematic approaches that optimize across multiple competing objectives simultaneously. Understanding efficiency trade-offs enables engineers to design systems that achieve maximum performance within practical constraints of time, energy, and cost.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Define the principles of algorithmic, compute, and data efficiency in AI systems</p></li>
<li><p>Identify and analyze trade-offs between algorithmic, compute, and data efficiency in system design</p></li>
<li><p>Apply strategies for achieving efficiency across diverse deployment contexts including edge, cloud, and TinyML</p></li>
<li><p>Examine historical evolution and emerging trends in machine learning efficiency</p></li>
<li><p>Evaluate broader ethical and environmental implications of efficient AI system design</p></li>
<li><p>Assess scaling laws and their impact on efficiency optimization strategies</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-efficient-ai-overview-6f6a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-overview-6f6a">Overview</h2>
<p>Consider the practical reality facing ML engineers today. Training GPT-3 cost $4.6 million and consumed 1,287 MWh of electricity, equivalent to powering 120 homes for a year <span class="citation" data-cites="Patterson_et_al_2021">(<a href="#ref-Patterson_et_al_2021" role="doc-biblioref">D. Patterson et al. 2021</a>)</span>. This model requires 350GB+ of memory<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to run, making it impossible to deploy on most edge devices<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that typically have less than 8GB RAM. Autonomous vehicles need real-time inference within 100ms latency constraints and 50W power budgets, while mobile applications must deliver acceptable performance using processors 1000x less powerful than data center GPUs.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Memory Usage</strong>: ML models consume both VRAM (for GPU processing) and system RAM. Large language models like GPT-3 require 350GB+ memory for inference, while typical edge devices have only 4-8GB RAM, creating a deployment gap that necessitates model compression and optimization techniques.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Edge Devices</strong>: Computing devices deployed at the “edge” of networks, close to data sources rather than in centralized data centers. Examples include smartphones (4-12GB RAM), IoT sensors (1KB-1MB RAM), autonomous vehicle computers (8-64GB RAM), and smart cameras. Enable real-time processing with reduced network latency.</p></div></div><p>These resource constraints force engineers to navigate critical trade-offs. Reducing a model’s size for edge deployment might decrease accuracy from 95% to 92%, but enables real-time processing that makes the difference between functional and useless systems. Cloud deployments can afford higher model complexity for improved accuracy, but at costs of $1000+ per month for inference serving and increased latency that may violate user experience requirements. Medical diagnostic systems face similar choices: portable devices for remote areas need models optimized for 10W power consumption and offline operation, while hospital systems can leverage powerful hardware for detailed analysis at the cost of higher energy demands.</p>
<p>These efficiency challenges are not merely engineering problems but essential constraints that shape what AI applications are possible. Understanding and managing these trade-offs determines whether machine learning systems can achieve their potential impact across diverse deployment contexts. This chapter builds upon the ML workflow understanding from <strong><a href="../core/workflow/workflow.html#sec-ai-workflow">Chapter 19: AI Workflow</a></strong> and deep learning architectures from <strong><a href="../core/dnn_architectures/dnn_architectures.html#sec-dnn-architectures">Chapter 4: DNN Architectures</a></strong> to explore how efficiency considerations permeate every stage of system development, from initial architecture decisions to production monitoring.</p>
<p>Beyond their immediate technical impact, the implications of these efficiency decisions extend far beyond performance and cost. Efficient systems enable deployment across diverse environments, from cloud infrastructures to edge devices, enhancing accessibility and adoption. They also reduce environmental impact by lowering energy consumption and carbon emissions, aligning technological progress with ethical and ecological responsibilities. Efficiency constraints often drive innovation, forcing the development of novel algorithms, architectures, and optimization techniques that advance the entire field.</p>
<div id="quiz-question-sec-efficient-ai-overview-6f6a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a trade-off when deploying ML models on edge devices?</p>
<ol type="a">
<li>Increased model accuracy with higher energy consumption.</li>
<li>Reduced model size for lower power usage but decreased accuracy.</li>
<li>Higher latency with improved model complexity.</li>
<li>Increased processing speed with larger model size.</li>
</ol></li>
<li><p>Explain why balancing model size and accuracy is crucial for autonomous vehicles.</p></li>
<li><p>What is a potential benefit of deploying ML models in cloud-based systems?</p>
<ol type="a">
<li>Decreased latency and energy consumption.</li>
<li>Lower environmental impact due to less energy usage.</li>
<li>Reduced hardware costs for portable devices.</li>
<li>Ability to use more complex models for improved accuracy.</li>
</ol></li>
<li><p>How do efficient ML systems contribute to sustainability?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-overview-6f6a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-efficient-ai-defining-system-efficiency-8e59" class="level2">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-defining-system-efficiency-8e59">Defining System Efficiency</h2>
<p>To address these multifaceted efficiency challenges systematically, we require a comprehensive framework. Machine learning efficiency cannot be achieved by optimizing single metrics in isolation. Instead, it requires coordinated optimization across three interconnected dimensions that together determine system viability.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="Definition of Machine Learning System Efficiency">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Machine Learning System Efficiency</summary><div><strong>Machine Learning System Efficiency</strong> refers to the optimization of machine learning systems across three interconnected dimensions: <em>algorithmic efficiency</em>, <em>compute efficiency</em>, and <em>data efficiency</em>. Its goal is to minimize <em>computational, memory, and energy</em> demands while maintaining or improving system performance. This efficiency ensures that machine learning systems are <em>scalable, cost-effective, and sustainable</em>, which allows them to adapt to diverse deployment contexts, ranging from <em>cloud data centers</em> to <em>edge devices</em>. Achieving system efficiency, however, often requires navigating <em>trade-offs</em> between dimensions, such as balancing <em>model complexity</em> with <em>hardware constraints</em> or reducing <em>data dependency</em> without compromising <em>generalization</em>.<p></p>
</div></details>
</div>
<p>The complexity of this optimization challenge emerges from how these three dimensions are deeply intertwined and often mutually reinforcing, creating a complex optimization landscape that defies simple solutions. Algorithmic efficiency reduces computational requirements through better algorithms and architectures, but may increase development complexity or require specialized hardware. Compute efficiency maximizes hardware utilization through optimized implementations and specialized processors, but may limit model expressiveness or require specific algorithmic approaches. Data efficiency enables learning with fewer examples through improved training procedures and data utilization, but may require more sophisticated algorithms or additional computational resources.</p>
<p>Navigating these interconnected trade-offs requires understanding how they manifest in practice, particularly when confronted with the scaling realities that have driven much of AI’s recent progress.</p>
<p>Understanding these interdependencies is necessary for designing systems that achieve maximum performance within practical constraints of time, energy, and cost. The following sections examine how scaling laws reveal these constraints and why brute-force scaling alone cannot address real-world efficiency requirements.</p>
</section>
<section id="sec-efficient-ai-ai-scaling-laws-a043" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-ai-scaling-laws-a043">AI Scaling Laws</h2>
<p>Machine learning systems have followed a consistent pattern: increasing model scale through parameters, training data, and computational resources typically improves performance. This empirical observation has driven progress across natural language processing, computer vision, and speech recognition, where larger models trained on extensive datasets consistently achieve state-of-the-art results.</p>
<p>However, scaling requires substantial resources, raising critical questions about efficiency and sustainability. Key concerns include the computational resources needed for marginal accuracy improvements, data requirements as task complexity increases, and the point where diminishing returns make further scaling economically or practically infeasible.</p>
<p>These concerns about scaling efficiency are not merely theoretical but reflect practical constraints that limit real-world AI deployment. To address these concerns systematically, researchers have developed scaling laws<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, empirical relationships that quantify how model performance relates to training resources. These laws provide a framework for analyzing scaling trade-offs and reveal why efficiency becomes increasingly important as systems expand in size and complexity, offering crucial insights for the multi-dimensional efficiency optimization we must pursue.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Scaling Laws</strong>: Empirical relationships discovered by OpenAI showing that language model performance follows predictable power-law relationships with model size (N), dataset size (D), and compute budget (C). These laws enable researchers to predict performance and optimal resource allocation before expensive training runs.</p></div></div><p>This section introduces scaling laws, examines their manifestation across model, compute, and data dimensions, and analyzes their implications for system design. This establishes a foundation for understanding the limitations of brute-force scaling and the need for efficient methodologies that balance performance with practical resource constraints.</p>
<section id="sec-efficient-ai-fundamental-principles-16fa" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-fundamental-principles-16fa">The Scaling Reality</h3>
<p>Consider the rapid evolution in AI capabilities over the past decade. GPT-1 (published 2018) had 117 million parameters and could complete simple sentences. GPT-2 (2019) scaled to 1.5 billion parameters and could write coherent paragraphs. GPT-3 (2020) jumped to 175 billion parameters and demonstrated human-like text generation across diverse topics. Each increase in model size brought dramatically improved capabilities, but at exponentially increasing costs.</p>
<p>This pattern extends beyond language models. In computer vision, doubling the size of neural networks typically yields consistent accuracy gains, provided proportional increases in training data are supplied. AlexNet (2012) had 60 million parameters, VGG-16 (2014) scaled to 138 million, and modern vision transformers exceed 600 million parameters. Each generation achieved better image recognition accuracy, but required proportionally more computational resources and training data.</p>
<p>Underlying this consistent progress is the scaling hypothesis: larger models possess increased capacity to capture intricate data patterns, facilitating improved accuracy and generalization. However, this scaling comes with critical constraints that make efficiency paramount. Training GPT-3 required approximately 314 sextillion<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> floating-point operations (314 followed by 21 zeros), equivalent to running a modern gaming PC continuously for over 350 years. The financial cost reached $4.6 million, and the energy consumption generated 502 tons of CO₂ equivalent <span class="citation" data-cites="strubell2019energy">(<a href="#ref-strubell2019energy" role="doc-biblioref">Strubell, Ganesh, and McCallum 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Sextillion</strong>: A number with 21 zeros (10²¹), representing an almost incomprehensible scale. To put this in perspective, there are approximately 7×10²² stars in the observable universe, making GPT-3’s training computation roughly 1/22nd of counting every star in the cosmos.</p></div><div id="ref-strubell2019energy" class="csl-entry" role="listitem">
Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019. <span>“Energy and Policy Considerations for Deep Learning in NLP.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 3645–50. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/p19-1355">https://doi.org/10.18653/v1/p19-1355</a>.
</div></div><p>These resource demands reveal why understanding scaling laws is necessary for efficiency. As illustrated in <a href="#fig-compute-trends" class="quarto-xref">Figure&nbsp;1</a>, computational demands of training state-of-the-art models are escalating at an unsustainable rate, growing faster than Moore’s Law improvements in hardware. This trajectory raises critical questions about the environmental impact and economic viability of continued brute-force scaling.</p>
<div id="fig-compute-trends" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute-trends.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: [@Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022.]"><img src="images/png/compute-trends.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: <span class="citation" data-cites="Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022">(<a href="#ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022" role="doc-biblioref">Sevilla et al. 2022a</a>.)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022" class="csl-entry" role="listitem">
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022a. <span>“Compute Trends Across Three Eras of Machine Learning.”</span> In <em>2022 International Joint Conference on Neural Networks (IJCNN)</em>, 1–8. IEEE. <a href="https://doi.org/10.1109/ijcnn55064.2022.9891914">https://doi.org/10.1109/ijcnn55064.2022.9891914</a>.
</div></div></figure>
</div>
<p>Scaling laws provide a quantitative framework for understanding these trade-offs. They reveal that model performance exhibits predictable patterns as resources increase, following power-law relationships where performance improves consistently but with diminishing returns<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Critically, these laws show that optimal resource allocation requires coordinating model size, dataset size, and computational budget rather than scaling any single dimension in isolation.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Diminishing Returns</strong>: Economic principle where each additional input yields progressively smaller output gains. In ML, doubling compute from 1 to 2 hours might improve accuracy by 5%, but doubling from 100 to 200 hours might improve it by only 0.5%.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Tokens</strong>: Individual units of text that language models process, created by breaking text into subword pieces using algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion tokens while PaLM used 780 billion tokens, requiring text corpora equivalent to millions of books from web crawls and digitized literature.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>FLOPs</strong>: Floating-Point Operations, measuring computational work performed. Modern deep learning models require 10²²-10²⁴ FLOPs for training: GPT-3 used ~3.14 × 10²³ FLOPs (314 sextillion operations), equivalent to running a high-end gaming PC continuously for over 350 years.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Transformer</strong>: Neural network architecture introduced by Vaswani et al. <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Chen et al. 2018</a>)</span> that revolutionized NLP through self-attention mechanisms. Unlike sequential RNNs, transformers enable parallel processing during training, forming the foundation of modern large language models including GPT, BERT, T5, and their derivatives.</p><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Chen, Mia Xu, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, et al. 2018. <span>“The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.”</span> In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 5998–6008. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/p18-1008">https://doi.org/10.18653/v1/p18-1008</a>.
</div></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Autoregressive Models</strong>: Language models that generate text by predicting each token based on all preceding tokens in the sequence. GPT-family models exemplify this approach, generating text left-to-right with causal attention masks to ensure each position only attends to previous positions.</p></div></div><p>Empirical studies of large language models (LLMs) clarify the interplay between parameters, data, and computational resources under fixed resource constraints. As illustrated in <a href="#fig-compute-optimal" class="quarto-xref">Figure&nbsp;2</a>, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>) that minimizes training loss. The left panel depicts ‘IsoFLOP curves,’ where each curve corresponds to a constant number of floating-point operations (FLOPs<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>) during transformer<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models.</p>
<div id="fig-compute-optimal" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-compute-optimal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute_optimal.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Optimal Compute Allocation: For fixed computational budgets, language model performance depends on balancing model size and training data volume; the left panel maps training loss across parameter counts, identifying an efficiency sweet spot for each FLOP level. The center and right panels quantify how optimal parameter counts and token requirements scale predictably with increasing compute, demonstrating the need for coordinated scaling of both model and data to maximize resource utilization in large language models. Source: [@hoffmann2022training]."><img src="images/png/compute_optimal.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-optimal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Optimal Compute Allocation</strong>: For fixed computational budgets, language model performance depends on balancing model size and training data volume; the left panel maps training loss across parameter counts, identifying an efficiency sweet spot for each FLOP level. The center and right panels quantify how optimal parameter counts and token requirements scale predictably with increasing compute, demonstrating the need for coordinated scaling of both model and data to maximize resource utilization in large language models. Source: <span class="citation" data-cites="hoffmann2022training">(<a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>For instance, in computer vision tasks, doubling the size of neural networks typically yields consistent accuracy gains, provided that proportional increases in training data are supplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> demonstrating that performance<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> scales predictably with both model parameters and training data volume.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>GPT-3</strong>: OpenAI’s 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming ~1,287 MWh of electricity. Its training data included 45TB of text from the internet, books, and other sources.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Perplexity</strong>: A measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss). GPT-3 achieved ~20 perplexity on WebText, meaning on average it’s as confused as if choosing randomly among 20 equally-likely next words.</p></div></div><p>However, scaling laws reveal critical constraints. While larger models can achieve superior performance, resource demands increase exponentially. As illustrated in <a href="#fig-compute-trends" class="quarto-xref">Figure&nbsp;1</a>, computational demands of training state-of-the-art models are escalating at an unsustainable rate. This raises important questions about the environmental impact and economic viability of continued scaling.</p>
<p>Understanding these relationships is important for informing decisions about system design and resource allocation. They outline both the potential benefits of scaling and its costs, guiding the development of more efficient and sustainable machine learning systems. This understanding provides the necessary context for our subsequent examination of algorithmic, compute, and data efficiency.</p>
</section>
<section id="sec-efficient-ai-empirical-scaling-laws-b941" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-empirical-scaling-laws-b941">Three Scaling Regimes</h3>
<p>Building on this foundation, understanding scaling laws requires recognizing that performance improvements follow predictable patterns, but these patterns change depending on resource availability. Data scaling provides the clearest example of this behavior, exhibiting three distinct regimes that reveal both the power and limitations of scaling approaches.</p>
<p>A key example of this behavior is the relationship between generalization error and dataset size, which exhibits three distinct regimes: a Small Data Region, a Power-law Region, and an Irreducible Error Region <span class="citation" data-cites="hestness2017deep">(<a href="#ref-hestness2017deep" role="doc-biblioref">Hestness et al. 2017</a>)</span>. As shown in <a href="#fig-data-scaling-regimes" class="quarto-xref">Figure&nbsp;3</a>, small datasets lead to high generalization error constrained by poor estimates (best-guess error). As more data becomes available, models enter the power-law region, where generalization error decreases predictably as a function of dataset size. Eventually, this trend saturates, approaching the irreducible error floor, beyond which further data yields negligible improvements. This visualization demonstrates the principle of diminishing returns and highlights the operational regime in which data scaling is most effective.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-data-scaling-regimes" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-data-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="54926858636f497bd98ce136bee5c4785ec5dc9d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: The relationship between dataset size and generalization error follows distinct scaling regimes. Increasing dataset size initially reduces generalization error following a power-law relationship, but eventually plateaus at an irreducible error floor determined by inherent data limitations or model capacity [@hestness2017deep]. This behavior exposes diminishing returns from data scaling and informs practical decisions about data collection efforts in machine learning systems."><img src="efficient_ai_files/mediabag/54926858636f497bd98ce136bee5c4785ec5dc9d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The relationship between dataset size and generalization error follows distinct scaling regimes. Increasing dataset size initially reduces generalization error following a power-law relationship, but eventually plateaus at an irreducible error floor determined by inherent data limitations or model capacity <span class="citation" data-cites="hestness2017deep">(<a href="#ref-hestness2017deep" role="doc-biblioref">Hestness et al. 2017</a>)</span>. This behavior exposes diminishing returns from data scaling and informs practical decisions about data collection efforts in machine learning systems.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-hestness2017deep" class="csl-entry" role="listitem">
Hestness, Joel, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. <span>“Deep Learning Scaling Is Predictable, Empirically.”</span> <em>arXiv Preprint arXiv:1712.00409</em>, December. <a href="http://arxiv.org/abs/1712.00409v1">http://arxiv.org/abs/1712.00409v1</a>.
</div></div></figure>
</div>
<p>This three-regime pattern appears across different resource dimensions. In the small data region, models make poor predictions due to insufficient examples. In the power-law region, performance improves predictably with more resources. In the saturation region, additional resources yield minimal gains as models approach fundamental limits.</p>
<p>For efficiency optimization, the implications of these scaling regimes are profound. Most practical benefit comes from operating in the power-law region, where resource investments produce reliable performance improvements. However, reaching this regime requires minimum resource thresholds, and staying in it demands careful resource allocation to avoid premature saturation.</p>
</section>
<section id="sec-efficient-ai-optimal-resource-allocation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-optimal-resource-allocation">Optimal Resource Allocation</h3>
<p>The key insight from scaling laws is that optimal performance requires coordinated scaling across multiple dimensions rather than maximizing any single resource. The relationship between model parameters, training data, and compute budget determines whether resources are used efficiently or wasted on suboptimal configurations.</p>
<p>This principle becomes clear when examining the empirical evidence from large language models. <span class="citation" data-cites="kaplan2020scaling">Kaplan et al. (<a href="#ref-kaplan2020scaling" role="doc-biblioref">2020</a>)</span> demonstrated that transformer-based language models scale predictably with three pivotal factors: the number of model parameters, the volume of the training dataset (measured in tokens), and the total computational budget (measured in floating-point operations). When these factors are augmented proportionally, models exhibit consistent performance improvements without requiring architectural modifications or task-specific tuning.</p>
<div class="no-row-height column-margin column-container"></div><p>When these factors are increased proportionally, models exhibit consistent performance improvements without requiring architectural modifications or task-specific tuning. This behavior underlies contemporary training strategies for large-scale language models and has influenced design decisions in both research and production environments.</p>
<p>The practical manifestation of these empirical patterns appears clearly in <a href="#fig-kaplan-scaling" class="quarto-xref">Figure&nbsp;4</a>, which presents test loss curves for models spanning a range of sizes, from <span class="math inline">\(10^3\)</span> to <span class="math inline">\(10^9\)</span> parameters. The figure reveals two key insights. First, larger models demonstrate superior sample efficiency, achieving target performance levels with fewer training tokens. Second, as computational resources increase, the optimal model size correspondingly grows, with loss decreasing predictably when compute is allocated efficiently. The curves also highlight a practical consideration in large-scale training: compute-optimal solutions often entail early stopping before full convergence, demonstrating the inherent trade-off between training duration and resource utilization.</p>
<div id="fig-kaplan-scaling" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-kaplan-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/kaplan_scaling_data_compute.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Scaling Laws &amp; Compute Optimality: Larger models consistently achieve better performance with increased training data and compute, but diminishing returns necessitate careful resource allocation during training. Optimal model size and training duration depend on the available compute budget, as evidenced by the convergence of loss curves at different parameter scales and training token counts. Source: [@kaplan2020scaling]."><img src="images/png/kaplan_scaling_data_compute.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kaplan-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Scaling Laws &amp; Compute Optimality</strong>: Larger models consistently achieve better performance with increased training data and compute, but diminishing returns necessitate careful resource allocation during training. Optimal model size and training duration depend on the available compute budget, as evidenced by the convergence of loss curves at different parameter scales and training token counts. Source: <span class="citation" data-cites="kaplan2020scaling">(<a href="#ref-kaplan2020scaling" role="doc-biblioref">Kaplan et al. 2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-kaplan2020scaling" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“Scaling Laws for Neural Language Models.”</span> <em>arXiv Preprint arXiv:2001.08361</em>, January. <a href="http://arxiv.org/abs/2001.08361v1">http://arxiv.org/abs/2001.08361v1</a>.
</div></div></figure>
</div>
<p>This work established a theoretical scaling relationship that defines the optimal allocation of compute between model size and dataset size. For a fixed compute budget, optimal resource allocation follows the relationship <span class="math inline">\(D \propto N^{0.74}\)</span> <span class="citation" data-cites="hoffmann2022training">(<a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>, where dataset size <span class="math inline">\(D\)</span> and model size <span class="math inline">\(N\)</span> grow in coordinated proportions. This means that as model size increases, the dataset should grow at roughly three-quarters the rate to maintain optimal compute efficiency. This defines the compute-optimal scaling frontier, where neither the model is undertrained nor the data underutilized.</p>
<div class="no-row-height column-margin column-container"></div><p>However, these theoretical predictions assume perfect compute utilization, which becomes increasingly challenging in distributed training scenarios. Real-world implementations face communication overhead that scales unfavorably with system size, creating bandwidth bottlenecks that can reduce effective compute utilization from the theoretical optimum. Beyond 100 nodes, communication overhead typically reduces expected performance gains by 20-40%, transforming the predicted 2.3x improvement from 10x compute increase to approximately 1.8x in practice.</p>
</section>
<section id="sec-efficient-ai-mathematical-framework" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-mathematical-framework">Mathematical Framework</h3>
<p>The predictable patterns observed in scaling behavior can be expressed mathematically using power-law relationships. This mathematical framework provides precision for resource planning and optimization, building on the intuitive understanding established in previous sections.</p>
<p>To formalize these observations, the general formulation of scaling laws is expressed as: <span class="math display">\[
\mathcal{L}(N) = A N^{-\alpha} + B
\]</span> where loss <span class="math inline">\(\mathcal{L}\)</span> decreases as resource quantity <span class="math inline">\(N\)</span> increases, following a power-law decay with rate <span class="math inline">\(\alpha\)</span>, plus a baseline constant <span class="math inline">\(B\)</span>. Here, <span class="math inline">\(\mathcal{L}(N)\)</span> represents the loss achieved with resource quantity <span class="math inline">\(N\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are task-dependent constants, and <span class="math inline">\(\alpha\)</span> is the scaling exponent that characterizes the rate of performance improvement. A larger value of <span class="math inline">\(\alpha\)</span> signifies that performance improvements are more efficient with respect to scaling. This formulation also encapsulates the principle of diminishing returns: incremental gains in performance decrease as <span class="math inline">\(N\)</span> increases.</p>
<p>These theoretical predictions find strong empirical support across multiple model configurations. As shown in <a href="#fig-loss-vs-n-d" class="quarto-xref">Figure&nbsp;5</a>, the early-stopped test loss <span class="math inline">\(\mathcal{L}(N, D)\)</span> varies predictably with both dataset size and model size, and learning curves across configurations can be aligned through appropriate parameterization. These results further substantiate the regularity of scaling behavior and provide a practical tool for guiding model development under resource constraints, though practitioners must account for the distributed systems realities that modulate these ideal curves in production deployments.</p>
<div id="fig-loss-vs-n-d" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-vs-n-d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="de2e4205574ea268bcc1883053f7f000bba4f588.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Early-stopped test loss varies predictably with both dataset size and model size, highlighting the importance of balanced scaling for optimal performance under fixed compute budgets."><img src="efficient_ai_files/mediabag/de2e4205574ea268bcc1883053f7f000bba4f588.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-vs-n-d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Early-stopped test loss varies predictably with both dataset size and model size, highlighting the importance of balanced scaling for optimal performance under fixed compute budgets.
</figcaption>
</figure>
</div>
<p>Similar trends have been observed in other domains. In computer vision, model families such as ResNet and EfficientNet exhibit consistent accuracy improvements when scaled along dimensions of depth, width, and resolution, provided the scaling adheres to principled heuristics. These empirical patterns reinforce the observation that the benefits of scale are governed by underlying regularities that apply broadly across architectures and tasks.</p>
<p>As long as the scaling regime remains balanced, in which the model underfits the data while compute capacity is fully utilized, performance continues to improve predictably. However, once these assumptions are violated, scaling may lead to overfitting, underutilized resources, or inefficiencies, as explored in subsequent sections.</p>
</section>
<section id="sec-efficient-ai-scaling-regimes-16d7" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-scaling-regimes-16d7">Scaling Regimes</h3>
<p>While the scaling laws discussed thus far have focused primarily on pre-training, recent research indicates that scaling behavior extends to other phases of model development and deployment. A more complete understanding emerges by examining three distinct scaling regimes that characterize different stages of the machine learning pipeline.</p>
<p>The first regime, pre-training scaling, encompasses the traditional domain of scaling laws: how model performance improves with larger architectures, expanded datasets, and increased compute during initial training. This has been extensively studied in the context of foundation models, where clear power-law relationships emerge between resources and capabilities.</p>
<p>Post-training scaling is the second regime that focuses on improvements achieved after initial training through techniques such as fine-tuning, prompt engineering, and task-specific data augmentation. This regime has gained prominence with the rise of foundation models, where adaptation rather than retraining often provides the most efficient path to enhanced performance.</p>
<p>The third regime, test-time scaling, addresses how performance can be improved by allocating additional compute during inference, without modifying the model’s parameters. This includes methods such as ensemble prediction, chain-of-thought prompting, and iterative refinement, which effectively allow models to spend more time processing each input.</p>
<p>As shown in <a href="#fig-scaling-regimes" class="quarto-xref">Figure&nbsp;6</a>, these regimes exhibit distinct characteristics in how they trade computational resources for improved performance. Pre-training scaling typically requires massive resources but provides broad capability improvements. Post-training scaling offers more targeted enhancements with moderate resource requirements. Test-time scaling provides flexible performance-compute trade-offs that can be adjusted per inference.</p>
<div id="fig-scaling-regimes" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="78391c07afda8b90af1af45f04f4817ba07d3d3e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Different scaling regimes offer distinct approaches to improving model performance with varying compute investments. Pre-training establishes broad capabilities through large-scale training from scratch, post-training refines existing models through additional training phases, and test-time scaling dynamically allocates compute during inference to enhance per-sample results. Understanding these regimes clarifies the trade-offs between upfront investment and flexible, on-demand resource allocation for optimal system performance."><img src="efficient_ai_files/mediabag/78391c07afda8b90af1af45f04f4817ba07d3d3e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Different scaling regimes offer distinct approaches to improving model performance with varying compute investments. Pre-training establishes broad capabilities through large-scale training from scratch, post-training refines existing models through additional training phases, and test-time scaling dynamically allocates compute during inference to enhance per-sample results. Understanding these regimes clarifies the trade-offs between upfront investment and flexible, on-demand resource allocation for optimal system performance.
</figcaption>
</figure>
</div>
<p>Understanding these regimes is crucial for system design, as it reveals multiple paths to improving performance beyond simply scaling up model size or training data. For resource-constrained deployments, post-training and test-time scaling may provide more practical approaches than full model retraining. Similarly, in high-stakes applications, test-time scaling offers a way to trade latency for accuracy when needed.</p>
<p>This framework provides a more nuanced view of scaling in machine learning systems. By considering all three regimes, designers can make more informed decisions about resource allocation and optimization strategies across the full model lifecycle. The interplay between these regimes also suggests opportunities for hybrid approaches that leverage the strengths of each scaling mode while managing their respective costs and limitations.</p>
</section>
<section id="sec-efficient-ai-system-design-054c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-system-design-054c">System Design</h3>
<p>Scaling laws provide insights into the behavior of machine learning systems as resource allocation increases. The consistent observation of power-law trends suggests that, within a well-defined operational regime, model performance is predominantly determined by scale rather than idiosyncratic architectural innovations. This observation has significant ramifications for system design, resource planning, and the evaluation of efficiency.</p>
<p>A salient characteristic of these laws is the phenomenon of diminishing returns. While augmenting model size or training data volume yields performance improvements, the rate of these improvements diminishes with increasing scale. For instance, doubling the parameter count from 100 million to 200 million may produce substantial gains, whereas a similar doubling from 100 billion to 200 billion may yield only incremental enhancements. This behavior is mathematically captured by the scaling exponent <span class="math inline">\(\alpha\)</span>, which dictates the slope of the performance curve. Lower values of <span class="math inline">\(\alpha\)</span> indicate that more aggressive scaling is necessary to achieve comparable performance gains.</p>
<p>Practically, this implies that unmitigated scaling is ultimately unsustainable. Each successive increment in performance necessitates a disproportionately larger investment in data, compute, or model size. Consequently, scaling laws underscore the escalating tension between model performance and resource expenditure, a central theme of this discourse. They also emphasize the imperative of balanced scaling, wherein increments in one resource dimension (e.g., model parameters) must be accompanied by commensurate increments in other dimensions (e.g., dataset size and compute budget) to maintain optimal performance progression.</p>
<p>Scaling laws can also serve as a diagnostic instrument for identifying performance bottlenecks. Performance plateaus despite increased resource allocation may indicate saturation in one dimension, such as inadequate data in relation to model size, or inefficient utilization of computational resources. This diagnostic capability renders scaling laws not only predictive but also prescriptive, enabling practitioners to ascertain the optimal allocation of resources for maximum efficacy.</p>
<p>Understanding scaling laws is not merely of theoretical interest but has direct implications for the practical design of efficient machine learning systems. By revealing how performance responds to increases in model size, data, and compute, scaling laws provide a principled framework for making informed design decisions across the full lifecycle of system development.</p>
<p>One key application is in resource budgeting. Scaling laws allow practitioners to estimate the returns on investment for different types of resources. For example, when facing a fixed computational budget, designers can use empirical scaling curves to determine whether performance gains are better achieved by increasing model size, expanding the dataset, or improving training duration. This enables more strategic allocation of limited resources, particularly in scenarios where cost, energy, or time constraints are dominant factors.</p>
<p>In OpenAI’s development of GPT-3, the authors followed scaling laws derived from earlier experiments to determine the appropriate training dataset size and model parameter count <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. Rather than conducting expensive architecture searches, they scaled a known transformer architecture along the compute-optimal frontier to 175 billion parameters and 300 billion tokens. This approach allowed them to predict model performance and resource requirements in advance, highlighting the practical value of scaling laws in large-scale system planning.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div><p>Scaling laws also inform decisions about model architecture. Rather than relying on exhaustive architecture search or ad hoc heuristics, system designers can use scaling trends to identify when architectural changes are likely to yield significant improvements and when gains are better pursued through scale alone. For instance, if a given model family follows a favorable scaling curve, it may be preferable to scale that architecture rather than switching to a more complex but untested design. Conversely, if scaling saturates early, it may indicate that architectural innovations are needed to overcome current limitations.</p>
<p>Scaling laws can also guide deployment strategy. In edge and embedded environments, system designers often face tight resource budgets. By understanding how performance degrades when a model is scaled down, it is possible to choose smaller configurations that deliver acceptable accuracy within the deployment constraints. This supports the use of model families with predictable scaling properties, enabling a continuum of options from high-performance cloud deployment to lightweight on-device inference.</p>
<p>Finally, scaling laws provide insight into efficiency limits. By quantifying the trade-offs between scale and performance, they highlight when brute-force scaling becomes inefficient and signal the need for alternative approaches. This includes methods such as model compression, efficient knowledge transfer, sparsity techniques, and hardware-aware model design, all of which aim to extract more value from existing resources without requiring further increases in raw scale. These principles apply to modern foundation models, where efficiency becomes critical for practical deployment.</p>
<p>In this way, scaling laws serve as a compass for system designers, helping them navigate the complex landscape of performance, efficiency, and practicality. They do not dictate a single path forward, but they provide the analytical foundation for choosing among competing options in a principled and data-driven manner.</p>
</section>
<section id="sec-efficient-ai-scaling-vs-efficiency-f579" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-scaling-vs-efficiency-f579">Scaling vs.&nbsp;Efficiency</h3>
<p>While scaling laws elucidate a pathway to performance enhancement through the augmentation of model size, dataset volume, and computational budget, they concurrently reveal the rapidly escalating resource demands associated with such progress. As models become increasingly large and sophisticated, the resources necessary to support their training and deployment expand disproportionately. This phenomenon introduces a fundamental tension within contemporary machine learning: the performance gains achieved through scaling are often accompanied by a significant compromise in system efficiency. While <strong><a href="../core/dnn_architectures/dnn_architectures.html#sec-dnn-architectures">Chapter 4: DNN Architectures</a></strong> explores the design principles of these sophisticated architectures, this tension highlights why efficiency must be considered from the earliest stages of architectural design.</p>
<p>A primary concern is the computational expenditure. Training large-scale models necessitates substantial processing power, typically requiring distributed infrastructures<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> comprising hundreds or thousands of accelerators. For instance, the training of state-of-the-art language models may require tens of thousands of GPU-days, consuming millions of kilowatt-hours of electricity and incurring financial costs that are prohibitive for many institutions. These distributed training systems introduce additional complexity around communication overhead, synchronization, and scaling efficiency. Detailed distributed training strategies and their efficiency implications are covered in <strong><a href="../core/training/training.html#sec-ai-training">Chapter 6: AI Training</a></strong>. As previously discussed, the energy demands of training have outpaced Moore’s Law, raising critical questions regarding the long-term sustainability of continued scaling.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Distributed Infrastructure</strong>: Computing systems that spread ML workloads across multiple machines connected by high-speed networks. OpenAI’s GPT-4 training likely used thousands of NVIDIA A100 GPUs connected via InfiniBand, requiring careful orchestration to avoid communication bottlenecks.</p></div></div><p>Data acquisition and curation present equally significant trade-offs. Large models demand extensive data volumes and high-quality, diverse datasets to realize their full potential. The collection, cleansing, and labeling of such datasets consume considerable time and resources. As models approach saturation of available high-quality data, especially in natural language processing, further performance gains through data scaling become increasingly challenging. This reality requires extracting greater value from existing data and emphasizes data efficiency as a complement to brute-force scaling.</p>
<p>The financial and environmental implications of scaling also warrant careful consideration. Training runs for large foundation models can incur millions of U.S. dollars in computational expenses alone, and the carbon footprint<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> associated with such training has garnered increasing scrutiny. These costs limit accessibility to cutting-edge research and exacerbate disparities in access to advanced AI systems. From a system design perspective, this underscores the imperative to develop more resource-efficient scaling strategies that minimize consumption without sacrificing performance. The democratization challenges introduced by efficiency barriers connect directly to the accessibility goals addressed in <strong><a href="../core/ai_for_good/ai_for_good.html#sec-ai-good">Chapter 18: AI for Good</a></strong>, where efficient AI systems enable broader societal impact. Comprehensive approaches to environmental sustainability in ML systems, including carbon footprint measurement, green computing practices, and sustainable AI development frameworks, are explored in <strong><a href="../core/sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 17: Sustainable AI</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Carbon Emissions</strong>: Training GPT-3 generated approximately 502 tons of CO₂ equivalent, comparable to the annual emissions of 123 gasoline-powered vehicles. Modern ML practices increasingly incorporate carbon tracking using tools like CodeCarbon and the ML CO2 Impact calculator to measure and minimize environmental impact.</p></div></div><p>These trade-offs highlight that while scaling laws provide a valuable framework for understanding performance growth, they do not offer an unencumbered path to improvement. Each incremental performance gain must be evaluated against the corresponding resource requirements. As machine learning systems approach the practical limits of scale, the focus must shift from mere scaling to efficient scaling. This transition necessitates a holistic approach to system design that balances performance, cost, energy, and environmental impact, ensuring that advancements in AI are not only effective but also sustainable and equitable.</p>
</section>
<section id="sec-efficient-ai-scaling-breakdown-2247" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-scaling-breakdown-2247">Scaling Breakdown</h3>
<p>While scaling laws exhibit remarkable consistency within specific operational regimes, they are not devoid of limitations. As machine learning systems expand, they inevitably encounter boundaries where the underlying assumptions of smooth, predictable scaling no longer hold. These breakdown points reveal critical inefficiencies and underscore the necessity for more refined system design.</p>
<p>A common failure mode is imbalanced scaling. For scaling laws to remain valid, model size, dataset size, and computational budget must be augmented in a coordinated manner. Over-investment in one dimension while maintaining others constant often results in suboptimal outcomes. For example, increasing model size without expanding the training dataset may induce overfitting, whereas increasing computational resources without model redesign may lead to inefficient resource utilization <span class="citation" data-cites="hoffmann2022training">(<a href="#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>. In such scenarios, performance plateaus or even declines despite increased resource expenditure.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hoffmann2022training" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training Compute-Optimal Large Language Models.”</span> <em>arXiv Preprint arXiv:2203.15556</em>, March. <a href="http://arxiv.org/abs/2203.15556v1">http://arxiv.org/abs/2203.15556v1</a>.
</div></div><p>Closely related is the issue of underutilized compute budgets. Large-scale models often require carefully tuned training schedules and learning rates to make full use of available resources. When compute is insufficiently allocated, due to premature stopping, batch size misalignment, or ineffective parallelism, models may fail to reach their performance potential despite significant infrastructure investment.</p>
<p>Another prevalent mode of failure is data saturation. Scaling laws presuppose that model performance will continue to improve with access to sufficient training data. However, in numerous domains, particularly in the fields of language and vision, the availability of high-quality, human-annotated data is finite. As models consume increasingly large datasets, they eventually reach a point of diminishing marginal utility, where additional data points contribute minimal new information. Beyond this threshold, larger models may exhibit memorization rather than generalization, leading to degraded performance on out-of-distribution tasks. This issue is particularly acute when scaling is pursued without commensurate enhancements in data diversity or quality.</p>
<p>Infrastructure bottlenecks also impose practical scaling constraints. As models grow in size, they demand greater memory bandwidth<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, interconnect capacity, and I/O throughput. These hardware limitations become increasingly challenging to overcome, even with specialized accelerators. For instance, distributing a trillion-parameter model across a cluster necessitates meticulous management of data parallelism, communication overhead, and fault tolerance. The complexity of orchestrating such large-scale systems introduces engineering challenges that can diminish the theoretical gains predicted by scaling laws.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Memory Bandwidth</strong>: The rate at which data can be read from or written to memory, measured in GB/s. NVIDIA H100 provides 3.35 TB/s memory bandwidth vs.&nbsp;typical DDR5 RAM’s 51 GB/s, a 65× difference critical for handling large model parameters.</p></div></div><p>Finally, semantic saturation presents a significant conceptual challenge. At extreme scales, models may approach the limits of what can be learned from their training distributions. Performance on benchmark tasks may continue to improve, but these improvements may no longer reflect meaningful gains in generalization or understanding. Instead, models may become increasingly brittle, susceptible to adversarial examples, or prone to generating plausible but inaccurate outputs, particularly in generative tasks.</p>
<p>These breakdown points demonstrate that scaling laws, while powerful, are not absolute. They describe empirical regularities under specific conditions, which become increasingly difficult to maintain at scale. As machine learning systems continue to evolve, discerning where and why scaling ceases to be effective and developing strategies that enhance performance without relying solely on scale becomes necessary.</p>
<p>To synthesize the primary causes of scaling failure, the following diagnostic matrix (<a href="#tbl-scaling-breakdown" class="quarto-xref">Table&nbsp;1</a>) outlines typical breakdown types, their underlying causes, and representative scenarios. This table serves as a reference point for anticipating inefficiencies and guiding more balanced system design.</p>
<p>In this section, we have explored the fundamental principles of AI scaling laws, examining their empirical foundations, practical implications, and inherent limitations. Scaling laws provide a valuable framework for understanding how model performance scales with resources, but they also highlight the importance of efficiency and sustainability. The trade-offs and challenges associated with scaling underscore the need for a holistic approach to system design, which balances performance with resource constraints. The following sections examine the specific dimensions of efficiency, including algorithmic, computational, and data-related aspects, exploring how these areas contribute to the development of more sustainable and effective machine learning systems.</p>
<div id="tbl-scaling-breakdown" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-scaling-breakdown-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Scaling Breakdown Types</strong>: Unbalanced scaling across model size, data volume, and compute resources leads to specific failure modes, such as overfitting or diminishing returns, impacting system performance and efficiency. The table categorizes these breakdowns, identifies their root causes, and provides representative scenarios to guide more effective system design and resource allocation.
</figcaption>
<div aria-describedby="tbl-scaling-breakdown-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Dimension Scaled</th>
<th style="text-align: left;">Type of Breakdown</th>
<th style="text-align: left;">Underlying Cause</th>
<th style="text-align: left;">Example Scenario</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Size</td>
<td style="text-align: left;">Overfitting</td>
<td style="text-align: left;">Model capacity exceeds available data</td>
<td style="text-align: left;">Billion-parameter model on limited dataset</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data Volume</td>
<td style="text-align: left;">Diminishing Returns</td>
<td style="text-align: left;">Saturation of new or diverse information</td>
<td style="text-align: left;">Scaling web text beyond useful threshold</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compute Budget</td>
<td style="text-align: left;">Underutilized Resources</td>
<td style="text-align: left;">Insufficient training steps or inefficient use</td>
<td style="text-align: left;">Large model with truncated training duration</td>
</tr>
<tr class="even">
<td style="text-align: left;">Imbalanced Scaling</td>
<td style="text-align: left;">Inefficiency</td>
<td style="text-align: left;">Uncoordinated increase in model/data/compute</td>
<td style="text-align: left;">Doubling model size without more data or time</td>
</tr>
<tr class="odd">
<td style="text-align: left;">All Dimensions</td>
<td style="text-align: left;">Semantic Saturation</td>
<td style="text-align: left;">Exhaustion of learnable patterns in the domain</td>
<td style="text-align: left;">No further gains despite scaling all inputs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-efficient-ai-toward-efficient-scaling-7f6d" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-toward-efficient-scaling-7f6d">Toward Efficient Scaling</h3>
<p>While the empirical success of scaling laws has driven substantial progress in artificial intelligence, these observations raise foundational questions that extend beyond resource allocation. The sustainability of the current scaling trajectory, and its adequacy in capturing the principles of efficient AI system design, must be critically examined.</p>
<p>The empirical regularities observed in scaling laws prompt deeper inquiry: can observation be translated into a theoretical framework that explains the mechanisms driving these patterns? Establishing such a framework would enhance scientific understanding and inform the design of more efficient algorithms and architectures. The limitations inherent in brute-force scaling, including diminishing returns and observed breakdowns, highlight the need for architectural innovations that reshape scaling behavior and address efficiency from an algorithmic standpoint.</p>
<p>The increasing demand for data emphasizes the importance of transitioning to a data-centric paradigm. As data saturation is approached, the efficiency of data acquisition, curation, and utilization becomes critical. This shift requires a deeper understanding of data dynamics and the development of strategies that maximize the utility of limited data resources. These considerations form the basis for the upcoming discussion on data efficiency.</p>
<p>The computational demands of large-scale models underscore the necessity of compute efficiency. Sustainable scaling depends on minimizing resource consumption while maintaining or improving performance. This objective motivates the exploration of hardware-optimized architectures and training methodologies that support efficient execution, to be discussed in the context of compute efficiency.</p>
<p>Algorithmic, compute, and data efficiency are not independent; their interdependence shapes the overall performance of machine learning systems. The emergence of novel capabilities in extremely large models suggests the potential for synergistic effects across these dimensions. Achieving real-world efficiency requires a holistic approach to system design in which these elements are carefully orchestrated. This perspective introduces the forthcoming discussion on system efficiency.</p>
<p>The ethical considerations surrounding access to compute and data resources demonstrate that efficiency is not solely a technical goal. Ensuring equitable distribution of the benefits of efficient AI represents a broader societal imperative. Subsequent sections will address these challenges, including the limits of optimization, the implications of Moore’s Law, and the balance between innovation and accessibility.</p>
<p>The future of scaling lies not in unbounded expansion but in the coordinated optimization of algorithmic, compute, and data resources. The sections that follow will examine each of these dimensions and their contributions to the development of efficient and sustainable machine learning systems. Although scaling laws offer a valuable perspective, they represent only one component of a more comprehensive framework.</p>
<div id="quiz-question-sec-efficient-ai-ai-scaling-laws-a043" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the relationship between model performance and resource allocation according to scaling laws?</p>
<ol type="a">
<li>Performance improves linearly with increased resources.</li>
<li>Performance remains constant regardless of resources.</li>
<li>Performance improves as a power function of resource allocation.</li>
<li>Performance decreases with increased resources.</li>
</ol></li>
<li><p>Explain how scaling laws can inform decisions about resource allocation in machine learning system design.</p></li>
<li><p>What is a potential consequence of unbalanced scaling in machine learning systems?</p>
<ol type="a">
<li>Improved performance without additional resources.</li>
<li>Increased data diversity.</li>
<li>Reduced computational requirements.</li>
<li>Overfitting due to excessive model size.</li>
</ol></li>
<li><p>True or False: According to scaling laws, increasing model size always leads to improved performance.</p></li>
<li><p>In a production system with limited computational resources, how might you apply the concept of scaling laws to optimize performance?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-ai-scaling-laws-a043" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-efficient-ai-pillars-ai-efficiency-c024" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-pillars-ai-efficiency-c024">The Efficiency Framework</h2>
<p>Scaling laws reveal a fundamental constraint: optimal performance requires coordinated optimization across multiple dimensions. No single resource, whether model parameters, training data, or compute budget, can be scaled indefinitely to achieve efficiency. Instead, practical ML systems must balance trade-offs across three interconnected dimensions that together determine system viability and performance.</p>
<p>To illustrate these interconnected constraints, consider a concrete example. Training a language model for mobile deployment faces simultaneous constraints: the model must fit in 2GB memory (compute efficiency), achieve acceptable accuracy with limited training data (data efficiency), and complete inference within 50ms latency (algorithmic efficiency). Optimizing any single dimension in isolation fails. A highly compressed algorithm may reduce memory usage but increase computation time. Abundant training data improves accuracy but requires larger models. Powerful hardware enables complex models but consumes more energy and costs more to deploy.</p>
<p>The three pillars of efficiency provide a systematic framework for navigating these interdependent trade-offs. Each pillar addresses a distinct aspect of system performance while maintaining critical connections to the others:</p>
<p><strong>Algorithmic Efficiency</strong> optimizes the model architecture and training procedures to achieve maximum performance per unit of computation. This includes everything from efficient attention mechanisms that reduce quadratic complexity to architectural innovations that improve parameter utilization. Modern techniques achieve 10-100x improvements in computation efficiency while maintaining or improving accuracy.</p>
<p><strong>Compute Efficiency</strong> maximizes the utilization of available hardware resources, from optimizing software implementations to designing specialized hardware accelerators. This dimension encompasses both training efficiency (reducing the time and energy required to train models) and inference efficiency (optimizing deployment costs and latency). Current approaches achieve 5-50x improvements in hardware utilization through techniques like mixed-precision training and specialized AI chips.</p>
<p><strong>Data Efficiency</strong> maximizes the information extracted from available training data while minimizing data requirements. This includes techniques like transfer learning, data augmentation, and active learning that enable models to achieve strong performance with fewer training examples. Modern methods can reduce data requirements by 10-1000x for specific tasks while maintaining comparable accuracy.</p>
<p>The power of this framework emerges from how these dimensions are interconnected, as depicted in <a href="#fig-evolution-efficiency" class="quarto-xref">Figure&nbsp;7</a>. Algorithmic innovations often enable better hardware utilization, while hardware advances unlock new algorithmic possibilities. Data-efficient techniques reduce computational requirements, while compute-efficient methods enable training on larger datasets. Understanding these synergies is essential for building practical ML systems.</p>
<div id="fig-evolution-efficiency" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-evolution-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="2abdd66045808c7d07317e46f41d4af3ac1aea8a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Algorithmic, computational, and data efficiency have each contributed to substantial gains in AI capabilities, though at different rates and with diminishing returns. Understanding these historical trends clarifies the interplay between these efficiency dimensions and informs strategies for scaling machine learning systems in data-limited environments."><img src="efficient_ai_files/mediabag/2abdd66045808c7d07317e46f41d4af3ac1aea8a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-evolution-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Algorithmic, computational, and data efficiency have each contributed to substantial gains in AI capabilities, though at different rates and with diminishing returns. Understanding these historical trends clarifies the interplay between these efficiency dimensions and informs strategies for scaling machine learning systems in data-limited environments.
</figcaption>
</figure>
</div>
<section id="sec-efficient-ai-algorithmic-efficiency-a3ba" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-algorithmic-efficiency-a3ba">Algorithmic Efficiency: Doing More with Less</h3>
<p>Algorithmic efficiency achieves maximum performance per unit of computation through optimized model architectures and training procedures. In an era where training costs can reach millions of dollars and inference latency determines user experience, algorithmic innovations provide the most direct path to practical AI deployment. Modern algorithmic efficiency techniques achieve 10-100x improvements in computational requirements while maintaining or improving accuracy.</p>
<p>The foundation for these dramatic improvements lies in a key observation: most neural networks are dramatically overparameterized. The lottery ticket hypothesis reveals that networks contain sparse subnetworks, typically just 10-20% of original parameters, that achieve comparable accuracy when trained in isolation <span class="citation" data-cites="frankle2019lottery">(<a href="#ref-frankle2019lottery" role="doc-biblioref">Frankle and Carbin 2019</a>)</span>. This discovery transforms compression into a principled approach: large models serve primarily as initialization strategies for finding efficient architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-frankle2019lottery" class="csl-entry" role="listitem">
Frankle, Jonathan, and Michael Carbin. 2019. <span>“The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.”</span> In <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=rJl-b3RcF7">https://openreview.net/forum?id=rJl-b3RcF7</a>.
</div></div><section id="sec-efficient-ai-early-efficiency-a537" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-early-efficiency-a537">Modern Compression Techniques</h4>
<p>Three major approaches dominate modern algorithmic efficiency: model compression, precision optimization, and efficient knowledge transfer. Each targets different aspects of model inefficiency and works synergistically with the other efficiency dimensions.</p>
<p>At the core of model compression lies the systematic removal of redundant components from neural networks through three primary algorithmic approaches: magnitude-based pruning, structured compression, and gradual sparsification. These techniques exploit the inherent sparsity that emerges during training, with research demonstrating that networks contain efficient subnetworks that achieve comparable accuracy with significantly fewer parameters, often just 10-20% of the original size.</p>
<p>The magnitude-based pruning algorithm exploits the fundamental insight that weight magnitude correlates with importance, enabling structured removal of parameters. For a weight matrix W, we remove weights where |W_ij| &lt; threshold, where the threshold is determined by percentile analysis: <code>mask = |W| &gt; percentile(|W|, pruning_ratio)</code>. Structured pruning achieves actual hardware speedups by removing entire channels or blocks rather than individual weights, typically achieving 2-4x inference speedup with 1-3% accuracy loss. Unstructured pruning can achieve higher compression ratios (90%+ sparsity) but requires specialized sparse computation libraries to realize speedups. Research demonstrates that ResNet-50 can be pruned to 20% of original parameters while maintaining 99% of ImageNet accuracy through careful layer-wise sensitivity analysis and gradual pruning schedules.</p>
<p>Building on compression techniques, precision optimization systematically reduces computational requirements through quantization algorithms that map high-precision floating-point values to lower-precision representations. Neural networks demonstrate inherent robustness to precision reduction, enabling significant computational savings while maintaining acceptable performance by exploiting redundancy in high-precision representations.</p>
<p>The most effective approach, quantization-aware training (QAT)<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>, maintains accuracy during precision reduction by simulating quantization effects during training using fake quantization operations: <code>x_quantized = scale * round(x / scale + zero_point) - zero_point</code>.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Quantization-Aware Training (QAT)</strong>: Training method that simulates quantization effects during the learning process, enabling models to adapt to reduced precision. BERT-base with QAT maintains 99.1% accuracy when quantized to INT8 vs.&nbsp;94.2% with post-training quantization, demonstrating superior adaptation to precision constraints. Post-training quantization calibrates using representative data to determine optimal scale and zero-point parameters: for INT8 quantization, we map FP32 range [-α, β] to INT8 range [0, 255] using <code>scale = (β - α) / 255</code> and <code>zero_point = -round(α / scale)</code>. Per-channel quantization improves accuracy by using separate scaling factors for each output channel, reducing quantization error from ~2% to ~0.5% for vision models. INT8 quantization achieves 4x memory reduction and 2-4x inference speedup on modern hardware while maintaining 98-99% of FP32 accuracy. Emerging INT4 and binary quantization push further: BERT-base quantized to INT4 maintains 95% accuracy while achieving 8x speedup and 75% memory reduction on specialized inference accelerators.</p></div></div></section>
<section id="sec-efficient-ai-deep-learning-era-4647" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-deep-learning-era-4647">Hardware-Algorithm Co-optimization</h4>
<p>While algorithmic innovations provide substantial theoretical improvements, their practical benefits vary dramatically across hardware platforms, requiring systematic co-design frameworks that analyze both algorithmic characteristics and hardware constraints. The roofline model<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> provides a quantitative framework for determining whether algorithms are memory-bound or compute-bound by plotting arithmetic intensity<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> (operations per byte) against achievable performance on specific hardware.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Roofline Model</strong>: Performance analysis framework that plots computational intensity vs.&nbsp;achieved performance to identify bottlenecks. Shows whether workloads are limited by memory bandwidth (low intensity) or compute throughput (high intensity), enabling targeted optimization strategies.</p></div><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Arithmetic Intensity</strong>: Ratio of floating-point operations to memory accesses (ops/byte). Neural network layers with intensity &lt;10 ops/byte are memory-bound on GPUs, while &gt;100 ops/byte are compute-bound. Convolutions typically achieve 20-50 ops/byte, making them moderately memory-bound. For neural networks, operations with arithmetic intensity below 10 ops/byte are typically memory-bound on GPUs, while those above 100 ops/byte become compute-bound. Convolution layers typically achieve 20-50 ops/byte arithmetic intensity, making them moderately memory-bound, while fully connected layers achieve 2-10 ops/byte, making them heavily memory-bound.</p></div><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Operator Fusion</strong>: Optimization technique that combines multiple computational operations into single kernels to reduce memory transfers. Fusing convolution→batch normalization→ReLU eliminates intermediate memory writes, improving performance by 20-40% while reducing energy consumption. Batch size optimization must balance throughput and memory constraints: GPU inference typically optimizes for batch sizes of 8-32 to maximize memory bandwidth utilization, while edge devices with limited memory require batch size 1 optimization that prioritizes low latency over throughput.</p></div></div><p>Memory access pattern optimization becomes critical for edge deployment efficiency. Techniques like operator fusion<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> combine multiple operations to minimize data movement between compute and memory: fusing convolution-batch normalization-activation reduces memory traffic by 3x while improving cache utilization.</p>
<p>A quantitative co-design framework enables systematic optimization across multiple efficiency dimensions:</p>
<ol type="1">
<li><strong>Hardware Analysis</strong>: Memory bandwidth (GB/s), compute units (FLOPS), precision support (FP32/FP16/INT8)</li>
<li><strong>Algorithm Mapping</strong>: Determine operations that match hardware capabilities (tensor cores for matrix multiplication, specialized units for activation functions)</li>
<li><strong>Memory Optimization</strong>: Minimize data movement through careful operator scheduling and memory layout optimization</li>
<li><strong>Precision Selection</strong>: Match algorithm precision to hardware capabilities (FP16 on tensor cores, INT8 on edge accelerators)</li>
</ol>
<p>INT8 quantization achieves 2.3x speedup on NVIDIA V100 GPUs with typically 1.2% accuracy loss for vision models, while specialized neural processing units deliver 4.1x speedup on ARM Cortex-A78 processors with 2.1% accuracy degradation <span class="citation" data-cites="gholami2021survey">(<a href="#ref-gholami2021survey" role="doc-biblioref">Gholami et al. 2021</a>)</span>. Emerging INT4 quantization on dedicated AI accelerators provides 8x speedup but requires careful calibration to limit accuracy loss to 3-5%. Energy efficiency improvements are even more dramatic: 8-bit quantization consumes 16x less energy than 32-bit floating-point operations, while pruning to 20% sparsity linearly reduces energy consumption proportional to the sparsity ratio.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gholami2021survey" class="csl-entry" role="listitem">
Gholami, Amir, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. 2021. <span>“A Survey of Quantization Methods for Efficient Neural Network Inference.”</span> <em>arXiv Preprint arXiv:2103.13630</em>, March. <a href="http://arxiv.org/abs/2103.13630v3">http://arxiv.org/abs/2103.13630v3</a>.
</div><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Knowledge Distillation</strong>: Technique where a large “teacher” model transfers knowledge to a smaller “student” model by training the student to mimic the teacher’s output probabilities. DistilBERT achieves 97% of BERT’s performance with 40% fewer parameters and 60% faster inference through distillation.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Temperature Scaling</strong>: Parameter T that controls probability distribution “softness” in knowledge distillation. Higher temperatures (T=5) create smoother distributions with more information about wrong answers, while lower temperatures (T=1) create sharper distributions. Optimal T typically ranges from 3-5 for most models. Feature-level distillation transfers intermediate representations by minimizing L2 distance between student and teacher hidden states, enabling architectural flexibility when student and teacher networks have different structures. Online distillation enables mutual learning between multiple student networks without requiring a pre-trained teacher, achieving comparable results while reducing training time. Practical implementations achieve 5-10x model compression: DistilBERT maintains 97% of BERT-base performance with 40% fewer parameters and 2x inference speedup, while MobileBERT achieves 4.3x speedup with only 1.5% accuracy loss on GLUE tasks through progressive knowledge transfer.</p></div></div><p>Knowledge transfer techniques systematically distill capabilities from large teacher models into efficient student models through specialized training algorithms that optimize both hard targets (ground truth labels) and soft targets (teacher predictions). These approaches enable significant model compression while preserving much of the original model’s capability, directly addressing data efficiency by requiring fewer training examples for compact models. Knowledge distillation<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> uses temperature scaling<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> to control information transfer: <code>L_distillation = α * KL_divergence(student_logits/T, teacher_logits/T) + (1-α) * CrossEntropy(student_logits, ground_truth)</code>, where temperature T (typically 3-5) softens probability distributions and α (typically 0.7-0.9) balances distillation versus task loss.</p>
</section>
<section id="sec-efficient-ai-architectural-innovation" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-architectural-innovation">Architectural Innovation for Efficiency</h4>
<p>Beyond compression techniques, modern efficiency requires architectures designed from the ground up for resource constraints. Models like MobileNet<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, EfficientNet<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, and SqueezeNet<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> demonstrate that compact designs can deliver high performance through architectural innovations rather than just scaling up existing designs. These architectures use specialized architectural patterns and automated design techniques to optimize the efficiency-accuracy trade-off.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>MobileNet</strong>: Efficient neural network architecture using depthwise separable convolutions, achieving ~50× fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs.&nbsp;VGG-16’s 138M, enabling deployment on smartphones with &lt;100MB memory.</p></div><div id="fn22"><p><sup>22</sup>&nbsp;<strong>EfficientNet</strong>: Architecture achieving state-of-the-art accuracy with superior parameter efficiency compared to previous models. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy with 66M parameters, compared to ResNet-152’s 77.0% accuracy with 60M parameters, demonstrating better accuracy despite similar parameter count.</p></div><div id="fn23"><p><sup>23</sup>&nbsp;<strong>SqueezeNet</strong>: Compact CNN architecture achieving AlexNet-level accuracy with 50× fewer parameters (1.25M vs.&nbsp;60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance.</p></div></div><p>Central to these architectural innovations is the insight that different deployment contexts require different efficiency trade-offs. Cloud inference prioritizes throughput and can tolerate higher memory usage, favoring architectures with parallel-friendly operations. Edge deployment prioritizes latency and memory efficiency, requiring architectures that minimize memory access and computation. Mobile deployment constrains energy usage, demanding architectures that optimize for energy-efficient operations.</p>
</section>
<section id="sec-efficient-ai-modern-efficiency-7708" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-modern-efficiency-7708">Interconnected Efficiency: Parameter-Efficient Adaptation</h4>
<p>The frontier of algorithmic efficiency lies in parameter-efficient fine-tuning<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> techniques that demonstrate how the three efficiency dimensions work together. Parameter-efficient fine-tuning methods update less than 1% of model parameters while achieving full fine-tuning performance. This approach simultaneously addresses all three efficiency pillars: algorithmic efficiency through reduced parameter updates, compute efficiency through lower memory requirements and faster training, and data efficiency by leveraging pre-trained representations that require fewer task-specific examples.</p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Parameter-Efficient Fine-tuning</strong>: Methods like LoRA and Adapters that update &lt;1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation.</p></div></div><p>The transformative potential of this approach becomes evident when considering its practical impact: fine-tuning GPT-3 traditionally requires storing gradients for 175 billion parameters, consuming over 700GB of GPU memory. LoRA reduces this to under 10GB by learning low-rank decompositions of weight updates, enabling efficient adaptation on single consumer GPUs. This technique enables data-efficient learning by requiring only hundreds of examples rather than thousands for effective adaptation, while maintaining the compute efficiency needed for practical deployment.</p>
</section>
<section id="sec-efficient-ai-efficiency-design-5416" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-design-5416">Production Efficiency Monitoring</h4>
<p>Translating algorithmic efficiency from development to production requires continuous monitoring of model performance and resource utilization. Key metrics include inference latency (95th percentile response time), throughput (requests per second), and memory utilization (peak and average usage). Modern deployment systems track these metrics in real-time, automatically scaling resources or switching to more efficient model variants when efficiency degrades.</p>
<p>In this production context, the interconnection with other efficiency dimensions becomes critical. Compute efficiency determines whether algorithmic optimizations translate to real-world speedups, while data efficiency affects how quickly models can adapt to distribution shifts without requiring expensive retraining. Successful production systems optimize across all three dimensions simultaneously rather than treating them as independent concerns.</p>
<div id="fig-algo-efficiency" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-algo-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="42e5ca2c27671b95ab12272cb46750f6894ba650.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Neural network training compute requirements decreased 44× between 2012 and 2019, outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements on model efficiency [@Hernandez_et_al_2020]. Innovations in model architecture and optimization techniques can drive substantial gains in AI system sustainability via this halving of compute every 16 months."><img src="efficient_ai_files/mediabag/42e5ca2c27671b95ab12272cb46750f6894ba650.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-algo-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Neural network training compute requirements decreased 44× between 2012 and 2019, outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> on model efficiency <span class="citation" data-cites="Hernandez_et_al_2020">(<a href="#ref-Hernandez_et_al_2020" role="doc-biblioref">Hernandez, Brown, et al. 2020</a>)</span>. Innovations in model architecture and optimization techniques can drive substantial gains in AI system sustainability via this halving of compute every 16 months.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Stochastic Gradient Descent (SGD)</strong>: Optimization algorithm using random data samples, introduced by Robbins and Monro (1951). Made neural network training practical by reducing memory requirements from full-batch to single-sample updates, enabling learning on larger datasets.</p></div></div></figure>
</div>
<p>Notably, as <a href="#fig-algo-efficiency" class="quarto-xref">Figure&nbsp;8</a> shows, the computational resources needed to train a neural network to achieve AlexNet<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>-level performance on ImageNet<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> classification had decreased by <span class="math inline">\(44\times\)</span> compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore’s Law<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations <span class="citation" data-cites="Hernandez_et_al_2020">(<a href="#ref-Hernandez_et_al_2020" role="doc-biblioref">Hernandez, Brown, et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>AlexNet</strong>: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution.</p></div><div id="fn27"><p><sup>27</sup>&nbsp;<strong>ImageNet</strong>: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017.</p></div><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Moore’s Law</strong>: Intel co-founder Gordon Moore’s 1965 observation that transistor density doubles every ~2 years. Hardware improvements follow ~2x every 18-24 months, while AI algorithmic efficiency improved 44x in 7 years (2012-2019).</p></div><div id="ref-Hernandez_et_al_2020" class="csl-entry" role="listitem">
Hernandez, Danny, Tom B. Brown, et al. 2020. <span>“Measuring the Algorithmic Efficiency of Neural Networks.”</span> <em>OpenAI Blog</em>. <a href="https://openai.com/research/ai-and-efficiency">https://openai.com/research/ai-and-efficiency</a>.
</div></div><p>The evolution of algorithmic efficiency, from algorithmic innovations to hardware-aware optimization, is of importance in machine learning. As the field advances, algorithmic efficiency will remain central to the design of systems that are high-performing, scalable, and sustainable.</p>
</section>
</section>
<section id="sec-efficient-ai-compute-efficiency-e72b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-compute-efficiency-e72b">Compute Efficiency</h3>
<p>Compute efficiency focuses on the effective use of hardware and computational resources to train and deploy machine learning models. It encompasses strategies for reducing energy consumption, optimizing processing speed, and leveraging hardware capabilities to achieve scalable and sustainable system performance. The evolution of compute efficiency is closely tied to advancements in hardware technologies, reflecting the growing demands of machine learning applications over time. While this chapter focuses on efficiency principles and trade-offs, the detailed technical implementation of hardware acceleration, including GPU architectures, TPU design, memory systems, and custom accelerators, is covered in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<section id="sec-efficient-ai-generalpurpose-computing-era-e3d1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-generalpurpose-computing-era-e3d1">General-Purpose Computing Era</h4>
<p>To understand the dramatic evolution of compute efficiency, we must first examine how the early days of machine learning were shaped by the limitations of general-purpose CPUs. Central Processing Units (CPUs) excel at sequential processing and complex decision-making but have limited parallelism, typically 4-16 cores optimized for diverse tasks rather than the repetitive matrix operations that dominate machine learning. Early research by Talpes and Marculescu at ISCA demonstrated that interconnection overheads and scaling limitations constrain the effectiveness of simply adding more CPU cores <span class="citation" data-cites="talpes2005scalability">(<a href="#ref-talpes2005scalability" role="doc-biblioref">Talpes and Marculescu, n.d.</a>)</span>. Studies of desktop applications showed that even by 2010, most software utilized limited thread-level parallelism, with only modest improvements from additional cores <span class="citation" data-cites="blake2010evolution">(<a href="#ref-blake2010evolution" role="doc-biblioref">Blake et al. 2010</a>)</span>. During this period, machine learning models had to operate within strict computational constraints, as specialized hardware for machine learning did not yet exist. Efficiency was achieved through algorithmic innovations, such as simplifying mathematical operations, reducing model size, and optimizing data handling to minimize computational overhead.</p>
<div class="no-row-height column-margin column-container"><div id="ref-talpes2005scalability" class="csl-entry" role="listitem">
Talpes, E., and D. Marculescu. n.d. <span>“Increased Scalability and Power Efficiency by Using Multiple Speed Pipelines.”</span> In <em>32nd International Symposium on Computer Architecture (ISCA’05)</em>, 310–21. ISCA ’05. IEEE. <a href="https://doi.org/10.1109/isca.2005.33">https://doi.org/10.1109/isca.2005.33</a>.
</div><div id="ref-blake2010evolution" class="csl-entry" role="listitem">
Blake, Geoffrey, Ronald G. Dreslinski, Trevor Mudge, and Krisztián Flautner. 2010. <span>“Evolution of Thread-Level Parallelism in Desktop Applications.”</span> In <em>Proceedings of the 37th Annual International Symposium on Computer Architecture</em>, 302–13. ISCA ’10. Saint-Malo, France: ACM. <a href="https://doi.org/10.1145/1815961.1816000">https://doi.org/10.1145/1815961.1816000</a>.
</div></div><p>Researchers worked to maximize the capabilities of CPUs by using parallelism where possible, though options were limited. Training times for models were often measured in days or weeks, as even relatively small datasets and models pushed the boundaries of available hardware. The focus on compute efficiency during this era was less about hardware optimization and more about designing algorithms that could run effectively within these constraints.</p>
</section>
<section id="sec-efficient-ai-accelerated-computing-era-0575" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-accelerated-computing-era-0575">Accelerated Computing Era</h4>
<p>This CPU-constrained era ended abruptly as the introduction of deep learning in the early 2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> showed the potential of neural networks, but their computational demands quickly surpassed the capabilities of traditional CPUs. As shown in <a href="#fig-comp_efficiency" class="quarto-xref">Figure&nbsp;9</a>, this marked the beginning of an era of exponential growth in compute usage. OpenAI’s analysis reveals that the amount of compute used in AI training has increased 300,000 times since 2012, doubling approximately every 3.4 months, a rate far exceeding Moore’s Law <span class="citation" data-cites="Amodei_et_al_2018">(<a href="#ref-Amodei_et_al_2018" role="doc-biblioref">Amodei, Hernandez, et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>ResNet</strong>: Residual Network architecture by He et al. <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span> enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time.</p><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div></div></div><div id="fig-comp_efficiency" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-comp_efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="33aa5f6d6d13074068a62908b481e9d47cfefd81.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: AI training experienced a 300,000-fold increase in computational requirements from 2012 to 2019, exceeding the growth rate predicted by Moore’s Law and driving demand for specialized hardware [@Amodei_et_al_2018]. This exponential growth underscores the increasing complexity of AI models and the need for efficient computing infrastructure to support continued progress."><img src="efficient_ai_files/mediabag/33aa5f6d6d13074068a62908b481e9d47cfefd81.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-comp_efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: AI training experienced a 300,000-fold increase in computational requirements from 2012 to 2019, exceeding the growth rate predicted by Moore’s Law and driving demand for specialized hardware <span class="citation" data-cites="Amodei_et_al_2018">(<a href="#ref-Amodei_et_al_2018" role="doc-biblioref">Amodei, Hernandez, et al. 2018</a>)</span>. This exponential growth underscores the increasing complexity of AI models and the need for efficient computing infrastructure to support continued progress.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Amodei_et_al_2018" class="csl-entry" role="listitem">
Amodei, Dario, Danny Hernandez, et al. 2018. <span>“AI and Compute.”</span> <em>OpenAI Blog</em>. <a href="https://openai.com/research/ai-and-compute">https://openai.com/research/ai-and-compute</a>.
</div></div></figure>
</div>
<p>This rapid growth was driven not only by the adoption of GPUs, which offered unparalleled parallel processing capabilities, but also by the willingness of researchers to scale up experiments by using large GPU clusters. Graphics Processing Units (GPUs) contain thousands of small cores designed for parallel computation, ideal for the matrix multiplications central to neural networks. While a CPU might have 16 cores, modern high-end GPUs like the NVIDIA H100 contain over 16,000 CUDA cores<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>. Specialized hardware accelerators such as Google’s Tensor Processing Units (TPUs) further revolutionized compute efficiency by designing chips specifically for machine learning workloads, optimizing for the specific data types and operations most common in neural networks. These innovations enabled significant reductions in training times for deep learning models, transforming tasks that once took weeks into operations completed in hours or days.</p>
<div class="no-row-height column-margin column-container"><div id="fn30"><p><sup>30</sup>&nbsp;<strong>CUDA Cores</strong>: NVIDIA’s parallel processing units optimized for floating-point operations. Unlike CPU cores (designed for complex sequential tasks), CUDA cores are simpler and work together, enabling a single H100 GPU to perform 16,896 parallel operations simultaneously for massive speedup in matrix computations.</p></div></div><p>The rise of large-scale compute also highlighted the complementary relationship between algorithmic innovation and hardware efficiency. Advances such as neural architecture search and massive batch processing leveraged the increasing availability of computational power, demonstrating that more compute could directly lead to better performance in many domains.</p>
</section>
<section id="sec-efficient-ai-sustainable-computing-era-98a6" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-sustainable-computing-era-98a6">Sustainable Computing Era</h4>
<p>As machine learning systems scale further, compute efficiency has become closely tied to sustainability. Training state-of-the-art large language models requires massive computational resources, leading to increased attention on the environmental impact of large-scale computing. The projected electricity usage of data centers, shown in <a href="#fig-datacenter-energy-usage" class="quarto-xref">Figure&nbsp;10</a>, highlights this concern. Between 2010 and 2030, electricity consumption is expected to rise sharply, particularly under the “Worst” scenario, where it could exceed 8,000 TWh by 2030 <span class="citation" data-cites="jones2018much">(<a href="#ref-jones2018much" role="doc-biblioref"><strong>jones2018much?</strong></a>)</span>.</p>
<div id="fig-datacenter-energy-usage" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-datacenter-energy-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4c48abde203d26b69977df82d4ef3b8663e9e9d2.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Between 2010 and 2030, data center electricity usage is projected to increase sharply, particularly under worst-case scenarios where consumption could exceed 8,000 TWh by 2030 [@jones2018much]. This projection underscores the critical need for improved energy efficiency in AI systems."><img src="efficient_ai_files/mediabag/4c48abde203d26b69977df82d4ef3b8663e9e9d2.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-datacenter-energy-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Between 2010 and 2030, data center electricity usage is projected to increase sharply, particularly under worst-case scenarios where consumption could exceed 8,000 TWh by 2030 <span class="citation" data-cites="jones2018much">(<a href="#ref-jones2018much" role="doc-biblioref"><strong>jones2018much?</strong></a>)</span>. This projection underscores the critical need for improved energy efficiency in AI systems.
</figcaption>
</figure>
</div>
<p>The dramatic demand for energy usage underscores the urgency for compute efficiency, as even large data centers face energy constraints due to limitations in electrical grid capacity and power availability in specific locations. To address these challenges, the focus today is on optimizing hardware utilization and minimizing energy consumption, both in cloud data centers and at the edge.</p>
<p>One key trend is the adoption of energy-aware scheduling and resource allocation techniques, which ensure that computational workloads are distributed efficiently across available hardware <span class="citation" data-cites="Patterson_et_al_2021">(<a href="#ref-Patterson_et_al_2021" role="doc-biblioref">D. Patterson et al. 2021</a>)</span>. Researchers are also developing methods to dynamically adjust precision levels during training and inference, using lower precision operations (e.g., mixed-precision training) to reduce power consumption without sacrificing accuracy.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Patterson_et_al_2021" class="csl-entry" role="listitem">
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. <span>“Carbon Emissions and Large Neural Network Training.”</span> <em>arXiv Preprint arXiv:2104.10350</em>, April. <a href="http://arxiv.org/abs/2104.10350v3">http://arxiv.org/abs/2104.10350v3</a>.
</div><div id="fn31"><p><sup>31</sup>&nbsp;<strong>Model Parallelism</strong>: Distributing model components across multiple processors, contrasting with data parallelism. Modern transformer models like GPT-3 require model parallelism due to their 175B parameters exceeding single GPU memory (~24GB for A100 variant).</p></div><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Data Parallelism</strong>: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously to achieve massive scale.</p></div></div><p>Another focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> and data parallelism<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources.</p>
<p>At the edge, compute efficiency is evolving to address the growing demand for real-time processing in energy-constrained environments. Innovations such as hardware-aware model optimization, lightweight inference engines, and adaptive computing architectures are paving the way for highly efficient edge systems. These advancements are critical for enabling applications like autonomous vehicles and smart home devices, where latency and energy efficiency are paramount.</p>
</section>
<section id="sec-efficient-ai-mobile-edge-deployment" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-mobile-edge-deployment">Mobile and Edge AI Deployment Patterns</h4>
<p>Modern mobile AI systems achieve efficiency through heterogeneous computing coordination, distributing AI workloads across specialized processors based on power, performance, and real-time constraints. Mobile devices contain 4-6 different processor types optimized for different AI workloads: Hexagon DSP for ultra-low power inference (1-50mW for always-on applications like voice detection), Adreno GPU for parallel compute (1-5W for computer vision), Kryo CPU for control logic (100mW-2W), dedicated NPU/AI engines for optimized matrix operations (2-8W), and modem DSP for AI-enhanced signal processing. Thermal management integration requires dynamic workload migration between processors as device temperature, battery level, and user experience requirements change.</p>
<p><strong>Device Capability Adaptation</strong> across the massive diversity in mobile device capabilities requires multi-tier model deployment strategies. Flagship devices (Snapdragon 8 Gen with 4nm process, 15+ TOPS AI performance) support 100-500MB models with FP16/INT8 precision for real-time, highest accuracy applications. Mid-range devices (Snapdragon 7 Gen with 6nm process, 5-10 TOPS) require 20-100MB models with INT8/INT4 precision for near real-time, good accuracy. Entry-level devices (Snapdragon 4 Gen with 12nm process, 1-3 TOPS) need 5-20MB models with INT4/binary precision for acceptable latency, basic accuracy. Runtime capability detection dynamically selects optimal model variants based on available AI TOPS, memory bandwidth, precision support, and thermal headroom.</p>
<p>Wireless-AI integration enables adaptive workload distribution between device and cloud based on network conditions, privacy requirements, power constraints, and real-time requirements. Modern mobile AI systems implement adaptive inference routing: when network latency exceeds 100ms or battery level drops below 20%, inference routes to on-device processing; when thermal state exceeds throttle threshold, processing offloads to cloud; otherwise, hybrid inference optimizes across device and cloud resources. 5G-AI coordination enables ultra-low latency edge compute at cell towers for autonomous vehicles, massive connectivity for IoT<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> sensor networks, and network slicing for dedicated bandwidth for critical AI applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Internet of Things (IoT)</strong>: Network of physical devices embedded with sensors, software, and connectivity to collect and exchange data. IDC forecasts 41.6 billion connected IoT devices by 2025, generating 79.4 zettabytes of data annually—representing exponential growth in distributed sensing and edge computing opportunities.</p></div></div><p>Power and thermal management continuously balances AI performance against battery life impact (AI inference can consume 20-40% of device power), thermal throttling (SoC temperature affects available AI performance), and user experience (background AI must not impact foreground responsiveness). Dynamic performance scaling adjusts AI performance based on current temperature, battery level, and user activity: temperatures above 45°C trigger throttled mode, battery levels below 15% enable power saver mode, active gaming requires balanced mode, otherwise maximum performance mode. Battery life optimization strategies include workload scheduling (defer non-critical AI during low battery), model switching (use smaller models when thermal budget is limited), and sensor gating (disable expensive sensors when not needed).</p>
</section>
<section id="sec-efficient-ai-production-case-studies" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-production-case-studies">Production Deployment Case Studies</h4>
<p>Real-world efficiency optimization reveals the practical challenges and quantitative trade-offs that theory alone cannot capture. These case studies demonstrate measured performance improvements and deployment metrics across diverse production environments.</p>
<p>Mobile computer vision applications requiring real-time object detection A production mobile app for visual search optimized YOLOv5 for real-time inference on smartphones. The baseline FP32 model (28MB, 140ms latency) was systematically optimized through multiple techniques. INT8 quantization reduced model size to 7MB and latency to 85ms with 2.1% mAP loss. Structured pruning to 30% sparsity further reduced latency to 52ms with additional 1.8% mAP loss. Knowledge distillation from YOLOv5-large to YOLOv5-small achieved 38ms latency at 4.2MB model size with only 3.5% total mAP degradation. Power consumption decreased from 2.1W to 0.8W, extending battery life by 160%. The optimized model achieved 95% of original accuracy while enabling real-time inference on mid-range devices with Snapdragon 7 Gen processors.</p>
<p>Autonomous vehicle perception systems with safety-critical latency requirements An autonomous vehicle perception system required &lt;10ms latency for collision avoidance while processing multiple sensor streams (6 cameras, 3 radars, 1 lidar). The baseline transformer-based fusion model (890MB, 45ms latency) was redesigned using efficient convolution architectures. EfficientNet-based backbone reduced parameters by 75% while maintaining fusion accuracy. Custom FPGA<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> acceleration achieved 4.8ms inference latency with 12W power consumption.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;<strong>Field-Programmable Gate Array (FPGA)</strong>: Reconfigurable digital circuits that can be programmed for specific computations after manufacturing. Unlike fixed GPUs, FPGAs can be optimized for exact ML model requirements, achieving 2-10x energy efficiency for inference with deterministic latency critical for autonomous systems. Quantization to INT4 for non-critical layers and FP16 for safety-critical components maintained 99.2% of original accuracy. The system processes 15 GB/s of sensor data with deterministic timing guarantees, meeting ISO 26262 functional safety requirements while operating in -40°C to +85°C temperature ranges.</p></div></div><p>Cloud language model serving optimized for cost-effective throughput A production language model API serving 10M+ requests daily optimized GPT-style models for cost efficiency. The baseline 6B parameter model required 24GB memory and achieved 12 tokens/second throughput at $0.15 per 1K tokens. Dynamic batching increased throughput to 45 tokens/second. INT8 quantization reduced memory to 6GB with &lt;1% quality degradation. Knowledge distillation to a 1.3B parameter student model achieved 89 tokens/second at $0.04 per 1K tokens while maintaining 94% of original model quality on evaluation benchmarks. Speculative decoding with smaller draft models increased effective throughput to 156 tokens/second for common queries. Total cost reduction: 73% while serving 4× more requests with acceptable quality trade-offs.</p>
<p>Edge IoT sensor networks requiring ultra-low power AI implementations A smart agriculture deployment required plant disease detection on solar-powered sensors with 1W power budgets. The baseline MobileNet-v3 model (15MB, 340ms) was optimized for microcontroller<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> deployment. Binary neural networks reduced model size to 1.8MB with 12ms inference time using only 45mW power.</p>
<div class="no-row-height column-margin column-container"><div id="fn35"><p><sup>35</sup>&nbsp;<strong>Microcontroller</strong>: Single-chip computer containing processor, memory, and peripherals optimized for embedded applications. ARM Cortex-M series used in IoT devices typically operate at 32-480MHz with 32KB-2MB RAM, consuming milliwatts compared to GPUs’ hundreds of watts. Federated learning enabled model updates without cloud connectivity, with local model updates requiring only 15KB data transfer. Wake-on-demand processing triggered by low-power motion sensors achieved 99.7% duty cycle efficiency. The system operates 6 months on 2600mAh battery with daily image analysis, achieving 91% disease classification accuracy compared to 94% for the full-precision baseline. Network-wide deployment across 500 sensors demonstrated scalable efficiency with &lt;2% false positive rates.</p></div></div></section>
<section id="sec-efficient-ai-compute-efficiencys-role-3789" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-compute-efficiencys-role-3789">Compute Efficiency’s Role</h4>
<p>Compute efficiency is a critical enabler of system-wide performance and scalability. By optimizing hardware utilization and energy consumption, it ensures that machine learning systems remain practical and cost-effective, even as models and datasets grow larger. Compute efficiency directly complements model and data efficiency. For example, compact models reduce computational requirements, while efficient data pipelines streamline hardware usage.</p>
<p>The evolution of compute efficiency highlights its essential role in addressing the growing demands of modern machine learning systems. From early reliance on CPUs to the emergence of specialized accelerators and sustainable computing practices, this dimension remains central to building scalable, accessible, and environmentally responsible machine learning systems.</p>
</section>
</section>
<section id="sec-efficient-ai-data-efficiency-d30c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-data-efficiency-d30c">Data Efficiency</h3>
<p>Data efficiency focuses on optimizing the amount and quality of data required to train machine learning models effectively. As datasets have grown in scale and complexity, managing data efficiently has become an increasingly critical challenge for machine learning systems. While historically less emphasized than model or compute efficiency, data efficiency has emerged as a pivotal dimension, driven by the rising costs of data collection, storage, and processing. Its evolution reflects the changing role of data in machine learning, from a scarce resource to a massive but unwieldy asset.</p>
<section id="sec-efficient-ai-data-scarcity-era-85a6" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-data-scarcity-era-85a6">Data Scarcity Era</h4>
<p>In the early days of machine learning, data efficiency was not a primary focus, as datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as <a href="https://archive.ics.uci.edu/">UCI’s Machine Learning Repository</a><a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>, which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA)<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>, were common methods for ensuring that models extracted the most valuable information from limited data.</p>
<div class="no-row-height column-margin column-container"><div id="fn36"><p><sup>36</sup>&nbsp;<strong>UCI Machine Learning Repository</strong>: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers, serving as a cornerstone for early ML research.</p></div><div id="fn37"><p><sup>37</sup>&nbsp;<strong>Principal Component Analysis (PCA)</strong>: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications.</p></div><div id="fn38"><p><sup>38</sup>&nbsp;<strong>MNIST</strong>: Modified National Institute of Standards and Technology database of handwritten digits, containing 70,000 28×28 pixel images. Created in 1998, became the “Hello World” of computer vision, though modern models achieve &gt;99% accuracy.</p></div><div id="ref-deng2012mnist" class="csl-entry" role="listitem">
Deng, Li. 2012. <span>“The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web].”</span> <em>IEEE Signal Processing Magazine</em> 29 (6): 141–42. <a href="https://doi.org/10.1109/msp.2012.2211477">https://doi.org/10.1109/msp.2012.2211477</a>.
</div><div id="ref-FeiFei2004LearningGV" class="csl-entry" role="listitem">
Fei-Fei, Li, R. Fergus, and P. Perona. n.d. <span>“Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories.”</span> In <em>2004 Conference on Computer Vision and Pattern Recognition Workshop</em>. IEEE. <a href="https://doi.org/10.1109/cvpr.2004.383">https://doi.org/10.1109/cvpr.2004.383</a>.
</div><div id="fn39"><p><sup>39</sup>&nbsp;<strong>CIFAR-10</strong>: Canadian Institute for Advanced Research dataset with 60,000 32×32 color images across 10 classes. Released in 2009, remains a standard benchmark despite its small image size by modern standards.</p></div><div id="ref-Krizhevsky09learningmultiple" class="csl-entry" role="listitem">
Krizhevsky, Alex. 2009. <span>“Learning Multiple Layers of Features from Tiny Images.”</span>
</div></div><p>During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> <span class="citation" data-cites="deng2012mnist">(<a href="#ref-deng2012mnist" role="doc-biblioref">Deng 2012</a>)</span>, Caltech 101 <span class="citation" data-cites="FeiFei2004LearningGV">(<a href="#ref-FeiFei2004LearningGV" role="doc-biblioref">Fei-Fei, Fergus, and Perona, n.d.</a>)</span> and CIFAR-10<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> <span class="citation" data-cites="Krizhevsky09learningmultiple">(<a href="#ref-Krizhevsky09learningmultiple" role="doc-biblioref">Krizhevsky 2009</a>)</span>, and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process.</p>
</section>
<section id="sec-efficient-ai-big-data-era-f494" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-big-data-era-f494">Big Data Era</h4>
<p>The advent of deep learning in the 2010s transformed the role of data in machine learning. Models such as AlexNet and GPT-3 demonstrated that larger datasets often led to better performance, particularly for complex tasks like image classification and natural language processing. This marked the beginning of the “big data” era, where the focus shifted from making the most of limited data to scaling data collection and processing to unprecedented levels.</p>
<p>This reliance on large datasets introduced inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data <span class="citation" data-cites="yosinski2014transferable">(<a href="#ref-yosinski2014transferable" role="doc-biblioref">Yosinski et al. 2014</a>)</span>. Data augmentation<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a> techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Active learning<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a> prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance <span class="citation" data-cites="Settles_2009">(<a href="#ref-Settles_2009" role="doc-biblioref">Settles 2012a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn40"><p><sup>40</sup>&nbsp;<strong>Transfer Learning</strong>: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with &lt;1000 labeled examples vs.&nbsp;millions needed from scratch.</p></div><div id="ref-yosinski2014transferable" class="csl-entry" role="listitem">
Yosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. <span>“How Transferable Are Features in Deep Neural Networks?”</span> <em>Advances in Neural Information Processing Systems</em> 27.
</div><div id="fn41"><p><sup>41</sup>&nbsp;<strong>Data Augmentation</strong>: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially valuable when labeled data is scarce.</p></div><div id="fn42"><p><sup>42</sup>&nbsp;<strong>Active Learning</strong>: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling strategies.</p></div><div id="ref-Settles_2009" class="csl-entry" role="listitem">
Settles, Burr. 2012a. <em>Active Learning</em>. <em>Computer Sciences Technical Report</em>. University of Wisconsin–Madison; Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01560-1">https://doi.org/10.1007/978-3-031-01560-1</a>.
</div></div><p>Despite these advancements, the “more data is better” paradigm dominated this period, with less attention paid to streamlining data usage. As a result, the environmental and economic costs of managing large datasets began to emerge as important concerns.</p>
</section>
<section id="sec-efficient-ai-modern-data-efficiency-era-f067" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-modern-data-efficiency-era-f067">Modern Data Efficiency Era</h4>
<p>As machine learning systems grow in scale, the inefficiencies of large datasets have become increasingly apparent. Recent work has focused on developing approaches that maximize the value of data while minimizing resource requirements. This shift reflects a growing understanding that bigger datasets do not always lead to better performance, particularly when considering the computational and environmental costs of training on massive scales.</p>
<p>Data-centric AI<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a> has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This approach focuses on enhancing data preprocessing, removing redundancy, and improving labeling efficiency. Research shows that careful curation and filtering of datasets can achieve comparable or superior model performance while using only a fraction of the original data volume. For instance, systematic analyses of web-scale datasets demonstrate that targeted filtering techniques can maintain model capabilities while reducing training data requirements <span class="citation" data-cites="penedo2024fineweb">(<a href="#ref-penedo2024fineweb" role="doc-biblioref">Penedo et al. 2024</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn43"><p><sup>43</sup>&nbsp;<strong>Data-Centric AI</strong>: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.</p></div><div id="fn44"><p><sup>44</sup>&nbsp;<strong>Self-Supervised Learning</strong>: Training method where models create their own labels from input data structure, like predicting masked words in BERT or next frames in videos. Enables learning from billions of unlabeled examples, revolutionizing NLP and computer vision.</p></div><div id="fn45"><p><sup>45</sup>&nbsp;<strong>Curriculum Learning</strong>: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance across various domains.</p></div></div><p>Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a> enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a> structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance.</p>
<p>The importance of data efficiency is particularly evident in foundation models<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a>. As these models grow in scale and capability, they are approaching the limits of available high-quality training data, especially for language tasks <a href="#fig-running-out-of-human-data" class="quarto-xref">Figure&nbsp;11</a>. This scarcity drives innovation in data processing and curation techniques, pushing the field to develop more sophisticated approaches to data efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="fn46"><p><sup>46</sup>&nbsp;<strong>Foundation Models</strong>: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E with billions of parameters.</p></div></div><div id="fig-running-out-of-human-data" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-running-out-of-human-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/running_out_of_data.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Dataset Growth: Foundation models are increasingly trained on vast datasets, reflecting the growing stock of human-generated text via The visual. This trend underscores the challenge of data scarcity in maintaining model performance as scale increases. Source: @villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024."><img src="images/png/running_out_of_data.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-running-out-of-human-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Dataset Growth</strong>: Foundation models are increasingly trained on vast datasets, reflecting the growing stock of human-generated text via The visual. This trend underscores the challenge of data scarcity in maintaining model performance as scale increases. Source: <span class="citation" data-cites="villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024">Sevilla et al. (<a href="#ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024" role="doc-biblioref">2022b</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024" class="csl-entry" role="listitem">
———. 2022b. <span>“Compute Trends Across Three Eras of Machine Learning.”</span> In <em>2022 International Joint Conference on Neural Networks (IJCNN)</em>, 1–8. IEEE. <a href="https://doi.org/10.1109/ijcnn55064.2022.9891914">https://doi.org/10.1109/ijcnn55064.2022.9891914</a>.
</div></div></figure>
</div>
<p>Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a> applications, datasets like Wake Vision demonstrate how model performance critically depends on careful data curation <span class="citation" data-cites="banbury2024wakevisiontailoreddataset">(<a href="#ref-banbury2024wakevisiontailoreddataset" role="doc-biblioref">Banbury et al. 2024</a>)</span>. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies can significantly improve performance on downstream tasks <span class="citation" data-cites="penedo2024fineweb">(<a href="#ref-penedo2024fineweb" role="doc-biblioref">Penedo et al. 2024</a>)</span>. <strong><a href="../core/benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 7: Benchmarking AI</a></strong> establishes rigorous methodologies for measuring these data quality improvements across different scales and applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn47"><p><sup>47</sup>&nbsp;<strong>TinyML</strong>: Machine learning on microcontrollers and edge devices with &lt;1KB-1MB memory and &lt;1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible due to resource constraints. These ultra-low-power chips contain a processor, memory, and peripherals on a single chip with dramatically limited resources.</p></div><div id="ref-banbury2024wakevisiontailoreddataset" class="csl-entry" role="listitem">
Banbury, Colby, Emil Njor, Andrea Mattia Garavagno, Mark Mazumder, Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries, Xenofon Fafoutis, and Vijay Janapa Reddi. 2024. <span>“Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications,”</span> May. <a href="http://arxiv.org/abs/2405.00892v5">http://arxiv.org/abs/2405.00892v5</a>.
</div><div id="ref-penedo2024fineweb" class="csl-entry" role="listitem">
Penedo, Guilherme, Hynek Kydlı́ček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. <span>“The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale.”</span> <em>arXiv Preprint arXiv:2406.17557</em>, June. <a href="http://arxiv.org/abs/2406.17557v2">http://arxiv.org/abs/2406.17557v2</a>.
</div></div><p>This modern era of data efficiency represents a shift in how machine learning systems approach data utilization. By focusing on quality over quantity and developing sophisticated techniques for data selection and processing, the field is moving toward more sustainable and effective approaches to model training and deployment.</p>
</section>
<section id="sec-efficient-ai-data-efficiencys-role-c216" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-data-efficiencys-role-c216">Data Efficiency’s Role</h4>
<p>Data efficiency is integral to the design of scalable and sustainable machine learning systems. By reducing the dependency on large datasets, data efficiency directly impacts both model and compute efficiency. For instance, smaller, higher-quality datasets reduce training times and computational demands, while enabling models to generalize more effectively. This dimension of efficiency is particularly critical for edge applications, where bandwidth and storage limitations make it impractical to rely on large datasets.</p>
<p>As the field advances, data efficiency will play an important role in addressing the challenges of scalability, accessibility, and sustainability. By rethinking how data is collected, processed, and utilized, machine learning systems can achieve higher levels of efficiency across the entire pipeline. These data efficiency principles complement the privacy-preserving techniques explored in <strong><a href="../core/privacy_security/privacy_security.html#sec-security-privacy">Chapter 15: Security & Privacy</a></strong>, where minimizing data requirements can enhance both efficiency and user privacy protection.</p>
<div id="quiz-question-sec-efficient-ai-pillars-ai-efficiency-c024" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the role of algorithmic efficiency in AI systems?</p>
<ol type="a">
<li>Maximizing performance by increasing computational resources.</li>
<li>Developing new hardware to support AI computations.</li>
<li>Collecting and storing large amounts of data for training.</li>
<li>Optimizing algorithms to perform well within given resource constraints.</li>
</ol></li>
<li><p>True or False: Compute efficiency primarily focuses on reducing the energy consumption and optimizing the use of hardware resources in AI systems.</p></li>
<li><p>Explain how data efficiency can impact the scalability of machine learning systems.</p></li>
<li><p>Order the following pillars of AI efficiency based on their primary focus: (1) Algorithmic Efficiency, (2) Compute Efficiency, (3) Data Efficiency.</p></li>
<li><p>In a production system with limited computational resources, what trade-offs might you consider when applying the three pillars of AI efficiency?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-pillars-ai-efficiency-c024" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-efficient-ai-system-efficiency-3cf1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-system-efficiency-3cf1">System Efficiency</h2>
<p>The efficiency of machine learning systems has become an important area of focus. Optimizing these systems ensures that they are not only high-performing but also adaptable, cost-effective, and environmentally sustainable. Understanding the concept of ML system efficiency, its key dimensions, and the interplay between them is necessary for uncovering how these principles can drive impactful, scalable, and responsible AI solutions.</p>
<section id="sec-efficient-ai-defining-system-efficiency-8e59" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-defining-system-efficiency-8e59">Defining System Efficiency</h3>
<p>Machine learning is a highly complex field, involving a multitude of components across a vast domain. Despite its complexity, there has not been a synthesis of what it truly means to have an efficient machine learning system. Here, we take a first step towards defining this concept.</p>
<div id="callout-definition*-1.2" class="callout callout-definition" title="Definition of Machine Learning System Efficiency">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Definition of Machine Learning System Efficiency</summary><div><strong>Machine Learning System Efficiency</strong> refers to the optimization of machine learning systems across three interconnected dimensions: <em>algorithmic efficiency</em>, <em>compute efficiency</em>, and <em>data efficiency</em>. Its goal is to minimize <em>computational, memory, and energy</em> demands while maintaining or improving system performance. This efficiency ensures that machine learning systems are <em>scalable, cost-effective, and sustainable</em>, which allows them to adapt to diverse deployment contexts, ranging from <em>cloud data centers</em> to <em>edge devices</em>. Achieving system efficiency, however, often requires navigating <em>trade-offs</em> between dimensions, such as balancing <em>model complexity</em> with <em>hardware constraints</em> or reducing <em>data dependency</em> without compromising <em>generalization</em>.<p></p>
</div></details>
</div>
<p>This definition highlights the holistic nature of efficiency in machine learning systems. Understanding how optimizations in one dimension affect others is necessary for designing systems that are not only performant but also scalable, adaptable, and sustainable <span class="citation" data-cites="patterson2021carbon">(<a href="#ref-patterson2021carbon" role="doc-biblioref">D. A. Patterson and Hennessy 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-patterson2021carbon" class="csl-entry" role="listitem">
Patterson, David A, and John L Hennessy. 2021. <span>“Carbon Emissions and Large Neural Network Optimization.”</span> <em>Communications of the ACM</em> 64 (7): 54–61.
</div></div><p>To better understand this interplay, we must examine how these dimensions reinforce one another and the challenges in balancing them. While each dimension contributes uniquely, the true complexity lies in their interdependencies. Historically, optimizations were often approached in isolation. However, recent years have seen a shift towards co-design, where multiple dimensions are optimized concurrently to achieve superior overall efficiency.</p>
</section>
<section id="sec-efficient-ai-efficiency-interdependencies-bff1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-interdependencies-bff1">Efficiency Interdependencies</h3>
<p>The efficiency of machine learning systems is inherently a multifaceted challenge that encompasses model design, computational resources, and data utilization. Understanding how these dimensions interact is important for building scalable, cost-effective, and high-performing systems that can adapt to diverse application demands.</p>
<p>This interplay is best captured through a conceptual visualization. <a href="#fig-interdependece" class="quarto-xref">Figure&nbsp;12</a> illustrates how these efficiency dimensions overlap and interact with each other in a simple Venn diagram. Each circle represents one of the efficiency dimensions, and their intersections highlight the areas where they influence one another, which we will explore next.</p>
<div id="fig-interdependece" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interdependece-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0a1d2fa659071c598c859bdc07adfe7d1f646873.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: The three efficiency dimensions (algorithmic, compute, and data) overlap and influence one another, creating systemic trade-offs in machine learning systems. Optimizing for one efficiency dimension often requires careful consideration of its impact on the others, shaping overall system performance and resource utilization."><img src="efficient_ai_files/mediabag/0a1d2fa659071c598c859bdc07adfe7d1f646873.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interdependece-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: The three efficiency dimensions (algorithmic, compute, and data) overlap and influence one another, creating systemic trade-offs in machine learning systems. Optimizing for one efficiency dimension often requires careful consideration of its impact on the others, shaping overall system performance and resource utilization.
</figcaption>
</figure>
</div>
<section id="sec-efficient-ai-algorithmic-efficiency-aids-compute-data-a77f" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-algorithmic-efficiency-aids-compute-data-a77f">Algorithmic Efficiency Aids Compute and Data</h4>
<p>Model efficiency is essential for efficient machine learning systems. By designing compact and streamlined models, we can significantly reduce computational demands, leading to faster and more cost-effective inference. These compact models not only consume fewer resources but are also easier to deploy across diverse environments, such as resource-constrained edge devices or energy-intensive cloud infrastructure.</p>
<p>Efficient models often require less data for training, as they avoid over-parameterization and focus on capturing essential patterns within the data. This results in shorter training times and reduced dependency on massive datasets, which can be expensive and time-consuming to curate. Optimizing algorithmic efficiency creates a ripple effect, enhancing both compute and data efficiency.</p>
<section id="sec-efficient-ai-mobile-deployment-example-c563" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-efficient-ai-mobile-deployment-example-c563">Mobile Deployment Example</h5>
<p>Mobile devices, such as smartphones, provide an accessible introduction to the interplay of efficiency dimensions. Consider a photo-editing application that uses machine learning to apply real-time filters. Compute efficiency is achieved through hardware accelerators like mobile GPUs or Neural Processing Units (NPUs)<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a>, ensuring tasks are performed quickly while minimizing battery usage.</p>
<div class="no-row-height column-margin column-container"><div id="fn48"><p><sup>48</sup>&nbsp;<strong>Neural Processing Units (NPUs)</strong>: Specialized processors designed for AI workloads on mobile devices. Apple’s A17 Pro contains a 16-core NPU delivering 35 TOPS (trillion operations per second) while consuming just 2-3 watts, enabling on-device AI without draining battery life.</p></div></div><p>This compute efficiency, in turn, is supported by algorithmic efficiency. The application relies on a lightweight neural network architecture, such as MobileNets, that reduces the computational load, allowing it to take full advantage of the mobile device’s hardware. Streamlined models also help reduce memory consumption, further enhancing computational performance and enabling real-time responsiveness.</p>
<p>Data efficiency strengthens both compute and algorithmic efficiency by ensuring the model is trained on carefully curated and augmented datasets. These datasets allow the model to generalize effectively, reducing the need for extensive retraining and lowering the demand for computational resources during training. By minimizing the complexity of the training data, the model can remain lightweight without sacrificing accuracy, reinforcing both model and compute efficiency.</p>
<p>Integrating these dimensions means mobile deployments achieve a seamless balance between performance, energy efficiency, and practicality. The interdependence of model, compute, and data efficiencies ensures that even resource-constrained devices can deliver advanced AI capabilities to users on the go.</p>
</section>
</section>
<section id="sec-efficient-ai-compute-efficiency-supports-model-data-3501" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-compute-efficiency-supports-model-data-3501">Compute Efficiency Supports Model and Data</h4>
<p>Compute efficiency is a key factor in optimizing machine learning systems. By maximizing hardware utilization and employing efficient algorithms, compute efficiency speeds up both model training and inference processes, ultimately cutting down on the time and resources needed, even when working with complex or large-scale models.</p>
<p>Efficient computation enables models to handle large datasets more effectively, minimizing bottlenecks associated with memory or processing power. Techniques such as parallel processing, hardware accelerators (e.g., GPUs, TPUs), and energy-aware scheduling contribute to reducing overhead while ensuring peak performance. As a result, compute efficiency not only supports model optimization but also enhances data handling, making it feasible to train models on high-quality datasets without unnecessary computational strain.</p>
<section id="sec-efficient-ai-edge-deployment-example-4e52" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-efficient-ai-edge-deployment-example-4e52">Edge Deployment Example</h5>
<p>Edge deployments, such as those in autonomous vehicles, highlight the intricate balance required between real-time constraints and energy efficiency. Compute efficiency is central, as vehicles rely on high-performance onboard hardware to process massive streams of sensor data, including data from cameras, LiDAR<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a>, and radar, in real time. These computations must be performed with minimal latency to ensure safe navigation and split-second decision-making.</p>
<div class="no-row-height column-margin column-container"><div id="fn49"><p><sup>49</sup>&nbsp;<strong>LiDAR</strong>: Light Detection and Ranging technology that uses laser pulses to create detailed 3D maps of surroundings. Autonomous vehicles generate ~70GB of LiDAR data daily, requiring real-time processing of millions of distance measurements per second for obstacle detection and navigation.</p></div></div><p>This compute efficiency is closely supported by algorithmic efficiency, as the system depends on compact, high-accuracy models designed for low latency. By employing streamlined neural network architectures or hybrid models combining deep learning and traditional algorithms, the computational demands on hardware are reduced. These optimized models not only lower the processing load but also consume less energy, reinforcing the system’s overall energy efficiency.</p>
<p>Data efficiency enhances both compute and algorithmic efficiency by reducing the dependency on vast amounts of training data. Through synthetic and augmented datasets, the model can generalize effectively across diverse scenarios, including varying lighting, weather, and traffic conditions, without requiring extensive retraining. This targeted approach minimizes computational costs during training and allows the model to remain efficient while adapting to a wide range of real-world environments.</p>
<p>Together, the interdependence of these efficiencies ensures that autonomous vehicles can operate safely and reliably while minimizing energy consumption. This balance not only improves real-time performance but also contributes to broader goals, such as reducing fuel consumption and enhancing environmental sustainability.</p>
</section>
</section>
<section id="sec-efficient-ai-data-efficiency-strengthens-model-compute-f801" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-data-efficiency-strengthens-model-compute-f801">Data Efficiency Strengthens Model and Compute</h4>
<p>Data efficiency is fundamental to bolstering both model and compute efficiency. By focusing on high-quality, compact datasets, the training process becomes more streamlined, requiring fewer computational resources to achieve comparable or superior model performance. This targeted approach reduces data redundancy and minimizes the overhead associated with handling excessively large datasets.</p>
<p>Data efficiency enables more focused model design. When datasets emphasize relevant features and minimize noise, models can achieve high performance with simpler architectures. This reduces computational requirements during both training and inference, allowing more efficient use of computing resources.</p>
<section id="sec-efficient-ai-cloud-deployment-example-f596" class="level5">
<h5 class="anchored" data-anchor-id="sec-efficient-ai-cloud-deployment-example-f596">Cloud Deployment Example</h5>
<p>Cloud deployments exemplify how system efficiency can be achieved across interconnected dimensions. Consider a recommendation system operating in a data center, where high throughput and rapid inference are critical. Compute efficiency is achieved by leveraging parallelized processing on GPUs or TPUs, which optimize the computational workload to ensure timely and resource-efficient performance. This high-performance hardware allows the system to handle millions of simultaneous queries while keeping energy and operational costs in check.</p>
<p>This compute efficiency is bolstered by algorithmic efficiency, as the recommendation system employs streamlined architectures, such as pruned or simplified models. By reducing the computational and memory footprint, these models enable the system to scale efficiently, processing large volumes of data without overwhelming the infrastructure. The streamlined design also reduces the burden on accelerators, improving energy usage and maintaining throughput.</p>
<p>Data efficiency strengthens both compute and algorithmic efficiency by enabling the system to learn and adapt without excessive data overhead. By focusing on actively labeled datasets, the system can prioritize high-value training data, ensuring better model performance with fewer computational resources. This targeted approach reduces the size and complexity of training tasks, freeing up resources for inference and scaling while maintaining high recommendation accuracy.</p>
<p>Together, the interdependence of these efficiencies enables cloud-based systems to achieve a balance of performance, scalability, and cost-effectiveness. By optimizing model, compute, and data dimensions in harmony, cloud deployments become a cornerstone of modern AI applications, supporting millions of users with efficiency and reliability.</p>
</section>
</section>
<section id="sec-efficient-ai-progression-takeaways-e267" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-progression-takeaways-e267">Progression and Takeaways</h4>
<p>Starting with Mobile ML deployments and progressing to Edge ML, Cloud ML, and Tiny ML, these examples illustrate how system efficiency adapts to diverse operational contexts. Mobile ML emphasizes battery life and hardware limitations, edge systems balance real-time demands with energy efficiency, cloud systems prioritize scalability and throughput, and Tiny ML demonstrates how AI can thrive in environments with severe resource constraints.</p>
<p>Despite these differences, the fundamental principles remain consistent: achieving system efficiency requires optimizing model, compute, and data dimensions. These dimensions are deeply interconnected, with improvements in one often reinforcing the others. For instance, lightweight models enhance computational performance and reduce data requirements, while efficient hardware accelerates model training and inference. Similarly, focused datasets streamline model training and reduce computational overhead.</p>
<p>By understanding the interplay between these dimensions, we can design machine learning systems that meet specific deployment requirements while maintaining flexibility across contexts. For instance, a model architected for edge deployment can often be adapted for cloud scaling or simplified for mobile use, provided we carefully consider the relationships between model architecture, computational resources, and data requirements.</p>
</section>
</section>
<section id="sec-efficient-ai-scalability-sustainability-0d26" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-scalability-sustainability-0d26">Scalability and Sustainability</h3>
<p>System efficiency serves as a fundamental driver of environmental sustainability in machine learning systems. When systems are optimized for efficiency, they can be deployed at scale while minimizing their environmental footprint. This relationship creates a positive feedback loop, as sustainable design practices naturally encourage further efficiency improvements.</p>
<p>The interconnection between efficiency, scalability, and sustainability forms a virtuous cycle, as shown in <a href="#fig-virtuous-efficiency-cycle" class="quarto-xref">Figure&nbsp;13</a>, that enhances the broader impact of machine learning systems. Efficient system design enables widespread deployment, which amplifies the positive environmental effects of sustainable practices. As organizations prioritize sustainability, they drive innovation in efficient system design, ensuring that advances in artificial intelligence align with global sustainability goals.</p>
<div id="fig-virtuous-efficiency-cycle" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-virtuous-efficiency-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="8ff5d037eed695d0dcda2db19bf7a200a9084b98.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Optimized machine learning systems achieve greater scalability, which in turn incentivizes sustainable design practices and further efficiency improvements, creating a reinforcing feedback loop for long-term impact. This cyclical relationship enables widespread deployment of AI solutions while minimizing environmental costs and fostering innovation in resource-conscious system design."><img src="efficient_ai_files/mediabag/8ff5d037eed695d0dcda2db19bf7a200a9084b98.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-virtuous-efficiency-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Optimized machine learning systems achieve greater scalability, which in turn incentivizes sustainable design practices and further efficiency improvements, creating a reinforcing feedback loop for long-term impact. This cyclical relationship enables widespread deployment of AI solutions while minimizing environmental costs and fostering innovation in resource-conscious system design.
</figcaption>
</figure>
</div>
<section id="sec-efficient-ai-efficiencyscalability-relationship-3540" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiencyscalability-relationship-3540">Efficiency-Scalability Relationship</h4>
<p>Efficient systems are inherently scalable. Reducing resource demands through lightweight models, targeted datasets, and optimized compute utilization allows systems to deploy broadly across diverse environments. For example, a speech recognition model that is efficient enough to run on mobile devices can serve millions of users globally without relying on costly infrastructure upgrades. Similarly, Tiny ML technologies, designed to operate on low-power hardware, make it possible to deploy thousands of devices in remote areas for applications like environmental monitoring or precision agriculture.</p>
<p>Scalability becomes feasible because efficiency reduces barriers to entry. Systems that are compact and energy-efficient require less infrastructure, making them more adaptable to different deployment contexts, from cloud data centers to edge and IoT devices. This adaptability is key to ensuring that advanced AI solutions reach users worldwide, fostering inclusion and innovation.</p>
</section>
<section id="sec-efficient-ai-scalabilitysustainability-relationship-a3ee" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-scalabilitysustainability-relationship-a3ee">Scalability-Sustainability Relationship</h4>
<p>When efficient systems scale, they amplify their contribution to sustainability. Energy-efficient designs deployed at scale reduce overall energy consumption and computational waste, mitigating the environmental impact of machine learning systems. For instance, deploying Tiny ML devices for on-device data processing avoids the energy costs of transmitting raw data to the cloud, while efficient recommendation engines in the cloud reduce the operational footprint of serving millions of users.</p>
<p>The wide-scale adoption of efficient systems not only reduces environmental costs but also fosters sustainable development in underserved regions. Efficient AI applications in healthcare, education, and agriculture can provide transformative benefits without imposing significant resource demands, aligning technological growth with ethical and environmental goals.</p>
</section>
<section id="sec-efficient-ai-sustainabilityefficiency-relationship-8bbb" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-sustainabilityefficiency-relationship-8bbb">Sustainability-Efficiency Relationship</h4>
<p>Sustainability itself reinforces the need for efficiency, creating a feedback loop that strengthens the entire system. Practices like minimizing data redundancy, designing energy-efficient hardware, and developing low-power models all emphasize efficient resource utilization. These efforts not only reduce the environmental footprint of AI systems but also set the stage for further scalability by making systems cost-effective and accessible.</p>
</section>
</section>
<section id="sec-efficient-ai-concrete-example-5f2a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-concrete-example-5f2a">A Concrete Example: Photo Search Application</h3>
<p>Consider a photo search application that helps users find specific images in their personal collections using natural language queries like “photos of my dog at the beach.” This example illustrates how the three efficiency dimensions work together in practice.</p>
<p><strong>Algorithmic Efficiency</strong> focuses on the model architecture. The system uses a compact vision-language model with 50 million parameters instead of a billion-parameter model, reducing memory requirements from 4GB to 200MB. This smaller model runs inference in 100 milliseconds instead of 2 seconds, enabling near-instantaneous search results. While the smaller model achieves 85% accuracy compared to 92% for the larger model, this trade-off is acceptable for the interactive user experience.</p>
<p><strong>Compute Efficiency</strong> addresses how the model runs on user devices. Instead of requiring expensive GPUs, the optimized model runs efficiently on smartphone processors, consuming only 10% battery per hour of active use. The system uses techniques like 8-bit quantization to reduce computation while maintaining quality, and employs batch processing<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a> to handle multiple image queries simultaneously when users search large albums.</p>
<div class="no-row-height column-margin column-container"><div id="fn50"><p><sup>50</sup>&nbsp;<strong>Batch Processing</strong>: Technique that groups multiple inputs together for simultaneous processing, improving computational efficiency. Instead of processing 100 images one-by-one (100 separate operations), batching processes them together in groups of 32, reducing overhead and improving GPU utilization by ~3-5×.</p></div></div><p><strong>Data Efficiency</strong> shapes how the model learns and adapts. Rather than requiring millions of labeled image-text pairs, the system leverages a foundation model pre-trained on diverse visual data, then adapts it using only thousands of user-specific examples. The system also learns continuously from user interactions—when users click on search results, this implicit feedback helps improve future searches without requiring explicit labeling.</p>
<p>The synergy between these dimensions creates emergent benefits: the smaller model (algorithmic efficiency) enables on-device processing (compute efficiency) which allows the system to learn from private user data (data efficiency) without sending personal photos to remote servers. This integration delivers both better performance and privacy protection—demonstrating how efficiency enables capabilities that would be impossible with less efficient approaches.</p>
<div id="quiz-question-sec-efficient-ai-system-efficiency-3cf1" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following best defines ‘Machine Learning System Efficiency’?</p>
<ol type="a">
<li>Maximizing algorithmic complexity to improve model accuracy.</li>
<li>Ensuring models are only deployed on high-performance hardware.</li>
<li>Focusing solely on reducing data requirements for training models.</li>
<li>Optimizing systems to minimize computational, memory, and energy demands while maintaining performance.</li>
</ol></li>
<li><p>Explain how algorithmic efficiency can enhance both compute and data efficiency in a machine learning system.</p></li>
<li><p>What is a potential trade-off when optimizing for compute efficiency in ML systems?</p>
<ol type="a">
<li>Increased model complexity and reduced scalability.</li>
<li>Decreased energy consumption but increased data redundancy.</li>
<li>Improved hardware utilization but potential compromise on model accuracy.</li>
<li>Enhanced data efficiency but increased computational overhead.</li>
</ol></li>
<li><p>In a production system, how might you apply the concept of system efficiency to improve sustainability?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-system-efficiency-3cf1" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-tradeoffs-challenges-946d">Efficiency Trade-offs and Challenges</h2>
<p>Thus far, we explored the individual dimensions of system efficiency and their potential for synergy. However, achieving this harmony in practice is far from straightforward.</p>
<p>In practice, balancing these dimensions often reveals underlying tensions where improvements in one area can impose constraints on others. This is especially pronounced in resource-constrained environments where efficiency becomes a prerequisite for system feasibility rather than merely an optimization goal.</p>
<section id="sec-efficient-ai-tradeoffs-source-5907" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-tradeoffs-source-5907">Trade-offs Source</h3>
<p>These tensions manifest in various ways across machine learning systems. For instance, simplifying a model to reduce computational demands might result in reduced accuracy, while optimizing compute efficiency for real-time responsiveness can conflict with energy efficiency goals. These trade-offs are not limitations but reflections of the intricate design decisions required to build adaptable and efficient systems.</p>
<p>Understanding the root of these trade-offs is essential for navigating the challenges of system design. Each efficiency dimension influences the others, creating a dynamic interplay that shapes system performance. The following sections delve into these interdependencies, beginning with the relationship between algorithmic efficiency and compute requirements.</p>
<section id="sec-efficient-ai-efficiency-compute-requirements-a1a1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-compute-requirements-a1a1">Efficiency and Compute Requirements</h4>
<p>Model efficiency focuses on designing compact and streamlined models that minimize computational and memory demands. By reducing the size or complexity of a model, it becomes easier to deploy on devices with limited resources, such as mobile phones or IoT<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a> sensors.</p>
<div class="no-row-height column-margin column-container"><div id="fn51"><p><sup>51</sup>&nbsp;<strong>IoT (Internet of Things)</strong>: Network of interconnected physical devices with embedded sensors, processors, and communication capabilities. IoT devices typically operate with severe resource constraints: 1KB-1MB memory, &lt;1mW power consumption, and intermittent connectivity, requiring specialized ultra-efficient AI approaches.</p></div></div><p>However, overly simplifying a model can reduce its accuracy, especially for complex tasks. To make up for this loss, additional computational resources may be required during training to fine-tune the model or during deployment to apply more sophisticated inference algorithms. Thus, while algorithmic efficiency can reduce computational costs, achieving this often places additional strain on compute efficiency.</p>
</section>
<section id="sec-efficient-ai-efficiency-realtime-needs-bc6a" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-realtime-needs-bc6a">Efficiency and Real-Time Needs</h4>
<p>Compute efficiency aims to minimize the resources required for tasks like training and inference, reducing energy consumption, processing time, and memory use. In many applications, particularly in cloud computing or data centers, this optimization works seamlessly with algorithmic efficiency to improve system performance.</p>
<p>However, in scenarios that require real-time responsiveness, including autonomous vehicles and augmented reality, compute efficiency is harder to maintain. <a href="#fig-efficiency-vs-latency" class="quarto-xref">Figure&nbsp;14</a> illustrates this challenge: real-time systems often require high-performance hardware to process large amounts of data instantly, which can conflict with energy efficiency goals or increase system costs. Balancing compute efficiency with stringent real-time application needs becomes a key challenge in such applications.</p>
<div id="fig-efficiency-vs-latency" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-efficiency-vs-latency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="688b312368a147e16b253e2ef475a08f88248e1f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Real-time system constraints in autonomous vehicles demand careful balance between computational efficiency and low latency. Increasing processing power to reduce delay can conflict with energy and cost limitations, yet sacrificing latency compromises safety by increasing reaction time and braking distance."><img src="efficient_ai_files/mediabag/688b312368a147e16b253e2ef475a08f88248e1f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-efficiency-vs-latency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Real-time system constraints in autonomous vehicles demand careful balance between computational efficiency and low latency. Increasing processing power to reduce delay can conflict with energy and cost limitations, yet sacrificing latency compromises safety by increasing reaction time and braking distance.
</figcaption>
</figure>
</div>
</section>
<section id="sec-efficient-ai-efficiency-model-generalization-2d9c" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-model-generalization-2d9c">Efficiency and Model Generalization</h4>
<p>Data efficiency seeks to minimize the amount of data required to train a model without sacrificing performance. By curating smaller, high-quality datasets, the training process becomes faster and less resource-intensive. Ideally, this reinforces both model and compute efficiency, as smaller datasets reduce the computational load and support more compact models.</p>
<p>However, reducing the size of a dataset can also limit its diversity, making it harder for the model to generalize to unseen scenarios. To address this, additional compute resources or model complexity may be required, creating a tension between data efficiency and the broader goals of system efficiency.</p>
</section>
</section>
<section id="sec-efficient-ai-common-tradeoffs-bde0" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-common-tradeoffs-bde0">Common Trade-offs</h3>
<p>The trade-offs between efficiency dimensions become particularly evident when examining specific scenarios. Decisions made in one area often have cascading effects on the rest of the system. For instance, choosing a larger, more complex model may improve accuracy, but it also increases computational demands and the size of the training dataset required. Similarly, reducing energy consumption may limit the ability to meet real-time performance requirements, particularly in latency-sensitive applications.</p>
<p>We explore three of the most common trade-offs encountered in machine learning system design:</p>
<ol type="1">
<li><p><strong>Model complexity vs.&nbsp;compute resources</strong>,</p></li>
<li><p><strong>Energy efficiency vs.&nbsp;real-time performance</strong>, and</p></li>
<li><p><strong>Data size vs.&nbsp;model generalization</strong>.</p></li>
</ol>
<p>Each of these trade-offs illustrates the nuanced decisions that system designers must make and the challenges involved in achieving efficient, high-performing systems.</p>
<p>These trade-offs become even more critical in resource-constrained environments. Consider a Tiny ML deployment for IoT-based environmental monitoring in remote agricultural fields, where the device must detect temperature anomalies while operating on a small battery for months without maintenance. Such systems face extreme constraints: the microcontroller has minimal processing power, tiny memory capacity, and no access to large training datasets. Success requires a holistic approach where a computationally lightweight model that fits in limited memory must still achieve reliable predictions from small, carefully selected datasets. A model that demands too much energy will drain the battery too quickly, while one requiring excessive training data becomes impractical for remote deployment.</p>
<section id="sec-efficient-ai-complexity-vs-resources-a86d" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-complexity-vs-resources-a86d">Complexity vs.&nbsp;Resources</h4>
<p>The relationship between model complexity and compute resources is one of the most fundamental trade-offs in machine learning system design. Complex models, such as deep neural networks with millions or even billions of parameters, are often capable of achieving higher accuracy by capturing intricate patterns in data. However, this complexity comes at a cost. These models require significant computational power and memory to train and deploy, often making them impractical for environments with limited resources.</p>
<p>For example, consider a recommendation system deployed in a cloud data center. A highly complex model may deliver better recommendations, but it increases the computational demands on servers, leading to higher energy consumption and operating costs. On the other hand, a simplified model may reduce these demands but might compromise the quality of recommendations, especially when handling diverse or unpredictable user behavior.</p>
<p>The trade-off becomes even more pronounced in resource-constrained environments such as mobile or edge devices. A compact, streamlined model designed for a smartphone or an autonomous vehicle may operate efficiently within the device’s hardware limits but might require more sophisticated data preprocessing or training procedures to compensate for its reduced capacity. This balancing act highlights the interconnected nature of efficiency dimensions, where gains in one area often demand sacrifices in another.</p>
</section>
<section id="sec-efficient-ai-energy-vs-performance-087b" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-energy-vs-performance-087b">Energy vs.&nbsp;Performance</h4>
<p>Energy efficiency and real-time performance often pull machine learning systems in opposite directions, particularly in applications requiring low-latency responses. Real-time systems, such as those in autonomous vehicles or augmented reality applications, rely on high-performance hardware to process large volumes of data quickly. This ensures responsiveness and safety in scenarios where even small delays can lead to significant consequences. However, achieving such performance typically increases energy consumption, creating tension with the goal of minimizing resource use.</p>
<p>For instance, an autonomous vehicle must process sensor data from cameras, LiDAR, and radar in real time to make navigation decisions. The computational demands of these tasks often require specialized accelerators, such as GPUs, which can consume significant energy. While optimizing hardware utilization and model architecture can improve energy efficiency to some extent, the demands of real-time responsiveness make it challenging to achieve both goals simultaneously.</p>
<p>In edge deployments, where devices rely on battery power or limited energy sources, this trade-off becomes even more critical. Striking a balance between energy efficiency and real-time performance often involves prioritizing one over the other, depending on the application’s requirements. This trade-off underscores the importance of context-specific design, where the constraints and priorities of the deployment environment dictate the balance between competing objectives.</p>
</section>
<section id="sec-efficient-ai-data-size-vs-generalization-ae8b" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-data-size-vs-generalization-ae8b">Data Size vs.&nbsp;Generalization</h4>
<p>The size and quality of the dataset used to train a machine learning model play a role in its ability to generalize to new, unseen data. Larger datasets generally provide greater diversity and coverage, enabling models to capture subtle patterns and reduce the risk of overfitting. However, the computational and memory demands of training on large datasets can be substantial, leading to trade-offs between data efficiency and computational requirements.</p>
<p>In resource-constrained environments such as Tiny ML deployments, the challenge of dataset size is particularly evident. For example, an IoT device monitoring environmental conditions might need a model that generalizes well to varying temperatures, humidity levels, or geographic regions. Collecting and processing extensive datasets to capture these variations may be impractical due to storage, computational, and energy limitations. In such cases, smaller, carefully curated datasets or synthetic data generated to mimic real-world conditions are used to reduce computational strain. However, this reduction often risks missing key edge cases, which could degrade the model’s performance in diverse environments.</p>
<p>Conversely, in cloud-based systems, where compute resources are more abundant, training on massive datasets can still pose challenges. Managing data redundancy, ensuring high-quality labeling, and handling the time and cost associated with large-scale data pipelines often require significant computational infrastructure. This trade-off highlights how the need to balance dataset size and model generalization depends heavily on the deployment context and available resources.</p>
<div id="quiz-question-sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the trade-off between model complexity and compute resources in ML systems?</p>
<ol type="a">
<li>Complex models require less computational power and memory.</li>
<li>Simplified models always outperform complex models in accuracy.</li>
<li>Complex models can achieve higher accuracy but demand more computational resources.</li>
<li>Model complexity has no impact on computational resources.</li>
</ol></li>
<li><p>Explain how energy efficiency and real-time performance create trade-offs in autonomous vehicle systems.</p></li>
<li><p>In Tiny ML deployments, a balance must be struck between model complexity and ________ to ensure efficient operation.</p></li>
<li><p>Order the following efficiency trade-offs based on their impact on system design: (1) Model complexity vs.&nbsp;compute resources, (2) Energy efficiency vs.&nbsp;real-time performance, (3) Data size vs.&nbsp;model generalization.</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-efficient-ai-managing-tradeoffs-80e8" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-managing-tradeoffs-80e8">Managing Trade-offs</h2>
<p>The trade-offs inherent in machine learning system design require thoughtful strategies to navigate effectively. Achieving the right balance often involves difficult decisions heavily influenced by the specific goals and constraints of the deployment environment. For example, a system designed for cloud deployment may prioritize scalability and throughput, while a Tiny ML system must focus on extreme resource efficiency.</p>
<p>To manage these challenges, designers can adopt a range of strategies that address the unique requirements of different contexts. By prioritizing efficiency dimensions based on the application, collaborating across system components, and leveraging automated optimization tools, it is possible to create systems that balance performance, cost, and resource use. This section explores these approaches and provides guidance for designing systems that are both efficient and adaptable.</p>
<section id="sec-efficient-ai-contextual-prioritization-bff3" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-contextual-prioritization-bff3">Contextual Prioritization</h3>
<p>Efficiency goals are rarely universal. The specific demands of an application or deployment scenario heavily influence which dimension of efficiency, whether it be model, compute, or data, takes precedence. Designing an efficient system requires a deep understanding of the operating environment and the constraints it imposes. Prioritizing the right dimensions based on context is the first step in effectively managing trade-offs.</p>
<p>For instance, in Mobile ML deployments, battery life is often the primary constraint. This places a premium on compute efficiency, as energy consumption must be minimized to preserve the device’s operational time. As a result, lightweight models are prioritized, even if it means sacrificing some accuracy or requiring additional data preprocessing. The focus is on balancing acceptable performance with energy-efficient operation.</p>
<p>In contrast, Cloud ML-based systems prioritize scalability and throughput. These systems must process large volumes of data and serve millions of users simultaneously. While compute resources in cloud environments are more abundant, energy efficiency and operational costs still remain important considerations. Here, algorithmic efficiency plays a critical role in ensuring that the system can scale without overwhelming the underlying infrastructure.</p>
<p>Edge ML systems present an entirely different set of priorities. Autonomous vehicles or real-time monitoring systems require low-latency processing to ensure safe and reliable operation. This makes real-time performance and compute efficiency paramount, often at the expense of energy consumption. However, the hardware constraints of edge devices mean that these systems must still carefully manage energy and computational resources to remain viable.</p>
<p>Finally, Tiny ML deployments demand extreme levels of efficiency due to the severe limitations of hardware and energy availability. For these systems, model and data efficiency are the top priorities. Models must be highly compact and capable of operating on microcontrollers with minimal memory and compute power. At the same time, the training process must rely on small, carefully curated datasets to ensure the model generalizes well without requiring extensive resources.</p>
<p>In each of these contexts, prioritizing the right dimensions of efficiency ensures that the system meets its functional and resource requirements. Recognizing the unique demands of each deployment scenario allows designers to navigate trade-offs effectively and tailor solutions to specific needs.</p>
</section>
<section id="sec-efficient-ai-testtime-compute-dbb3" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-testtime-compute-dbb3">Test-Time Compute</h3>
<p>We can further enhance system adaptability through dynamic resource allocation during inference, a concept often referred to as “Test-Time Compute.” This approach recognizes that resource needs may fluctuate even within a specific deployment context. By adjusting the computational effort expended at inference time, systems can fine-tune their performance to meet immediate demands.</p>
<p>For example, in a cloud-based video analysis system, standard video streams might be processed with a streamlined, low-compute model to maintain high throughput. However, when a critical event is detected, the system could dynamically allocate more computational resources to a more complex model, enabling higher precision analysis of the event. This flexibility allows for a trade-off between latency and accuracy on demand.</p>
<p>Similarly, in mobile applications, a voice assistant might use a lightweight model for routine commands, conserving battery life. But when faced with a complex query, the system could temporarily activate a more resource-intensive model for improved accuracy. This ability to adjust compute based on the complexity of the task or the importance of the result is a powerful tool for optimizing system performance in real-time.</p>
<p>Implementing “Test-Time Compute” introduces new challenges. Dynamic resource allocation requires sophisticated monitoring and control mechanisms to ensure that the system remains stable and responsive. There is a point of diminishing returns; increasing compute beyond a certain threshold may not yield significant performance improvements, making it crucial to strike a balance between resource usage and desired outcomes. The ability to dynamically increase compute can create disparities in access to high-performance AI, raising equity concerns about who benefits from advanced AI capabilities.</p>
<p>Despite these challenges, “Test-Time Compute” offers a valuable strategy for enhancing system adaptability and optimizing performance in dynamic environments. It complements the contextual prioritization approach by enabling systems to respond effectively to varying demands within specific deployment scenarios.</p>
</section>
<section id="sec-efficient-ai-codesign-5516" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-codesign-5516">Co-Design</h3>
<p>Efficient machine learning systems are rarely the product of isolated optimizations. Achieving balance across model, compute, and data efficiency requires an end-to-end perspective, where each component of the system is designed in tandem with the others. This holistic approach, often referred to as co-design, involves aligning model architectures, hardware platforms, and data pipelines to work seamlessly together.</p>
<p>One of the key benefits of co-design is its ability to mitigate trade-offs by tailoring each component to the specific requirements of the system. For instance, consider a speech recognition system deployed on a mobile device. The model must be compact enough to fit within the device’s Tiny ML memory constraints while still delivering real-time performance. By designing the model architecture to leverage the capabilities of hardware accelerators, such as NPUs, it becomes possible to achieve low-latency inference without excessive energy consumption. Similarly, careful preprocessing and augmentation of the training data can ensure robust performance, even with a smaller, streamlined model.</p>
<p>Co-design becomes essential in resource-constrained environments like Edge ML and Tiny ML deployments. Models must align precisely with hardware capabilities. For example, 8-bit models require hardware support for efficient integer operations, while pruned models benefit from sparse tensor operations. Similarly, edge accelerators often optimize specific operations like convolutions or matrix multiplication, influencing model architecture choices. This creates a tight coupling between hardware and model design decisions. Detailed hardware architecture considerations, including accelerator design principles, memory hierarchy optimization, and custom silicon development for ML workloads, are covered comprehensively in <strong><a href="../core/hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 9: AI Acceleration</a></strong>.</p>
<p>This approach extends beyond the interaction of models and hardware. Data pipelines, too, play a central role in co-design. For example, in applications requiring real-time adaptation, such as personalized recommendation systems, the data pipeline must deliver high-quality, timely information that minimizes computational overhead while maximizing model effectiveness. By integrating data management into the design process, it becomes possible to reduce redundancy, streamline training, and support efficient deployment.</p>
<p>End-to-end co-design ensures that the trade-offs inherent in machine learning systems are addressed holistically. By designing each component with the others in mind, it becomes possible to balance competing priorities and create systems that are not only efficient but also robust and adaptable.</p>
</section>
<section id="sec-efficient-ai-automation-0e1f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-automation-0e1f">Automation</h3>
<p>Navigating the trade-offs between model, compute, and data efficiency is a complex task that often involves numerous iterations and expert judgment. Automation and optimization tools have emerged as powerful solutions for managing these challenges, streamlining the process of balancing efficiency dimensions while reducing the time and expertise required.</p>
<p>One widely used approach is automated machine learning (AutoML)<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a>, which enables the exploration of different model architectures, hyperparameter configurations, and feature engineering techniques. Building on the systematic approach to ML workflows introduced in <strong><a href="../core/workflow/workflow.html#sec-ai-workflow">Chapter 19: AI Workflow</a></strong>, AutoML tools automate many of the efficiency optimization decisions that traditionally required extensive manual tuning.</p>
<div class="no-row-height column-margin column-container"><div id="fn52"><p><sup>52</sup>&nbsp;<strong>AutoML</strong>: Automated machine learning that systematically searches through model architectures, hyperparameters, and data preprocessing options. Google’s AutoML achieved 84.3% ImageNet accuracy vs.&nbsp;human experts’ 78.5%, while reducing model development time from months to hours using neural architecture search and automated hyperparameter optimization. By automating these aspects of the design process, AutoML can identify models that achieve an optimal balance between performance and efficiency. For instance, an AutoML pipeline might search for a lightweight model architecture that delivers high accuracy while fitting within the resource constraints of an edge device <span class="citation" data-cites="hutter2019automated">(<a href="#ref-hutter2019automated" role="doc-biblioref">Hutter, Kotthoff, and Vanschoren 2019</a>)</span>. This approach reduces the need for manual trial-and-error, making optimization faster and more accessible.</p><div id="ref-hutter2019automated" class="csl-entry" role="listitem">
Hutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. 2019. <em>Automated Machine Learning: Methods, Systems, Challenges</em>. <em>Automated Machine Learning</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5">https://doi.org/10.1007/978-3-030-05318-5</a>.
</div></div><div id="fn53"><p><sup>53</sup>&nbsp;<strong>Neural Architecture Search (NAS)</strong>: Automated method for discovering optimal neural network architectures using algorithmic search rather than human design. EfficientNet-B7, discovered via NAS, achieved 84.3% ImageNet accuracy with 37M parameters vs.&nbsp;hand-designed ResNeXt-101’s 80.9% with 84M parameters. For example, NAS can design models that leverage quantization or sparsity techniques, ensuring compatibility with energy-efficient accelerators like TPUs or microcontrollers <span class="citation" data-cites="elsken2019neural">(<a href="#ref-elsken2019neural" role="doc-biblioref">Elsken, Metzen, and Hutter 2019</a>)</span>. This automated co-design of models and hardware helps mitigate trade-offs by aligning efficiency goals across dimensions. The specific implementation techniques for quantization, pruning, and other optimization methods that NAS systems often discover are detailed systematically in <strong><a href="../core/optimizations/optimizations.html#sec-model-optimizations">Chapter 11: Model Optimizations</a></strong>.</p><div id="ref-elsken2019neural" class="csl-entry" role="listitem">
Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2019. <span>“Neural Architecture Search.”</span> In <em>Automated Machine Learning</em>, 20:63–77. 55. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5\_3">https://doi.org/10.1007/978-3-030-05318-5\_3</a>.
</div></div></div><p>Neural architecture search (NAS)<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a> takes automation a step further by designing model architectures tailored to specific hardware or deployment scenarios. NAS algorithms evaluate a wide range of architectural possibilities, selecting those that maximize performance while minimizing computational demands.</p>
<p>Data efficiency, too, benefits from automation. Tools that automate dataset curation, augmentation, and active learning reduce the size of training datasets without sacrificing model performance. These tools prioritize high-value data points, ensuring that models are trained on the most informative examples. This not only speeds up training but also reduces computational overhead, reinforcing both compute and algorithmic efficiency <span class="citation" data-cites="settles2009active">(<a href="#ref-settles2009active" role="doc-biblioref">Settles 2012b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-settles2009active" class="csl-entry" role="listitem">
———. 2012b. <em>Active Learning</em>. <em>University of Wisconsin-Madison Department of Computer Sciences</em>. Vol. 1648. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01560-1">https://doi.org/10.1007/978-3-031-01560-1</a>.
</div></div><p>While automation tools are not a panacea, they play a critical role in addressing the complexity of trade-offs. By leveraging these tools, system designers can achieve efficient solutions more quickly and at lower cost, freeing them to focus on broader design challenges and deployment considerations. <strong><a href="../core/frameworks/frameworks.html#sec-ai-frameworks">Chapter 5: AI Frameworks</a></strong> explores how modern ML frameworks incorporate these automation capabilities, making efficiency optimization more accessible to practitioners.</p>
</section>
<section id="sec-efficient-ai-systematic-evaluation-8c2a" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-systematic-evaluation-8c2a">Systematic Evaluation</h3>
<p>Beyond the technical automation of efficiency optimization lies the broader challenge of systematic evaluation. The pursuit of efficiency in machine learning systems necessitates a structured approach to assessing trade-offs that extends beyond purely technical considerations. As systems transition from research prototypes to production deployments, the criteria for success must encompass not only algorithmic performance but also economic viability and operational sustainability. This broader perspective recognizes that efficiency optimization decisions carry implications across multiple dimensions of system design and organizational priorities.</p>
<p>The costs associated with efficiency improvements manifest across several distinct categories. Engineering effort represents a significant investment, encompassing research into optimization techniques, experimental validation of approaches, and integration of solutions into existing infrastructure. These development costs must be weighed against the ongoing operational expenses of running less efficient systems. For instance, while implementing advanced model compression techniques may require substantial upfront engineering investment, the resulting reduction in computational requirements can yield sustained decreases in infrastructure costs over time.</p>
<p>The benefits of efficiency improvements similarly span multiple domains. Beyond direct cost reductions in computational resources, efficient systems often enable qualitatively new capabilities, such as real-time processing in resource-constrained environments or deployment to edge devices that would otherwise be infeasible. These strategic advantages can create competitive differentiation and expand the scope of possible applications, justifying efficiency investments that might not appear cost-effective when evaluated solely on operational metrics.</p>
</section>
<section id="sec-efficient-ai-continuous-assessment-d74b" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-continuous-assessment-d74b">Continuous Assessment</h3>
<p>This systematic evaluation framework must be complemented by ongoing assessment mechanisms. The dynamic nature of machine learning systems in production environments necessitates continuous monitoring of efficiency characteristics. As models evolve, data distributions shift, and infrastructure changes, the efficiency properties of systems can degrade in subtle but significant ways. This reality underscores the importance of establishing systematic approaches to monitor and maintain efficiency over time, ensuring that optimization efforts yield sustained benefits rather than temporary improvements.</p>
<p>Effective efficiency assessment encompasses both technical performance metrics and higher-level system behaviors. Resource utilization patterns reveal how effectively systems leverage available computational capacity, while throughput and latency measurements indicate whether efficiency optimizations translate into improved user experience. However, these technical metrics must be interpreted in the context of system quality—improvements in computational efficiency that compromise accuracy or reliability may ultimately increase total system cost through degraded user satisfaction or increased error handling overhead.</p>
<p>The temporal dimension of efficiency assessment is equally important. Real-time monitoring enables rapid detection of efficiency regressions, allowing teams to respond quickly to deployments or infrastructure changes that negatively impact system performance. Historical analysis provides insight into longer-term trends, revealing whether efficiency improvements are sustainable under changing operational conditions. For example, model compression techniques that initially reduce computational requirements may become less effective as traffic patterns evolve or as models are retrained on new data distributions.</p>
<div id="quiz-question-sec-efficient-ai-managing-tradeoffs-80e8" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the primary focus of Tiny ML deployments?</p>
<ol type="a">
<li>Extreme resource efficiency</li>
<li>Real-time performance and compute efficiency</li>
<li>Scalability and throughput</li>
<li>Algorithmic efficiency for cloud environments</li>
</ol></li>
<li><p>True or False: In Mobile ML deployments, compute efficiency is prioritized over model accuracy to extend battery life.</p></li>
<li><p>Explain how ‘Test-Time Compute’ can enhance system adaptability in dynamic environments.</p></li>
<li><p>In Edge ML systems, low-latency processing is prioritized to ensure ________ operation.</p></li>
<li><p>Order the following strategies based on their role in managing trade-offs: (1) Co-design, (2) Automation, (3) Contextual Prioritization.</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-managing-tradeoffs-80e8" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-efficient-ai-efficiencyfirst-mindset-39e7" class="level2">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-efficiencyfirst-mindset-39e7">Efficiency-First Mindset</h2>
<p>Designing an efficient machine learning system requires a holistic approach. While it is tempting to focus on optimizing individual components, such as the model architecture or the hardware platform, true efficiency emerges when the entire system is considered as a whole. This end-to-end perspective ensures that trade-offs are balanced across all stages of the machine learning pipeline, from data collection to deployment.</p>
<p>Efficiency is not a static goal but a dynamic process shaped by the context of the application. A system designed for a cloud data center will prioritize scalability and throughput, while an edge deployment will focus on low latency and energy conservation. These differing priorities influence decisions at every step of the design process, requiring careful alignment of the model, compute resources, and data strategy.</p>
<p>An end-to-end perspective can transform system design, enabling machine learning practitioners to build systems that effectively balance trade-offs. Through case studies and examples, we will highlight how efficient systems are designed to meet the unique challenges of their deployment environments, whether in the cloud, on mobile devices, or in resource-constrained Tiny ML applications.</p>
<section id="sec-efficient-ai-endtoend-perspective-fc95" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-endtoend-perspective-fc95">End-to-End Perspective</h3>
<p>Efficiency in machine learning systems is achieved not through isolated optimizations but by considering the entire pipeline as a unified whole. Each stage, including data collection, model training, hardware deployment, and inference, contributes to the overall efficiency of the system. Decisions made at one stage can ripple through the rest, influencing performance, resource use, and scalability.</p>
<p>For example, data collection and preprocessing are often the starting points of the pipeline. The quality and diversity of the data directly impact model performance and efficiency. <strong><a href="../core/data_engineering/data_engineering.html#sec-data-engineering">Chapter 8: Data Engineering</a></strong> provides comprehensive coverage of how data pipeline design decisions cascade through the entire system, affecting efficiency at every subsequent stage. Curating smaller, high-quality datasets can reduce computational costs during training while simplifying the model’s design. However, insufficient data diversity may affect generalization, necessitating compensatory measures in model architecture or training procedures. By aligning the data strategy with the model and deployment context, designers can avoid inefficiencies downstream.</p>
<p>Model training is another critical stage. The choice of architecture, optimization techniques, and hyperparameters must consider the constraints of the deployment hardware. A model designed for high-performance cloud systems may emphasize accuracy and scalability, leveraging large datasets and compute resources. Conversely, a model intended for edge devices must balance accuracy with size and energy efficiency, often requiring compact architectures and quantization techniques tailored to specific hardware.</p>
<p>Deployment and inference demand precise hardware alignment. Each platform offers distinct capabilities. GPUs excel at parallel matrix operations, TPUs optimize specific neural network computations, and microcontrollers provide energy-efficient scalar processing. For example, a smartphone speech recognition system might leverage an NPU’s dedicated convolution units for millisecond-level inference times at low power consumption, while an autonomous vehicle’s FPGA-based accelerator processes multiple sensor streams with microsecond-level latency. This hardware-software integration determines real-world efficiency.</p>
<p>An end-to-end perspective ensures that trade-offs are addressed holistically, rather than shifting inefficiencies from one stage of the pipeline to another. By treating the system as an integrated whole, machine learning practitioners can design solutions that are not only efficient but also robust and scalable across diverse deployment scenarios. This systems thinking approach becomes particularly critical when deploying to resource-constrained environments, as explored in <strong><a href="../core/ondevice_learning/ondevice_learning.html#sec-ondevice-learning">Chapter 13: On-Device Learning</a></strong> for mobile and edge contexts.</p>
</section>
<section id="sec-efficient-ai-scenarios-15e0" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-scenarios-15e0">Scenarios</h3>
<p>The efficiency needs of machine learning systems differ significantly depending on the lifecycle stage and deployment environment. From research prototypes to production systems, and from high-performance cloud applications to resource-constrained edge deployments, each scenario presents unique challenges and trade-offs. Understanding these differences is crucial for designing systems that meet their operational requirements effectively.</p>
<section id="sec-efficient-ai-prototypes-vs-production-c0ea" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-prototypes-vs-production-c0ea">Prototypes vs.&nbsp;Production</h4>
<p>In the research phase, the primary focus is often on model performance, with efficiency taking a secondary role. Prototypes are typically trained and tested using abundant compute resources, allowing researchers to experiment with large architectures, extensive hyperparameter tuning, and diverse datasets. While this approach enables the exploration of cutting-edge techniques, the resulting systems are often too resource-intensive for real-world use.</p>
<p>In contrast, production systems must prioritize efficiency to operate within practical constraints. Deployment environments, including cloud data centers, mobile devices, and IoT sensors, impose strict limitations on compute power, memory, and energy consumption. Transitioning from a research prototype to a production-ready system often involves significant optimization, such as model pruning, quantization, or retraining on targeted datasets. Production systems also require continuous monitoring of efficiency metrics, cost tracking, alerting for performance degradation, and operational frameworks for managing efficiency trade-offs at scale—comprehensive production efficiency management strategies are detailed in <strong><a href="../core/ops/ops.html#sec-ml-operations">Chapter 12: ML Operations</a></strong>. This shift highlights the need to balance performance and efficiency as systems move from concept to deployment.</p>
</section>
<section id="sec-efficient-ai-cloud-apps-vs-constrained-systems-22ae" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-cloud-apps-vs-constrained-systems-22ae">Cloud Apps vs.&nbsp;Constrained Systems</h4>
<p>Cloud-based systems, such as those used for large-scale analytics or recommendation engines, are designed to handle massive workloads. Scalability is the primary concern, requiring models and infrastructure that can support millions of users simultaneously. The ML systems design principles covered in <strong><a href="../core/ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong> provide the architectural foundations for building these scalable, efficiency-optimized cloud deployments. While compute resources are relatively abundant in cloud environments, energy efficiency and operational costs still remain critical considerations. Techniques such as model compression and hardware-specific optimizations help manage these trade-offs, ensuring the system scales efficiently.</p>
<p>In contrast, edge and mobile systems operate under far stricter constraints. Real-time performance, energy efficiency, and hardware limitations are often the dominant concerns. For example, a speech recognition application on a smartphone must balance model size and latency to provide a seamless user experience without draining the device’s battery. Similarly, an IoT sensor deployed in a remote location must operate for months on limited power, requiring an ultra-efficient model and compute pipeline. These scenarios demand solutions that prioritize efficiency over raw performance. <strong><a href="../core/frontiers/frontiers.html#sec-agi-systems">Chapter 21: AGI Systems</a></strong> explores how these efficiency principles scale to future AI systems, where resource optimization becomes even more critical as system complexity increases.</p>
</section>
<section id="sec-efficient-ai-frequent-retraining-vs-stability-9b56" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-frequent-retraining-vs-stability-9b56">Frequent Retraining vs.&nbsp;Stability</h4>
<p>Some systems, such as recommendation engines or fraud detection platforms, require frequent retraining to remain effective in dynamic environments. These systems depend heavily on data efficiency, using actively labeled datasets and sampling strategies to minimize retraining costs. Compute efficiency also plays a role, as scalable infrastructure is needed to process new data and update models regularly.</p>
<p>Other systems, such as embedded models in medical devices or industrial equipment, require long-term stability with minimal updates. In these cases, upfront optimizations in model and data efficiency are critical to ensure the system performs reliably over time. Reducing dependency on frequent updates minimizes computational and operational overhead, making the system more sustainable in the long run. <strong><a href="../core/robust_ai/robust_ai.html#sec-robust-ai">Chapter 14: Robust AI</a></strong> examines how reliability requirements in critical applications influence efficiency optimization strategies.</p>
<div id="quiz-question-sec-efficient-ai-efficiencyfirst-mindset-39e7" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the importance of an end-to-end perspective in designing efficient ML systems?</p>
<ol type="a">
<li>It focuses solely on optimizing model architecture.</li>
<li>It balances trade-offs across the entire system to avoid shifting inefficiencies.</li>
<li>It ensures efficiency by considering each stage of the pipeline independently.</li>
<li>It prioritizes hardware deployment over data collection.</li>
</ol></li>
<li><p>Explain how the efficiency needs of cloud-based systems differ from those of edge deployments.</p></li>
<li><p>True or False: In a production ML system, efficiency is a static goal that remains constant regardless of the deployment context.</p></li>
<li><p>In a smartphone speech recognition system, balancing model size and latency is crucial to provide a seamless user experience without ________ the device’s battery.</p></li>
<li><p>How might you apply an efficiency-first mindset when transitioning a research prototype to a production system?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-efficiencyfirst-mindset-39e7" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-efficient-ai-broader-challenges-fe2d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-broader-challenges-fe2d">Broader Challenges</h2>
<p>While efficiency in machine learning is often framed as a technical challenge, it is also deeply tied to broader questions about the purpose and impact of AI systems. Designing efficient systems involves navigating not only practical trade-offs but also complex ethical and philosophical considerations, such as the following:</p>
<ul>
<li><p>What are the limits of optimization?</p></li>
<li><p>How do we ensure that efficiency benefits are distributed equitably?</p></li>
<li><p>How do efficiency innovations impact responsible AI practices?</p></li>
<li><p>Can the pursuit of efficiency stifle innovation or creativity in the field?</p></li>
</ul>
<p>We must explore these questions as engineers, inviting reflection on the broader implications of system efficiency. By examining the limits of optimization, equity concerns, and the tension between innovation and efficiency, we can have a deeper understanding of the challenges involved in balancing technical goals with ethical and societal values. <strong><a href="../core/responsible_ai/responsible_ai.html#sec-responsible-ai">Chapter 16: Responsible AI</a></strong> provides a comprehensive framework for addressing these ethical considerations in AI system design.</p>
<section id="sec-efficient-ai-optimization-limits-a52b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-optimization-limits-a52b">Optimization Limits</h3>
<p>Optimization plays a central role in building efficient machine learning systems, but it is not an infinite process. As systems become more refined, each additional improvement often requires exponentially more effort, time, or resources, while delivering increasingly smaller benefits. This phenomenon, known as diminishing returns, is a common challenge in many engineering domains, including machine learning.</p>
<p>The No Free Lunch (NFL) theorems<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a> for optimization further illustrate the inherent limitations of optimization efforts. According to the NFL theorems, no single optimization algorithm can outperform all others across every possible problem.</p>
<div class="no-row-height column-margin column-container"><div id="fn54"><p><sup>54</sup>&nbsp;<strong>No Free Lunch (NFL) Theorems</strong>: Mathematical proof by Wolpert and Macready (1997) showing that averaged over all possible optimization problems, every algorithm performs equally well. In ML context, this means no universal optimization technique exists—methods must be tailored to specific problem domains and data characteristics. This implies that the effectiveness of an optimization technique is highly problem-specific, and improvements in one area may not translate to others <span class="citation" data-cites="wolpert1997no">(<a href="#ref-wolpert1997no" role="doc-biblioref">Wolpert and Macready 1997</a>)</span>.</p><div id="ref-wolpert1997no" class="csl-entry" role="listitem">
Wolpert, D. H., and W. G. Macready. 1997. <span>“No Free Lunch Theorems for Optimization.”</span> <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 67–82. <a href="https://doi.org/10.1109/4235.585893">https://doi.org/10.1109/4235.585893</a>.
</div></div></div><p>For example, compressing a machine learning model can initially reduce memory usage and compute requirements significantly with minimal loss in accuracy. However, as compression progresses, maintaining performance becomes increasingly challenging. Achieving additional gains may necessitate sophisticated techniques, such as hardware-specific optimizations or extensive retraining, which increase both complexity and cost. These costs extend beyond financial investment in specialized hardware and training resources to include the time and expertise required to fine-tune models, iterative testing efforts, and potential trade-offs in model robustness and generalizability. As such, pursuing extreme efficiency often leads to diminishing returns, where escalating costs and complexity outweigh incremental benefits.</p>
<p>The NFL theorems highlight that no universal optimization solution exists, emphasizing the need to balance efficiency pursuits with practical considerations. Recognizing the limits of optimization is critical for designing systems that are not only efficient but also practical and sustainable. Over-optimization risks wasted resources and reduced adaptability, complicating future system updates or adjustments to changing requirements. Identifying when a system is “good enough” ensures resources are allocated effectively, focusing on efforts with the greatest overall impact.</p>
<p>Similarly, optimizing datasets for training efficiency may initially save resources but excessively reducing dataset size risks compromising diversity and weakening model generalization. Likewise, pushing hardware to its performance limits may improve metrics such as latency or power consumption, yet the associated reliability concerns and engineering costs can ultimately outweigh these gains.</p>
<p>Understanding the limits of optimization is essential for creating systems that balance efficiency with practicality and sustainability. This perspective helps avoid over-optimization and ensures resources are invested in areas with the most meaningful returns.</p>
</section>
<section id="sec-efficient-ai-moores-law-case-study-2e70" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-moores-law-case-study-2e70">Moore’s Law Case Study</h3>
<p>One of the most insightful examples of the limits of optimization can be seen in Moore’s Law and the economic curve it depends on. While Moore’s Law is often celebrated as a predictor of exponential growth in computational power, its success relied on an intricate economic balance. The relationship between integration and cost, as illustrated in the accompanying plot, provides a compelling analogy for the diminishing returns seen in machine learning optimization.</p>
<p><a href="#fig-moores-law-plot" class="quarto-xref">Figure&nbsp;15</a> shows the relative manufacturing cost per component as the number of components in an integrated circuit increases. Initially, as more components are packed onto a chip (<span class="math inline">\(x\)</span>-axis), the cost per component (<span class="math inline">\(y\)</span>-axis) decreases. This is because higher integration reduces the need for supporting infrastructure such as packaging and interconnects, creating economies of scale. For example, in the early years of integrated circuit design, moving from hundreds to thousands of components per chip drastically reduced costs and improved performance <span class="citation" data-cites="moore2021cramming">(<a href="#ref-moore2021cramming" role="doc-biblioref">Moore 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-moores-law-plot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-moores-law-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f9f984606d637e71ea667911c29735a879b2f63a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Moore’s Law Economics: Declining per-component manufacturing costs initially drove exponential growth in integrated circuit complexity, but diminishing returns eventually limited further cost reductions. This relationship mirrors optimization challenges in machine learning, where increasing model complexity yields diminishing gains in performance relative to computational expense. Source: [@moore2021cramming]."><img src="efficient_ai_files/mediabag/f9f984606d637e71ea667911c29735a879b2f63a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moores-law-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Moore’s Law Economics</strong>: Declining per-component manufacturing costs initially drove exponential growth in integrated circuit complexity, but diminishing returns eventually limited further cost reductions. This relationship mirrors optimization challenges in machine learning, where increasing model complexity yields diminishing gains in performance relative to computational expense. Source: <span class="citation" data-cites="moore2021cramming">(<a href="#ref-moore2021cramming" role="doc-biblioref">Moore 2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-moore2021cramming" class="csl-entry" role="listitem">
Moore, Gordon. 2021. <span>“Cramming More Components onto Integrated Circuits (1965).”</span> In <em>Ideas That Created the Future</em>, 261–66. The MIT Press. <a href="https://doi.org/10.7551/mitpress/12274.003.0027">https://doi.org/10.7551/mitpress/12274.003.0027</a>.
</div></div></figure>
</div>
<p>However, as integration continues, the curve begins to rise. This inflection point occurs because the challenges of scaling become more pronounced. Components packed closer together face reliability issues, such as increased heat dissipation and signal interference. Addressing these issues requires more sophisticated manufacturing techniques, such as advanced lithography, error correction, and improved materials. These innovations increase the complexity and cost of production, driving the curve upward. This U-shaped curve captures the fundamental trade-off in optimization: early improvements yield substantial benefits, but beyond a certain point, each additional gain comes at a greater cost.</p>
<section id="sec-efficient-ai-ml-optimization-parallels-0fbb" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-ml-optimization-parallels-0fbb">ML Optimization Parallels</h4>
<p>The dynamics of this curve mirror the challenges faced in machine learning optimization. For instance, compressing a deep learning model to reduce its size and energy consumption follows a similar trajectory. Initial optimizations, such as pruning redundant parameters or reducing precision, often lead to significant savings with minimal impact on accuracy. However, as the model is further compressed, the losses in performance become harder to recover. Techniques such as quantization or hardware-specific tuning can restore some of this performance, but these methods add complexity and cost to the design process.</p>
<p>Similarly, in data efficiency, reducing the size of training datasets often improves computational efficiency at first, as less data requires fewer resources to process. Yet, as the dataset shrinks further, it may lose diversity, compromising the model’s ability to generalize. Addressing this often involves introducing synthetic data or sophisticated augmentation techniques, which demand additional engineering effort.</p>
<p>The Moore’s Law plot (<a href="#fig-moores-law-plot" class="quarto-xref">Figure&nbsp;15</a>) serves as a visual reminder that optimization is not an infinite process. The cost-benefit balance is always context-dependent, and the point of diminishing returns varies based on the goals and constraints of the system. Machine learning practitioners, like semiconductor engineers, must identify when further optimization ceases to provide meaningful benefits. Over-optimization can lead to wasted resources, reduced adaptability, and systems that are overly specialized to their initial conditions.</p>
</section>
</section>
<section id="sec-efficient-ai-equity-concerns-9026" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-equity-concerns-9026">Equity Concerns</h3>
<p>Efficiency in machine learning has the potential to reduce costs, improve scalability, and expand accessibility. However, the resources needed to achieve efficiency, including advanced hardware, curated datasets, and state-of-the-art optimization techniques, are often concentrated in well-funded organizations or regions. This disparity creates inequities in who can leverage efficiency gains, limiting the reach of machine learning in low-resource contexts. By examining compute, data, and algorithmic efficiency inequities, we can better understand these challenges and explore pathways toward democratization.</p>
<section id="sec-efficient-ai-uneven-access-8ace" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-uneven-access-8ace">Uneven Access</h4>
<p>The training costs of state-of-the-art AI models have reached unprecedented levels. For example, training costs for state-of-the-art models like GPT-4 and Google’s Gemini Ultra are estimated to require tens to hundreds of millions of dollars worth of compute <span class="citation" data-cites="perrault2024artificial">(<a href="#ref-perrault2024artificial" role="doc-biblioref">Maslej et al. 2024</a>)</span>. Computational efficiency depends on access to specialized hardware and infrastructure. The discrepancy in access is significant: training even a small language model (SLM) like LLlama with 7 billion parameters can require millions of dollars in computing resources, while many research institutions operate with significantly lower annual compute budgets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-perrault2024artificial" class="csl-entry" role="listitem">
Maslej, Nestor, Loredana Fattorini, C. Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, John Etchemendy, et al. 2024. <span>“Artificial Intelligence Index Report 2024.”</span> <em>CoRR</em>. <a href="https://doi.org/10.48550/ARXIV.2405.19522">https://doi.org/10.48550/ARXIV.2405.19522</a>.
</div><div id="ref-oecd_ai_2021" class="csl-entry" role="listitem">
OECD.AI. 2021. <span>“Measuring the Geographic Distribution of AI Computing Capacity.”</span> &lt;https://oecd.ai/en/policy-circle/computing-capacity&gt;.
</div></div><p>Research conducted by <a href="https://oecd.ai/en/">OECD.AI</a> indicates that 90% of global AI computing capacity is centralized in only five countries, posing significant challenges for researchers and professionals in other regions <span class="citation" data-cites="oecd_ai_2021">(<a href="#ref-oecd_ai_2021" role="doc-biblioref">OECD.AI 2021</a>)</span>.</p>
<p>A concrete illustration of this disparity is the compute divide in academia versus industry. Academic institutions often lack the hardware needed to replicate state-of-the-art results, particularly when competing with large technology firms that have access to custom supercomputers or cloud resources. This imbalance not only stifles innovation in underfunded sectors but also makes it harder for diverse voices to contribute to advancing machine learning.</p>
<p>Energy-efficient compute technologies, such as accelerators designed for Tiny ML or Mobile ML, present a promising avenue for democratization. By enabling powerful processing on low-cost, low-power devices, these technologies allow organizations without access to high-end infrastructure to build and deploy impactful systems. For instance, energy-efficient Tiny ML models can be deployed on affordable microcontrollers, opening doors for applications in healthcare, agriculture, and education in underserved regions.</p>
</section>
<section id="sec-efficient-ai-lowresource-challenges-a3d1" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-lowresource-challenges-a3d1">Low-Resource Challenges</h4>
<p>Data efficiency is essential in contexts where high-quality datasets are scarce, but the challenges of achieving it are unequally distributed. For example, natural language processing (NLP) for low-resource languages suffers from a lack of sufficient training data, leading to significant performance gaps compared to high-resource languages like English. Efforts like the Masakhane project, which builds open-source datasets for African languages, show how collaborative initiatives can address this issue. However, scaling such efforts globally requires far greater investment and coordination.</p>
<p>Even when data is available, the ability to process and curate it efficiently depends on computational and human resources. Large organizations routinely employ data engineering teams and automated pipelines for curation and augmentation, enabling them to optimize data efficiency and improve downstream performance. In contrast, smaller groups often lack access to the tools or expertise needed for such tasks, leaving them at a disadvantage in both research and practical applications.</p>
<p>Democratizing data efficiency requires more open sharing of pre-trained models and datasets. Initiatives like Hugging Face’s open access to transformers or multilingual models by organizations like Meta’s No Language Left Behind aim to make state-of-the-art NLP models available to researchers and practitioners worldwide. These efforts help reduce the barriers to entry for data-scarce regions, enabling more equitable access to AI capabilities.</p>
</section>
<section id="sec-efficient-ai-efficiency-accessibility-71c4" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiency-accessibility-71c4">Efficiency for Accessibility</h4>
<p>Model efficiency plays a crucial role in democratizing machine learning by enabling advanced capabilities on low-cost, resource-constrained devices. Compact, efficient models designed for edge devices or mobile phones have already begun to bridge the gap in accessibility. For instance, AI-powered diagnostic tools running on smartphones are transforming healthcare in remote areas, while low-power Tiny ML models enable environmental monitoring in regions without reliable electricity or internet connectivity.</p>
<p>Technologies like <a href="https://ai.google.dev/edge/litert">TensorFlow Lite</a> and <a href="https://pytorch.org/mobile/home/">PyTorch Mobile</a> allow developers to deploy lightweight models on everyday devices, expanding access to AI applications in resource-constrained settings. These tools demonstrate how algorithmic efficiency can serve as a practical pathway to equity, particularly when combined with energy-efficient compute hardware.</p>
<p>However, scaling the benefits of algorithmic efficiency requires addressing barriers to entry. Many efficient architectures, such as those designed through NAS, remain resource-intensive to develop. Open-source efforts to share pre-optimized models, like MobileNet or EfficientNet, play a critical role in democratizing access to efficient AI by allowing under-resourced organizations to deploy state-of-the-art solutions without needing to invest in expensive optimization processes.</p>
</section>
<section id="sec-efficient-ai-democratization-pathways-1b49" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-democratization-pathways-1b49">Democratization Pathways</h4>
<p>Efforts to close the equity gap in machine learning must focus on democratizing access to tools and techniques that enhance efficiency. Open-source initiatives, such as community-driven datasets and shared model repositories, provide a foundation for equitable access to efficient systems. Affordable hardware platforms, such as Raspberry Pi devices or open-source microcontroller frameworks, further enable resource-constrained organizations to build and deploy AI solutions tailored to their needs.</p>
<p>Collaborative partnerships between well-resourced organizations and underrepresented groups also offer opportunities to share expertise, funding, and infrastructure. For example, initiatives that provide subsidized access to cloud computing platforms or pre-trained models for underserved regions can empower diverse communities to leverage efficiency for social impact.</p>
<p>Through efforts in model, computation, and data efficiency, the democratization of machine learning can become a reality. These efforts not only expand access to AI capabilities but also foster innovation and inclusivity, ensuring that the benefits of efficiency are shared across the global community.</p>
</section>
</section>
<section id="sec-efficient-ai-balancing-innovation-efficiency-6be4" class="level3">
<h3 class="anchored" data-anchor-id="sec-efficient-ai-balancing-innovation-efficiency-6be4">Balancing Innovation and Efficiency</h3>
<p>The pursuit of efficiency in machine learning often brings with it a tension between optimizing for what is known and exploring what is new. On one hand, efficiency drives the practical deployment of machine learning systems, enabling scalability, cost reduction, and environmental sustainability. On the other hand, focusing too heavily on efficiency can stifle innovation by discouraging experimentation with untested, resource-intensive ideas.</p>
<section id="sec-efficient-ai-stability-vs-experimentation-e472" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-stability-vs-experimentation-e472">Stability vs.&nbsp;Experimentation</h4>
<p>Efficiency often favors established techniques and systems that have already been proven to work well. For instance, optimizing neural networks through pruning, quantization, or distillation typically involves refining existing architectures rather than developing entirely new ones. While these approaches provide incremental improvements, they may come at the cost of exploring novel designs or paradigms that could yield transformative breakthroughs.</p>
<p>Consider the shift from traditional machine learning methods to deep learning. Early neural network research in the 1990s and 2000s required significant computational resources and often failed to outperform simpler methods on practical tasks. Despite this, researchers continued to push the boundaries of what was possible, eventually leading to the breakthroughs in deep learning that define modern AI. If the field had focused exclusively on efficiency during that period, these innovations might never have emerged.</p>
</section>
<section id="sec-efficient-ai-resourceintensive-innovation-8a8d" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-resourceintensive-innovation-8a8d">Resource-Intensive Innovation</h4>
<p>Pioneering research often requires significant resources, from massive datasets to custom hardware. For example, large language models like GPT-4 or PaLM are not inherently efficient; their training processes consume enormous amounts of compute power and energy. Yet, these models have opened up entirely new possibilities in language understanding, prompting advancements that eventually lead to more efficient systems, such as smaller fine-tuned versions for specific tasks.</p>
<p>However, this reliance on resource-intensive innovation raises questions about who gets to participate in these advancements. Well-funded organizations can afford to explore new frontiers, while smaller institutions may be constrained to incremental improvements that prioritize efficiency over novelty. Balancing the need for experimentation with the realities of resource availability is a key challenge for the field.</p>
</section>
<section id="sec-efficient-ai-efficiencycreativity-constraint-1ae1" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-efficiencycreativity-constraint-1ae1">Efficiency-Creativity Constraint</h4>
<p>Efficiency-focused design often requires adhering to strict constraints, such as reducing model size, energy consumption, or latency. While these constraints can drive ingenuity, they can also limit the scope of what researchers and engineers are willing to explore. For instance, edge computing applications often demand ultra-compact models, leading to a narrow focus on compression techniques rather than entirely new approaches to machine learning on constrained devices.</p>
<p>At the same time, the drive for efficiency can have a positive impact on innovation. Constraints force researchers to think creatively, leading to the development of new methods that maximize performance within tight resource budgets. Techniques like NAS and attention mechanisms arose, in part, from the need to balance performance and efficiency, demonstrating that innovation and efficiency can coexist when approached thoughtfully.</p>
</section>
<section id="sec-efficient-ai-striking-balance-d49a" class="level4">
<h4 class="anchored" data-anchor-id="sec-efficient-ai-striking-balance-d49a">Striking a Balance</h4>
<p>The tension between innovation and efficiency highlights the need for a balanced approach to system design and research priorities. Organizations and researchers must recognize when it is appropriate to prioritize efficiency and when to embrace the risks of experimentation. For instance, applied systems for real-world deployment may demand strict efficiency constraints, while exploratory research labs can focus on pushing boundaries without immediate concern for resource optimization.</p>
<p>Ultimately, the relationship between innovation and efficiency is not adversarial but complementary. Efficient systems create the foundation for scalable, practical applications, while resource-intensive experimentation drives the breakthroughs that redefine what is possible. Balancing these priorities ensures that machine learning continues to evolve while remaining accessible, impactful, and sustainable.</p>
<div id="quiz-question-sec-efficient-ai-broader-challenges-fe2d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the concept of diminishing returns in the context of ML optimization?</p>
<ol type="a">
<li>A single optimization algorithm can outperform all others across every problem.</li>
<li>Optimization efforts always lead to proportional improvements in system performance.</li>
<li>Initial improvements in ML systems require less effort and resources compared to later stages.</li>
<li>Optimization can continue indefinitely without any increase in complexity or cost.</li>
</ol></li>
<li><p>Explain how the No Free Lunch (NFL) theorems relate to ML optimization and the implications for system design.</p></li>
<li><p>True or False: Achieving extreme efficiency in ML systems can lead to over-optimization, resulting in wasted resources and reduced adaptability.</p></li>
<li><p>In the context of equity in ML efficiency, which of the following is a significant barrier to democratizing access to AI capabilities?</p>
<ol type="a">
<li>The high cost and resource requirements of training state-of-the-art models.</li>
<li>The availability of open-source datasets and models.</li>
<li>The universal applicability of optimization algorithms.</li>
<li>The rapid advancements in Tiny ML technologies.</li>
</ol></li>
<li><p>Discuss the tension between efficiency and innovation in ML system design and provide an example of how this tension might manifest in practice.</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-broader-challenges-fe2d" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="fallacies-and-pitfalls" class="level2">
<h2 class="anchored" data-anchor-id="fallacies-and-pitfalls">Fallacies and Pitfalls</h2>
<p>Efficiency in AI systems involves complex trade-offs between multiple competing objectives—accuracy, latency, memory usage, energy consumption, and cost—that often pull in different directions. The mathematical elegance of scaling laws can create false confidence about predictable optimization paths, while the diverse requirements of deployment contexts from cloud to edge create misconceptions about universal efficiency strategies.</p>
<p><strong>Fallacy:</strong> <em>Efficiency optimizations always improve system performance across all metrics.</em></p>
<p>This misconception leads teams to apply efficiency techniques without understanding their trade-offs and side effects. Optimizing for computational efficiency might degrade accuracy, improving memory efficiency could increase latency, and reducing model size often requires more complex training procedures. Efficiency gains in one dimension frequently create costs in others that may be unacceptable for specific deployment scenarios. Effective efficiency optimization requires careful analysis of which metrics matter most for the target application and acceptance that some performance aspects will necessarily be sacrificed.</p>
<p><strong>Pitfall:</strong> <em>Assuming that scaling laws predict efficiency requirements linearly across all model sizes.</em></p>
<p>Teams often extrapolate efficiency requirements based on scaling law relationships without considering the breakdown points where these laws no longer apply. Scaling laws provide useful guidance for moderate increases in model size, but they fail to account for emergent behaviors, architectural constraints, and infrastructure limitations that appear at extreme scales. Applying scaling law predictions beyond their validated ranges can lead to wildly inaccurate resource estimates and deployment failures. Successful efficiency planning requires understanding both the utility and limits of scaling law frameworks.</p>
<p><strong>Fallacy:</strong> <em>Edge deployment efficiency requirements are simply scaled-down versions of cloud requirements.</em></p>
<p>This belief assumes that edge deployment is merely cloud deployment with smaller models and less computation. Edge environments introduce qualitatively different constraints including real-time processing requirements, power consumption limits, thermal management needs, and connectivity variability. Optimization strategies that work in cloud environments often fail catastrophically in edge contexts. Edge efficiency requires different approaches that prioritize predictable performance, energy efficiency, and robust operation under varying conditions rather than peak throughput.</p>
<p><strong>Pitfall:</strong> <em>Focusing on algorithmic efficiency while ignoring system-level efficiency factors.</em></p>
<p>Many practitioners optimize algorithmic complexity metrics like FLOPs or parameter counts without considering how these improvements translate to actual system performance. Real system efficiency depends on memory access patterns, data movement costs, hardware utilization characteristics, and software stack overhead that may not correlate with theoretical complexity metrics. A model with fewer parameters might still perform worse in practice due to irregular memory access patterns or poor hardware mapping. Comprehensive efficiency optimization requires measuring and optimizing actual system performance rather than relying solely on algorithmic complexity indicators.</p>
</section>
<section id="sec-efficient-ai-summary-66bb" class="level2">
<h2 class="anchored" data-anchor-id="sec-efficient-ai-summary-66bb">Summary</h2>
<p>Efficiency has emerged as a fundamental design principle that transforms how we approach machine learning systems, moving beyond simple performance optimization toward comprehensive resource stewardship. This chapter revealed how scaling laws provide empirical insights into the relationship between model performance and computational resources, establishing efficiency not as a constraint but as a strategic advantage that enables broader accessibility, sustainability, and innovation. The interdependencies between model, compute, and data efficiency create a complex landscape where decisions in one dimension cascade throughout the entire system, requiring a holistic perspective that balances trade-offs across the complete machine learning pipeline.</p>
<p>The practical challenges of designing efficient systems highlight the critical importance of context-aware decision making, where deployment environments shape efficiency priorities. Cloud systems leverage abundant resources for scalability and throughput, while edge deployments optimize for real-time performance within strict power constraints, and Tiny ML applications push the boundaries of what’s achievable with minimal resources. These diverse requirements demand sophisticated strategies including end-to-end co-design, automated optimization tools, and careful prioritization of efficiency dimensions based on operational constraints. The emergence of scaling law breakdowns and the tension between innovation and efficiency underscore that optimal system design requires navigating not just technical trade-offs but also broader considerations of equity, sustainability, and long-term impact.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Efficiency is a strategic enabler that democratizes access to AI capabilities across diverse deployment contexts</li>
<li>Scaling laws provide predictive frameworks for resource allocation, but their limits reveal opportunities for architectural innovation</li>
<li>Trade-offs between model, compute, and data efficiency are interconnected and context-dependent, requiring holistic optimization strategies</li>
<li>Automation tools and end-to-end co-design approaches can transform efficiency constraints into opportunities for system synergy</li>
</ul>
</div>
</div>
<p>Understanding these efficiency principles establishes the foundation for the specific optimization techniques explored in subsequent chapters, where quantization, pruning, and knowledge distillation provide concrete tools for achieving the efficiency goals outlined here. As machine learning systems continue scaling in complexity and reach, the principles of efficient design will remain essential for creating systems that are not only performant but also sustainable, accessible, and aligned with broader societal goals of responsible AI development.</p>


<div id="quiz-question-sec-efficient-ai-summary-66bb" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a key challenge in achieving efficiency in machine learning systems?</p>
<ol type="a">
<li>Balancing model complexity with computational resources</li>
<li>Maximizing data throughput at all costs</li>
<li>Ignoring scaling laws to focus on immediate performance</li>
<li>Prioritizing innovation over sustainability</li>
</ol></li>
<li><p>Explain how understanding the interdependencies between algorithmic, compute, and data dimensions can lead to more efficient ML systems.</p></li>
<li><p>True or False: The principles of efficiency in ML systems primarily focus on short-term performance gains.</p></li>
<li><p>In machine learning systems, understanding the empirical foundations of ________ is crucial for efficient resource utilization.</p></li>
<li><p>In a production system, how might you apply the concept of efficiency to address equity concerns in access to compute and data?</p></li>
</ol>
<p><a href="#quiz-answer-sec-efficient-ai-summary-66bb" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-efficient-ai-overview-6f6a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a trade-off when deploying ML models on edge devices?</strong></p>
<ol type="a">
<li>Increased model accuracy with higher energy consumption.</li>
<li>Reduced model size for lower power usage but decreased accuracy.</li>
<li>Higher latency with improved model complexity.</li>
<li>Increased processing speed with larger model size.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Reduced model size for lower power usage but decreased accuracy. This trade-off is common in edge devices where power and processing resources are limited.</p>
<p><em>Learning Objective</em>: Understand the trade-offs involved in deploying ML models on edge devices.</p></li>
<li><p><strong>Explain why balancing model size and accuracy is crucial for autonomous vehicles.</strong></p>
<p><em>Answer</em>: Balancing model size and accuracy in autonomous vehicles is crucial because it ensures real-time processing and decision-making within the power constraints of edge devices in vehicles. For example, a smaller model may fit the low-power requirements but could sacrifice some accuracy, impacting the vehicle’s ability to make precise decisions. This balance is important because it affects both safety and performance.</p>
<p><em>Learning Objective</em>: Analyze the importance of trade-offs in model deployment for autonomous vehicles.</p></li>
<li><p><strong>What is a potential benefit of deploying ML models in cloud-based systems?</strong></p>
<ol type="a">
<li>Decreased latency and energy consumption.</li>
<li>Lower environmental impact due to less energy usage.</li>
<li>Reduced hardware costs for portable devices.</li>
<li>Ability to use more complex models for improved accuracy.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Ability to use more complex models for improved accuracy. Cloud-based systems can handle higher model complexity, which often results in better accuracy, although it may increase latency and energy consumption.</p>
<p><em>Learning Objective</em>: Identify the benefits and trade-offs of deploying ML models in cloud-based systems.</p></li>
<li><p><strong>How do efficient ML systems contribute to sustainability?</strong></p>
<p><em>Answer</em>: Efficient ML systems contribute to sustainability by reducing energy consumption and carbon emissions, which aligns with ethical and ecological responsibilities. For example, deploying models that require less computational power helps lower the environmental impact of data centers. This is important because it supports the sustainable growth of technology while minimizing its ecological footprint.</p>
<p><em>Learning Objective</em>: Understand the relationship between ML system efficiency and sustainability.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-overview-6f6a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-ai-scaling-laws-a043" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the relationship between model performance and resource allocation according to scaling laws?</strong></p>
<ol type="a">
<li>Performance improves linearly with increased resources.</li>
<li>Performance remains constant regardless of resources.</li>
<li>Performance improves as a power function of resource allocation.</li>
<li>Performance decreases with increased resources.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Performance improves as a power function of resource allocation. This is correct because scaling laws indicate that model performance follows a power-law relationship with resources like model size, dataset size, and computational budget.</p>
<p><em>Learning Objective</em>: Understand the fundamental principle of scaling laws in relation to resource allocation.</p></li>
<li><p><strong>Explain how scaling laws can inform decisions about resource allocation in machine learning system design.</strong></p>
<p><em>Answer</em>: Scaling laws provide a quantitative framework to analyze how model performance varies with resource allocation, guiding decisions on optimal resource distribution. For example, they help determine the balance between model size and dataset size for a given compute budget, ensuring efficient use of resources. This is important because it allows for strategic planning and cost-effective scaling in system design.</p>
<p><em>Learning Objective</em>: Apply scaling laws to make informed decisions about resource allocation in ML system design.</p></li>
<li><p><strong>What is a potential consequence of unbalanced scaling in machine learning systems?</strong></p>
<ol type="a">
<li>Improved performance without additional resources.</li>
<li>Increased data diversity.</li>
<li>Reduced computational requirements.</li>
<li>Overfitting due to excessive model size.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Overfitting due to excessive model size. This is because unbalanced scaling, such as increasing model size without adequate data, can lead to overfitting where the model learns noise instead of general patterns.</p>
<p><em>Learning Objective</em>: Identify the risks associated with unbalanced scaling in ML systems.</p></li>
<li><p><strong>True or False: According to scaling laws, increasing model size always leads to improved performance.</strong></p>
<p><em>Answer</em>: False. This is false because while increasing model size can improve performance, it must be balanced with sufficient data and compute resources to avoid diminishing returns and inefficiencies.</p>
<p><em>Learning Objective</em>: Challenge the misconception that increasing model size alone guarantees better performance.</p></li>
<li><p><strong>In a production system with limited computational resources, how might you apply the concept of scaling laws to optimize performance?</strong></p>
<p><em>Answer</em>: In a resource-constrained production system, scaling laws can guide the optimal allocation of available resources. For instance, by analyzing scaling curves, one can determine the most effective balance between model size and dataset size to maximize performance without exceeding computational limits. This ensures efficient use of resources while maintaining acceptable accuracy levels.</p>
<p><em>Learning Objective</em>: Apply scaling laws to optimize resource allocation in production ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-ai-scaling-laws-a043" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-pillars-ai-efficiency-c024" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the role of algorithmic efficiency in AI systems?</strong></p>
<ol type="a">
<li>Maximizing performance by increasing computational resources.</li>
<li>Developing new hardware to support AI computations.</li>
<li>Collecting and storing large amounts of data for training.</li>
<li>Optimizing algorithms to perform well within given resource constraints.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Optimizing algorithms to perform well within given resource constraints. Algorithmic efficiency focuses on designing and refining algorithms to maximize performance without excessive resource use. Options A, C, and D relate to compute and data efficiency, not algorithmic efficiency.</p>
<p><em>Learning Objective</em>: Understand the concept of algorithmic efficiency and its importance in AI systems.</p></li>
<li><p><strong>True or False: Compute efficiency primarily focuses on reducing the energy consumption and optimizing the use of hardware resources in AI systems.</strong></p>
<p><em>Answer</em>: True. Compute efficiency involves strategies to minimize energy usage and optimize hardware utilization, ensuring that AI systems are scalable and sustainable.</p>
<p><em>Learning Objective</em>: Recognize the primary focus of compute efficiency in AI systems.</p></li>
<li><p><strong>Explain how data efficiency can impact the scalability of machine learning systems.</strong></p>
<p><em>Answer</em>: Data efficiency impacts scalability by reducing the amount of data needed for training, thus lowering computational demands and storage requirements. For example, using techniques like data augmentation and active learning can achieve high model performance with less data. This is important because it enables scalable systems even in data-limited environments.</p>
<p><em>Learning Objective</em>: Analyze the role of data efficiency in enhancing the scalability of machine learning systems.</p></li>
<li><p><strong>Order the following pillars of AI efficiency based on their primary focus: (1) Algorithmic Efficiency, (2) Compute Efficiency, (3) Data Efficiency.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Algorithmic Efficiency, (3) Data Efficiency, (2) Compute Efficiency. Algorithmic efficiency focuses on optimizing algorithms, data efficiency on maximizing information from data, and compute efficiency on optimizing hardware and energy use.</p>
<p><em>Learning Objective</em>: Differentiate between the primary focuses of the three pillars of AI efficiency.</p></li>
<li><p><strong>In a production system with limited computational resources, what trade-offs might you consider when applying the three pillars of AI efficiency?</strong></p>
<p><em>Answer</em>: In a production system, trade-offs might include balancing model complexity (algorithmic efficiency) with available hardware (compute efficiency) and data volume (data efficiency). For example, model compression can reduce resource use but may affect accuracy. It’s important to optimize across all pillars to maintain performance while minimizing costs. This is crucial for deploying efficient and scalable systems.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in applying AI efficiency pillars in resource-constrained environments.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-pillars-ai-efficiency-c024" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-system-efficiency-3cf1" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best defines ‘Machine Learning System Efficiency’?</strong></p>
<ol type="a">
<li>Maximizing algorithmic complexity to improve model accuracy.</li>
<li>Ensuring models are only deployed on high-performance hardware.</li>
<li>Focusing solely on reducing data requirements for training models.</li>
<li>Optimizing systems to minimize computational, memory, and energy demands while maintaining performance.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Optimizing systems to minimize computational, memory, and energy demands while maintaining performance. This definition encompasses the holistic approach to system efficiency across algorithmic, compute, and data dimensions.</p>
<p><em>Learning Objective</em>: Understand the comprehensive definition of system efficiency in ML systems.</p></li>
<li><p><strong>Explain how algorithmic efficiency can enhance both compute and data efficiency in a machine learning system.</strong></p>
<p><em>Answer</em>: Algorithmic efficiency enhances compute efficiency by reducing computational demands through streamlined models, leading to faster and more cost-effective inference. It also reduces data efficiency by requiring less data for training, avoiding over-parameterization, and focusing on essential patterns. For example, compact models like MobileNets can be deployed on resource-constrained devices, minimizing computational and data requirements.</p>
<p><em>Learning Objective</em>: Analyze the interdependencies between algorithmic, compute, and data efficiencies.</p></li>
<li><p><strong>What is a potential trade-off when optimizing for compute efficiency in ML systems?</strong></p>
<ol type="a">
<li>Increased model complexity and reduced scalability.</li>
<li>Decreased energy consumption but increased data redundancy.</li>
<li>Improved hardware utilization but potential compromise on model accuracy.</li>
<li>Enhanced data efficiency but increased computational overhead.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Improved hardware utilization but potential compromise on model accuracy. Optimizing compute efficiency often involves using streamlined models or hardware accelerators, which might limit the complexity and accuracy of models.</p>
<p><em>Learning Objective</em>: Identify trade-offs involved in optimizing compute efficiency.</p></li>
<li><p><strong>In a production system, how might you apply the concept of system efficiency to improve sustainability?</strong></p>
<p><em>Answer</em>: In a production system, applying system efficiency involves optimizing algorithmic, compute, and data dimensions to reduce energy consumption and resource usage. For instance, using energy-efficient hardware and streamlined models can minimize environmental impact while maintaining performance. This is important because it aligns AI development with sustainability goals, reducing the carbon footprint and operational costs.</p>
<p><em>Learning Objective</em>: Apply system efficiency principles to enhance sustainability in real-world ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-system-efficiency-3cf1" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the trade-off between model complexity and compute resources in ML systems?</strong></p>
<ol type="a">
<li>Complex models require less computational power and memory.</li>
<li>Simplified models always outperform complex models in accuracy.</li>
<li>Complex models can achieve higher accuracy but demand more computational resources.</li>
<li>Model complexity has no impact on computational resources.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Complex models can achieve higher accuracy but demand more computational resources. This is correct because complex models capture intricate patterns but require significant power and memory, especially in resource-constrained environments. Options A, B, and D are incorrect as they misrepresent the relationship between model complexity and computational demands.</p>
<p><em>Learning Objective</em>: Understand the trade-off between model complexity and computational resources.</p></li>
<li><p><strong>Explain how energy efficiency and real-time performance create trade-offs in autonomous vehicle systems.</strong></p>
<p><em>Answer</em>: Energy efficiency and real-time performance often conflict in autonomous vehicles. Real-time systems need high-performance hardware to process data quickly, ensuring safety and responsiveness. However, this increases energy consumption, challenging the goal of minimizing resource use. For example, using GPUs for real-time data processing consumes significant energy. This trade-off is crucial in balancing performance and efficiency in such applications.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between energy efficiency and real-time performance in practical ML applications.</p></li>
<li><p><strong>In Tiny ML deployments, a balance must be struck between model complexity and ________ to ensure efficient operation.</strong></p>
<p><em>Answer</em>: compute resources. In Tiny ML deployments, limited compute resources necessitate a balance with model complexity to achieve efficient operation.</p>
<p><em>Learning Objective</em>: Recognize the constraints and trade-offs in resource-limited ML deployments.</p></li>
<li><p><strong>Order the following efficiency trade-offs based on their impact on system design: (1) Model complexity vs.&nbsp;compute resources, (2) Energy efficiency vs.&nbsp;real-time performance, (3) Data size vs.&nbsp;model generalization.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Model complexity vs.&nbsp;compute resources, (2) Energy efficiency vs.&nbsp;real-time performance, (3) Data size vs.&nbsp;model generalization. This order reflects the progression from fundamental trade-offs impacting resource allocation to those affecting system responsiveness and data handling.</p>
<p><em>Learning Objective</em>: Understand the hierarchy and impact of different efficiency trade-offs in ML system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-efficiency-tradeoffs-challenges-946d" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-managing-tradeoffs-80e8" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the primary focus of Tiny ML deployments?</strong></p>
<ol type="a">
<li>Extreme resource efficiency</li>
<li>Real-time performance and compute efficiency</li>
<li>Scalability and throughput</li>
<li>Algorithmic efficiency for cloud environments</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Extreme resource efficiency. Tiny ML deployments prioritize extreme resource efficiency due to hardware and energy limitations, focusing on compact models and minimal compute power.</p>
<p><em>Learning Objective</em>: Understand the primary focus and constraints of Tiny ML deployments.</p></li>
<li><p><strong>True or False: In Mobile ML deployments, compute efficiency is prioritized over model accuracy to extend battery life.</strong></p>
<p><em>Answer</em>: True. This is true because Mobile ML deployments prioritize compute efficiency to minimize energy consumption and extend battery life, even if it means sacrificing some model accuracy.</p>
<p><em>Learning Objective</em>: Recognize the trade-offs in Mobile ML deployments between compute efficiency and model accuracy.</p></li>
<li><p><strong>Explain how ‘Test-Time Compute’ can enhance system adaptability in dynamic environments.</strong></p>
<p><em>Answer</em>: ‘Test-Time Compute’ enhances adaptability by allowing systems to dynamically adjust computational resources during inference. For example, a cloud-based video analysis system might use low-compute models for standard tasks but allocate more resources for critical events. This flexibility helps balance latency and accuracy, optimizing performance as needed.</p>
<p><em>Learning Objective</em>: Analyze the benefits and challenges of implementing ‘Test-Time Compute’ in dynamic environments.</p></li>
<li><p><strong>In Edge ML systems, low-latency processing is prioritized to ensure ________ operation.</strong></p>
<p><em>Answer</em>: reliable. Low-latency processing is prioritized in Edge ML systems to ensure reliable operation, especially in applications like autonomous vehicles.</p>
<p><em>Learning Objective</em>: Identify the primary efficiency focus in Edge ML systems.</p></li>
<li><p><strong>Order the following strategies based on their role in managing trade-offs: (1) Co-design, (2) Automation, (3) Contextual Prioritization.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Contextual Prioritization, (1) Co-design, (2) Automation. Contextual prioritization sets the stage by identifying key efficiency dimensions, co-design aligns components for holistic efficiency, and automation optimizes these dimensions.</p>
<p><em>Learning Objective</em>: Understand the sequence and interrelation of strategies for managing trade-offs in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-managing-tradeoffs-80e8" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-efficiencyfirst-mindset-39e7" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the importance of an end-to-end perspective in designing efficient ML systems?</strong></p>
<ol type="a">
<li>It focuses solely on optimizing model architecture.</li>
<li>It balances trade-offs across the entire system to avoid shifting inefficiencies.</li>
<li>It ensures efficiency by considering each stage of the pipeline independently.</li>
<li>It prioritizes hardware deployment over data collection.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. It balances trade-offs across the entire system to avoid shifting inefficiencies. This is correct because an end-to-end perspective ensures that decisions at one stage do not negatively impact others, maintaining overall system efficiency. Options A, B, and D focus on isolated components rather than the holistic approach required.</p>
<p><em>Learning Objective</em>: Understand the significance of an end-to-end perspective in achieving system efficiency.</p></li>
<li><p><strong>Explain how the efficiency needs of cloud-based systems differ from those of edge deployments.</strong></p>
<p><em>Answer</em>: Cloud-based systems prioritize scalability and throughput, leveraging abundant compute resources, while edge deployments focus on real-time performance and energy efficiency, operating under strict hardware constraints. For example, a cloud-based recommendation engine must handle large-scale data processing, whereas a smartphone app must optimize for low latency and battery life. This is important because it highlights the need for tailored efficiency strategies based on deployment context.</p>
<p><em>Learning Objective</em>: Analyze the differing efficiency requirements of cloud and edge systems.</p></li>
<li><p><strong>True or False: In a production ML system, efficiency is a static goal that remains constant regardless of the deployment context.</strong></p>
<p><em>Answer</em>: False. Efficiency is a dynamic process shaped by the application context, requiring different priorities and trade-offs depending on whether the system is deployed in the cloud, on mobile devices, or in Tiny ML applications. This is important because it emphasizes the need for adaptive design strategies.</p>
<p><em>Learning Objective</em>: Challenge the misconception that efficiency is a static goal in ML systems.</p></li>
<li><p><strong>In a smartphone speech recognition system, balancing model size and latency is crucial to provide a seamless user experience without ________ the device’s battery.</strong></p>
<p><em>Answer</em>: draining. Balancing model size and latency ensures efficient operation without excessive power consumption.</p>
<p><em>Learning Objective</em>: Understand the trade-offs involved in optimizing ML systems for mobile devices.</p></li>
<li><p><strong>How might you apply an efficiency-first mindset when transitioning a research prototype to a production system?</strong></p>
<p><em>Answer</em>: Applying an efficiency-first mindset involves optimizing the prototype for practical constraints, such as reducing model size through pruning or quantization, and aligning with deployment hardware capabilities. For example, retraining the model on targeted datasets can improve efficiency without sacrificing performance. This is important because it ensures the system is viable for real-world use while maintaining effectiveness.</p>
<p><em>Learning Objective</em>: Apply efficiency-first principles to the transition from research to production in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-efficiencyfirst-mindset-39e7" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-broader-challenges-fe2d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the concept of diminishing returns in the context of ML optimization?</strong></p>
<ol type="a">
<li>A single optimization algorithm can outperform all others across every problem.</li>
<li>Optimization efforts always lead to proportional improvements in system performance.</li>
<li>Initial improvements in ML systems require less effort and resources compared to later stages.</li>
<li>Optimization can continue indefinitely without any increase in complexity or cost.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Initial improvements in ML systems require less effort and resources compared to later stages. This is correct because diminishing returns imply that as systems become more refined, each additional improvement requires exponentially more effort, time, or resources, while delivering smaller benefits.</p>
<p><em>Learning Objective</em>: Understand the concept of diminishing returns in ML optimization.</p></li>
<li><p><strong>Explain how the No Free Lunch (NFL) theorems relate to ML optimization and the implications for system design.</strong></p>
<p><em>Answer</em>: The NFL theorems state that no single optimization algorithm can outperform all others across every possible problem, implying that optimization is highly problem-specific. In system design, this means that practitioners must choose optimization techniques based on the specific context and goals of their ML system, rather than relying on a one-size-fits-all solution. This is important because it highlights the need for tailored approaches to achieve efficient and effective ML systems.</p>
<p><em>Learning Objective</em>: Analyze the implications of the NFL theorems on ML system design.</p></li>
<li><p><strong>True or False: Achieving extreme efficiency in ML systems can lead to over-optimization, resulting in wasted resources and reduced adaptability.</strong></p>
<p><em>Answer</em>: True. This is true because over-optimization can result in systems that are overly specialized to their initial conditions, complicating future updates or adjustments to changing requirements. It highlights the importance of balancing efficiency with practicality and sustainability.</p>
<p><em>Learning Objective</em>: Evaluate the risks of over-optimization in ML systems.</p></li>
<li><p><strong>In the context of equity in ML efficiency, which of the following is a significant barrier to democratizing access to AI capabilities?</strong></p>
<ol type="a">
<li>The high cost and resource requirements of training state-of-the-art models.</li>
<li>The availability of open-source datasets and models.</li>
<li>The universal applicability of optimization algorithms.</li>
<li>The rapid advancements in Tiny ML technologies.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. The high cost and resource requirements of training state-of-the-art models. This is correct because these costs and resource needs are often concentrated in well-funded organizations, creating inequities in who can leverage efficiency gains.</p>
<p><em>Learning Objective</em>: Identify barriers to equitable access in ML efficiency.</p></li>
<li><p><strong>Discuss the tension between efficiency and innovation in ML system design and provide an example of how this tension might manifest in practice.</strong></p>
<p><em>Answer</em>: The tension between efficiency and innovation arises because efficiency often prioritizes established techniques that are known to work, potentially stifling exploration of new, resource-intensive ideas. For example, while optimizing neural networks through pruning or quantization improves efficiency, it may discourage the development of novel architectures that could lead to breakthroughs. This tension is crucial because balancing these priorities ensures that ML continues to evolve while remaining accessible and impactful.</p>
<p><em>Learning Objective</em>: Understand the relationship between efficiency and innovation in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-broader-challenges-fe2d" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-efficient-ai-summary-66bb" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a key challenge in achieving efficiency in machine learning systems?</strong></p>
<ol type="a">
<li>Balancing model complexity with computational resources</li>
<li>Maximizing data throughput at all costs</li>
<li>Ignoring scaling laws to focus on immediate performance</li>
<li>Prioritizing innovation over sustainability</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Balancing model complexity with computational resources is a key challenge because it involves understanding trade-offs to maintain efficiency while scaling. Options B, C, and D do not align with the principles of efficient ML system design.</p>
<p><em>Learning Objective</em>: Understand the challenges involved in balancing different dimensions of efficiency in ML systems.</p></li>
<li><p><strong>Explain how understanding the interdependencies between algorithmic, compute, and data dimensions can lead to more efficient ML systems.</strong></p>
<p><em>Answer</em>: Understanding these interdependencies allows for designing systems that optimize resource utilization, align with operational contexts, and meet long-term objectives. For example, improving algorithmic efficiency can reduce compute demands, enabling better scalability. This is important because it ensures that systems are both performant and sustainable.</p>
<p><em>Learning Objective</em>: Analyze the impact of interdependencies on system efficiency and design.</p></li>
<li><p><strong>True or False: The principles of efficiency in ML systems primarily focus on short-term performance gains.</strong></p>
<p><em>Answer</em>: False. The principles of efficiency focus on long-term sustainability and scalability, ensuring that systems can grow and adapt while maintaining performance and resource balance.</p>
<p><em>Learning Objective</em>: Challenge misconceptions about the focus of efficiency principles in ML systems.</p></li>
<li><p><strong>In machine learning systems, understanding the empirical foundations of ________ is crucial for efficient resource utilization.</strong></p>
<p><em>Answer</em>: scaling laws. Scaling laws reveal how model performance scales with resources, guiding efficient resource utilization.</p>
<p><em>Learning Objective</em>: Recall the role of scaling laws in resource utilization and system efficiency.</p></li>
<li><p><strong>In a production system, how might you apply the concept of efficiency to address equity concerns in access to compute and data?</strong></p>
<p><em>Answer</em>: By optimizing resource allocation and leveraging efficient models, systems can be designed to minimize barriers to access. For instance, using scalable architectures that require less compute can democratize access to AI technologies. This is important because it ensures that AI benefits are widely distributed.</p>
<p><em>Learning Objective</em>: Apply efficiency concepts to address equity in ML system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-efficient-ai-summary-66bb" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/training/training.html" class="pagination-link" aria-label="AI Training">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">AI Training</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/optimizations/optimizations.html" class="pagination-link" aria-label="Model Optimizations">
        <span class="nav-page-text">Model Optimizations</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>