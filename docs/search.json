[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Systems",
    "section": "",
    "text": "Preface\nWelcome to Machine Learning Systems, your gateway to the fast-paced world of machine learning (ML) systems. This book is an extension of the CS249r course at Harvard University, taught by Prof. Vijay Janapa Reddi, and is the result of a collaborative effort involving students, professionals, and the broader community of AI practitioners.\nWe’ve created this open-source book to demystify the process of building efficient and scalable ML systems. Our goal is to provide a comprehensive guide that covers the principles, practices, and challenges of developing robust ML pipelines for deployment. This isn’t a static textbook—it’s a living, evolving resource designed to keep pace with advancements in the field.\nAs a living and breathing resource, this book is a continual work in progress, reflecting the ever-evolving nature of machine learning systems. Advancements in the ML landscape drive our commitment to keeping this resource updated with the latest insights, techniques, and best practices. We warmly invite you to join us on this journey by contributing your expertise, feedback`, and ideas.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#global-outreach",
    "href": "index.html#global-outreach",
    "title": "Machine Learning Systems",
    "section": "Global Outreach",
    "text": "Global Outreach\nThank you to all our readers and visitors. Your engagement with the material keeps us motivated.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-we-wrote-this-book",
    "href": "index.html#why-we-wrote-this-book",
    "title": "Machine Learning Systems",
    "section": "Why We Wrote This Book",
    "text": "Why We Wrote This Book\nWhile there are plenty of resources that focus on the algorithmic side of machine learning, resources on the systems side of things are few and far between. This gap inspired us to create this book—a resource dedicated to the principles and practices of building efficient and scalable ML systems.\nOur vision for this book and its broader mission is deeply rooted in the transformative potential of AI and the need to make AI education globally accessible to all. To learn more about the inspiration behind this project and the values driving its creation, we encourage you to read the Author’s Note.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#want-to-help-out",
    "href": "index.html#want-to-help-out",
    "title": "Machine Learning Systems",
    "section": "Want to Help Out?",
    "text": "Want to Help Out?\nThis is a collaborative project, and your input matters! If you’d like to contribute, check out our contribution guidelines. Feedback, corrections, and new ideas are welcome. Simply file a GitHub issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#whats-next",
    "href": "index.html#whats-next",
    "title": "Machine Learning Systems",
    "section": "What’s Next?",
    "text": "What’s Next?\nIf you’re ready to dive deeper into the book’s structure, learning objectives, and practical use, visit the About the Book section for more details.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/frontmatter/foreword.html",
    "href": "contents/frontmatter/foreword.html",
    "title": "Author’s Note",
    "section": "",
    "text": "AI is bound to transform the world in profound ways, much like computers and the Internet revolutionized every aspect of society in the 20th century. From systems that generate creative content to those driving breakthroughs in drug discovery, AI is ushering in a new era—one that promises to be even more transformative in its scope and impact. But how do we make it accessible to everyone?\nWith its transformative power comes an equally great responsibility for those who access it or work with it. Just as we expect companies to wield their influence ethically, those of us in academia bear a parallel responsibility: to share our knowledge openly, so it benefits everyone—not just a select few. This conviction inspired the creation of this book—an open-source resource aimed at making AI education, particularly in AI engineering, and systems, inclusive, and accessible to everyone from all walks of life.\nMy passion for creating, curating, and editing this content has been deeply influenced by landmark textbooks that have profoundly shaped both my academic and personal journey. Whether I studied them cover to cover or drew insights from key passages, these resources fundamentally shaped the way I think. I reflect on the books that guided my path: works by Turing Award winners such as David Patterson and John Hennessy—pioneers in computer architecture and system design—and foundational research papers by luminaries like Yann LeCun, Geoffrey Hinton, and Yoshua Bengio. In some small part, my hope is that this book will inspire students to chart their own unique paths.\nI am optimistic about what lies ahead for AI. It has the potential to solve global challenges and unlock creativity in ways we have yet to imagine. To achieve this, however, we must train the next generation of AI engineers and practitioners—those who can transform novel AI algorithms into working systems that enable real-world application. This book is a step toward curating the material needed to build the next generation of AI engineers who will transform today’s visions into tomorrow’s reality.\nThis book is a work in progress, but knowing that even one learner benefits from its content motivates me to continually refine and expand it. To that end, if there’s one thing I ask of readers, it’s this: please show your support by starring the GitHub repository here. Your star ⭐ reflects your belief in this mission—not just to me, but to the growing global community of learners, educators, and practitioners. This small act is more than symbolic—it amplifies the importance of making AI education accessible.\nI am a student of my own writing, and every chapter of this book has taught me something new—thanks to the numerous people who have played, and continue to play, an important role in shaping this work. Professors, students, practitioners, and researchers contributed by offering suggestions, sharing expertise, identifying errors, and proposing improvements. Every interaction, whether a detailed critique or a simple correction from a GitHub contributor, has been a lesson in itself. These contributions have not only refined the material but also deepened my understanding of how knowledge grows through collaboration. This book is, therefore, not solely my work; it is a shared endeavor, reflecting the collective spirit of those dedicated to sharing their knowledge and effort.\nThis book is dedicated to the loving memory of my father. His passion for education, endless curiosity, generosity in sharing knowledge, and unwavering commitment to quality challenge me daily to strive for excellence in all I do. In his honor, I extend this dedication to teachers and mentors everywhere, whose efforts and guidance transform lives every day. Your selfless contributions remind me to persevere.\nLast but certainly not least, this work would not be possible without the unwavering support of my wonderful wife and children. Their love, patience, and encouragement form the foundation that enables me to pursue my passion and bring this work to life. For this, and so much more, I am deeply grateful.\n— Prof. Vijay Janapa Reddi",
    "crumbs": [
      "Author's Note"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html",
    "href": "contents/frontmatter/about/about.html",
    "title": "About the Book",
    "section": "",
    "text": "Overview",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-overview-6ca0",
    "href": "contents/frontmatter/about/about.html#sec-book-overview-6ca0",
    "title": "About the Book",
    "section": "",
    "text": "Purpose of the Book\nWelcome to this collaborative textbook. It originated as part of the CS249r: Tiny Machine Learning course that Prof. Vijay Janapa Reddi teaches at Harvard University.\nThe goal of this book is to provide a resource for educators and learners seeking to understand the principles and practices of machine learning systems. This book is continually updated to incorporate the latest insights and effective teaching strategies with the intent that it remains a valuable resource in this fast-evolving field. So please check back often!\n\n\nContext and Development\nThe book originated as a collaborative effort with contributions from students, researchers, and practitioners. While maintaining its academic rigor and real-world applicability, it continues to evolve through regular updates and careful curation to reflect the latest developments in machine learning systems.\n\n\nWhat to Expect\nThis textbook explores the foundational principles, practical workflows, and critical challenges of building and deploying machine learning systems. Starting with foundational concepts, it progresses through engineering principles, examines operational considerations for deploying AI systems, and concludes by reflecting on the societal and technological implications of machine learning. Throughout the book, you’ll find quizzes as quick self-checks to reinforce your understanding and test your knowledge at key learning milestones.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-learning-goals-4827",
    "href": "contents/frontmatter/about/about.html#sec-book-learning-goals-4827",
    "title": "About the Book",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nKey Learning Outcomes\nThis book is structured with Bloom’s Taxonomy in mind, which defines six levels of learning, ranging from foundational knowledge to advanced creative thinking:\n\n\n\n\n\n\nFigure 1: Bloom’s Taxonomy (2021 edition).\n\n\n\n\nRemembering: Recalling basic facts and concepts.\nUnderstanding: Explaining ideas or processes.\nApplying: Using knowledge in new situations.\nAnalyzing: Breaking down information into components.\nEvaluating: Making judgments based on criteria and standards.\nCreating: Producing original work or solutions.\n\n\n\nLearning Objectives\nThis book supports readers in:\n\nUnderstanding Fundamentals: Explain the foundational principles of machine learning, including theoretical underpinnings and practical applications.\nAnalyzing System Components: Evaluate the critical components of AI systems and their roles within various architectures.\nDesigning Workflows: Outline workflows for developing machine learning systems, from data collection to deployment.\nOptimizing Models: Apply methods to enhance performance, such as hyperparameter tuning and regularization.\nEvaluating Ethical Implications: Analyze societal impacts and address potential biases in AI systems.\nExploring Applications: Investigate real-world use cases across diverse domains.\nConsidering Deployment Challenges: Address security, scalability, and maintainability in real-world systems.\nEnvisioning Future Trends: Reflect on emerging challenges and technologies in machine learning.\nSelf-Assessment Through Quizzes: Use quick self-check quizzes throughout the book to test understanding and identify areas for further study.\n\n\n\nAI Learning Companion\nThroughout this resource, you’ll find SocratiQ—an AI learning assistant designed to enhance your learning experience. Inspired by the Socratic method of teaching, SocratiQ combines interactive quizzes, personalized assistance, and real-time feedback to help you reinforce your understanding and create new connections. As part of our experiment with Generative AI technologies, SocratiQ encourages critical thinking and active engagement with the material.\nSocratiQ is still a work in progress, and we welcome your feedback to make it better. For more details about how SocratiQ works and how to get the most out of it, visit the AI Learning Companion page.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-use-book-bd7c",
    "href": "contents/frontmatter/about/about.html#sec-book-use-book-bd7c",
    "title": "About the Book",
    "section": "How to Use This Book",
    "text": "How to Use This Book\n\nBook Structure\nThe book is organized into four main parts, each building on the previous one:\n\nThe Essentials (Chapters 1-4) Core principles, components, and architectures that underpin machine learning systems.\nEngineering Principles (Chapters 5-13) Covers workflows, data engineering, optimization strategies, and operational challenges in system design.\nAI Best Practice (Chapters 14-18) Focuses on key considerations for deploying AI systems in real-world environments, including security, privacy, robustness, and sustainability.\nClosing Perspectives (Chapter 19-20) Synthesizes key lessons and explores emerging trends shaping the future of ML systems.\n\n\n\nSuggested Reading Paths\n\nBeginners: Start with The Essentials to build a strong conceptual base before progressing to other parts.\nPractitioners: Focus on Engineering Principles and AI in Practice for hands-on, real-world insights.\nResearchers: Dive into AI in Practice and Closing Perspectives to explore advanced topics and societal implications.\n\n\n\nModular Design\nThe book is modular, allowing readers to explore chapters independently or sequentially. Each chapter includes supplementary resources:\n\nQuizzes as quick self-checks to test comprehension and reinforce key concepts.\nSlides summarizing key concepts.\nVideos providing in-depth explanations.\nExercises reinforcing understanding.\nLabs offering practical, hands-on experience.\n\nWhile several of these resources are still a work in progress, we believe it’s better to share valuable insights and tools as they become available rather than wait for everything to be perfect. After all, progress is far more important than perfection, and your feedback will help us improve and refine this resource over time.\nAdditionally, we try to reuse and build upon the incredible work created by amazing experts in the field, rather than reinventing everything from scratch. This philosophy reflects the fundamental essence of community-driven learning: collaboration, sharing knowledge, and collectively advancing our understanding.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-transparency-collaboration-db62",
    "href": "contents/frontmatter/about/about.html#sec-book-transparency-collaboration-db62",
    "title": "About the Book",
    "section": "Transparency and Collaboration",
    "text": "Transparency and Collaboration\nThis book is a community-driven project, with content generated collaboratively by numerous contributors over time. The content creation process may have involved various editing tools, including generative AI technology. As the main author, editor, and curator, Prof. Vijay Janapa Reddi maintains human oversight to ensure the content is accurate and relevant.\nHowever, no one is perfect, and inaccuracies may still exist. Your feedback is highly valued, and we encourage you to provide corrections or suggestions. This collaborative approach is crucial for maintaining high-quality information and making it globally accessible.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-copyright-licensing-4f29",
    "href": "contents/frontmatter/about/about.html#sec-book-copyright-licensing-4f29",
    "title": "About the Book",
    "section": "Copyright and Licensing",
    "text": "Copyright and Licensing\nThis book is open-source and developed collaboratively through GitHub. Unless otherwise stated, this work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).\nContributors retain copyright over their individual contributions, dedicated to the public domain or released under the same open license as the original project. For more information on authorship and contributions, visit the GitHub repository.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/about/about.html#sec-book-join-community-c104",
    "href": "contents/frontmatter/about/about.html#sec-book-join-community-c104",
    "title": "About the Book",
    "section": "Join the Community",
    "text": "Join the Community\nThis textbook is more than just a resource—it’s an invitation to collaborate and learn together. Engage in community discussions to share insights, tackle challenges, and learn alongside fellow students, researchers, and practitioners.\nWhether you’re a student starting your journey, a practitioner solving real-world challenges, or a researcher exploring advanced concepts, your contributions will enrich this learning community. Introduce yourself, share your goals, and let’s collectively build a deeper understanding of machine learning systems.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "contents/frontmatter/changelog/changelog.html",
    "href": "contents/frontmatter/changelog/changelog.html",
    "title": "Book Changelog",
    "section": "",
    "text": "2025 Changes\nThis Machine Learning Systems textbook is constantly evolving. This changelog automatically records all updates and improvements, helping you stay informed about what’s new and refined.",
    "crumbs": [
      "Book Changelog"
    ]
  },
  {
    "objectID": "contents/frontmatter/changelog/changelog.html#changes",
    "href": "contents/frontmatter/changelog/changelog.html#changes",
    "title": "Book Changelog",
    "section": "",
    "text": "📅 Published on Jul 06, 2025\n\n\n📄 Frontmatter\n\n\nIndex: The frontmatter index has been updated with new hashes that consider section content, section IDs have been revised due to changes in manager code, and section headers have been updated using a script.\nAbout: The about.qmd file has been updated with quizzes for self-assessment, a bug fix in the repair section, an update to the script for section generation, and changes to section IDs for consistent linking. The section headers have also been updated using the script.\nAcknowledgements: The acknowledgements section has been updated with a revised script for section generation, improved section IDs for consistent linking, and new section headers.\nSocraticAI: The machine learning systems textbook has been updated with improvements to the script for section generation, consistent linking through updated section IDs, and the implementation of PDF part summaries. Additionally, the section headers have been updated using the script and a new hash that considers section content has been introduced.\n\n\n\n\n📖 Chapters\n\n\nFoundations: The updates to the “foundations.qmd” file include the implementation of PDF summaries for different parts of the book and the addition of a new section that outlines the organization of the book parts.\nDesign Principles: The updates to the ‘design_principles.qmd’ file include the implementation of PDF part summaries and the addition of a new book part organization.\nBest Practices: The updates to the “best_practices.qmd” file include the implementation of PDF summaries for each part of the book and the addition of a new organization structure for the book parts.\nImpact Outlook: The “impact_outlook.qmd” file has been updated with PDF part summaries and a new organization of the book parts.\nPhD Survival Guide: The resources guide has been relocated to the backmatter and its anchor links have been updated.\nPhD Survival Guide: The “PhD Survival Guide” has been moved from the appendix to the backmatter/resources section.\nChapter 1: Introduction: The introduction chapter of the machine learning systems textbook has been updated with a refined definition of ML systems engineering. New sections and section headers have been added for better navigation and clarity. The chapter now includes auto-generated self-check quizzes, with improved generation and formatting. Quiz metadata has been added to the introduction, and the quiz answers have been corrected and improved. Some quizzes have been added and removed from various chapters during the process. A new callout feature for slides, videos, and exercises has also been introduced.\nChapter 2: ML Systems: The machine learning systems textbook has been updated with the addition of quizzes to the efficient AI and ML systems chapters, including self-check answers. The quizzes are now automatically injected from JSON files, and the logic for quiz insertion and answer extraction has been improved. The section headers and IDs have been updated for consistent linking, and a first major draft of all quizzes has been generated and integrated.\nChapter 3: DL Primer: The DL Primer chapter now includes automatically injected quizzes from JSON files, with improved quiz insertion logic and answer extraction. A major draft of all quizzes has been integrated into the chapter. Section anchors for self-check answers have been added, and “Quiz Answers” have been renamed to “Self-Check Answers”. The chapter also features updated section IDs for consistent linking and new section headers.\nChapter 4: DNN Architectures: The textbook’s “DNN Architectures” chapter has been enhanced with the addition of quizzes, specifically in the “Efficient AI” section, and the introduction of section anchors for self-check answers. The section IDs have been updated for consistent linking, and the “Quiz Answers” section has been renamed to “Self-Check Answers”. Additionally, example Colab notebooks have been removed from the content.\nChapter 5: AI Workflow: The machine learning systems textbook has been updated with the addition of quizzes to the efficient AI chapter, the generation and integration of the first major draft of all quizzes, and the insertion and review of questions. Section headers and IDs have been updated for consistent linking, and the “Quiz Answers” section has been renamed to “Self-Check Answers”.\nChapter 6: Data Engineering: The machine learning systems textbook has been updated with the addition of quizzes in the efficient AI chapter, the inclusion of section anchors for self-check answers, and the renaming of “Quiz Answers” to “Self-Check Answers”. The first major draft of all quizzes has been generated and integrated, and the section IDs have been updated for consistent linking. Additionally, a new hash that considers section content has been implemented.\nChapter 7: AI Frameworks: The textbook has been updated with the addition of quizzes to the efficient AI chapter, a new section on framework components, and a major draft of all quizzes. The script for section generation has also been revised and section IDs have been updated for consistent linking.\nChapter 8: AI Training: The textbook has been updated with the addition of quizzes to the efficient AI chapter, section anchors for self-check answers, and new section headers. The quiz insertion logic and answer extraction have been improved, and “Quiz Answers” have been renamed to “Self-Check Answers”. The first major draft of all quizzes has been generated, integrated, and reviewed. The script for section generation has been updated, and section IDs have been cleaned and updated for consistent linking.\nChapter 9: Efficient AI: The Efficient AI chapter in the machine learning systems textbook has been updated with the addition of quizzes and section anchors for self-check answers. The quizzes have been automatically injected from JSON files and the quiz answers have been renamed to “Self-Check Answers”. The section IDs have been updated for consistent linking and the script for section generation has been improved.\nChapter 10: Model Optimizations: The machine learning systems textbook has been updated with the addition of quizzes in the efficient AI chapter, the generation and integration of a major draft of all quizzes, and the insertion and review of questions. Section anchors for self-check answers have been added, section headers have been updated, and section IDs have been cleaned and updated for consistent linking. A bug in the -repair followed by repair script has been fixed.\nChapter 11: AI Acceleration: The machine learning systems textbook has been updated with the addition of quizzes to the efficient AI chapter, the integration of a first major draft of all quizzes, and the insertion and first review of questions. Section headers and IDs have been updated for consistent linking and better organization. The script for section generation has been improved, and the “Quiz Answers” section has been renamed to “Self-Check Answers”.\nChapter 12: Benchmarking AI: The textbook’s “Benchmarking” chapter has been updated with a major draft of all quizzes, which have been automatically generated and integrated. The quiz answers have been corrected and renamed to “Self-Check Answers”. Additionally, new section headers and anchors have been added for these answers. The “Efficient AI” chapter has also been enhanced with quizzes.\nChapter 13: ML Operations: The machine learning systems textbook has been updated with the addition of quizzes to the efficient AI chapter, the generation and integration of a major draft of all quizzes, and the insertion and review of questions. Section headers and IDs have been updated for consistent linking and improved navigation. The script for section generation has been enhanced, and a new hash considering section content has been implemented.\nChapter 14: On-Device Learning: The on-device learning chapter of the machine learning systems textbook has been enriched with the addition of quizzes, section anchors for self-check answers, and an improved quiz insertion logic. The chapter also underwent a thorough review and correction of quiz answers. In addition, the section headers and IDs have been updated for consistent linking, and a first major draft of all quizzes has been generated and integrated.\nChapter 15: Security & Privacy: The machine learning systems textbook has been updated with the addition of quizzes to the efficient AI chapter, the introduction of section anchors for self-check answers, and the renaming of “Quiz Answers” to “Self-Check Answers”. The first major draft of all quizzes has been generated, integrated, and scripts have been improved. Section headers and IDs have been updated for consistent linking and improved navigation.\nChapter 16: Responsible AI: The textbook’s “Responsible AI” chapter has been updated with the addition of quizzes to the “Efficient AI” section, a major draft of all quizzes has been generated and integrated, and the section IDs have been updated for consistent linking. Additionally, the “Quiz Answers” section has been renamed to “Self-Check Answers”, and the script for section generation has been improved.\nChapter 17: Sustainable AI: The textbook’s “Sustainable AI” chapter has been significantly updated with the addition of quizzes to the “Efficient AI” section, the generation and integration of a first major draft of all quizzes, and the review and insertion of new questions. Section headers and IDs have been updated for consistency, and a typo in a diagram annotation has been corrected.\nChapter 18: Robust AI: The robust AI chapter of the machine learning systems textbook has been updated with the addition of quizzes in the efficient AI section and the generation of a first major draft of all quizzes. Section headers and IDs have been updated for consistency and improved linking. Example Colab notebooks have been removed.\nChapter 19: AI for Good: The AI for Good chapter in the machine learning systems textbook has been updated with the addition of quizzes, section anchors for self-check answers, and new section headers. The quiz insertion logic and answer extraction have been improved, and the “Quiz Answers” section has been renamed to “Self-Check Answers”. The first major draft of all quizzes has been generated, integrated, and reviewed. The section IDs have been updated for consistent linking and the script for section generation has been updated.\nChapter 20: Conclusion: The conclusion section of the machine learning systems textbook has been updated with new section anchors for self-check answers, a bug fix related to the ‘repair’ function, an updated script for section generation, and revised section headers.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Labs Overview: The overview.qmd file in the labs section of the machine learning systems textbook has been updated with a new hash that takes into account the content of the sections, changes to section IDs due to modifications in the manager code, and updates to section headers using a script.\nLab: Lab Setup: The getting_started.qmd file in the labs content has been updated with new hashes that consider section content, changes to section IDs due to updates in manager code, and modifications to section headers using a script.\nLab: Arduino Setup: The setup.qmd file in the Arduino Nicla Vision lab has been updated with a bug fix in the repair function, improvements to the section generation script, consistent linking through updated section IDs, and changes to section headers. The section IDs have also been updated to consider section content and reflect changes to the manager code.\nLab: Arduino Image Classification: The image_classification.qmd file in the Arduino Nicla Vision lab has been updated with added section headers, a bug fix in the repair function, an updated script for section generation, and consistent linking through updated section IDs. The section content has also been considered in the new hash updates.\nLab: Arduino Object Detection: The object detection lab in the Arduino Nicla Vision chapter has undergone updates in the script for section generation, along with updates to section IDs for consistent linking. Additionally, the section headers have been updated using the script, and there have been changes to the manager code which have been reflected in the section IDs. The lab content has also been updated with a new hash that considers section content.\nLab: Arduino Keyword Spotting: The updates to the machine learning systems textbook include a bug fix in the repair section, an update to the script for section generation, consistent linking through updated section IDs, and new changes to the manager code reflected in the section IDs. The section headers have also been updated using the script.\nLab: Arduino Motion Classification: The motion_classification.qmd file in the Arduino Nicla Vision lab has been updated with a revised script for section generation, updated section IDs for consistent linking, and new hashes that consider section content. Additionally, the section headers have been updated using the script, and unnecessary blank lines have been removed.\nLab: XIAO Setup: The setup.qmd file in the xiao_esp32s3 setup lab has been updated to fix a bug in the repair process, incorporate a new hash that takes into account section content, reflect changes to the manager code in the section IDs, and modify section headers using a script.\nLab: XIAO Image Classification: The image_classification.qmd file in the xiao_esp32s3 section has been updated with a bug fix in the repair function, improvements to the section generation script, and updates to section IDs for consistent linking. The hashing method has been revised to consider section content, and the section headers have been updated using the new script.\nLab: XIAO Object Detection: The object detection lab in the machine learning systems textbook has been updated with a bug fix in the repair function, improvements to the section generation script, consistent linking through updated section IDs, and changes to section headers. The updates also include a new hash function that considers section content and adjustments to section IDs due to changes in manager code.\nLab: XIAO Keyword Spotting: The updates include a bug fix in the repair function, improvements to the script for section generation, consistent linking through updated section IDs, and changes to section headers. The update also includes a new hash function that considers section content and changes to the manager code affecting section IDs.\nLab: XIAO Motion Classification: The motion_classification.qmd file in the machine learning systems textbook has been updated with a corrected script for section generation, a new hash that considers section content, and changes to section IDs and headers due to updates in the manager code. A bug in the ‘-repair followed by repair’ function has also been fixed.\nLab: Setup And No Code Apps: The updates include a bug fix in the repair process, improvements to the script for section generation, updates to section IDs for consistent linking, and changes to section headers. The updates also include a new hash function that considers section content and modifications to the manager code.\nLab: Arduino Image Classification: The image_classification.qmd file in the Grove Vision AI V2 lab has been updated with a bug fix in the repair function, improvements to the section generation script, and updates to section IDs for consistent linking. The section headers have also been updated using the script and a new hash function has been implemented that considers section content.\nLab: Raspberry Pi Setup: The setup.qmd file in the raspi setup lab has been updated with a new script for section generation, consistent section IDs for accurate linking, and revised section headers. A bug related to the ‘repair’ function has also been fixed.\nLab: Pi Image Classification: The image classification lab in the Raspberry Pi section has been updated with a revised script for section generation, improved section IDs for consistent linking, and a new hash function that takes into account section content.\nLab: Pi Object Detection: The object detection lab in the Raspberry Pi chapter has been updated with a bug fix in the repair script, improvements to the section generation script, and updates to section IDs for consistent linking. Additionally, the section headers have been updated using the script, and a new hash that considers section content has been implemented.\nLab: Pi Large Language Models: The machine learning systems textbook has been updated with a fixed bug in the repair process, an updated script for section generation, consistent linking through updated section IDs, and a new hash that considers section content. Additionally, changes have been made to the manager code that affect section IDs, and section headers have been updated using the script.\nLab: Pi Vision Language Models: The vlm.qmd file in the raspi lab has been updated with a bug fix in the repair function, improvements to the section generation script, and updates to section IDs for consistent linking. Additionally, the file has been updated with a new hash that considers section content, and section headers have been revised using the updated script.\nLab: Kws Feature Eng: The updates to the machine learning systems textbook include bug fixes in the ‘repair’ section, improvements to the script used for section generation, and updates to section IDs for consistent linking. Additionally, the section headers have been updated using the script, and a new feature has been added that considers section content when generating section IDs.\nLab: Dsp Spectral Features Block: The dsp_spectral_features_block.qmd file has been updated with a bug fix, an updated script for section generation, consistent linking through updated section IDs, and a new hash that considers section content.\nLab: Nicla Vision: The Nicla Vision lab in the Arduino chapter has been updated with a bug fix in the repair function, improvements in section generation script, a new hash function that takes into account section content, changes to section IDs due to manager code updates, and revised section headers.\nLab: Raspi: The Raspberry Pi lab content has been updated with a bug fix in the repair section, modifications in the section generation script, and changes to section IDs and headers as per the new manager code.\nLab: Grove Vision Ai V2: The Grove Vision AI V2 lab content has been updated with a bug fix in the repair function, improvements to the section generation script, and changes to section IDs and headers based on updates to the manager code.\nLab: Xiao Esp32S3: The machine learning systems textbook has been updated with fixes to a bug in the repair function, improvements to the section generation script, a new hash that considers section content, changes to section IDs due to updates in manager code, and modifications to section headers using the script.\nLab: Shared: The shared labs section in the machine learning systems textbook has been updated with a revised heading.\n\n\n\n\n📚 Appendix\n\n\nPhD Survival Guide: The appendix section “PhD Survival Guide” has been updated with new resources, section content, and headers, along with changes to section IDs in accordance with the updated manager code.\n\n\n\n\n\n📅 Published on Jun 10, 2025\n\n\n📄 Frontmatter\n\n\nSocratiqAI: The ‘socratiq.qmd’ file in the AI section of the frontmatter contents has been updated with corrections to minor content errors and possibly some content updates or additions, as indicated by the ‘Update_socratiq’ commit message.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter has been updated with improved text processing, the addition of missing footnotes, and refined language for better clarity and precision.\nChapter 2: ML Systems: The core content of the machine learning systems textbook has been enhanced with the addition of resource sections, the refinement of language for improved clarity and consistency, and the inclusion of new figures. Additionally, the TinyML example callout has been removed.\nChapter 3: DL Primer: The updates to the machine learning systems textbook include the addition of resource sections to the core content, correction of minor content errors, improvements in text processing, updates to chapter 3, addition of missing footnotes, refinement of content for clarity and consistency, clarification of the difference between training and inference, addition of missing figure references and new figures, and initial work on chapters 3 and 4.\nChapter 4: DNN Architectures: The deep learning architectures section has been refined with improved explanations, particularly for CNNs, and the addition of illustrations for data movement patterns. New resource sections have been added to the core content, and figures have been included. The labeling and referencing of code blocks in Chapter 4 have also been updated. Initial work has been done on Chapters 3 and 4.\nChapter 5: AI Workflow: The core content of the machine learning systems textbook has been enhanced with the addition of resource sections, improved text processing in QMD files for better clarity and consistency, and the inclusion of new figures.\nChapter 6: Data Engineering: The data engineering chapter was updated with added resource sections, improved text processing, and clarified figure references. A data pipeline overview diagram was added, and the Mermaid diagram was replaced with TikZ before being restored. A broken web scraping Colab link was removed.\nChapter 7: AI Frameworks: The core content of the machine learning systems textbook has been enhanced with the addition of resource sections, improvements in text processing, updates to chapter 8 on frameworks, and the inclusion of new figures and diagrams.\nChapter 8: AI Training: The core content of the machine learning systems textbook has been enhanced with the addition of resource sections, improved text processing in QMD files, a clarified explanation of activation checkpointing, an updated Chapter 8 on training, and the inclusion of new figures and diagrams.\nChapter 9: Efficient AI: The core content of the efficient AI chapter has been enhanced with the addition of resource sections, improvements in text processing for QMD files, and clarification on compute-optimal scaling frontier and efficiency dimensions in AI scaling. The language in the scaling laws section has been refined for better clarity. The trade-off between efficiency and latency has been further explained. Missing figure references have been added and diagrams have been updated for improved clarity.\nChapter 10: Model Optimizations: The core content of the machine learning systems textbook has been enriched with the addition of resource sections, clarification of AutoML and NAS descriptions, refinement of model optimization techniques documentation, and the inclusion of missing figure references and new figures.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file has been significantly updated with the addition of resource sections to the core content, clarification of placement and allocation definitions, refinement of explanations for improved clarity, correction of code block language for better understanding, an update to the chapter 11 hw acceleration, and the inclusion of new figures and diagrams. Also, the matrix multiplication example has been corrected and the benefits of tiling for AI accelerators have been clarified.\nChapter 12: Benchmarking AI: The benchmarking documentation in the machine learning systems textbook has been updated with added resource sections, improved text processing in QMD files, and enhanced clarity and consistency of text.\nChapter 13: ML Operations: The core content of the machine learning systems textbook has been enhanced with the addition of resource sections, missing footnotes, and figures. The content in chapter 13 and MLOps has been updated for clarity and accuracy. The operations diagram and text have been revised, and table references in ondevice and ops pages have been updated. Redundant “Figure” prefixes have been removed for better readability.\nChapter 14: On-Device Learning: The on-device learning chapter has been updated with added resource sections, clarified adapter-based adaptation equations, refined explanations and concepts, updated table references, and redundant “Figure” prefixes have been removed.\nChapter 15: Security & Privacy: The privacy and security chapter has been significantly updated with the introduction of new sections on trustworthy ML systems, secure model design and deployment, and a case study on traffic sign trickery. The discussions on adversarial attacks, data poisoning, security vulnerabilities, and the relevance of Jeep Cherokee hack and Mirai botnet have been expanded. The explanations of side-channel attacks, model theft, and defenses have been clarified and refined. New diagrams illustrating ML lifecycle threat and threat mitigation flow have been added. The chapter also saw the removal of the terminology section and the addition of updated citations, references, and resource sections.\nChapter 16: Responsible AI: The Responsible AI chapter in the machine learning systems textbook has been extensively updated, with new sections added on governance structures, safety and robustness considerations, privacy architectures, and fairness constraints. The content on responsible AI principles and practices has been refined and expanded, including more detailed discussions on privacy and data governance, safety and robustness, and fairness in machine learning. The chapter now also includes a comparison table for Responsible AI principles, a discussion on design tradeoffs, and elaboration on system explainability considerations. Furthermore, the chapter introduces deployment contexts for responsible AI, clarifies accountability, and provides practical applications of Responsible AI principles. The introduction to Responsible AI and its definition have also been refined.\nChapter 17: Sustainable AI: The sustainable AI section of the machine learning systems textbook has been updated with new resource sections, improved language for clarity and consistency, and the removal of instructions for figure updates.\nChapter 18: Robust AI: The robust AI section of the machine learning systems textbook has been updated with added resource sections, improved explanations for better clarity, the addition of a figure environment for error masking, and refactored content for improved clarity.\nChapter 19: AI for Good: The AI for Good chapter has been updated with added resource sections, improved text processing, and refined content for better clarity.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The updates to the Grove Vision AI V2 section of the machine learning systems textbook include improved documentation, the addition of an Image Classification Lab, and the introduction of a new lab specifically focused on Grove Vision AI v2.\nLab: Arduino Image Classification: The image_classification.qmd file in the Grove Vision AI V2 lab has been updated with new sections, rewrites, additional examples, and changes in figures to enhance the understanding of image classification in the context of machine learning systems.\nLab: Arduino Object Detection: A new lab titled “Grove Vision AI v2” has been added to the object detection section, focusing on object detection techniques.\nLab: Arduino Object Detection: The object detection section of the Grove Vision AI V2 lab in the Seeed Xiao ESP32S3 chapter has been updated with new content, examples, and figures.\nLab: Grove Vision Ai V2: The Grove Vision AI V2 lab has been added and its documentation has been improved.\nLab: Grove Vision Ai V2: The file update includes new sections on the Seeed Xiao ESP32S3 and Grove Vision AI V2, with added examples and figure changes to enhance understanding of the concepts.\nLab: Lab Setup: The “Getting Started” lab in the machine learning systems textbook has been updated with revisions to general lab files.\nLab: Labs Overview: The General Lab Files in the machine learning systems textbook have been updated.\nLab: Setup And No Code Apps: The Grove Vision AI V2 lab has been added, with significant improvements made to the documentation, including enhanced clarity and a corrected description of inference latency.\nLab: Setup And No Code Apps: The setup and no-code applications chapter for the Grove Vision AI V2 in the Seeed Xiao ESP32S3 lab section has been updated with new content. — — — —\nLab: XIAO Image Classification: The image classification lab in the Seeed Xiao ESP32S3 section has been updated to correct a typo.\n\n\n\n\n\n📅 Published on May 14, 2025\n\n\n📖 Chapters\n\n\nChapter 14: On-Device Learning: The on-device learning content has been refactored and clarified, with updates made to chapter 14. — — — —\n\n\n\n\n\n📅 Published on May 04, 2025\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter of the machine learning systems textbook has been updated.\nChapter 2: ML Systems: The chapter 2 of the machine learning systems textbook has been updated, and a script has been added to find any missing references.\nChapter 3: DL Primer: The dimension order for W^L has been swapped and a script has been added to find any missing references in the ‘dl_primer’ chapter of the machine learning systems textbook.\nChapter 4: DNN Architectures: The chapter on deep neural network architectures has been updated, and a script has been added to identify any missing references.\nChapter 5: AI Workflow: The “Workflow” chapter in the machine learning systems textbook has been updated with new sections, revised content, additional examples, and changes to figures.\nChapter 6: Data Engineering: The data engineering chapter has been updated with new sections, revised content, additional examples, and changes to figures for enhanced understanding of the topic.\nChapter 7: AI Frameworks: The chapter on machine learning frameworks in the core contents has been updated, with corrections made to existing content.\nChapter 8: AI Training: The chapter on training in the machine learning systems textbook has been updated with minor issue fixes and improvements in label checking.\nChapter 9: Efficient AI: The chapter 9 on efficient AI in the machine learning systems textbook has been updated, including a fix from Bravo, potentially indicating corrections or improvements in the content.\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has been updated with new sections, rewrites, additional examples, and figure changes.\nChapter 11: AI Acceleration: The hardware acceleration discussion and explanation have been refined, the explanation of hardware specialization has been enhanced, the AI compute primitives explanation has been clarified, and certain acronyms have been corrected.\nChapter 12: Benchmarking AI: The “Purpose” heading has been renamed to “Motivation”, the explanation of benchmarking metrics and power measurements has been clarified, and a script has been added to find any missing references.\nChapter 13: ML Operations: The core concepts and case studies in the MLOps section have been expanded, the styling of TikZ figures has been consolidated, a script to find any missing references has been added, and a missing exercise reference has been fixed and removed.\nChapter 14: On-Device Learning: The on-device learning chapter in the machine learning systems textbook has been significantly updated with new sections on Federated Learning, challenges and limitations, and design constraints. It now includes a more detailed exploration of on-device model adaptation strategies and learning with less data, as well as explanations of experience replay, data compression, and privacy concerns in federated learning. The chapter also provides a clearer definition of on-device learning, guidance on system design, and a conclusion. Additional footnotes and citations have been added for further clarification and reference.\nChapter 15: Security & Privacy: The updates to the ‘privacy_security.qmd’ file include fixes to minor issues and enhancements to label checking procedures.\nChapter 17: Sustainable AI: The updates include the consolidation of TikZ figure styling and the addition of a script to identify any missing references in the Sustainable AI chapter.\nChapter 18: Robust AI: The updates to the ‘Robust AI’ chapter include fixes to minor issues, improvements to label checking, addition of a script to find any missing references, and resolution of a missing package issue.\nChapter 19: AI for Good: The accuracy of the PlantVillage Nuru footnote has been updated and a script has been added to find any missing references in the AI for Good chapter.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The image classification section in the Arduino Nicla Vision lab has been updated with revised instructions, refreshed text and images, and improved documentation for better clarity.\nLab: Arduino Keyword Spotting: The ‘kws.qmd’ file in the ‘nicla_vision’ section of the Arduino labs has been updated with fixes.\nLab: Arduino Motion Classification: The ‘motion_classification.qmd’ file in the Arduino Nicla Vision lab section has been updated with fixes, potentially improving the clarity or accuracy of the content.\nLab: Arduino Object Detection: The object detection section in the Arduino Nicla Vision lab has been updated with clarified instructions and enhanced object detection content.\nLab: Arduino Setup: The instructions in the Nicla Setup section of the Arduino lab in the machine learning systems textbook have been updated and clarified.\nLab: Dsp Spectral Features Block: The LABS part_shared in the dsp_spectral_features_block.qmd file has been updated.\nLab: Kws Feature Eng: The LABS part_shared in the kws_feature_eng.qmd file has been updated.\nLab: Pi Image Classification: The ‘image_classification.qmd’ file in the ‘raspi’ lab section of the machine learning systems textbook has been updated, featuring changes in the LABS part_raspi.\nLab: Pi Large Language Models: The LABS section under the ‘raspi’ directory has been updated, likely featuring modifications to the lab exercises, example additions, or changes to figures and diagrams.\nLab: Pi Object Detection: The LABS section of the Raspberry Pi object detection chapter has been updated.\nLab: Pi Vision Language Models: The VLM lab guide in the Raspberry Pi section of the LABS has been refactored for improved clarity.\nLab: Raspberry Pi Setup: The LABS section in the ‘part_raspi’ has been updated with changes to the setup instructions for Raspberry Pi in the ‘setup.qmd’ file.\nLab: Raspi: The LABS section of the part_raspi file has been updated.\nLab: Xiao Esp32S3: The LABS part 2 section of the seeed xiao esp32s3 chapter has been updated. — — — —\nLab: XIAO Image Classification: The updates in the ‘image_classification.qmd’ file for the ‘seeed/xiao_esp32s3’ lab in the machine learning systems textbook include revisions in LABS part 2 and various fixes.\nLab: XIAO Keyword Spotting: The LABS part 2 section of the seeed xiao esp32s3 chapter has been updated with new content, including revisions, additional examples, and potential alterations to figures.\nLab: XIAO Motion Classification: The “motion_classification.qmd” file in the “seeed/xiao_esp32s3/motion_classification” lab has been updated with modifications to the second part of the LABS section.\nLab: XIAO Object Detection: The ‘object_detection.qmd’ file in the ‘seeed/xiao_esp32s3/object_detection’ section of the LABS part 2 has been updated, potentially including changes such as new sections, rewrites, added examples, or figure modifications.\nLab: XIAO Setup: The LABS part 2 section for the seeed xiao esp32s3 has been updated.\n\n\n\n\n\n📅 Published on Mar 25, 2025\n\n\n📄 Frontmatter\n\n\nAbout: The ‘About’ section in the frontmatter has been updated based on vale testing results.\nAcknowledgements: The contributors list in the acknowledgements section has been updated.\nChapter: Old Sus Ai: The ‘old_sus_ai.qmd’ file in the ‘sustainable_ai’ section of the core contents has been updated with improvements and the removal of outdated content.\nForeword: The foreword section of the machine learning systems textbook has been updated based on feedback from vale testing.\nSocratiq: The updates to the machine learning systems textbook include the correction of all broken links in the contents/frontmatter/ai/socratiq.qmd file.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction.qmd file in the core introduction section underwent a review of section headers after an auto-update, a first pass-through for edits, and an update to chapter 1. Additionally, an unfinished sentence was completed.\nChapter 2: ML Systems: The machine learning systems textbook has been updated with the addition of footnotes, the correction of missing references, and the removal of short section headers in the machine learning systems chapter. Additionally, the second chapter of the book has been updated.\nChapter 3: DL Primer: The dl_primer chapter was updated with a new definition, footnotes were revised, and the section headers were reviewed after an auto-update.\nChapter 4: DNN Architectures: The updates to the machine learning systems textbook include a pass on footnotes, fixing all broken links, and correcting a missing figure reference in the DNN Architectures section.\nChapter 5: AI Workflow: The updates to the machine learning systems textbook include the addition of a new definition, updated references, and revisions made in the first pass through, along with updates to chapter 5 on workflow.\nChapter 6: Data Engineering: The data engineering chapter was updated with a first pass through and a manual merge of updates from the sidenotes branch. The text was massaged a bit with a new definition added. All broken links were fixed, and a missing reference to a figure was corrected. The section headers were reviewed and tweaked after an auto-update. The mermaid diagram was also enabled.\nChapter 7: AI Frameworks: A figure for ONNX has been added, broken links have been fixed, and updates from the sidenotes branch have been manually merged into the chapter on frameworks.\nChapter 8: AI Training: The updates to the training chapter include a review and potential modifications of section headers, a first pass through for general improvements, updates from the sidenotes branch manually merged, and corrections to missing figure references.\nChapter 9: Efficient AI: The updated version of the machine learning systems textbook includes new sections on Scaling Laws and a wrap-up section. There have been additions to the Scaling Laws discussion, some figures related to epoch AI have been inserted, and the text has been edited for clarity. The chapter headers have also been reviewed and renamed where necessary.\nChapter 10: Model Optimizations: The chapter on optimizations has been updated and revised, with corrections to broken links and missing figure references, as well as the addition of footnotes. There have also been updates from the sidenotes branch manually merged into the main content.\nChapter 11: AI Acceleration: The updates to the machine learning systems textbook include a review and update of acronyms, the removal of redundant table and figure references, an update to the caption and text information about a figure, and the manual merge of updates from the sidenotes branch.\nChapter 12: Benchmarking AI: The updates to the benchmarking chapter include fixing all broken links, reviewing and potentially revising section headers after an automatic update, and correcting a missing figure reference.\nChapter 13: ML Operations: The machine learning systems textbook has undergone significant updates, particularly in the ‘ops.qmd’ file. Changes include the addition of new references and case studies, updates to the learning objectives, and tweaks to the case study. The MLOps chapter and its key components section have been updated, along with the DevOps part and the introduction of historical context. The core components have been structured into groups, and the overview has been revised. Feedback was incorporated to swap the bullet list for a narrative style and to replace embedded Ops with Operational design principles.\nChapter 14: On-Device Learning: The updates to the on-device learning chapter include a first pass through for content, and the repair of all previously broken links.\nChapter 15: Security & Privacy: The updates to the ‘privacy_security.qmd’ file include a first pass through and the fixing of all broken links.\nChapter 16: Responsible AI: The updates to the responsible_ai.qmd file include a first pass through, and the fixing of all broken links.\nChapter 17: Sustainable AI: The chapter on Sustainable AI has been updated with a new section discussing Jevon’s paradox, including the addition of a related plot. The caption has also been updated, and some broken links have been fixed. Improvements have been made throughout the chapter, and footnotes and acronyms have been added for clarity. The table formatting has been revised for better readability.\nChapter 18: Robust AI: The updates to the “Robust AI” chapter include an improved purpose and updated learning objectives, along with revisions to the conclusion and footnotes. New text has been added, including an introduction paragraph, a discussion on shifts, and sections on poisoning attacks and transient faults. The attacks section has been updated, and minor tweaks have been made to the detection and mitigation sections. The overview has been improved with an added introduction, and the real-world section has been enhanced. All broken links have been fixed and dead section links removed.\nChapter 19: AI for Good: The updates to the “AI for Good” chapter include the addition of new definitions, a review and correction of all broken links, and modifications to the footnotes for consistency.\nChapter 20: Conclusion: The conclusion section of the machine learning systems textbook has been revised and unnecessary sections have been removed in the first pass through.\nChapter: Generative Ai: The generative AI chapter in the machine learning systems textbook has been initially reviewed and updated.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The update does not provide specific content-level changes such as new sections, rewrites, example additions, or figure changes. The commit message indicates a formatting adjustment, which is not a meaningful content change.\nLab: Arduino Keyword Spotting: The update does not provide specific content-level changes such as new sections, rewrites, example additions, or figure changes. It only mentions a formatting adjustment, changing the markdown styles.\nLab: Arduino Motion Classification: The updates to the motion_classification.qmd file in the Arduino Nicla Vision lab include fixing all broken links.\nLab: Arduino Setup: The setup.qmd file in the Arduino Nicla Vision lab has been updated with all broken links fixed.\nLab: Dsp Spectral Features Block: No content-level changes were made in this update, only spelling corrections and markdown style adjustments.\nLab: Kws Feature Eng: The update does not provide any specific content-level changes such as new sections, rewrites, example additions, or figure changes. The commit message only indicates a formatting adjustment, changing from one markdown style to another.\nLab: Pi Large Language Models: The update does not provide any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes, but rather involves a cleaning of Markdown styles.\nLab: Pi Object Detection: The updates to the object_detection.qmd file in the raspi lab include the correction of all previously broken links.\nLab: Pi Vision Language Models: The updates to the contents/labs/raspi/vlm/vlm.qmd file include the correction of all previously broken links.\nLab: Raspberry Pi Setup: The setup.qmd file in the raspi setup lab has been updated to correct spelling errors identified by a codespell check.\nLab: XIAO Image Classification: The image_classification.qmd file in the seeed/xiao_esp32s3 lab has undergone spelling corrections due to a codespell check, but no significant content-level changes were made.\nLab: XIAO Keyword Spotting: No meaningful content-level changes were made in this update. The changes were primarily focused on spelling corrections and markdown style adjustments.\nLab: XIAO Motion Classification: The update does not provide specific content-level changes to the ‘motion_classification.qmd’ file in the ‘xiao_esp32s3’ section of the ‘seeed’ labs, as the commit message only mentions a cleaning of Markdown styles.\n\n\n\n\n📚 Appendix\n\n\nPhd Survival Guide: The “phd_survival_guide.qmd” file in the appendix of the machine learning systems textbook has been updated to fix all broken links. — — — —\n\n\n\n\n\n📅 Published on Mar 03, 2025\n\n\n📄 Frontmatter\n\n\nAbout: The update does not contain any meaningful content-level changezew sections, rewrites, example additions, or figure changes. It only includes formatting adjustments.\nAcknowledgements: The acknowledgements section has been updated with a revised list of contributors.\nSocratiq: The changelog does not indicate any content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were primarily focused on formatting and linting corrections.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were primarily focused on formatting adjustments and lint fixes.\nChapter 2: ML Systems: The changelog does not indicate any content-level changes such as new sections, rewrites, example additions, or figure changes. All the updates pertain to formatting and linting fixes.\nChapter 3: DL Primer: The chapter 3 of the machine learning systems textbook has been updated.\nChapter 4: DNN Architectures: The chapter on deep neural network architectures has been updated.\nChapter 5: AI Workflow: The meaningful content-level changes include the removal of redundant definitions with the help of a script, corrections made to the text, and updates made to chapter 5 in the workflow.qmd file.\nChapter 6: Data Engineering: The chapter 6 of the machine learning systems textbook has been updated in the file data_engineering.qmd.\nChapter 7: AI Frameworks: The updates to the machine learning systems textbook include the addition of model and data parallelism images for distributed execution in the frameworks chapter, the removal of redundant definitions, and the deletion of an unnecessary log file from chapter 7.\nChapter 8: AI Training: The core training chapter of the machine learning systems textbook has been updated with the addition of descriptions for single and multi GPU systems, and redundant definitions such as GPUs have been removed.\nChapter 9: Efficient AI: The ‘Efficient AI’ chapter in the machine learning systems textbook has been updated to remove redundant definitions with the help of a script, and some labels have been corrected.\nChapter 10: Model Optimizations: The optimizations chapter of the machine learning systems textbook has been significantly updated with new sections on structured optimization, model representation, numerics, and a conclusion. There have been additions of various figures, including those for KD, PTQ + QAT, and sparsity visual, as well as updates to the range for fp and section 2. The chapter also includes new content on LTH + iterative pruning + calibration, and redundant definitions and acronyms have been removed.\nChapter 11: AI Acceleration: The hardware acceleration chapter has been significantly updated with additions including a section on host accelerators, a discussion on numerics, and a section on hybrid mapping. There have been major updates to the matrix/vector section and data movement section. New figures have been added, such as the NVSwitch image for multi-GPU, a transistor count plot, and a models vs. memory bandwidth plot. The chapter also includes improvements to the compiler section and mapping strategies section.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with a replacement of a PNG image with tikz code, the removal of an exercise, the fixing of a broken reference, the addition of an image for datacentric AI, and the removal of redundant definitions. A citation related to benchmarking has also been corrected.\nChapter 13: ML Operations: The meaningful content-level changes include the removal of redundant definitions such as GPUs with the assistance of a script.\nChapter 14: On-Device Learning: The redundant definitions such as GPUs have been removed from the on-device learning section.\nChapter 15: Security & Privacy: The redundant definitions such as GPUs have been removed from the privacy and security chapter.\nChapter 16: Responsible AI: The changelog does not indicate any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes to the “Responsible AI” chapter of the machine learning systems textbook. The updates were primarily focused on formatting and linting fixes.\nChapter 17: Sustainable AI: The sustainable AI section in the machine learning systems textbook has been updated to remove redundant definitions with the assistance of a script.\nChapter 18: Robust AI: The redundant definitions such as (GPUs) have been removed from the robust AI section of the machine learning systems textbook.\nChapter 19: AI for Good: The redundant definitions such as GPUs in the AI for Good chapter have been removed with the aid of a script.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The arduino/nicla_vision LABS part in the image_classification.qmd file has been updated.\nLab: Arduino Keyword Spotting: The arduino/nicla_vision LABS section has been updated.\nLab: Arduino Motion Classification: The arduino/nicla_vision LABS part in the motion_classification.qmd file has been updated.\nLab: Arduino Object Detection: The Arduino/Nicla Vision lab part in the machine learning systems textbook has been updated, with potential modifications to sections, examples, or figures related to object detection.\nLab: Arduino Setup: The arduino/nicla_vision LABS section has been updated.\nLab: Kws Feature Eng: The changelog does not indicate any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The updates are primarily related to fixing formatting and linting issues.\nLab: Labs Overview: The changelog does not indicate any content-level changes such as new sections, rewrites, example additions, or figure changes in the machine learning systems textbook. The updates were related to linting fixes and undoing incorrect linting of QMD files.\nLab: Nicla Vision: The arduino/nicla_vision LABS section of the machine learning systems textbook has been updated. — — — —\nLab: Pi Vision Language Models: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. All changes were related to formatting and linting issues.\n\n\n\n\n\n📅 Published on Feb 08, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\nSocratiq: The socratiq.qmd file in the AI section of the frontmatter contents has been updated with a link to the Socratiq research paper. — — — —\n\n\n\n\n\n📅 Published on Feb 07, 2025\n\n\n📄 Frontmatter\n\n\nAbout: The precheck process has been updated to only run on qmd and bib files in the ‘About’ section.\nAcknowledgements: The acknowledgements section has been updated multiple times to include new contributors to the machine learning systems textbook.\nChangelog: The changelog for the machine learning systems textbook has been automatically updated multiple times, with significant trimming of the text and an update to include it in the frontmatter.\nIndex: The precheck has been updated to only run on qmd and bib files in the frontmatter section of the machine learning systems textbook.\nSocratiq: The precheck has been updated to only run on qmd and bib files in the ‘socratiq.qmd’ section of the AI chapter.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter has been updated with a summary, and the code has been revised to remove a library in use, now utilizing only _quarto.yml. Additionally, the precheck has been updated to run exclusively on qmd and bib files.\nChapter 2: ML Systems: The ml_systems.qmd file has been updated with corrections and a precheck has been implemented to only run on qmd and bib files.\nChapter 3: DL Primer: The precheck has been updated to only run on qmd and bib files in the “Deep Learning Primer” chapter.\nChapter 4: DNN Architectures: The dnn_architectures.qmd file in the core section has been updated with a revised precheck system that now only runs on qmd and bib files.\nChapter 5: AI Workflow: The precheck process has been updated to only run on qmd and bib files in the workflow chapter of the machine learning systems textbook.\nChapter 6: Data Engineering: The precheck process has been updated to only run on qmd and bib files in the data engineering chapter.\nChapter 7: AI Frameworks: The core/frameworks section of the machine learning systems textbook has been updated to remove an in-use library from the code and to modify the precheck to only run on qmd and bib files.\nChapter 8: AI Training: The diagram in the training section has been updated and its formatting issues have been fixed, and an unused library from the code has been removed. The precheck has also been updated to only run on qmd and bib files.\nChapter 9: Efficient AI: The Efficient AI chapter has been updated with the addition of R code for the first time, along with enabling debugging for the R code. There were also changes made to address feedback received from Jeff.\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has been updated with a revised precheck that now only runs on qmd and bib files.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the core section has been updated to modify the precheck process to only run on qmd and bib files.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been significantly updated with a focus on the Power section. New content includes the addition of MLPerf Power Trends, a new plot for power ranges, and graphs to motivate benchmarking. The text has been tweaked for better flow and the model and data have been updated based on feedback. Additionally, the FastML science graph has been added to the benchmarking challenges chapter.\nChapter 13: ML Operations: The precheck process has been updated to only run on qmd and bib files in the core/ops/ops.qmd section of the machine learning systems textbook.\nChapter 14: On-Device Learning: The on-device learning chapter has been updated to include a new precheck system that only runs on qmd and bib files.\nChapter 15: Security & Privacy: The privacy and security chapter in the machine learning systems textbook has been updated to include a precheck that only runs on qmd and bib files.\nChapter 16: Responsible AI: The precheck process has been updated to only run on qmd and bib files in the responsible AI section of the machine learning systems textbook.\nChapter 17: Sustainable AI: The precheck process has been updated to only run on qmd and bib files in the sustainable_ai chapter.\nChapter 18: Robust AI: The robust AI chapter in the machine learning systems textbook has been updated with a refined precheck process that now only runs on qmd and bib files.\nChapter 19: AI for Good: The significant updates include the removal of an in-use library from the code, caching of PNG instead of a remote URL, an update to the precheck to only run on qmd + bib files, and updates to Chapter 19.\nChapter 20: Conclusion: The conclusion chapter has been updated to include a precheck that only runs on qmd and bib files.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The precheck process for the ‘image_classification.qmd’ file in the Arduino Nicla Vision lab has been updated to only run on qmd and bib files.\nLab: Arduino Keyword Spotting: The precheck has been updated to only run on qmd and bib files in the Arduino Nicla Vision KWS lab.\nLab: Arduino Motion Classification: The ‘motion_classification.qmd’ file in the Arduino Nicla Vision lab section has been updated with a revised precheck system that now only operates on qmd and bib files.\nLab: Arduino Setup: The setup instructions for the Nicla Vision lab in the Arduino section have been updated, with modifications made to the precheck to only run on qmd and bib files.\nLab: Dsp Spectral Features Block: The precheck process for the dsp_spectral_features_block.qmd file has been updated to only run on qmd and bib files.\nLab: Kws Feature Eng: The precheck process in the ‘kws_feature_eng.qmd’ file has been updated to only run on qmd and bib files.\nLab: Lab Setup: The ‘getting_started.qmd’ lab in the machine learning systems textbook has been updated with a revised precheck function that now only runs on qmd and bib files.\nLab: Labs Overview: The precheck process has been updated to only run on qmd and bib files in the ‘overview’ lab of the machine learning systems textbook.\nLab: Pi Image Classification: The precheck has been updated to only run on qmd and bib files in the image_classification.qmd file of the raspi lab in the machine learning systems textbook.\nLab: Pi Large Language Models: The precheck process in the ‘llm.qmd’ file under the ‘raspi’ lab in the machine learning systems textbook has been updated to only run on qmd and bib files.\nLab: Pi Object Detection: The ‘object_detection.qmd’ file in the ‘raspi’ lab of the machine learning systems textbook has been updated to modify the precheck process to only run on .qmd and .bib files.\nLab: Pi Vision Language Models: The precheck has been updated to only run on qmd and bib files in the ‘raspi/vlm’ lab section of the machine learning systems textbook.\nLab: Raspberry Pi Setup: The precheck process in the ‘setup.qmd’ file has been updated to only run on qmd and bib files.\nLab: Raspi: The precheck process has been updated to only run on qmd and bib files in the raspi lab section of the machine learning systems textbook.\nLab: Shared: The precheck has been updated to only run on qmd and bib files in the shared.qmd file of the labs section.\nLab: XIAO Image Classification: The precheck process for the ‘image_classification.qmd’ file in the ‘seeed/xiao_esp32s3/image_classification’ lab has been updated to only run on qmd and bib files.\nLab: XIAO Keyword Spotting: The precheck process in the ‘kws.qmd’ file under the ‘seeed/xiao_esp32s3/kws’ lab has been updated to only run on .qmd and .bib files.\nLab: XIAO Motion Classification: The precheck process in the ‘motion_classification.qmd’ file under the ‘seeed/xiao_esp32s3/motion_classification’ lab has been updated to only run on qmd and bib files.\nLab: XIAO Object Detection: The precheck process for the ‘object_detection.qmd’ file in the ‘seeed/xiao_esp32s3/object_detection’ lab has been updated to only run on qmd and bib files.\nLab: XIAO Setup: The precheck process for the ‘setup.qmd’ file in the ‘xiao_esp32s3’ lab section has been updated to only run on qmd and bib files.\n\n\n\n\n📚 Appendix\n\n\nPhd Survival Guide: The precheck process has been updated to only run on qmd and bib files in the PhD survival guide appendix. — — — —\n\n\n\n\n\n📅 Published on Feb 02, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated with the addition of new contributors.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The titles of all callout sections in the Introduction chapter have been updated to title block format.\nChapter 2: ML Systems: The callout titles in the “ML Systems” chapter have been updated to a title block format.\nChapter 3: DL Primer: The callout titles in the “Deep Learning Primer” chapter have been updated to the title block format.\nChapter 4: DNN Architectures: The “DNN Architectures” section of the machine learning systems textbook has been updated to correct errors and the callout titles have been transformed into title block format.\nChapter 5: AI Workflow: The callout titles in the “workflow.qmd” section of the machine learning systems textbook have been updated to a title block format.\nChapter 6: Data Engineering: The callout titles in the “Data Engineering” chapter have been updated to a title block format.\nChapter 7: AI Frameworks: The tikz package usage has been relocated from the main body to the header file in the ‘frameworks.qmd’ section of the machine learning systems textbook.\nChapter 8: AI Training: The core training chapter has been updated with minor adjustments to AI training content, including an update to figure 8.8 and the addition of a few new diagrams. There were also fixes made to the Python code and figure sizing issues were addressed.\nChapter 9: Efficient AI: The ‘efficient_ai.qmd’ file in the ‘core/efficient_ai’ section of the machine learning systems textbook has been updated with corrections to the bibliography and modifications to all callout titles, transforming them into title block format.\nChapter 10: Model Optimizations: The callout titles in the ‘Optimizations’ chapter have been updated to title block format.\nChapter 11: AI Acceleration: The titles of all callouts in the “Hardware Acceleration” chapter have been updated to a title block format.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been significantly updated with improvements to the learning objectives, a reorganization of content, and an enhanced definition of benchmarking. The historical section and case study have been updated, and the data and model section has been shortened with added metrics content. Several missing or broken references and figures have been fixed or updated, and some figures from a previous version have been reintroduced.\nChapter 13: ML Operations: The ops.qmd file in the core/ops section of the machine learning systems textbook has been updated with all callout titles now converted into title block format.\nChapter 14: On-Device Learning: The ‘On-Device Learning’ chapter has been updated with all callout titles now converted into title block format.\nChapter 15: Security & Privacy: The “Privacy and Security” chapter in the machine learning systems textbook has been updated with all callout titles now converted into title block format.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated, with all callout titles being transformed into title block format.\nChapter 17: Sustainable AI: The ‘Sustainable AI’ chapter in the machine learning systems textbook has been updated with all callout titles now converted into title block format.\nChapter 18: Robust AI: The titles of all callouts in the “Robust AI” chapter have been updated to title block format.\nChapter 19: AI for Good: The titles of all callouts in the “AI for Good” chapter have been updated to the title block format. — — — —\n\n\n\n\n\n📅 Published on Jan 28, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated multiple times to include new contributors, and logos have been added.\nPainting Pots: The appendix section of the machine learning systems textbook has been updated with new examples and figures in the ‘painting_pots.qmd’ file, along with significant rewrites for improved clarity and understanding.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The redundant case study in the Introduction section of the core content has been removed.\nChapter 2: ML Systems: The ml_systems.qmd file has been updated with the addition of some radar plots.\nChapter 4: DNN Architectures: The dnn_architectures.qmd file in the core section has been updated with modifications based on Bravo’s recommendations and some wording adjustments.\nChapter 5: AI Workflow: The workflow.qmd file in the core section has been updated with new sections, rewrites, example additions, and figure changes to enhance the understanding of machine learning systems workflow.\nChapter 6: Data Engineering: The data engineering chapter has been updated with new content, including additional citations for the data section and edits to later sections. References and mentions of exercises have been removed. Work is in progress on keywords.\nChapter 7: AI Frameworks: Figures for the chips have been added and small bibliography references have been included in the updated content of the machine learning systems textbook.\nChapter 8: AI Training: The training chapter has been significantly improved with the addition of new figures for the chips, mermaid chart drawings, and an improved evolution section with a new figure. A new definition and a footnote have been added, and references to the hardware section have been updated. Learning objectives and a conclusion have also been added to the chapter.\nChapter 9: Efficient AI: The efficient_ai.qmd file in the machine learning systems textbook has been updated with new learning objectives, updated references, added figures, and new content including a discussion on Moore’s law. The purpose of the content has also been clarified and the title has been tweaked.\nChapter 10: Model Optimizations: The “Optimizations” chapter in the machine learning systems textbook has been updated to remove dead references.\nChapter 11: AI Acceleration: The “Hardware Acceleration” section in the machine learning systems textbook has been updated to remove certain references.\nChapter 19: AI for Good: The “AI for Good” chapter has been updated with new learning objectives, additional references, and spotlight use cases. It also includes added multimedia content such as videos and images, alongside text modifications for clarity and improvement.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Pi Image Classification: The image_classification.qmd file in the Raspberry Pi labs section has been updated with new content, including potential rewrites, additional examples, and changes to figures related to image classification.\nLab: Pi Object Detection: The object_detection.qmd file in the raspi lab section of the machine learning systems textbook has been updated, featuring meaningful content-level changes such as new sections, rewrites, example additions, and figure changes.\n\n\n\n\n📚 Appendix\n\n\nPhd Survival Guide: The appendix file “phd_survival_guide.qmd” has been updated with additional favorite resources and an updated link. — — — —\n\n\n\n\n\n📅 Published on Jan 17, 2025\n\n\n📄 Frontmatter\n\n\nAbout: The ‘About’ section has been updated with new content, including additional examples, revisions of existing sections, and modifications to figures for improved clarity and understanding.\nAcknowledgements: The acknowledgements section has been updated with the addition of new contributors.\nSocratiq: Without specific details from the commit messages, it’s impossible to provide a summary of the content-level changes in the file contents/frontmatter/ai/socratiq.qmd. Please provide detailed commit messages.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter has been updated to address Bravo’s feedback.\nChapter 2: ML Systems: The ml_systems.qmd file was updated to fix issues that arose from a previous merge, with no specific content-level changes like new sections, rewrites, example additions, or figure changes mentioned.\nChapter 3: DL Primer: Without specific commit messages detailing the changes made to the file, it’s impossible to provide a summary of content-level changes such as new sections, rewrites, example additions, or figure changes. Please provide detailed commit messages.\nChapter 4: DNN Architectures: The section on DNN architectures has been updated with a clarification to the parameter storage bound for RNNs, and an unused footnote has been removed.\nChapter 6: Data Engineering: The data engineering chapter has been updated to address Bravo’s feedback, which may include modifications to sections, rewrites, addition of examples, and changes to figures.\nChapter 7: AI Frameworks: The machine learning systems textbook has been updated with major changes including the addition of a timeline plot and new graphs, updated learning objectives, and new text. The framework overview has been added and updated with a new definition. Significant work has been done on the computational graph section, and a historical part has been added. The purpose of the content has also been refined.\nChapter 12: Benchmarking AI: The “Benchmarking” section in the machine learning systems textbook has been updated to fix reference issues.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Pi Large Language Models: The lab content in the Raspberry Pi section of the machine learning systems textbook has been updated to correct copyediting leftovers.\nLab: Pi Vision Language Models: The vlm.qmd file in the raspi lab section of the machine learning systems textbook has been updated with meaningful content-level changes. — — — —\n\n\n\n\n\n📅 Published on Jan 12, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors to the project.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction section has undergone adjustments in the section headers and resolved a broken merge issue.\nChapter 2: ML Systems: The updates to the machine learning systems textbook include the addition of a definition for hybrid ML, a new decision playbook framework, and updated definitions for each section. There’s also a new analogy related to tectonics introduced.\nChapter 5: AI Workflow: The workflow chapter in the machine learning systems textbook has been updated with Zishen’s fixes and unnecessary grammar fix requests have been removed.\nChapter 6: Data Engineering: The data engineering chapter has been updated with a new data labeling section, several fixes from Zishen and Bravo, and a replacement of a previously locked figure which had caused build issues. — — — —\n\n\n\n\n\n📅 Published on Jan 11, 2025\n\n\n📄 Frontmatter\n\n\nAbout: The “about.qmd” section of the machine learning systems textbook has been edited.\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\nSocratiq: The socratiq.qmd file in the AI section of the frontmatter has been edited.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction.qmd file in the core introduction section has been updated with added footnotes.\nChapter 2: ML Systems: The core ML systems chapter has been updated with the addition of a decision playbook framework, updated definitions, and each section now includes specific definitions. A tectonic analogy has also been incorporated.\nChapter 5: AI Workflow: The workflow chapter in the machine learning systems textbook has been updated to improve the clarity of the content by removing unnecessary grammar fix requests.\nChapter 6: Data Engineering: The data engineering chapter has been updated with improvements and tweaks, including the addition of references. Specific updates were made to the sections on synthetic data, crowdsourcing, web scraping, and problem definition. The overview section also received minor updates. — — — —\n\n\n\n\n\n📅 Published on Jan 09, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction.qmd file in the core/introduction section of the machine learning systems textbook has been updated based on Marco’s feedback.\nChapter 5: AI Workflow: The workflow chapter in the machine learning systems textbook has been updated to address and fix feedback from Bravo.\nChapter 6: Data Engineering: The data_engineering.qmd file in the core content of the machine learning systems textbook has been updated to improve the grammar and clarity of the content.\nChapter 7: AI Frameworks: The “Frameworks” chapter in the machine learning systems textbook has been updated to remove requests for grammar pass fixes.\nChapter 8: AI Training: The training chapter in the machine learning systems textbook has been updated to address and rectify Bravo’s feedback.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the core section has been updated to address and fix feedback from Bravo.\nChapter 16: Responsible AI: The responsible AI section of the core contents has been updated to address and incorporate Bravo’s feedback. — — — —\n\n\n\n\n\n📅 Published on Jan 07, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\nForeword: The foreword of the machine learning systems textbook has been updated with modifications in the wording.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction section has been updated to clarify the distinction between Artificial Intelligence and Machine Learning.\nChapter 3: DL Primer: The dl_primer.qmd file in the core section was updated with new images and code to better explain the training loop and inference process, including the addition of code snapshots for sections 3.5 and 3.6. The caption for a figure was also updated.\nChapter 4: DNN Architectures: The “DNN Architectures” section of the Machine Learning Systems textbook has been updated with new images and added visualization figures and tools as per Zishen’s recommendation in Chapter 4. — — — —\n\n\n\n\n\n📅 Published on Jan 03, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\nSocratiq: The ‘socratiq.qmd’ file in the AI section of the frontmatter has been updated with fixes addressing content-level changes.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter of the machine learning systems textbook has been updated with fixes to content-level issues.\nChapter 2: ML Systems: The ML Systems chapter in the machine learning systems textbook has been updated with fixes to content, including corrections and improvements to sections, examples, and figures.\nChapter 4: DNN Architectures: The “DNN Architectures” chapter in the machine learning systems textbook has been updated with fixes to the content.\nChapter 6: Data Engineering: The data engineering chapter has been updated with corrections to existing content.\nChapter 20: Conclusion: The conclusion chapter of the machine learning systems textbook has been updated with fixes. — — — —\n\n\n\n\n\n📅 Published on Jan 02, 2025\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements section has been updated to include new contributors.\n\n\n\n\n📖 Chapters\n\n\nChapter 4: DNN Architectures: The “dnn_architectures.qmd” file in the core section has been updated to remove unnecessary commented text and incorporate suggested fixes from user Bravo.\nChapter 20: Conclusion: The conclusion chapter of the machine learning systems textbook has been updated with suggested fixes from Bravo.\nChapter: Generative Ai: The ‘Generative AI’ chapter in the machine learning systems textbook has been updated to remove unnecessary commented text.",
    "crumbs": [
      "Book Changelog"
    ]
  },
  {
    "objectID": "contents/frontmatter/changelog/changelog.html#changes-1",
    "href": "contents/frontmatter/changelog/changelog.html#changes-1",
    "title": "Book Changelog",
    "section": "2024 Changes",
    "text": "2024 Changes\n\n📅 Published on Nov 19, 2024\n\n\n📄 Frontmatter\n\n\nChapter: Acknowledgements: The acknowledgements section has been updated multiple times to include new contributors to the textbook.\nSocratiq: The updated machine learning systems textbook now includes an AI podcast, a blog on updated widget suggestions, support for .png format for gif images in the PDF build, and a new toggle button switch feature. There have also been changes to the text, relocation of button text, and the ‘widget_access’ has been renamed to ‘socratiq’.\n\n\n\n\n📖 Chapters\n\n\nChapter 15: Security & Privacy: The privacy and security chapter in the machine learning systems textbook has been updated with a new section on machine unlearning, a reordered table for improved readability, and a new federated case study replacing a previously discussed one. The explanations of power consumption attacks have been clarified with the help of revised figures. The introduction of the section has been made less academic, and the explanations throughout the chapter are now less repetitive and clearer. Definitions have been grouped for better understanding, and case studies have been renamed for consistency.\nChapter 16: Responsible AI: The updates to the responsible_ai.qmd file include a revised figure placement, a summary of policies listed in the chapter, and a clarification of the figure explanation.\nChapter 17: Sustainable AI: The updates to the Sustainable AI chapter include the addition of a new water footprint image, a new Life Cycle Assessment (LCA) figure, and the removal of a repeated statement.\nChapter 19: AI for Good: The section on AI for Good has been updated to include a motivation for using TinyML. — — — —\n\n\n\n\n\n📅 Published on Nov 16, 2024\n\n\n📄 Frontmatter\n\n\nChapter: About: The ‘about.qmd’ file in the core section has been updated with a fixed relative path and a reorganization of the file content.\nChapter: Acknowledgements: The acknowledgements section has been updated with new contributors, the preface material has been reorganized, and there have been updates to ensure correct number building, although full build testing is still in progress.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The updates to the machine learning systems textbook do not include any content-level changes such as new sections, rewrites, example additions, or figure changes. The changes were primarily focused on fixing reference links, adjusting definition formatting, and correcting style consistency.\nChapter 2: ML Systems: The Introduction section of the ML Systems chapter has been replaced with an Overview to provide a more accurate representation of the content.\nChapter 3: DL Primer: The “Introduction” section in the “dl_primer.qmd” file has been replaced with an “Overview” section.\nChapter 5: AI Workflow: The “Workflow” section of the core content in the machine learning systems textbook has been updated with connections between different roles sections.\nChapter 6: Data Engineering: The Introduction section of the data_engineering.qmd file has been replaced with an Overview section.\nChapter 7: AI Frameworks: The Introduction section in the ‘frameworks.qmd’ file has been replaced with an Overview section.\nChapter 8: AI Training: The Introduction section in the training.qmd file has been replaced with an Overview section.\nChapter 9: Efficient AI: The “Introduction” section in the “Efficient AI” chapter has been replaced with an “Overview” section.\nChapter 10: Model Optimizations: The Introduction section in the ‘Optimizations’ chapter has been replaced with an Overview to provide a more accurate representation of the content.\nChapter 11: AI Acceleration: The Introduction section in the ‘hw_acceleration.qmd’ file has been replaced with an Overview section.\nChapter 12: Benchmarking AI: The Introduction section in the Benchmarking chapter has been replaced with an Overview to provide a more accurate description of the content.\nChapter 13: ML Operations: The core operations chapter has been restructured to group fragmented topics, connect roles sections, and create a less fragmented data management section. Redundant information previously discussed in other chapters has been removed. The Introduction has been replaced with an Overview to better suit the textbook style, and the abstract style introduction has been converted to a textbook style introduction.\nChapter 14: On-Device Learning: The Introduction section in the ‘ondevice_learning.qmd’ file has been replaced with an Overview to better represent the content.\nChapter 15: Security & Privacy: The Introduction section in the Privacy and Security chapter has been replaced with an Overview section.\nChapter 16: Responsible AI: The ‘Responsible AI’ section of the machine learning systems textbook has been updated with improved explanations for the table and definitions. Additionally, the ‘Introduction’ has been replaced with an ‘Overview’ to avoid redundancy.\nChapter 17: Sustainable AI: The Introduction section in the Sustainable AI chapter has been replaced with an Overview to provide a more comprehensive understanding of the topic.\nChapter 18: Robust AI: The Introduction section in the Robust AI chapter has been replaced with an Overview section.\nChapter 19: AI for Good: The “Introduction” section in the “AI for Good” chapter has been replaced with an “Overview” section.\nChapter 20: Conclusion: The “Introduction” section in the “Conclusion” chapter has been renamed to “Overview”.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The Introduction section in the ‘image_classification.qmd’ file has been replaced with an Overview section to provide a more accurate representation of the content.\nLab: Arduino Keyword Spotting: The Introduction section in the contents/labs/arduino/nicla_vision/kws/kws.qmd file has been replaced with an Overview.\nLab: Arduino Motion Classification: The Introduction section in the ‘motion_classification.qmd’ file has been replaced with an Overview section to better reflect the content.\nLab: Arduino Object Detection: The Introduction section in the object_detection.qmd file has been replaced with an Overview section.\nLab: Arduino Setup: The Introduction section in the contents/labs/arduino/nicla_vision/setup/setup.qmd file has been replaced with an Overview section.\nLab: Dsp Spectral Features Block: The Introduction section of the dsp_spectral_features_block.qmd file has been replaced with an Overview section. — — — —\nLab: Kws Feature Eng: The Introduction section in the ‘kws_feature_eng.qmd’ file has been replaced with an Overview section.\nLab: Pi Image Classification: The Introduction section in the image_classification.qmd file has been replaced with an Overview section.\nLab: Pi Large Language Models: The Introduction section in the contents/labs/raspi/llm/llm.qmd file has been replaced with an Overview section.\nLab: Pi Object Detection: The Introduction section in the ‘object_detection.qmd’ file has been replaced with an Overview section.\nLab: Raspberry Pi Setup: The Introduction section in the “contents/labs/raspi/setup/setup.qmd” file has been replaced with an Overview section.\nLab: XIAO Image Classification: The Introduction section in the ‘image_classification.qmd’ file has been replaced with an Overview section to provide a more accurate representation of the content.\nLab: XIAO Keyword Spotting: The Introduction section in the contents/labs/seeed/xiao_esp32s3/kws/kws.qmd file has been replaced with an Overview section.\nLab: XIAO Motion Classification: The Introduction section in the ‘motion_classification.qmd’ file has been replaced with an Overview section.\nLab: XIAO Object Detection: The Introduction section in the object_detection.qmd file has been replaced with an Overview section.\nLab: XIAO Setup: The Introduction section in the contents/labs/seeed/xiao_esp32s3/setup/setup.qmd file has been replaced with an Overview section.\n\n\n\n\n\n📅 Published on Sep 20, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter has been revised with all sections now complete, broken figure references have been fixed, and content related to embedded AI has been removed. Additionally, changes have been accepted in the data engineering and efficient AI chapters to resolve merge conflicts.\nChapter 2: ML Systems: All chapters in the machine learning systems textbook have been revised and completed.\nChapter 3: DL Primer: The changelog for the machine learning systems textbook includes the completion and revision of all chapters, the fixing of broken links and reference build issues, and the correction of figure references.\nChapter 5: AI Workflow: The workflow chapter in the machine learning systems textbook has been revised and completed with corrected references and figures.\nChapter 6: Data Engineering: The textbook has been updated with the completion of all chapters, including revisions, and possible addition of figure references.\nChapter 7: AI Frameworks: The updates to the machine learning systems textbook include revisions to all chapters, fixing of figure references, and the completion of all chapters.\nChapter 8: AI Training: No meaningful content-level changes were made, only a character formatting adjustment was done.\nChapter 9: Efficient AI: The updates to the “Efficient AI” chapter include revisions to all chapters for completeness and corrections to figure references.\nChapter 10: Model Optimizations: The summary sentence is not available as the provided commit message only indicates a formatting or typo correction, which should be ignored according to the instructions.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with revisions to all chapters and corrections to figure references.\nChapter 12: Benchmarking AI: The “Benchmarking” section in the machine learning systems textbook has been updated with revisions to all chapters, removal of unnecessary figures, and resolution of merge conflicts in the data engineering and efficient AI chapters by accepting incoming changes.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with revisions to all chapters, the completion of all chapters, and the correction of figure references.\nChapter 14: On-Device Learning: The on-device learning chapter in the machine learning systems textbook has been updated with fixed references and paths, and it has been marked as complete.\nChapter 15: Security & Privacy: The privacy and security chapter has been revised with fixed figure references, and potential conflicts with changes in the data engineering and efficient AI chapters have been resolved.\nChapter 16: Responsible AI: The responsible_ai.qmd file has been updated with new sections on ethical considerations in AI, rewrites of certain sections for improved clarity, additions of practical examples to illustrate concepts, and changes in figures for better visualization.\nChapter 17: Sustainable AI: The Sustainable AI section has been thoroughly proofread and revised, all chapters have been completed, and figure references have been fixed.\nChapter 18: Robust AI: The robust_ai.qmd file has been updated with new sections on robust AI principles, significant rewrites of existing content for clarity, the addition of practical examples to illustrate key concepts, and changes to figures for improved visual representation.\nChapter 19: AI for Good: The AI for Good chapter has been fully revised and completed, with corrections made to broken and figure references.\nChapter 20: Conclusion: The conclusion chapter has been updated with new sections on the future of machine learning systems, rewrites of existing content for clarity, the addition of practical examples, and changes to figures for better visual representation.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The dsp_spectral_features_block.qmd file has been updated with new sections on spectral features in machine learning, additional examples to enhance understanding, and modifications in figures for better visual representation.\nLab: Arduino Image Classification: The image_classification.qmd file has been updated with new sections on advanced classification techniques, rewritten explanations for better comprehension, added practical examples for hands-on understanding, and modified figures for enhanced visual representation.\nLab: Arduino Image Classification: The image_classification.qmd file in the Arduino Nicla Vision lab has been updated with new content, including additional examples and revisions to the image classification section.\nLab: Arduino Keyword Spotting: The updates to the “contents/labs/arduino/nicla_vision/kws/kws.qmd” file include the addition of new sections, rewrites of existing content, the inclusion of new examples, and changes to figures to enhance the understanding of the topic.\nLab: Arduino Motion Classification: The ‘motion_classification.qmd’ file in the Arduino Nicla Vision section of the labs has been updated with fixes from BravoBaldo, potentially including corrections, improvements or additions to the content.\nLab: Arduino Object Detection: The object_detection.qmd file in the Arduino Nicla Vision lab has been updated with fixes from BravoBaldo, potentially including corrections or improvements to sections, examples, or figures related to object detection.\nLab: Arduino Setup: The setup.qmd file in the Arduino Nicla Vision lab has been updated with new sections, rewrites, example additions, and figure changes to enhance the understanding of the setup process.\nLab: Dsp Spectral Features Block: The dsp_spectral_features_block.qmd file in the labs/shared section has been updated with new sections, rewrites, example additions, and figure changes to enhance the understanding of spectral features in digital signal processing.\nLab: Kws Feature Eng: The updates to the “kws_feature_eng.qmd” file include new sections on feature engineering for keyword spotting, rewrites of existing content for clarity, the addition of practical examples to illustrate key concepts, and changes to figures to improve their relevance and accuracy.\nLab: Lab Setup: The “Getting Started” lab in the machine learning systems textbook has been updated with fixes from BravoBaldo, including content-level changes such as new sections, rewrites, example additions, and figure changes.\nLab: Labs: The labs.qmd file in the machine learning systems textbook has been updated with new sections, rewrites, example additions, and figure changes.\nLab: Nicla Vision: The updates to the “nicla_vision.qmd” file in the Arduino labs section of the machine learning systems textbook include new sections on Nicla Vision, rewrites of existing content for clarity, additions of practical examples, and changes to figures for better understanding.\nLab: Pi Image Classification: The update does not include any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The changes were primarily focused on fixing character formatting and correcting typos.\nLab: Pi Large Language Models: Without the specific commit messages, it’s impossible to provide a summary of the content-level changes made to the file. Please provide the commit messages to proceed.\nLab: Pi Object Detection: The object detection lab in the Raspberry Pi section has been updated with corrections to typos.\nLab: Raspberry Pi Setup: The updates to the ‘setup.qmd’ file in the ‘raspi/setup’ lab section include the addition of new files.\nLab: Raspi: The raspi.qmd file in the labs section has been updated with fixes from BravoBaldo, which may include corrections or improvements to the content, examples, or figures.\nLab: Shared: The shared.qmd file in the labs section has been updated with new sections, rewrites, example additions, and figure changes to enhance the understanding of machine learning systems. — — — —\nLab: Xiao Esp32S3: The updates to the “contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.qmd” file include new sections on the Xiao ESP32S3, additional examples for better understanding, and changes in figures for enhanced clarity.\nLab: XIAO Image Classification: The image_classification.qmd file in the Seeed Xiao ESP32S3 lab section has been updated with fixes, possibly including corrections or improvements to the content, examples, or figures, thanks to contributions from BravoBaldo.\nLab: XIAO Keyword Spotting: The ‘kws.qmd’ file in the ‘seeed/xiao_esp32s3’ lab section has been updated with fixes from BravoBaldo, potentially including corrections or improvements to the content, examples, or figures related to the keyword spotting (KWS) topic.\nLab: XIAO Motion Classification: The ‘motion_classification.qmd’ file in the ‘seeed/xiao_esp32s3/motion_classification’ lab has been updated with corrections from BravoBaldo and an image fix.\nLab: XIAO Object Detection: The ‘object_detection.qmd’ file in the ‘seeed/xiao_esp32s3’ lab section of the machine learning systems textbook has been updated with fixes and improvements contributed by BravoBaldo.\nLab: XIAO Setup: The setup instructions for the Seeed Xiao ESP32S3 in the labs section have been updated and corrected.\n\n\n\n\n\n📅 Published on Sep 12, 2024\n\n\n📖 Chapters\n\n\nChapter 13: ML Operations: The ‘ops.qmd’ file in the machine learning systems textbook has been updated based on feedback from Baldo, which may include new sections, rewrites, example additions, or figure changes.\nChapter 17: Sustainable AI: The updates primarily involve corrections and improvements recommended by (BravoBaldo?), however, no significant content-level changes such as new sections, rewrites, example additions, or figure changes were made.\nChapter 18: Robust AI: The “Robust AI” chapter in the machine learning systems textbook has been updated with corrections and improvements suggested by Baldo.\nChapter 19: AI for Good: The ‘AI for Good’ chapter in the machine learning systems textbook has been updated with fixes and improvements, thanks to contributions from Baldo.\nChapter 20: Conclusion: The conclusion chapter of the machine learning systems textbook has been updated based on feedback from Baldo.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Pi Image Classification: The image classification lab in the Raspberry Pi section has been updated with corrected links and typos.\nLab: Pi Object Detection: The Object Detection Lab has been uploaded in the ‘raspi’ section under ‘labs’, providing new content and practical examples on object detection using Raspberry Pi. — — — —\n\n\n\n\n\n📅 Published on Sep 06, 2024\n\n\n📖 Chapters\n\n\nChapter 16: Responsible AI: The responsible AI chapter in the machine learning systems textbook has been updated with corrections to the bibliography and text content. — — — —\n\n\n\n\n\n📅 Published on Sep 04, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The captions for even side pages in the introduction have been corrected.\nChapter 2: ML Systems: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The changes were only related to grammar fixes.\nChapter 3: DL Primer: The update includes grammar corrections in the “dl_primer.qmd” file of the machine learning systems textbook.\nChapter 6: Data Engineering: The data_engineering.qmd file was updated with grammar fixes to improve readability and comprehension.\nChapter 7: AI Frameworks: The changelog does not indicate any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were only related to grammar fixes.\nChapter 8: AI Training: No content-level changes have been made to the training.qmd file, only grammar corrections were implemented.\nChapter 9: Efficient AI: The “Efficient AI” chapter in the machine learning systems textbook has been updated with improved explanations.\nChapter 10: Model Optimizations: There were no content-level changes, only grammar fixes were made in the optimizations chapter of the machine learning systems textbook.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with improved explanations.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with grammar corrections for improved readability.\nChapter 13: ML Operations: The update does not include any content-level changes like new sections, rewrites, example additions, or figure changes, but only consists of grammar fixes.\nChapter 14: On-Device Learning: The on-device learning chapter in the machine learning systems textbook has been updated with grammar corrections for improved readability.\nChapter 15: Security & Privacy: The ‘Privacy and Security’ chapter in the machine learning systems textbook has been updated with corrections and improvements in the content’s grammar.\nChapter 16: Responsible AI: The responsible_ai.qmd file in the Responsible AI chapter has been updated with grammar corrections for improved readability and understanding.\nChapter 17: Sustainable AI: The sustainable_ai.qmd file in the machine learning systems textbook has been updated with grammar corrections.\nChapter 18: Robust AI: No content-level changes have been made to the robust_ai.qmd file, the updates were solely related to grammar fixes.\nChapter 19: AI for Good: The ‘AI for Good’ chapter in the machine learning systems textbook has been updated with grammar corrections for improved readability.\nChapter 20: Conclusion: The conclusion section of the machine learning systems textbook has been updated with grammar corrections for improved readability and understanding.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The image_classification.qmd file in the Arduino Nicla Vision lab has been updated with grammar corrections to improve readability.\nLab: Kws Feature Eng: The updates to the file contents/labs/shared/kws_feature_eng/kws_feature_eng.qmd consist of grammar corrections, but no content-level changes such as new sections, rewrites, example additions, or figure changes have been made. — — — —\n\n\n\n\n\n📅 Published on Sep 02, 2024\n\n\n📖 Chapters\n\n\nChapter 2: ML Systems: The ml_systems.qmd file was updated to correct a dangling sentence.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with a more student-focused explanation of hardware design principles, an introduction, and a corrected table.\nChapter 13: ML Operations: The ops.qmd file has been updated with a new section on model serving, including an added figure and updated references. The content has been improved and shortened, and certain issues raised by BravoBaldo have been addressed.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Pi Image Classification: The image_classification.qmd file in the Raspi lab section has been updated with new files, potentially including additional examples, figures, or sections related to image classification. — — — —\nLab: Raspberry Pi Setup: The setup.qmd file in the raspi lab section of the machine learning systems textbook has been updated and new files have been uploaded.\n\n\n\n\n\n📅 Published on Aug 29, 2024\n\n\n📖 Chapters\n\n\nChapter 13: ML Operations: The contents/ops/ops.qmd file has been updated with fixes based on suggestions from BravoBaldo, potentially involving changes to sections, rewrites, additional examples, or figure modifications.\nChapter 14: On-Device Learning: The on-device learning chapter has been updated with corrections and improvements based on feedback from user BravoBaldo.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Kws Feature Eng: The update fixed formatting issues in the “kws_feature_eng.qmd” file of the machine learning systems textbook.\nLab: Labs: The labs.qmd file in the machine learning systems textbook has been updated to fix an issue with table merging. — — — —\nLab: Pi Image Classification: New files have been uploaded to the ‘image_classification’ lab in the ‘raspi’ section of the machine learning systems textbook.\n\n\n\n\n\n📅 Published on Aug 27, 2024\n\n\n📖 Chapters\n\n\nChapter 7: AI Frameworks: The updates to the machine learning systems textbook include corrections to broken links and adjustments to section labels for better referencing.\nChapter 9: Efficient AI: The updates to the machine learning systems textbook include the removal of duplicated information between chapters 8 and 9, the addition of background information on the representation of floating points, corrections to the explanations of structure importance methods, a fix to a figure that was pointing to an incorrect image, and changes to all figures’ credits to sources. Feedback from chapter 8’s students was also incorporated.\nChapter 10: Model Optimizations: The optimizations chapter has been updated to remove unnecessary historical background, duplicate tables, and redundant information overlapping with chapter 8. The explanation of knowledge distillation has been improved, the challenges section has been adjusted to be less repetitive and more informative, and the explanations of structure importance methods have been corrected.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated to fix broken links, correct the qbit count, and complete an incomplete sentence. Additionally, a duplicate title issue was resolved.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with fixes and improvements, thanks to contributions from (BravoBaldo?).\nChapter 13: ML Operations: The contents/ops/ops.qmd file has been updated with minor wording changes.\nChapter 15: Security & Privacy: The Power Attack and Side-Channel Attack sections in the Privacy and Security chapter have been edited, and broken links within the content have been fixed.\nChapter 17: Sustainable AI: The “Sustainable AI” chapter in the machine learning systems textbook has been updated with corrected broken links.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Labs: The file update includes new sections added to the labs, rewrites of existing content for clarity, the inclusion of additional examples for better understanding, and changes to figures for improved visualization.\nLab: Xiao Esp32S3: The table in the “Seeed Xiao ESP32S3” lab section has been corrected for better readability and understanding. — — — —\n\n\n\n\n\n📅 Published on Aug 22, 2024\n\n\n📖 Chapters\n\n\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with the use of subscript.\nChapter 17: Sustainable AI: The sustainable_ai.qmd file in the machine learning systems textbook has been updated with subscripts for improved readability and understanding.\nChapter 19: AI for Good: The AI for Good chapter in the machine learning systems textbook has been updated with the use of subscript.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Labs: The update includes new files that have been uploaded to the ‘labs’ section of the machine learning systems textbook.\nLab: Raspberry Pi Setup: The setup.qmd file in the raspi setup lab has been updated with new files uploaded.\nLab: Raspi: The raspi.qmd file in the labs section has been updated with new content, although the specific changes are not detailed in the commit message. — — — —\n\n\n\n\n\n📅 Published on Aug 21, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction.qmd file in the machine learning systems textbook has been updated with enhanced content and corrections to provide a more in-depth understanding of the subject.\nChapter 2: ML Systems: The ml_systems.qmd file has been updated with enhanced utilities and pivotal fixes.\nChapter 3: DL Primer: The “dl_primer” section of the machine learning systems textbook has been updated with enhanced content and corrections.\nChapter 5: AI Workflow: The “workflow.qmd” file in the machine learning systems textbook has been updated with enhancements and fixes, potentially improving the clarity and accuracy of the content.\nChapter 6: Data Engineering: The data engineering chapter has been updated with enhanced explanations, pivotal and delve fixes, and potentially the utilization of new methods or tools.\nChapter 7: AI Frameworks: The “Frameworks” chapter in the machine learning systems textbook has been updated with enhanced content, the utilization of new examples, and fixes in the “Delve” section.\nChapter 8: AI Training: The meaningful content-level changes include the correction of a broken Colab link, the enhancement of certain features, the utilization of new tools or methods, and an update to the table formatting.\nChapter 9: Efficient AI: The “Efficient AI” chapter in the machine learning systems textbook has been updated with enhancements and fixes, likely improving the clarity and accuracy of the content.\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has been updated with pivotal and delve fixes, and enhancements have been made for better utilization.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with corrected table references, enhanced content, fixes in the delve section, and an update from a standard table to a grid table.\nChapter 12: Benchmarking AI: The “Benchmarking” chapter in the machine learning systems textbook has been updated with enhanced explanations, utilization of new examples, and corrections to the “Delve” section.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with corrected table references, enhanced content, updated grid tables, and improved centering.\nChapter 14: On-Device Learning: The on-device learning chapter has been enhanced and updated with the addition of a grid table.\nChapter 15: Security & Privacy: The privacy_security.qmd file was updated with corrections to table references, conversion of a table to a grid table, and improvements to the ‘delve’ section.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated with enhancements and corrections, and a grid table has been added for better data representation.\nChapter 17: Sustainable AI: The sustainable AI chapter in the machine learning systems textbook has been updated with enhancements and fixes, including a more in-depth exploration of certain topics and the utilization of new examples.\nChapter 18: Robust AI: The robust AI chapter in the machine learning systems textbook has been updated to fix citation reference issues and enhance certain aspects, though the specifics of the enhancement are not detailed.\nChapter 19: AI for Good: The “AI for Good” chapter in the machine learning systems textbook has been updated to improve and enhance certain sections, although specific details of the changes are not provided.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The ‘dsp_spectral_features_block’ section of the machine learning systems textbook has been updated to remove redundant code.\nLab: Arduino Image Classification: The “Image Classification” section of the machine learning systems textbook has been updated to remove unnecessary code and improve the overall functionality.\nLab: Arduino Image Classification: The image_classification.qmd file in the Arduino Nicla Vision lab has been updated with enhancements and fixes, potentially improving the explanation or examples related to image classification.\nLab: Lab Setup: The “getting_started.qmd” file in the machine learning systems textbook has been updated with the initial version of the Raspberry Pi section.\nLab: Labs: An initial version of the rasPi section has been added to the labs.\nLab: Pi Image Classification: The initial version of the Raspberry Pi image classification lab has been added to the machine learning systems textbook.\nLab: Pi Large Language Models: The initial version of the Raspberry Pi section in the machine learning systems textbook has been added.\nLab: Pi Object Detection: The initial version of the Raspberry Pi section in the object detection lab has been added.\nLab: Raspberry Pi Setup: The initial version of the Raspberry Pi setup guide has been added to the labs section.\nLab: Raspi: The initial version of the Raspberry Pi section has been added to the labs content of the machine learning systems textbook. — — — —\nLab: XIAO Image Classification: The “image_classification.qmd” file in the “seeed/xiao_esp32s3” lab section has been updated with fixes to the delve.\nLab: XIAO Keyword Spotting: The ‘kws.qmd’ file in the ‘xiao_esp32s3’ section of the ‘seeed’ labs has been updated, but the specific changes are unclear due to the vague commit message ‘utilizze’.\n\n\n\n\n\n📅 Published on Aug 15, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter of the machine learning systems textbook has been updated with enhanced explanations and corrections to provide a more in-depth understanding.\nChapter 2: ML Systems: The ml_systems.qmd file in the machine learning systems textbook has been updated with pivotal fixes, enhanced features, and improved utilization methods.\nChapter 3: DL Primer: The “dl_primer.qmd” file in the machine learning systems textbook has been updated with enhancements and corrections to improve the content’s clarity and accuracy.\nChapter 5: AI Workflow: The workflow chapter in the machine learning systems textbook has been updated with enhancements and fixes, potentially improving explanations or examples.\nChapter 6: Data Engineering: The data engineering chapter has been updated with crucial fixes, enhanced content, and the utilization of new examples, along with improvements in the ‘delve’ section.\nChapter 7: AI Frameworks: The updates to the “Frameworks” chapter in the machine learning systems textbook include enhancements and fixes to the content, as well as the utilization of new examples.\nChapter 8: AI Training: The updated content in the machine learning systems textbook includes a re-arrangement and update of wording in certain sections, an update to the Neural Network notation, consolidation of common pitfalls, and additions related to regularization and hyperparameter search.\nChapter 9: Efficient AI: The ‘Efficient AI’ chapter in the machine learning systems textbook has been updated with enhanced content, detailed explanations, and potential revisions in examples and figures. The changes also include fixes and improvements from the ‘dev’ branch.\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has been updated with crucial fixes, enhanced features, and further in-depth explanations.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with corrections to table references, enhancements, and necessary fixes. Additionally, the table has been updated and converted into a grid table.\nChapter 12: Benchmarking AI: The “Benchmarking” chapter in the machine learning systems textbook has been updated with enhanced explanations, utilization of new methodologies, and fixes to the delve sections.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with corrections to table references and enhancements, along with an update to the grid table.\nChapter 14: On-Device Learning: The on-device learning chapter has been updated with enhancements and fixes, the utilization of a new grid table, and possibly new examples or figures.\nChapter 15: Security & Privacy: The privacy_security.qmd file in the machine learning systems textbook has been updated with enhanced content, including corrections to table references, improvements to the utilization section, and fixes in the delve section. Additionally, the table has been updated to a grid table and the file has been merged with the latest version of the dev branch.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated with improvements and enhancements, including the addition of a grid table.\nChapter 17: Sustainable AI: The sustainable_ai.qmd file in the machine learning systems textbook has been updated with enhanced explanations, utilization of new methods, and fixes to the delve sections.\nChapter 18: Robust AI: The updates to the “Robust AI” chapter include a correction to a citation reference issue and the enhancement of certain features for improved utility.\nChapter 19: AI for Good: The “AI for Good” chapter in the machine learning systems textbook has been updated with improvements and enhancements.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Arduino Image Classification: The image_classification.qmd file in the machine learning systems textbook has been updated to improve and fix the content related to image classification.\nLab: Arduino Image Classification: The image classification section in the Arduino Nicla Vision lab has been enhanced and fixed.\nLab: XIAO Image Classification: The “image_classification.qmd” file in the “seeed/xiao_esp32s3/image_classification” lab has been updated with fixes to the delve section.\nLab: XIAO Keyword Spotting: The updates include the utilization of the Xiao ESP32S3 for keyword spotting in the ‘Seeed’ lab section of the machine learning systems textbook. — — — —\n\n\n\n\n\n📅 Published on Aug 07, 2024\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The image width issues for PDF rendering in the dsp_spectral_features_block section have been fixed.\nLab: Arduino Image Classification: The image width issues for PDF rendering in the image classification section have been fixed.\nLab: Arduino Image Classification: The image width issues for PDF rendering in the ‘image_classification.qmd’ section of the Arduino Nicla Vision lab have been fixed.\nLab: Arduino Keyword Spotting: The image width issues for PDF rendering in the Arduino Nicla Vision KWS lab content have been fixed.\nLab: Arduino Motion Classification: The image width issues for PDF rendering in the ‘motion_classification.qmd’ section of the Arduino Nicla Vision lab have been fixed.\nLab: Arduino Object Detection: The object detection section in the Arduino Nicla Vision lab has been updated to fix issues with image width for PDF rendering.\nLab: Arduino Setup: The image width issues for PDF rendering in the ‘Arduino Nicla Vision Setup’ lab section have been fixed.\nLab: Dsp Spectral Features Block: The image width issues for PDF rendering in the dsp_spectral_features_block section of the labs have been fixed. — — — —\nLab: Kws Feature Eng: The image width issues for PDF rendering in the ‘kws_feature_eng.qmd’ file of the ‘shared/kws_feature_eng’ lab have been fixed.\nLab: XIAO Image Classification: The image width issues for PDF rendering in the ‘image_classification.qmd’ file of the ‘seeed/xiao_esp32s3/image_classification’ lab have been fixed.\nLab: XIAO Keyword Spotting: The image width issues for PDF rendering in the ‘kws.qmd’ file of the ‘xiao_esp32s3’ section under ‘seeed’ labs have been fixed.\nLab: XIAO Motion Classification: The image width issues for PDF rendering in the ‘motion_classification.qmd’ section of the Seeed Xiao ESP32S3 lab in the machine learning systems textbook have been fixed.\nLab: XIAO Object Detection: The object detection lab in the Seeed Xiao ESP32S3 section has been updated to fix issues with image width for PDF rendering.\nLab: XIAO Setup: The image width issues for PDF rendering in the ‘setup.qmd’ file of the ‘xiao_esp32s3’ section under ‘seeed’ labs have been fixed.\n\n\n\n\n\n📅 Published on Aug 06, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction.qmd file has been updated with a new build for both HTML and PDF versions, specifically for Edward Tufte.\nChapter 2: ML Systems: The grid tables in the ML Systems chapter have been initially set up, and the source credit formatting has been updated for consistency.\nChapter 3: DL Primer: The machine learning systems textbook has undergone several updates, including the resolution of merge conflicts between the development version and chapter 6, an enhanced explanation of tensors, and revisions to chapter 3. Additionally, all broken URL links have been fixed, and there have been some minor tweaks to the wording.\nChapter 5: AI Workflow: The workflow chapter has been updated with student feedback, and the tables have been revised and aligned to the left using markdown formatting.\nChapter 6: Data Engineering: The data engineering chapter has been updated with a first pass on grid tables, a left alignment for all tables, a fixed missing reference, and a text update. Additionally, a new exercise featuring the wake vision colab has been added.\nChapter 7: AI Frameworks: ’ for consistency. Feedback from students has been incorporated and broken links have been fixed.\nChapter 8: AI Training: The updates include fixing all broken URL links, a first pass on grid tables, and an update to the “Credit” section which is now labeled as “Source”.\nChapter 9: Efficient AI: The image path and figure ID in the ‘efficient_ai.qmd’ section have been updated.\nChapter 10: Model Optimizations: The optimizations chapter has been updated with minor writing improvements, added in-text citations, and the adjustment of table contents.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with corrected URL links and the ‘Credit’ section has been revised to ‘Source’.\nChapter 12: Benchmarking AI: The source citation in the “Benchmarking” chapter was updated.\nChapter 13: ML Operations: The updated file includes fixes for broken URL links and tables that weren’t previously updated for Grid formatting. Additionally, the “Credit” section has been updated to “Source,” with a consistent formatting style.\nChapter 14: On-Device Learning: The broken URL links in the “On-Device Learning” chapter have been fixed and the source citation format has been updated for consistency.\nChapter 15: Security & Privacy: The privacy and security section in the machine learning systems textbook has been edited and all broken links have been fixed.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated with corrected URL links.\nChapter 17: Sustainable AI: The sustainable_ai.qmd file in the machine learning systems textbook has been updated with consistent formatting style and the ‘Credit’ section has been changed to ‘Source’. Additionally, the file has been built for HTML and PDF for Edward Tufte.\nChapter 18: Robust AI: ” for consistency.\nChapter 19: AI for Good: The “AI for Good” chapter has been updated with corrected URL links.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The image width issues for PDF rendering in the dsp_spectral_features_block section have been fixed.\nLab: Arduino Image Classification: The image width issues for PDF rendering have been fixed, redundant elements have been removed, and all broken video links in the image classification section have been corrected.\nLab: Arduino Image Classification: The updates to the image_classification.qmd file in the Arduino Nicla Vision lab include fixes to image width issues for PDF rendering, removal of redundant elements, and corrections to broken video links.\nLab: Arduino Keyword Spotting: The updates to the ‘kws.qmd’ file in the ‘nicla_vision’ section of the ‘arduino’ lab include corrections to image width issues for PDF rendering and the fixing of all broken URL links.\nLab: Arduino Motion Classification: The image width issues in the motion classification section of the Arduino Nicla Vision lab have been fixed for better PDF rendering.\nLab: Arduino Object Detection: The updates to the object_detection.qmd file in the Arduino Nicla Vision lab include corrections to image width for proper PDF rendering and the fixing of all remaining broken video links.\nLab: Arduino Setup: The setup.qmd file in the Arduino Nicla Vision lab has been updated to fix image width issues for PDF rendering.\nLab: Dsp Spectral Features Block: The image width issues for PDF rendering in the ‘dsp_spectral_features_block’ section of the machine learning systems textbook have been fixed.\nLab: Kws Feature Eng: The image width issues for PDF rendering in the ‘kws_feature_eng.qmd’ file of the ‘shared’ section under ‘labs’ have been fixed.\nLab: Nicla Vision: The “Nicla Vision” lab in the Arduino section has been updated with consistent formatting style for the source credits, but there were no content-level changes like new sections, rewrites, example additions, or figure changes.\nLab: Shared: The tables in the shared.qmd file have been updated to have a left alignment.\nLab: Xiao Esp32S3: The ‘Source’ section in the ‘seeed/xiao_esp32s3’ lab content has been updated. — — — —\nLab: XIAO Image Classification: The image width issues for PDF rendering in the “Image Classification” section of the Seeed Xiao ESP32S3 lab have been fixed.\nLab: XIAO Keyword Spotting: The updates to the machine learning systems textbook include corrections to image width issues for PDF rendering in the kws.qmd file and the fixing of broken links.\nLab: XIAO Motion Classification: The image width issues for PDF rendering in the motion classification lab of the Xiao ESP32S3 section have been fixed.\nLab: XIAO Object Detection: The image width issues for PDF rendering in the object detection lab section of the Seeed Xiao ESP32S3 chapter have been fixed.\nLab: XIAO Setup: The image width issues for PDF rendering in the ‘setup.qmd’ file of the ‘xiao_esp32s3’ section under the ‘seeed’ lab in the machine learning systems textbook have been fixed.\n\n\n\n\n\n📅 Published on Jun 25, 2024\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The link for video 3.1 in the “dl_primer” section has been fixed. — — — —\n\n\n\n\n\n📅 Published on Jun 20, 2024\n\n\n📖 Chapters\n\n\nChapter 2: ML Systems: The ml_systems.qmd file has been updated to fix a broken reference build and incorporate feedback from a student’s perspective, potentially involving modifications to sections, examples, or figures.\n\n\n\n\n🧑‍💻 Labs\n\n\nLab: Shared: The shared.qmd file in the labs section has been updated to fix broken links. — — — —\n\n\n\n\n\n📅 Published on Jun 19, 2024\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements page has been updated with comments disabled on certain pages.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction material has been updated with improvements based on feedback from the Data review team, and the foreword content has been removed. There was also a correction made to an error in a file reference.\nChapter 2: ML Systems: The content of the machine learning systems textbook has been enhanced based on feedback and suggestions from the Data review team.\nChapter 3: DL Primer: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. All updates pertain to formatting and typo corrections.\nChapter 5: AI Workflow: The update does not include any content-level changes such as new sections, rewrites, example additions, or figure changes. The changes were related to Markdown lint fixes, which are formatting adjustments.\nChapter 6: Data Engineering: The changelog does not contain any content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were only related to citation formatting and markdown lint fixes.\nChapter 7: AI Frameworks: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes.\nChapter 8: AI Training: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes.\nChapter 9: Efficient AI: A reference to missing videos was added to the “Efficient AI” chapter.\nChapter 10: Model Optimizations: The updates do not include any content-level changes such as new sections, rewrites, example additions, or figure changes. The changes were primarily focused on citation formatting and typo corrections.\nChapter 11: AI Acceleration: A link to Google’s Edge TPU website has been added.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with additional figures, including the mlperf training progress figure. Some content has been trimmed and updated, with specific changes influenced by Colby’s updates.\nChapter 13: ML Operations: The update does not include any content-level changes such as new sections, rewrites, example additions, or figure changes. The commit message indicates only formatting adjustments were made.\nChapter 14: On-Device Learning: The changelog does not indicate any content-level changes such as new sections, rewrites, example additions, or figure changes to the on-device learning chapter. The changes were related to Markdown lint fixes, which are formatting or typo corrections.\nChapter 15: Security & Privacy: The case study header in the Privacy and Security chapter was corrected.\nChapter 16: Responsible AI: The update does not contain any content-level changes such as new sections, rewrites, example additions, or figure changes. The commit message indicates that only markdown lint fixes were made, which are formatting changes.\nChapter 17: Sustainable AI: The changelog does not indicate any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The update only includes markdown lint fixes, which are formatting adjustments.\nChapter 18: Robust AI: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were only related to citation formatting and markdown lint fixes.\nChapter 19: AI for Good: The update does not include any content-level changes such as new sections, rewrites, example additions, or figure changes, but rather involves fixes related to Markdown linting.\nChapter 20: Conclusion: The changelog does not indicate any content-level changes such as new sections, rewrites, example additions, or figure changes. The commit message only refers to MD lint fixes, which are formatting or typo corrections.\nGenerative Ai: The changelog does not provide specific details about content-level changes such as new sections, rewrites, example additions, or figure changes. The updates mentioned are only related to wording tweaks, which are considered formatting or typo-only changes.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The updates to the “dsp_spectral_features_block.qmd” file include a fix to the resources and a minor modification in the title.\nKws Feature Eng: The kws_feature_eng.qmd file in the labs/shared directory has been updated, reflecting changes in the machine learning systems textbook.\nKws}: The kws_nicla.qmd file has been relocated from the kws_nicla directory to the labs/arduino/nicla_vision/kws directory, indicating a possible reorganization or restructuring of the content.\nLab: Arduino Image Classification: The changelog does not indicate any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes to the image_classification.qmd file. The commit message only mentions “MD lint fixes,” which are typically formatting or typo corrections.\nLab: Arduino Image Classification: The image_classification.qmd file in the Arduino Nicla Vision lab has been updated with the integration of the lab into the main content, addition of all necessary Arduino lab files, and a fix to the resources used in the lab.\nLab: Arduino Keyword Spotting: The content in the ‘kws.qmd’ file of the Arduino Nicla Vision lab has been significantly restructured with corrections in the placement of content and resources. Additionally, the file has been integrated into the labs.\nLab: Arduino Motion Classification: The ‘motion_classification.qmd’ file in the Arduino Nicla Vision lab section of the machine learning systems textbook has been updated with corrected content placement and integration into labs.\nLab: Arduino Niclavision: The arduino_niclavision.qmd file in the labs section has been updated with new examples, revisions to existing content, and modifications to figures to enhance understanding of the topic.\nLab: Arduino Object Detection: The object detection lab in the Arduino Nicla Vision section has been updated with integrated resources and additional content from the labs.\nLab: Arduino Setup: The setup.qmd file in the Arduino Nicla Vision lab has been updated with corrected resources, renamed sections, and has been integrated into the labs.\nLab: Dsp Spectral Features Block: The dsp_spectral_features_block.qmd file in the machine learning systems textbook has been updated with a title modification and the integration of this content into labs.\nLab: Dsp Spectral Features Block: The dsp_spectral_features_block.qmd file has been moved from the arduino/nicla_vision directory to the shared directory, suggesting that the content related to DSP spectral features block is now applicable to a broader context, not just the Arduino Nicla Vision.\nLab: Dsp Spectral Features Block: The “dsp_spectral_features_block.qmd” file in the Arduino Nicla Vision lab section has been updated with new examples, rewrites for clarity, and changes to figures for better understanding of the DSP Spectral Features Block.\nLab: Kws Feature Eng: The updates to the ‘kws_feature_eng.qmd’ file include the integration of the content into labs, a minor tweak to the content, and a fix to the resources referenced in the text.\nLab: Kws Feature Eng: The ‘kws_feature_eng.qmd’ file in the ‘nicla_vision’ section of the Arduino labs has been updated with new sections, additional examples, and changes to figures to enhance understanding of keyword spotting feature engineering in machine learning systems.\nLab: Kws Nicla: The updates to the contents/labs/arduino/nicla_vision/kws_nicla/kws_nicla.qmd file include new sections on the Nicla Vision system, rewrites of the Arduino lab content, the addition of examples for the KWS Nicla system, and changes to the figures to better illustrate the concepts.\nLab: Lab Setup: The “Getting Started” section of the machine learning systems textbook has been updated with new content and an updated overview section. Additionally, a placeholder for more detailed information has been created.\nLab: Labs: The machine learning systems textbook has been updated with a reorganized structure, wording tweaks, and an updated overview section with a new placeholder for details. The labs section has been integrated, and the table in the content has been transposed and updated. There are also updated images and fixed paths for better navigation.\nLab: Motion Classify Ad: The ‘motion_classify_ad.qmd’ file in the Arduino Nicla Vision section has been updated with new examples, figure changes, and significant rewrites for improved clarity and understanding.\nLab: Nicla Vision: The updates to the “Nicla Vision” lab in the Arduino section of the machine learning systems textbook include the addition of the missing Keyword Spotting (KWS) section, updates to the introduction text to avoid duplication, integration of the lab into the main content, fixing of broken links, and updating of images. The lab files for Arduino have also been added and the overall structure of the content has been improved.\nLab: Nicla Vision}: The Arduino Niclavision lab content has been moved to a new location under the Arduino/Nicla Vision directory, and potentially updated with new sections, rewrites, example additions, or figure changes.\nLab: Niclav Sys: The changelog summary for the file “contents/labs/arduino/nicla_vision/niclav_sys/niclav_sys.qmd” is not available as no specific commit messages were provided. Please provide the commit messages to generate a meaningful changelog summary.\nLab: Object Detection Fomo: The object_detection_fomo.qmd file in the Arduino Nicla Vision lab has been updated with new sections, rewrites, additional examples, and changes to figures to enhance understanding of object detection.\nLab: Object Detection}: The ‘Object Detection FOMO’ section in the Arduino Nicla Vision lab has been renamed and updated to ‘Object Detection’, indicating a possible content revision or focus shift in this section.\nLab: Seeed Xiao Esp32S3: The contents/labs/seeed_xiao_esp32S3.qmd file has been updated with new sections, rewrites, example additions, and figure changes. — — — —\nLab: Shared: A new overview has been added to the Shared Labs section.\nLab: Xiao Esp32S3: The machine learning systems textbook has undergone several updates, including the importation and integration of SEEED labs, the addition of structure, and a restructuring of folders. The introduction text has been improved and consolidated into a single file to avoid duplication. An image credit has been added, an acronym has been removed, and build error issues caused by figure labels have been fixed.\nLab: Xiao Esp32S3}: The file for the Xiao ESP32S3 lab has been moved to its own dedicated directory.\nLab: Xiao Esp32S3}: The file “seeed_xiao_esp32S3.qmd” has been relocated from the “labs” directory to the “seeed” directory.\nLab: XIAO Image Classification: The image_classification.qmd file in the SEEED labs section has been updated with the importation of SEEED labs, integration of labs into the system, and a correction in the resources.\nLab: XIAO Keyword Spotting: The SEEED labs were imported and integrated into the textbook, and the resources section was updated in the ‘kws.qmd’ file under the ‘xiao_esp32s3’ section of the ‘seeed’ labs.\nLab: XIAO Motion Classification: The motion_classification.qmd file in the SEEED labs section has been updated with the importation and integration of SEEED labs, a fix to the resources, and the addition of a link to an internal document.\nLab: XIAO Object Detection: The object detection lab in the SEEED section has been updated with corrected resources and is now imported into the SEEED labs.\nLab: XIAO Setup: The setup.qmd file in the SEEED labs section has been updated with the importation of SEEED labs, a renaming process, and an integration into labs. Additionally, resource issues have been fixed.\nMotion Classification}: The motion classification content has been relocated from the ‘motion_classify_ad’ directory to the ‘labs/arduino/nicla_vision/motion_classification’ directory, indicating a possible reorganization or refinement of the content structure.\nObject Detection Fomo: The object detection FOMO chapter has been updated with the integration of new lab exercises.\nSetup}: The file ‘niclav_sys.qmd’ has been relocated to ‘labs/arduino/nicla_vision/setup/setup.qmd’, indicating a possible reorganization of content or change in the structure of the textbook.\n\n\n\n\n\n📅 Published on Jun 11, 2024\n\n\n📖 Chapters\n\n\nChapter 2: ML Systems: The ml_systems.qmd file has been updated with the addition of video callouts and end of section resources, and all exercise call out blocks have been folded for a more streamlined appearance.\nChapter 3: DL Primer: Video callouts and end of section resources were added to the “dl_primer” chapter.\nChapter 5: AI Workflow: Video callouts and end of section resources have been added to the workflow chapter of the machine learning systems textbook. — — — —\nChapter 6: Data Engineering: The data engineering chapter has been updated with the addition of video callouts and end of section resources, along with a revision of exercise call out blocks for improved aesthetics.\nChapter 7: AI Frameworks: The updates to the “Frameworks” chapter in the machine learning systems textbook include the addition of video callouts and end-of-section resources, as well as a more organized presentation of exercise callout blocks.\nChapter 8: AI Training: Added video callouts and end of section resources, improved the presentation of exercise call out blocks, and fixed the rendering of tables in the training chapter of the machine learning systems textbook.\nChapter 9: Efficient AI: The “Efficient AI” section of the machine learning systems textbook has been updated with the addition of video callouts and end-of-section resources.\nChapter 10: Model Optimizations: Video callouts and end of section resources were added to the optimizations chapter, and all exercise call out blocks were folded for a cleaner look.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with video callouts and end of section resources, along with improvements in cross-references for videos. Additionally, all exercise callout blocks have been folded for a more streamlined appearance.\nChapter 12: Benchmarking AI: Added video callouts and end of section resources, and improved the presentation of exercise call out blocks in the benchmarking chapter.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with video callouts and end of section resources, and all exercise call out blocks have been folded for a cleaner look.\nChapter 14: On-Device Learning: The ‘On-Device Learning’ section of the machine learning systems textbook has been updated with added video callouts and end-of-section resources, and all exercise callout blocks have been folded for a cleaner look.\nChapter 15: Security & Privacy: Added video callouts and end of section resources, and improved the visual presentation of exercise call out blocks in the Privacy and Security chapter.\nChapter 16: Responsible AI: Video callouts and end of section resources have been added to the responsible AI section.\nChapter 17: Sustainable AI: Video callouts and end of section resources were added to the sustainable AI chapter, and all exercise call out blocks were folded for a cleaner look.\nChapter 18: Robust AI: The robust AI section of the machine learning systems textbook has been updated with video callouts and end of section resources, an expanded general description, and additional references and links. There’s also new information added about Bayesian Neural Networks.\nChapter 19: AI for Good: The “AI for Good” section has been updated with the addition of video callouts and end of section resources, and all exercise call out blocks have been folded for a more streamlined appearance.\nGenerative Ai: The “Generative AI” section of the machine learning systems textbook has been updated with the addition of upcoming content text.\n\n\n\n\n\n📅 Published on Jun 01, 2024\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction section of the machine learning systems textbook has been enhanced for better readability and grammar, and the reference or URL links that were previously removed have been reinstated. — — — —\nChapter 2: ML Systems: The machine learning systems textbook has been updated to fix rendering issues and improve the presentation of labs, exercises, and slides.\nChapter 3: DL Primer: The Colab badge that was broken during a global replacement in a previous commit has been fixed, and the usage of (exr?)- for Colabs has been updated. Additionally, the “coming soon” section now includes bullet points.\nChapter 5: AI Workflow: The “Coming Soon” section in the workflow chapter has been updated with bullet points, and the default note for slides has been adjusted for optimal PDF rendering.\nChapter 6: Data Engineering: The Data Engineering chapter has been updated to fix a minor markdown issue in text and URL highlighting.\nChapter 7: AI Frameworks: The Colab badge that was broken during a global replacement in a previous commit has been fixed, and the ‘(exr?)-’ tag is now being used for Colabs. The ‘coming soon’ section has been updated with bullet points. The default note for slides has been adjusted to render well in PDF.\nChapter 8: AI Training: The Colab badge that was broken during a global replacement in a previous commit has been fixed, and the ‘(exr?)-’ notation is now being used for Colabs. The ‘coming soon’ section has been updated with bullet points. The default note for slides has been adjusted to ensure it renders well in PDF format.\nChapter 9: Efficient AI: The updates to the machine learning systems textbook include corrections to rendering issues and headers in the “Efficient AI” chapter, as well as adjustments to the default note for slides to ensure optimal rendering in PDF format.\nChapter 10: Model Optimizations: The optimizations chapter has been updated with enhanced content for the ‘coming soon’ section, and improvements have been made to the rendering of notes in slides for better PDF output.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file was updated to fix a broken Colab badge, implement the use of (exr?)- for Colabs, and modify the ‘coming soon’ section to include bullet points.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with a fixed Colab badge, improved rendering for slides in PDF format, and a restructured ‘coming soon’ section with bullet points.\nChapter 13: ML Operations: The Colab badge that was broken during a global replacement has been fixed, and the ‘(exr?)-’ tag is now being used for Colabs. Additionally, the ‘coming soon’ section has been updated with bullet points.\nChapter 14: On-Device Learning: The on-device learning chapter in the machine learning systems textbook has been updated with corrections to headers and the Colab badge, along with updates to the ‘coming soon’ section.\nChapter 15: Security & Privacy: The privacy and security chapter in the machine learning systems textbook has been updated with improved Colab badge functionality and restructured “coming soon” sections.\nChapter 16: Responsible AI: The “Coming Soon” section in the Responsible AI chapter has been updated with bullet points, and the default note for slides has been adjusted to improve PDF rendering.\nChapter 17: Sustainable AI: The Colab badge that was broken during a previous global replacement has been fixed, and the ‘Coming Soon’ section has been updated with bullet points.\nChapter 18: Robust AI: The updates fixed rendering issues and improved the integration with Colab, but did not include any significant content-level changes such as new sections, rewrites, example additions, or figure changes.\nChapter 19: AI for Good: The AI for Good chapter has been updated with improvements to the Colab badge, modifications to the ‘coming soon’ section now featuring bullet points, and adjustments to the default note for slides to enhance PDF rendering.\nGenerative Ai: The updates to the ‘Generative AI’ chapter include fixes to issues related to incorrect rendering.\n\n\n\n\n\n📅 Published on May 26, 2024\n\n\n📄 Frontmatter\n\n\nAcknowledgements: The acknowledgements.qmd file has been initiated with its first draft and image logos have been added.\nMl Systems}: The embedded systems chapter has been moved and updated to focus on machine learning systems, with new sections added, examples updated, and figures revised to align with the new focus.\n\n\n\n\n📖 Chapters\n\n\nChapter 1: Introduction: The introduction chapter has been enhanced with the addition of section headers for cross-referencing, a cover image, and an image for Mark’s article. A reference section and structure have also been introduced.\nChapter 2: ML Systems: The updated file includes added section headers for cross-referencing, fixed figure captions and references, captions added to all tables, and a word change from “algorithms” to “models”. The “Embedded Systems” section was removed to focus solely on ML systems. There were also edits made to chapters 1-4, and the final colabs were added.\nChapter 3: DL Primer: The conclusion section of the machine learning systems textbook has been updated, along with changes to the Data Diversity and Quality section. New section headers have been added for cross-referencing, and figure captions and references have been corrected. Captions have been added to all tables and short captions for videos. Videos have also been added to enhance the content. The resources introduction has been updated, and the Colab notebooks have been updated in the dl_primer.qmd file.\nChapter 5: AI Workflow: The updated content in the machine learning systems textbook includes fixed slide links, cleaned up erroneous sections, added section headers for cross-referencing, corrected bibliographic file errors, added captions to all tables, and made a stylistic change to the ‘Coming Soon’ text.\nChapter 6: Data Engineering: The data_engineering.qmd file has been updated with new section headers for cross-referencing, captions added to all tables, more slides, and exercises. The file also underwent some reference fixes and link corrections. The SVG to PNG change for colab-badge.svg was made to enable PDF builds.\nChapter 7: AI Frameworks: The updates to the machine learning systems textbook include fixing broken slide links, adding section headers for cross-referencing, correcting figure captions and references, adding captions to all tables, and updating the content in the frameworks.qmd file. Additionally, SVG was fixed to PNG for colab-badge.svg to enable PDF builds, and the ‘Coming soon’ text was stylistically changed.\nChapter 8: AI Training: The updated training.qmd file now includes added section headers for cross-referencing, fixed figure captions and references, and added captions to all tables and videos. A missing figure has been fixed, and videos have been added to enhance the content. The file also underwent a cleanup of erroneous sec-slides, sec-exercises, and sec-labs. The SVG for the colab-badge has been fixed to PNG to enable PDF builds.\nChapter 9: Efficient AI: The updated file includes added references, fixed image path references, cleaned up erroneous sections, added section headers for cross-referencing, fixed figure captions and references, added captions to all tables, added more slides, and set ‘collapse’ to false.\nChapter 10: Model Optimizations: The optimizations.qmd file in the machine learning systems textbook has been updated with the addition of new sections for cross-referencing, more slides, and exercises. Videos and captions for the videos and tables have also been added. Broken links have been fixed, and a local save option has been implemented for PDF builds that don’t support remote links. The figure captions and references have been corrected, and the SVG has been converted to PNG for PDF builds.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with added section headers for cross-referencing, fixed broken links, corrected figure captions and references, and added short captions for videos. In addition, the SVG has been fixed to PNG for colab-badge.svg to enable PDF builds, and the file hw_acceleration.qmd has been updated.\nChapter 12: Benchmarking AI: The benchmarking chapter has been updated with new section headers for cross-referencing, weeks of grammar edits, and a change from SVG to PNG for the colab-badge image to enable PDF builds.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with several changes including the addition of section headers for cross-referencing, fixing of figure captions and references, addition of captions to all tables, and inclusion of short captions for videos. The file has also been updated with the addition of videos and the renaming of ‘embedded ops’ to ‘ops’. The SVG has been fixed to PNG for colab-badge.svg to enable PDF builds.\nChapter 14: On-Device Learning: The ondevice_learning.qmd file has been significantly updated with the addition of more colabs, slides, and videos. The conclusion section has been revised, and exercises have been included. All tables now have captions, and short captions have been added for the videos. The file has also been made compatible for PDF builds by fixing SVG to PNG for colab-badge.svg.\nChapter 15: Security & Privacy: The privacy_security.qmd file in the machine learning systems textbook has been updated with added section headers for cross-referencing, captions for all tables and videos, and the addition of ‘colabs’ to the file. The links to slides have been fixed and cleaned up, and the SVG for the colab-badge has been changed to PNG to enable PDF builds.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated with added section headers for cross-referencing, fixed broken links, and included short captions for the videos.\nChapter 17: Sustainable AI: The conclusion section of the sustainable_ai chapter was updated, a new section about exercises was added to the resources section, and figure captions and references were corrected. Additionally, the duplicate ‘introduction’ identifier was removed along with erroneous sections, and the SVG was fixed to PNG for colab-badge.svg to enable PDF builds.\nChapter 18: Robust AI: The Robust AI chapter has been significantly updated with the addition of new sections, including a resources section and learning objectives. There have been several changes to the figures, including the addition of new images, renaming, and fixing of paths and references. The chapter has also been proofread multiple times, resulting in grammar and punctuation fixes, and the text has been restructured into paragraphs for better readability. Contributions from Prof. Song Han and Prof. Yanjing have been incorporated, and all Colabs have been added to robust_ai.qmd.\nChapter 19: AI for Good: The updates to the “AI for Good” chapter include the addition of section headers for cross-referencing, the inclusion of short captions for videos, and a change from SVG to PNG for the colab-badge image to enable PDF builds.\nChapter 20: Conclusion: The conclusion section of the machine learning systems textbook has been updated with minor grammar fixes and improvements, particularly around the discussion of frameworks. Additionally, a new cover image has been added, and the initial draft of the conclusion has been written.\nEmbedded Ml: The embedded machine learning section of the textbook has been updated with edits to chapters 1-4.\nEmbedded Sys: The embedded systems section of the machine learning systems textbook has been updated with edits to chapters 1-4, the addition of more slides, and a change in settings to prevent collapse.\nGenerative Ai: The generative_ai.qmd file has been updated with added section headers for cross-referencing.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The changelog does not contain any meaningful content-level changes like new sections, rewrites, example additions, or figure changes, as the updates were only related to punctuation and grammar corrections.\nKws Feature Eng: The changelog does not contain any content-level changes such as new sections, rewrites, example additions, or figure changes, only punctuation fixes were made.\nLab: Arduino Image Classification: No content-level changes were made to the image_classification.qmd file, only punctuation fixes were implemented. — — — —\nMotion Classify Ad: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes, as the update only pertains to punctuation fixes.\nNiclav Sys: The changelog does not contain any meaningful content-level changes such as new sections, rewrites, example additions, or figure changes. The updates were only related to punctuation fixes.\n\n\n\n\n\n📅 Published on Mar 21, 2024\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The “Resources” section in the “dl_primer.qmd” file has been updated with introductory text for each part and a collapse feature has been enabled. More slides have been added and moved to the end of the page. An empty “Resources” section with headers has also been added to the end of all QMDs.\nChapter 5: AI Workflow: The “Resources” section in the workflow.qmd file has been updated with introductory text for each part and a new feature to collapse these sections has been added. Additionally, the slides have been relocated to the end of the page. An empty “Resources” section has also been added to all QMDs along with headers. — — — —\nChapter 6: Data Engineering: The data_engineering.qmd file has been updated with examples on how to refer to exercises, an introduction text for each part in the Resources section, exercise callouts, and a new “Resources” section at the end of all QMDs. Additionally, slides have been moved to the end of the page.\nChapter 7: AI Frameworks: The “Frameworks” chapter in the machine learning systems textbook has been updated with an introduction to each part in the “Resources” section, which has also been made collapsible. More slides have been added and moved to the end of the page. Additionally, the chapter has been updated with Colab integration.\nChapter 8: AI Training: The “Resources” section of the training.qmd file has been updated with introductory text for each part and a collapse feature. Additionally, more slides have been added and moved to the end of the page. An empty “Resources” section with headers has also been added to all QMDs.\nChapter 9: Efficient AI: The “Resources” section was added to all QMDs with introductory text for each part and a new feature to collapse the section was enabled. Additionally, more slides were incorporated into the content.\nChapter 10: Model Optimizations: The “Resources” section in the optimizations chapter has been updated with introductory text for each part, and a collapsible feature has been enabled. Additionally, the slides have been relocated to the end of the page. An empty “Resources” section has also been added to the end of all QMDs with appropriate headers.\nChapter 11: AI Acceleration: The “hw_acceleration.qmd” file has been updated with an added “Resources” section at the end, which includes introductory text for each part.\nChapter 12: Benchmarking AI: The “Benchmarking” chapter in the machine learning systems textbook has been updated with a new “Resources” section at the end, which now includes introductory text for each part. Additionally, the slides have been relocated to the end of the page.\nChapter 13: ML Operations: The “Resources” section in the ops.qmd file has been updated with introductory text for each part and now has a collapse feature. More slides have been added and moved to the end of the page. An empty “Resources” section has also been added to the end of all QMDs along with headers.\nChapter 14: On-Device Learning: The “On-Device Learning” chapter has been updated with an introduction text for each part in the Resources section, more slides have been added, and these slides have been moved to the end of the page. Additionally, an empty “Resources” section with headers has been added to the end of all QMDs.\nChapter 15: Security & Privacy: The “Privacy and Security” chapter in the machine learning systems textbook has been updated with an introduction text for each part in the “Resources” section, additional slides for enhanced understanding, and a reorganization of content where slides have been moved to the end of the page.\nChapter 16: Responsible AI: The “Responsible AI” section of the machine learning systems textbook has been updated with an introductory text for each part in the “Resources” section, which has also been enabled with a collapse feature. Additionally, the slides have been moved to the end of the page, and an empty “Resources” section with headers has been added to all QMDs.\nChapter 17: Sustainable AI: The “Sustainable AI” chapter has been updated with an introductory text for each part in the “Resources” section, which has also been moved to the end of the page. Additionally, a new “Resources” section has been added to all QMDs.\nChapter 19: AI for Good: The “AI for Good” chapter has been updated with an introduction text for each part in the “Resources” section, which has also been moved to the end of the page. Additionally, a new “Resources” section has been added to all QMDs.\nEmbedded Ml: The “Embedded ML” chapter has been updated with an enriched “Resources” section, which now includes introductory text for each part and has a collapse feature enabled. More slides have been added and moved to the end of the page. An empty “Resources” section has been added to the end of all QMDs with corresponding headers. The exercises section has been cleaned up for better clarity and organization.\nEmbedded Sys: The “Resources” section in the “Embedded Systems” chapter has been updated with introductory text for each part and a new feature to collapse content has been added. Additionally, more slides have been included and moved to the end of the page.\n\n\n\n\n\n📅 Published on Mar 12, 2024\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The dl_primer.qmd file in the machine learning systems textbook has been updated with additional slides and the implementation of (Non-) ASCII checker scripts, along with corresponding fixes.\nChapter 5: AI Workflow: The updates to the workflow.qmd file include the addition of more slides, corrections to the notes from the previous week, the introduction of (Non-) ASCII checker scripts, and the completion of six chapters.\nChapter 6: Data Engineering: The data engineering chapter has been updated with new colab notebooks, more slides have been added, a web scraping exercise has been introduced after a subsection and also at the end as a separate part, last week’s notes have been corrected, (Non-) ASCII checker scripts have been added and fixed, and six chapters have been updated.\nChapter 7: AI Frameworks: The updates to the “Frameworks” chapter of the machine learning systems textbook include the addition of Colab notebooks, more slides, fixes to the notes from the previous week, and the implementation of (Non-) ASCII checker scripts.\nChapter 8: AI Training: The training.qmd file in the machine learning systems textbook has been updated with additional slides and a new section on (Non-) ASCII checker scripts.\nChapter 9: Efficient AI: The ‘efficient_ai’ chapter has been updated with the addition of non-ASCII checker scripts and relevant fixes. —\nChapter 10: Model Optimizations: The optimizations chapter of the machine learning systems textbook has been updated with corrections to the previous week’s notes, the addition of a (Non-) ASCII checker script, and content for six new chapters.\nChapter 11: AI Acceleration: The ‘hw_acceleration.qmd’ file in the machine learning systems textbook has been updated with the removal of a figure reference in the text, deletion of the mermaid section, and the addition of Non-ASCII checker scripts.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with additional slides, corrections to the previous week’s notes, the inclusion of (Non-) ASCII checker scripts, and content spanning six new chapters.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with additional slides, corrections to the previous week’s notes, and the inclusion of (Non-) ASCII checker scripts.\nChapter 14: On-Device Learning: The “On-Device Learning” chapter has been updated with additional slides and the inclusion of (Non-) ASCII checker scripts, along with corresponding fixes.\nChapter 15: Security & Privacy: The “Privacy and Security” section of the machine learning systems textbook has been updated with additional slides, corrected notes from the previous week, and content spanning six new chapters.\nChapter 16: Responsible AI: The “Responsible AI” section of the machine learning systems textbook has been updated with additional slides.\nChapter 17: Sustainable AI: The “Sustainable AI” chapter has been updated with additional slides and the inclusion of (Non-) ASCII checker scripts, along with corresponding fixes.\nChapter 19: AI for Good: The “AI for Good” chapter has been updated with additional slides and implemented (Non-) ASCII checker scripts, along with corresponding fixes.\nEmbedded Ml: The embedded machine learning chapter has been updated with the removal of debug code, the addition of a nested example, more slides, and custom callouts. Furthermore, arrow capability has been added to the style file along with checks.\nEmbedded Sys: The embedded systems chapter has been updated with additional slides and the inclusion of (Non-) ASCII checker scripts, along with corresponding fixes.\n\n\n\n\n🧑‍💻 Labs\n\n\nNiclav Sys: The updates to the ‘niclav_sys.qmd’ file include corrections to the links and the addition of (Non-) ASCII checker scripts, along with associated fixes.\n\n\n\n\n\n📅 Published on Feb 03, 2024\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The video rendering issue in the Deep Learning Primer has been fixed.\nChapter 11: AI Acceleration: The video rendering section in the hardware acceleration chapter has been fixed.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated to improve list consistency and remove unpopulated list items.\nChapter 13: ML Operations: The MCU example for smartwatch in the operations chapter has been updated and a new reference citation has been added.\nChapter 14: On-Device Learning: The updates to the file primarily focused on correcting the rendering of itemised lists in the ‘On-Device Learning’ chapter.\nChapter 15: Security & Privacy: The video rendering issue in the Privacy section has been fixed, and improvements have been made to the hyperlinking in the GDPR and CCPA sections of the Security chapter, including added clarity on the CCPA summary.\nChapter 17: Sustainable AI: The updates to the sustainable AI section of the machine learning systems textbook include the addition of a citation for the OECD blueprint paper.\nChapter 19: AI for Good: The “AI for Good” chapter has been updated to fix video rendering issues and resolve problems with YouTube shortened URLs.\n\n\n\n\n\n📅 Published on Feb 02, 2024\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The dl_primer.qmd file has been updated with the replacement of svg images with png for pdf builds. — — — —\nChapter 6: Data Engineering: The data engineering chapter now includes a new web scraping exercise on Colab, updates to all bibtex references, modifications to the callout content, and a resolution to png error issues through conversion to jpg.\nChapter 8: AI Training: The “Training” chapter in the machine learning systems textbook has been updated with automatic updates to all bibtex references.\nChapter 10: Model Optimizations: A reference for quantization-aware pruning was added, an incomplete sparsity matrix filter illustration was removed as it was included further below, and all bibtex references were updated automatically in the optimizations chapter.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been updated with corrected image references/links, fixed bibliography issues, and updated bibtex references. Additionally, changes may have been made in four chapters, although the specific content changes are not detailed in the commit messages.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated with corrected reference rendering, four new chapters, and automatic updates to all bibtex references.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been updated with corrections to several broken image references/links, specifically the rendering of figure 14.3, and an automatic update to all bibtex references.\nChapter 14: On-Device Learning: The on-device learning chapter in the machine learning systems textbook has been updated with changes including the removal of a hyperlinked image due to a non-existent source, and updates to a bullet list.\nChapter 15: Security & Privacy: The updates to the machine learning systems textbook include fixing several broken image references/links, correcting grammar, fixing issues with reference rendering, and resolving issues with a video URL and its rendering that was previously blocking the display of remaining content. Additionally, all bibtex references have been updated automatically.\nChapter 16: Responsible AI: The updates in the responsible_ai.qmd file include the completion of part-2, the correction of a redundant citation issue related to the usage of ‘@’, and an automatic update of all bibtex references.\nChapter 17: Sustainable AI: The updates in the sustainable AI chapter include fixes to several broken image references and links, completion of the second part, automatic update of all bibtex references, and a repair of a broken chapter link.\nChapter 19: AI for Good: The ‘AI for Good’ chapter has been updated with corrections to broken image references/links, addition of four new chapters, and automatic updates to all bibtex references.\nEmbedded Ml: The embedded machine learning chapter has been updated with new PNG images.\nEmbedded Sys: The ‘embedded_sys.qmd’ file has been updated with automatic updates to all bibtex references.",
    "crumbs": [
      "Book Changelog"
    ]
  },
  {
    "objectID": "contents/frontmatter/changelog/changelog.html#changes-2",
    "href": "contents/frontmatter/changelog/changelog.html#changes-2",
    "title": "Book Changelog",
    "section": "2023 Changes",
    "text": "2023 Changes\n\n📅 Published on Dec 19, 2023\n\n\n📖 Chapters\n\n\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has been updated with added figures and corrections to broken references. — — — —\n\n\n\n\n\n📅 Published on Dec 18, 2023\n\n\n📖 Chapters\n\n\nChapter 7: AI Frameworks: The frameworks chapter in the machine learning systems textbook has been updated with modifications to the frameworks colab, courtesy of contributions from Marcelo. — — — —\nChapter 10: Model Optimizations: The update does not include any content-level changes such as new sections, rewrites, example additions, or figure changes, it was merely a fix for a markdown issue with windows.\nChapter 12: Benchmarking AI: The content of the ‘benchmarking’ section has been migrated to the ‘benchmarks/leaderboards’ section, and an issue causing more than two references to show up due to incorrect use of comma instead of semicolon as a separator has been fixed.\nChapter 17: Sustainable AI: The sustainable AI chapter in the machine learning systems textbook has been updated with content migration to the benchmarks/leaderboards section, addition of new material in Chapter 17, and corrections in wording regarding power draw. Issues with multiple references and markdown compatibility with Windows have also been fixed.\n\n\n\n\n\n📅 Published on Dec 13, 2023\n\n\n📖 Chapters\n\n\nChapter 7: AI Frameworks: The frameworks chapter has been updated with improvements to the Colab section, thanks to contributions from Marcelo.\nChapter 8: AI Training: The file “training.qmd” in the machine learning systems textbook has been updated with a revised path.\nChapter 9: Efficient AI: The URL link in the “Efficient AI” chapter was fixed.\nChapter 10: Model Optimizations: The reference for the attention paper has been updated in the optimizations section. — — — —\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has been updated to fix reference spacing.\n\n\n\n\n\n📅 Published on Dec 12, 2023\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The DL primer activation function has been removed and the computation graph has been relocated to the training section.\nChapter 5: AI Workflow: The “workflow.qmd” file in the machine learning systems textbook has been updated to ensure consistency in the usage of the term “TinyML”.\nChapter 6: Data Engineering: The term “tinyML” has been updated to “TinyML” for consistency throughout the content in the data engineering chapter.\nChapter 7: AI Frameworks: The TinyML terminology has been standardized throughout the content in the “Frameworks” chapter.\nChapter 8: AI Training: The DL primer activation function was removed from the training content and the computation graph was moved to the training section.\nChapter 10: Model Optimizations: The optimizations.qmd file has been updated with a corrected reference for the attention paper and the term ‘tinyML’ has been standardized to ‘TinyML’ for consistency.\nChapter 11: AI Acceleration: The “hw_acceleration” section of the machine learning systems textbook has been updated to ensure consistent use of the term “TinyML”.\nChapter 12: Benchmarking AI: The “Benchmarking” section of the machine learning systems textbook has been updated for consistency in terminology, changing ‘tinyML’ to ‘TinyML’ throughout.\nChapter 14: On-Device Learning: The ‘On-Device Learning’ chapter in the machine learning systems textbook has been updated with word changes to improve clarity and comprehension.\nChapter 16: Responsible AI: The “Responsible AI” chapter in the machine learning systems textbook has been updated for consistency, with the term “tinyML” now standardized to “TinyML”.\nChapter 18: Robust AI: The robust_ai.qmd file in the machine learning systems textbook has been cleaned up for improved clarity and comprehension.\nEmbedded Ml: The embedded machine learning chapter has been updated with word changes for improved clarity and understanding.\nEmbedded Sys: The “embedded_sys.qmd” file was updated to ensure consistency in the usage of the term “TinyML” throughout the text.\nGenerative Ai: The generative_ai.qmd file in the machine learning systems textbook has been cleaned up for improved readability and understanding.\n\n\n\n\n🧑‍💻 Labs\n\n\nKws Nicla: The TinyML terminology has been updated for consistency throughout the kws_nicla.qmd file.\nLab: Arduino Image Classification: The image_classification.qmd file was updated to maintain consistency in the use of the term “TinyML” throughout the text. — — — —\n\n\n\n\n\n📅 Published on Dec 11, 2023\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The Deep Learning primer section on activation function was removed and the computation graph was moved to the training section, a massive reorganization of files into a new folder structure was implemented, and distributed references were introduced so each chapter now has its own reference files. Additionally, subfolders were created within the images folder based on file type.\nChapter 5: AI Workflow: The workflow chapter of the machine learning systems textbook has undergone a massive reorganization of files into a new folder structure, with the creation of subfolders within the images folder based on file type, and the distribution of references so each chapter now has its own reference files.\nChapter 6: Data Engineering: The textbook has undergone a significant reorganization with files moved into a new folder structure, and references have been distributed so that each chapter now has its own reference files. Additionally, the term “tinyML” has been updated to “TinyML” for consistency throughout the text.\nChapter 7: AI Frameworks: The textbook has undergone a significant reorganization with files sorted into a new folder structure, including the creation of subfolders within the images directory based on file type. Each chapter now has its own dedicated references files. A broken URL has been fixed.\nChapter 8: AI Training: The DL primer activation function has been removed and the computation graph has been moved to the training section. There has also been a significant reorganization of the files into a new folder structure, including the creation of subfolders within the images folder based on file type. Additionally, references have been distributed so that each chapter now has its own reference files, and these references have been corrected to appear before the period.\nChapter 9: Efficient AI: The “Efficient AI” chapter of the machine learning systems textbook has been updated with references to mentioned datasets, ResNet-SE, and ResNeXt papers. The chapter also underwent a reorganization of files into a new folder structure, and the addition of a cover image. Furthermore, a broken figure has been fixed, and references have been distributed so that each chapter now has its own reference files.\nChapter 10: Model Optimizations: The optimizations.qmd file in the machine learning systems textbook has been updated with two missing references and a duplicate sentence about the lottery ticket hypothesis has been removed. Additionally, the file structure has undergone a massive reorganization, with the creation of subfolders within images/ based on filetype, and the distribution of references so each chapter has its own files. A new feature has also been added for book generation.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file has been updated with changes in future trends related to hardware acceleration, added references for Machine Learning/Reinforcement Learning for Architecture DSE, GA, RL for chip floorplanning, and RL for logic synthesis. The file structure has also been massively reorganized, including the creation of subfolders within the images folder based on file type.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has undergone a significant reorganization of its file structure, including the creation of subfolders within the images directory based on file type, and the distribution of references so that each chapter now has its own reference files.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been significantly reorganized with distributed references for each chapter, creation of subfolders within images based on file type, and a massive restructuring of the files into a new folder structure.\nChapter 14: On-Device Learning: The ondevice_learning.qmd file has undergone a significant reorganization into a new folder structure, with the creation of subfolders within images/ based on filetype. Additionally, the chapter now has its own references file due to the distribution of references.\nChapter 15: Security & Privacy: The privacy and security chapter in the machine learning systems textbook has been reorganized with distributed references, meaning each chapter now has its own references files. Additionally, the images have been sorted into subfolders based on file type.\nChapter 16: Responsible AI: The textbook has undergone a massive reorganization with files being distributed into a new folder structure, including the creation of subfolders within the images folder based on file type. Additionally, each chapter now has its own separate references files.\nChapter 17: Sustainable AI: The sustainable_ai.qmd file has undergone a significant reorganization into a new folder structure, with the addition of distributed references where each chapter now has its own reference files, and the creation of subfolders within the images folder based on file type.\nChapter 18: Robust AI: The robust_ai.qmd file has undergone a significant reorganization into a new folder structure, and references have been distributed so that each chapter now has its own reference files.\nChapter 19: AI for Good: The AI for Good chapter underwent a significant reorganization of its files into a new folder structure, with the creation of subfolders within the images folder based on file type, and the distribution of references files to each individual chapter.\nEmbedded Ml: The embedded machine learning chapter underwent a significant reorganization of its files into a new folder structure, distributed references so each chapter has its own reference files, and created subfolders within images based on file type.\nEmbedded Sys: The TinyML terminology has been standardized throughout the text, references have been distributed so each chapter has its own files, and a significant reorganization of files into a new folder structure has been implemented.\nGenerative Ai: The generative_ai.qmd file has undergone a significant reorganization into a new folder structure, with image subfolders now categorized based on file type, and each chapter now containing its own separate references files.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The dsp_spectral_features_block chapter has undergone a significant reorganization with distributed references, creation of subfolders within images based on file type, and a massive restructuring of files into a new folder structure.\nKws Feature Eng: The kws_feature_eng.qmd file in the machine learning systems textbook has undergone a significant reorganization into a new folder structure, with the creation of subfolders within the images/ based on file type and the distribution of references files into individual chapters.\nKws Nicla: The textbook has undergone a significant reorganization with files being distributed into a new folder structure, including the creation of subfolders within the images folder based on file type. Additionally, each chapter now has its own references files.\nLab: Arduino Image Classification: The image classification chapter has undergone a significant reorganization of its files into a new folder structure, including the creation of subfolders within the images folder based on filetype. Additionally, each chapter now has its own dedicated references files. — — — —\nMotion Classify Ad: The motion_classify_ad.qmd file in the machine learning systems textbook has undergone a significant reorganization into a new folder structure, with the creation of subfolders within the ‘images/’ directory based on file type, and the distribution of reference files to their respective chapters.\nNiclav Sys: The textbook has undergone a massive reorganization with files being sorted into a new folder structure, each chapter now has its own references files, and subfolders have been created within the images folder based on file type.\nObject Detection Fomo: The object_detection_fomo.qmd file has undergone a significant reorganization into a new folder structure, with the addition of individual reference files for each chapter and the creation of subfolders within the images folder based on file type.\n\n\n\n\n\n📅 Published on Dec 10, 2023\n\n\n📖 Chapters\n\n\nChapter 3: DL Primer: The updates involved a major reorganization of files into a new folder structure and the creation of subfolders within the images folder based on file type.\nChapter 5: AI Workflow: The workflow.qmd file has been significantly reorganized with a new folder structure, and subfolders have been created within the images directory based on file type.\nChapter 6: Data Engineering: The data_engineering.qmd file has been significantly reorganized with a new folder structure, and subfolders have been created within the images directory based on file type.\nChapter 7: AI Frameworks: The contents/frameworks/frameworks.qmd file has been significantly restructured with a massive reorganization of the files into a new folder structure, and the creation of subfolders within the images/ directory based on filetype.\nChapter 8: AI Training: The training.qmd file has undergone a significant reorganization into a new folder structure, and the images have been sorted into subfolders based on their file type.\nChapter 9: Efficient AI: The file efficient_ai.qmd in the ‘efficient_ai’ section has undergone a significant reorganization with files being sorted into a new folder structure, and subfolders have been created within ‘images/’ based on file type.\nChapter 10: Model Optimizations: The optimizations chapter in the machine learning systems textbook has undergone a significant reorganization of its file and folder structure, and subfolders have been created within the images section based on file type.\nChapter 11: AI Acceleration: The hw_acceleration.qmd file in the machine learning systems textbook has been significantly reorganized, with a new folder structure implemented and subfolders created within the images directory based on file type.\nChapter 12: Benchmarking AI: The benchmarking chapter in the machine learning systems textbook has undergone a significant reorganization of its files and images into a new folder structure for improved navigation and accessibility.\nChapter 13: ML Operations: The ops.qmd file in the machine learning systems textbook has been significantly reorganized, with a new folder structure implemented and subfolders created within the images folder based on file type.\nChapter 14: On-Device Learning: The update involved a significant reorganization of files into a new folder structure and the creation of subfolders within the images directory based on file type.\nChapter 15: Security & Privacy: The file contents/privacy_security/privacy_security.qmd has been significantly reorganized, with a new folder structure implemented and subfolders created within the images/ directory based on file type.\nChapter 16: Responsible AI: The ‘Responsible AI’ chapter in the machine learning systems textbook has undergone a significant reorganization of its files and images, with new subfolders created based on file type for better organization and accessibility.\nChapter 17: Sustainable AI: The update involved a major reorganization of files into a new folder structure and the creation of subfolders within the images directory based on file type.\nChapter 18: Robust AI: The robust_ai.qmd file in the machine learning systems textbook has undergone a significant reorganization for improved structure and accessibility.\nChapter 19: AI for Good: The update involved a major reorganization of files into a new folder structure and the creation of subfolders within the ‘images/’ directory based on file type.\nEmbedded Ml: The updates involved a major reorganization of the files into a new folder structure and the creation of subfolders within the images folder based on file type for the ‘Embedded Machine Learning’ chapter.\nEmbedded Sys: The embedded systems chapter has undergone a significant reorganization of files into a new folder structure, and subfolders have been created within the images directory based on file type.\nGenerative Ai: The changelog does not provide specific content-level changes such as new sections, rewrites, example additions, or figure changes in the “generative_ai.qmd” file. The updates mentioned are related to file and folder organization, specifically the creation of subfolders within the “images/” directory based on file type and a massive reorganization of files.\n\n\n\n\n🧑‍💻 Labs\n\n\nDsp Spectral Features Block: The file contents/dsp_spectral_features_block/dsp_spectral_features_block.qmd has been extensively reorganized for better structure, and subfolders have been created within the images/ directory based on file type.\nKws Feature Eng: The update involved a massive reorganization of files into a new folder structure and the creation of subfolders within the images folder based on file type.\nKws Nicla: The file contents/kws_nicla/kws_nicla.qmd has been significantly reorganized with a new folder structure, and subfolders have been created within the images/ directory based on file type.\nLab: Arduino Image Classification: The image_classification.qmd file has been significantly restructured with a new organization of files into specific subfolders within images/, based on their file types.\nMotion Classify Ad: The file motion_classify_ad.qmd in the machine learning systems textbook has undergone a significant reorganization, with files being systematically arranged into a new folder structure, and subfolders being created within the images folder based on file type.\nNiclav Sys: The textbook has undergone a massive reorganization with files being sorted into a new folder structure, and subfolders have been created within the images folder based on file type.\nObject Detection Fomo: The object_detection_fomo.qmd file has undergone a significant reorganization with the creation of subfolders within the images/ directory based on file type.",
    "crumbs": [
      "Book Changelog"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Funding Agencies and Companies\nThis book, inspired by the TinyML edX course and CS294r at Harvard University, is the result of years of hard work and collaboration with many students, researchers and practitioners. We are deeply indebted to the folks whose groundbreaking work laid its foundation.\nAs our understanding of machine learning systems deepened, we realized that fundamental principles apply across scales, from tiny embedded systems to large-scale deployments. This realization shaped the book’s expansion into an exploration of machine learning systems with the aim of providing a foundation applicable across the spectrum of implementations.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html#sec-acknowledgements-funding-agencies-companies-2469",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html#sec-acknowledgements-funding-agencies-companies-2469",
    "title": "Acknowledgements",
    "section": "",
    "text": "Academic Support\nWe are grateful for the academic support that has made it possible to hire teaching assistants to help improve instructional material and quality:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Profit and Institutional Support\nWe gratefully acknowledge the support of the following non-profit organizations and institutions that have contributed to educational outreach efforts, provided scholarship funds to students in developing countries, and organized workshops to teach using the material:\n\n\n\n\n\n\n\n\n\n\n\n\nCorporate Support\nThe following companies contributed hardware kits used for the labs in this book and/or supported the development of hands-on educational materials:",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/acknowledgements/acknowledgements.html#sec-acknowledgements-contributors-2b36",
    "href": "contents/frontmatter/acknowledgements/acknowledgements.html#sec-acknowledgements-contributors-2b36",
    "title": "Acknowledgements",
    "section": "Contributors",
    "text": "Contributors\nWe express our sincere gratitude to the open-source community of learners, educators, and contributors. Each contribution, whether a chapter section or a single-word correction, has significantly enhanced the quality of this resource. We also acknowledge those who have shared insights, identified issues, and provided valuable feedback behind the scenes.\nA comprehensive list of all GitHub contributors, automatically updated with each new contribution, is available below. For those interested in contributing further, please consult our GitHub page for more information.\n\n\n\n\n\n\n\n\nVijay Janapa Reddi\n\n\njasonjabbour\n\n\nIkechukwu Uchendu\n\n\nZeljko Hrcek\n\n\nKai Kleinbard\n\n\n\n\nNaeem Khoshnevis\n\n\nMarcelo Rovai\n\n\nSara Khosravi\n\n\nDouwe den Blanken\n\n\nshanzehbatool\n\n\n\n\nElias\n\n\nJared Ping\n\n\nJeffrey Ma\n\n\nItai Shapira\n\n\nMaximilian Lam\n\n\n\n\nJayson Lin\n\n\nSophia Cho\n\n\nAndrea\n\n\nAlex Rodriguez\n\n\nKorneel Van den Berghe\n\n\n\n\nZishen Wan\n\n\nColby Banbury\n\n\nMark Mazumder\n\n\nDivya Amirtharaj\n\n\nAbdulrahman Mahmoud\n\n\n\n\nSrivatsan Krishnan\n\n\nmarin-llobet\n\n\nEmeka Ezike\n\n\nAghyad Deeb\n\n\nHaoran Qiu\n\n\n\n\nAditi Raju\n\n\nELSuitorHarvard\n\n\nEmil Njor\n\n\nMichael Schnebly\n\n\nJared Ni\n\n\n\n\noishib\n\n\nYu-Shun Hsiao\n\n\nJae-Won Chung\n\n\nHenry Bae\n\n\nJennifer Zhou\n\n\n\n\nArya Tschand\n\n\nEura Nofshin\n\n\nPong Trairatvorakul\n\n\nMatthew Stewart\n\n\nMarco Zennaro\n\n\n\n\nAndrew Bass\n\n\nShvetank Prakash\n\n\nFin Amin\n\n\nAllen-Kuang\n\n\nGauri Jain\n\n\n\n\ngnodipac886\n\n\nThe Random DIY\n\n\nBruno Scaglione\n\n\nFatima Shah\n\n\nSercan Aygün\n\n\n\n\nAlex Oesterling\n\n\nBaldassarre Cesarano\n\n\nAbenezer\n\n\nTheHiddenLayer\n\n\nabigailswallow\n\n\n\n\nyanjingl\n\n\nhappyappledog\n\n\nYang Zhou\n\n\nAritra Ghosh\n\n\nAndy Cheng\n\n\n\n\nBilge Acun\n\n\nJessica Quaye\n\n\nJason Yik\n\n\nEmmanuel Rassou\n\n\nShreya Johri\n\n\n\n\nSonia Murthy\n\n\nVijay Edupuganti\n\n\nCostin-Andrei Oncescu\n\n\nAnnie Laurie Cook\n\n\nJothi Ramaswamy\n\n\n\n\nBatur Arslan\n\n\nCurren Iyer\n\n\nFatima Shah\n\n\nEdward Jin\n\n\na-saraf\n\n\n\n\nsonghan\n\n\nZishen",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html",
    "href": "contents/frontmatter/socratiq/socratiq.html",
    "title": "SocratiQ AI",
    "section": "",
    "text": "AI Learning Companion\nWelcome to SocratiQ (pronounced ``Socratic’’), an AI learning assistant seamlessly integrated throughout this resource. Inspired by the Socratic method of teaching—emphasizing thoughtful questions and answers to stimulate critical thinking—SocratiQ is part of our experiment with what we call as Generative Learning. By combining interactive quizzes, personalized assistance, and real-time feedback, SocratiQ is meant to reinforce your understanding and help you create new connections. SocratiQ is still a work in progress, and we welcome your feedback.\nLearn more: Read our research paper on SocratiQ’s design and pedagogy here.\nYou can enable SocratiQ by clicking the button below:\nSocratiQ’s goal is to adapt to your needs while generating targeted questions and engaging in meaningful dialogue about the material. Unlike traditional textbook study, SocratiQ offers an interactive, personalized learning experience that can help you better understand and retain complex concepts. It is only available as an online feature.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-ai-learning-companion-bc63",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-ai-learning-companion-bc63",
    "title": "SocratiQ AI",
    "section": "",
    "text": "Listen to this AI-generated podcast about SocratiQ. \n\n    \n    \n    \n    \n\n\n\nSocratiQ: OFF\n\n\n\n\n\n\n\n\n\nDirect URL Access\n\n\n\n\n\nYou can directly control SocratiQ by adding ?socratiq= parameters to your URL:\n\nTo activate: mlsysbook.ai/?socratiq=true\nTo deactivate: mlsysbook.ai/?socratiq=false\n\nThis gives you with quick access to toggle SocratiQ’s functionality directly from your browser’s address bar if you are on a page and do not want to return here to toggle functionality.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-quick-start-guide-dbe1",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-quick-start-guide-dbe1",
    "title": "SocratiQ AI",
    "section": "Quick Start Guide",
    "text": "Quick Start Guide\n\nEnable SocratiQ using the button below or URL parameters\nUse keyboard shortcut (Cmd/Ctrl + /) to open SocratiQ anytime\nSet your academic level in Settings\nStart learning! Look for quiz buttons at the end of sections\n\nPlease note that this is an experimental feature. We are experimenting with the idea of creating a dynamic and personalized learning experience by harnessing the power of generative AI. We hope that this approach will transform how you interact with and absorb the complex concepts.\n\n\n\n\n\n\nWarning\n\n\n\nAbout AI Responses: While SocratiQ uses advanced AI to generate quizzes and provide assistance, like all AI systems, it may occasionally provide imperfect or incomplete answers. However, we’ve designed and tested it to ensure it’s effective for supporting your learning journey. If you’re unsure about any response, refer to the textbook content or consult your instructor.\n\n\nOnce you’ve enabled SocratiQ it will always be available when you visit this site.\nYou can access SocratiQ at any time using a keyboard shortcut shown in Figure 1, which brings up the interface shown in Figure 2.\n\n\n\n\n\n\nFigure 1: Keyboard shortcut for SocratiQ.\n\n\n\n\n\n\n\n\n\nFigure 2: The main SocratiQ interface, showing the key components of your AI learning assistant.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-button-overview-b40f",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-button-overview-b40f",
    "title": "SocratiQ AI",
    "section": "Button Overview",
    "text": "Button Overview\nThe top nav bar provides quick access to the following features:\n\nAdjust your settings at any time.\nTrack your progress by viewing the dashboard.\nStart new or save your conversations with SocratiQ.\n\n\n\n\n\n\n\nFigure 3: View of the top nav menu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-personalize-learning-81e4",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-personalize-learning-81e4",
    "title": "SocratiQ AI",
    "section": "Personalize Your Learning",
    "text": "Personalize Your Learning\nBefore diving into your studies, take a moment to configure SocratiQ for your academic level. This initial setup ensures that all interactions, from quiz questions to explanations, are tailored to your background knowledge. Figure 4 shows where you can adjust these preferences.\nYou can augment any AI SocratiQ response using the dropdown menu at the top of each message.\n\n\n\n\n\n\nFigure 4: The settings panel where you can customize SocratiQ to match your academic level.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-learning-socratiq-75e4",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-learning-socratiq-75e4",
    "title": "SocratiQ AI",
    "section": "Learning with SocratiQ",
    "text": "Learning with SocratiQ\n\nQuizzes\nAs you progress through each section of the textbook, you have the option to ask SocratiQ to automatically generate quizzes tailored to reinforce key concepts. These quizzes are conveniently inserted at the end of every major subsection (e.g., 1.1, 1.2, 1.3, and so on), as illustrated in Figure 6.\n\n\n\n\n\n\nFigure 5: Redo an AI message by choosing a new experience level.\n\n\n\n\n\n\n\n\n\nFigure 6: Quizzes are generated at the end of every section.\n\n\n\nEach quiz typically consists of 3-5 multiple-choice questions and takes only 1-2 minutes to complete. These questions are designed to assess your understanding of the material covered in the preceding section, as shown in Figure 7 (a).\nUpon submitting your answers, SocratiQ provides immediate feedback along with detailed explanations for each question, as demonstrated in Figure 7 (b).\n\n\n\n\n\n\n\n\n\n\n\n(a) Example of AI-generated quiz questions.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Example of AI-generated feedback and explanations for quizzes.\n\n\n\n\n\n\n\nFigure 7: SocratiQ uses a Large Language Model (LLM) to automatically generate and grade quizzes.\n\n\n\n\n\nExample Learning Flow\n\nRead a section\nSelect challenging text → Ask SocratiQ for explanation\nTake the section quiz\nReview related content suggestions\nTrack progress in dashboard\n\n\n\nGetting Help with Concepts\nWhen you encounter challenging concepts, SocratiQ offers two powerful ways to get help. First, you can select any text from the textbook and ask for a detailed explanation, as demonstrated in Figure 8.\n\n\n\n\n\n\nFigure 8: Selecting specific text to ask for clarification.\n\n\n\nOnce you’ve selected the text, you can ask questions about it, and SocratiQ will provide detailed explanations based on that context, as illustrated in Figure 9.\n\n\n\n\n\n\nFigure 9: Example of how SocratiQ provides explanations based on selected text.\n\n\n\nFigure 11 shows the response for the ask in Figure 9.\n\n\n\n\n\n\nFigure 10: Referencing different sections from the textbook.\n\n\n\nAdditionally, you can also reference Sections, as shown in Figure 10, Sub-sections and keywords directly as you converse with SocratiQ. Use the @ symbol to reference a section, sub-section or keyword. You can also click the + Context button right above the input.\n\n\n\n\n\n\nFigure 11: An interactive chat session with SocratiQ, demonstrating how to get clarification on concepts.\n\n\n\nTo enhance your learning experience, SocratiQ doesn’t just answer your questions, it also suggests related content from the textbook that might be helpful for deeper understanding, as shown in Figure 12.\n\n\n\n\n\n\nFigure 12: SocratiQ suggests related content based on your questions to help deepen your understanding.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-tracking-progress-4aaf",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-tracking-progress-4aaf",
    "title": "SocratiQ AI",
    "section": "Tracking Your Progress",
    "text": "Tracking Your Progress\n\nPerformance Dashboard\nSocratiQ maintains a comprehensive record of your learning journey. The progress dashboard (Figure 13) displays your quiz performance statistics, learning streaks, and achievement badges. This dashboard updates real-time.\n\n\n\n\n\n\nFigure 13: The progress dashboard showing your learning statistics and achievements.\n\n\n\nAs you continue to engage with the material and complete quizzes, you’ll earn various badges that recognize your progress, as shown in Figure 14.\n\n\n\n\n\n\nFigure 14: Examples of achievement badges you can earn through consistent engagement.\n\n\n\n\n\nAchievement Badges\nAs you progress through the quizzes, you’ll earn special badges to mark your achievements! Here’s what you can earn:\n\n\n\n\n\n\n\n\nBadge\nName\nHow to Earn\n\n\n\n\n\nFirst Steps\nComplete your first quiz\n\n\n\nOn a Streak\nMaintain a streak of perfect scores\n\n\n\nQuiz Medalist\nComplete 10 quizzes\n\n\n\nQuiz Champion\nComplete 20 quizzes\n\n\n\nQuiz Legend\nComplete 30 quizzes\n\n\n x n\nQuiz AGI Super Human\nComplete 40 or more quizzes\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nKeep taking quizzes to collect all badges and improve your learning journey! Your current badges will appear in the quiz statistics dashboard.\n\n\nIf you’d like a record of your progress you can generate a PDF report. It will show your progress, average performance and all the questions you’ve attempted. The PDF is a generated with a unique hash and can be uniquely validated.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-data-storage-736d",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-data-storage-736d",
    "title": "SocratiQ AI",
    "section": "Data Storage",
    "text": "Data Storage\n\n\n\n\n\n\nImportant\n\n\n\nImportant Note: All progress data is stored locally in your browser. Clearing your browser history or cache will erase your entire learning history, including quiz scores, streaks, and achievement badges.\n\n\nYou can also delete all of your saved conversations by clicking the New Chat button in the nav bar.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-technical-requirements-b18f",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-technical-requirements-b18f",
    "title": "SocratiQ AI",
    "section": "Technical Requirements",
    "text": "Technical Requirements\nTo use SocratiQ effectively, you’ll need:\n\nChrome or Safari browser\nJavaScript enabled\nStable internet connection\n\n\n\n\n\n\n\nFigure 15: You can click the Download Report button to view your report. You can verify that your PDF has been created by SocratiQ by clicking the verify button and uploading your generated PDF.\n\n\n\n\n\n\n\n\n\nFigure 16: Load or delete previous chats or start a new chat.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-common-issues-troubleshooting-1553",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-common-issues-troubleshooting-1553",
    "title": "SocratiQ AI",
    "section": "Common Issues and Troubleshooting",
    "text": "Common Issues and Troubleshooting\n\nIf SocratiQ isn’t responding: Refresh the page\nIf quizzes don’t load: Check your internet connection\nIf progress isn’t saving: Ensure cookies are enabled\n\nFor persistent issues, please contact us at vj[@]eecs.harvard.edu.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-providing-feedback-83df",
    "href": "contents/frontmatter/socratiq/socratiq.html#sec-socratiq-ai-providing-feedback-83df",
    "title": "SocratiQ AI",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nYour feedback helps us improve SocratiQ.\nYou can report technical issues, suggest improvements to quiz questions, or share thoughts about AI responses using the feedback buttons located throughout the interface. You can submit a GitHub issue.",
    "crumbs": [
      "SocratiQ AI"
    ]
  },
  {
    "objectID": "contents/parts/foundations.html",
    "href": "contents/parts/foundations.html",
    "title": "Foundations",
    "section": "",
    "text": "This part establishes the core concepts necessary for understanding machine learning systems. It begins with a high-level overview of the field, including the motivation for a systems-oriented approach to machine learning. Subsequent chapters introduce the fundamentals of machine learning, deep learning architectures, and the computational framing used throughout the textbook. These chapters provide the terminology, abstractions, and technical background that will be built upon in later parts of the book.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html",
    "href": "contents/core/introduction/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Purpose\nWhat does it mean to engineer machine learning systems—not just design models?\nAs machine learning becomes deeply embedded in the fabric of modern technology, it is no longer sufficient to treat it purely as an algorithmic or theoretical pursuit. We start by framing the field of Machine Learning Systems Engineering as a discipline that unites the science of learning with the practical realities of deployment, scale, and infrastructure. It explores how this emerging discipline extends beyond model development to encompass the full lifecycle of intelligent systems—from data to deployment, from theory to engineering practice. Understanding this perspective is essential for anyone seeking to build AI systems that are not only powerful, but also reliable, efficient, and grounded in real-world constraints.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ai-pervasiveness-8869",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ai-pervasiveness-8869",
    "title": "1  Introduction",
    "section": "1.1 AI Pervasiveness",
    "text": "1.1 AI Pervasiveness\nArtificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids, and enable billions of wireless devices to communicate seamlessly. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators. In space exploration, it helps rovers navigate distant planets and telescopes detect new celestial phenomena.\nThroughout history, certain technologies have fundamentally transformed human civilization, defining their eras. The 18th and 19th centuries were shaped by the Industrial Revolution, where steam power and mechanization transformed how humans could harness physical energy. The 20th century was defined by the Digital Revolution, where the computer and internet transformed how we process and share information. Now, the 21st century appears to be the era of Artificial Intelligence, a shift noted by leading thinkers in technological evolution (Brynjolfsson and McAfee 2014; Domingos 2016).\n\nBrynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies, 1st Edition. W. W. Norton Company.\n\nDomingos, Pedro. 2016. “The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World.” Choice Reviews Online 53 (07): 53–3100. https://doi.org/10.5860/choice.194685.\nThe vision driving AI development extends far beyond the practical applications we see today. We aspire to create systems that can work alongside humanity, enhancing our problem-solving capabilities and accelerating scientific progress. Imagine AI systems that could help us understand consciousness, decode the complexities of biological systems, or unravel the mysteries of dark matter. Consider the potential of AI to help address global challenges like climate change, disease, or sustainable energy production. This is not just about automation or efficiency—it’s about expanding the boundaries of human knowledge and capability.\nThe impact of this revolution operates at multiple scales, each with profound implications. At the individual level, AI personalizes our experiences and augments our daily decision-making capabilities. At the organizational level, it transforms how businesses operate and how research institutions make discoveries. At the societal level, it reshapes everything from transportation systems to healthcare delivery. At the global level, it offers new approaches to addressing humanity’s greatest challenges, from climate change to drug discovery.\nWhat makes this transformation unique is its unprecedented pace. While the Industrial Revolution unfolded over centuries and the Digital Revolution over decades, AI capabilities are advancing at an extraordinary rate. Technologies that seemed impossible just years ago, including systems that can understand human speech, generate novel ideas, or make complex decisions, are now commonplace. This acceleration suggests we are only beginning to understand how profoundly AI will reshape our world.\nWe stand at a historic inflection point. Just as the Industrial Revolution required us to master mechanical engineering to harness the power of steam and machinery, and the Digital Revolution demanded expertise in electrical and computer engineering to build the internet age, the AI Revolution presents us with a new engineering challenge. We must learn to build systems that can learn, reason, and potentially achieve superhuman capabilities in specific domains.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ai-ml-basics-041a",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ai-ml-basics-041a",
    "title": "1  Introduction",
    "section": "1.2 AI and ML Basics",
    "text": "1.2 AI and ML Basics\nThe exploration of artificial intelligence’s transformative impact across society presents a fundamental question: How can we create these intelligent capabilities? Understanding the relationship between AI and ML provides the theoretical and practical framework necessary to address this question.\n\n\nChapter connection→ Ch 3.2: Classification algorithms in recommendation systems\n→ Ch \\(\\ref{sec-introduction-expert-systems-era-c061}\\): coming soon.\n↩︎ Ch 12.3: Reinforcement learning in autonomous systems\n\n\n\nArtificial Intelligence represents the systematic pursuit of understanding and replicating intelligent behavior—specifically, the capacity to learn, reason, and adapt to new situations. It encompasses fundamental questions about the nature of intelligence, knowledge, and learning. How do we recognize patterns? How do we learn from experience? How do we adapt our behavior based on new information? AI as a field explores these questions, drawing insights from cognitive science, psychology, neuroscience, and computer science.\nMachine Learning, in contrast, constitutes the methodological approach to creating systems that demonstrate intelligent behavior. Instead of implementing intelligence through predetermined rules, machine learning systems utilize gradient descent and other optimization techniques to identify patterns and relationships. This methodology reflects fundamental learning processes observed in biological systems. For instance, object recognition in machine learning systems parallels human visual learning processes, requiring exposure to numerous examples to develop robust recognition capabilities. Similarly, natural language processing systems acquire linguistic capabilities through extensive analysis of textual data.\nThe relationship between AI and ML exemplifies the connection between theoretical understanding and practical engineering implementation observed in other scientific fields. For instance, physics provides the theoretical foundation for mechanical engineering’s practical applications in structural design and machinery, while AI’s theoretical frameworks inform machine learning’s practical development of intelligent systems. Similarly, electrical engineering’s transformation of electromagnetic theory into functional power systems parallels machine learning’s implementation of intelligence theories into operational ML systems.\n\n\n\n\n\n\nAI and ML: Key Definitions\n\n\n\n\nArtificial Intelligence (AI): The goal of creating machines that can match or exceed human intelligence—representing humanity’s quest to build systems that can think, reason, and adapt.\nMachine Learning (ML): The scientific discipline of understanding how systems can learn and improve from experience—providing the theoretical foundation for building intelligent systems.\n\n\n\nThe emergence of machine learning as a viable scientific discipline approach to artificial intelligence resulted from extensive research and fundamental paradigm shifts in the field. The progression of artificial intelligence encompasses both theoretical advances in understanding intelligence and practical developments in implementation methodologies. This development mirrors the evolution of other scientific and engineering disciplines—from mechanical engineering’s advancement from basic force principles to contemporary robotics, to electrical engineering’s progression from fundamental electromagnetic theory to modern power and communication networks. Analysis of this historical trajectory reveals both the technological innovations leading to current machine learning approaches and the emergence of deep reinforcement learning that inform contemporary AI system development.\n\nSelf-Check: Question 1.1\n\nExplain how the relationship between AI and ML is similar to the relationship between physics and mechanical engineering.\nTrue or False: Machine Learning systems implement intelligence through predetermined rules.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ai-evolution-0dd8",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ai-evolution-0dd8",
    "title": "1  Introduction",
    "section": "1.3 AI Evolution",
    "text": "1.3 AI Evolution\nThe evolution of AI, depicted in the timeline shown in Figure 1.1, highlights key milestones such as the development of the perceptron in 1957 by Frank Rosenblatt, a foundational element for modern neural networks. Imagine walking into a computer lab in 1965. You’d find room-sized mainframes running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, while groundbreaking for their time, were a far cry from today’s machine learning systems that can detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA chatbot in 1966, to significant breakthroughs such as IBM’s Deep Blue defeating chess champion Garry Kasparov in 1997. More recent advancements include the introduction of OpenAI’s GPT-3 in 2020 and GPT-4 in 2023, demonstrating the dramatic evolution and increasing complexity of AI systems over the decades.\n\n\n\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\definecolor{bluegraph}{RGB}{0,102,204}\n    \\pgfmathsetlengthmacro\\MajorTickLength{\n      \\pgfkeysvalueof{/pgfplots/major tick length} * 1.5\n    }\n\\tikzset{%\n   textt/.style={line width=0.5pt,draw=bluegraph,text width=26mm,align=flush center,\n                        font=\\usefont{T1}{phv}{m}{n}\\footnotesize,fill=cyan!7},\n   Line/.style={line width=0.85pt,draw=bluegraph,dash pattern=on 3pt off 2pt,\n   {Circle[bluegraph,length=4.5pt]}-   }\n}\n\n\\begin{axis}[clip=false,\n  axis line style={thick},\n  axis lines*=left,\n  axis on top,\n  width=18cm,\n  height=20cm,\n  xmin=1950,\n  xmax=2023,\n  ymin=0.000000,\n  ymax=0.00033,\n  xtick={1950,1960,1970,1980,1990,2000,2010,2020},\n  extra x ticks={1955,1965,1975,1985,1995,2005,2015},\n  extra x tick labels={},\n  xticklabels={1950,1960,1970,1980,1990,2000,2010,2020},\n  ytick={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  yticklabels={0.0000,0.00005, 0.00010, 0.00015, 0.00020, 0.00025, 0.00030},\n  grid=none,\n    tick label style={/pgf/number format/assume math mode=true},\n    xticklabel style={font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n},\n   yticklabel style={\n  font=\\footnotesize\\usefont{T1}{phv}{m}{n},\n  /pgf/number format/fixed,\n  /pgf/number format/fixed zerofill,\n  /pgf/number format/precision=5\n},\nscaled y ticks=false,\ntick style = {line width=1.0pt},\ntick align = outside,\nmajor tick length=\\MajorTickLength,\n]\n\\fill[fill=BrownL!70](axis cs:1974,0)rectangle(axis cs:1980,0.00031)\n        node[above,align=center,xshift=-7mm]{1st AI \\\\ Winter};\n\\fill[fill=BrownL!70](axis cs:1987,0)rectangle(axis cs:1993,0.00031)\n        node[above,align=center,xshift=-7mm]{2nd AI \\\\ Winter};\n\\addplot[line width=2pt,color=RedLine,smooth,samples=100] coordinates {\n(1950,0.0000006281)\n(1951,0.0000000683)\n(1952,0.0000003056)\n(1953,0.0000002927)\n(1954,0.0000004296)\n(1955,0.0000004593)\n(1956,0.0000016705)\n(1957,0.0000006570)\n(1958,0.0000021902)\n(1959,0.0000032832)\n(1960,0.0000126863)\n(1961,0.0000063721)\n(1962,0.0000240680)\n(1963,0.0000141502)\n(1964,0.0000111442)\n(1965,0.0000143832)\n(1966,0.0000147726)\n(1967,0.0000169539)\n(1968,0.0000167880)\n(1969,0.0000175559)\n(1970,0.0000155680)\n(1971,0.0000206809)\n(1972,0.0000223804)\n(1973,0.0000218203)\n(1974,0.0000256138)\n(1975,0.0000282924)\n(1976,0.0000247784)\n(1977,0.0000404966)\n(1978,0.0000358032)\n(1979,0.0000436903)\n(1980,0.0000472788)\n(1981,0.0000561471)\n(1982,0.0000767864)\n(1983,0.0001064465)\n(1984,0.0001592212)\n(1985,0.0002133700)\n(1986,0.0002559067)\n(1987,0.0002608470)\n(1988,0.0002623321)\n(1989,0.0002358150)\n(1990,0.0002301105)\n(1991,0.0002051343)\n(1992,0.0001789229)\n(1993,0.0001560935)\n(1994,0.0001508219)\n(1995,0.0001401406)\n(1996,0.0001169577)\n(1997,0.0001150365)\n(1998,0.0001051385)\n(1999,0.0000981740)\n(2000,0.0001010236)\n(2001,0.0000976966)\n(2002,0.0001038084)\n(2003,0.0000980004)\n(2004,0.0000989412)\n(2005,0.0000977251)\n(2006,0.0000899964)\n(2007,0.0000864005)\n(2008,0.0000911872)\n(2009,0.0000852932)\n(2010,0.0000822649)\n(2011,0.0000913442)\n(2012,0.0001104912)\n(2013,0.0001023061)\n(2014,0.0001022477)\n(2015,0.0000919719)\n(2016,0.0001134797)\n(2017,0.0001384348)\n(2018,0.0002057324)\n(2019,0.0002328642)\n}\nnode[left,pos=1,align=center,black]{Last year of\\\\ date: 2019};\n\n\\node[textt,text width=20mm](1950)at(axis cs:1957,0.00014){\\textcolor{red}{1950}\\\\\nAlan Turing publishes \\textbf{``Computing Machinery and Intelligence''} in the journal \\textit{Mind}.};\n\\node[red,align=center,above=2mm of 1950]{Milestones\\\\ in AI};\n\\draw[Line] (axis cs:1950,0) -- (1950.235);\n%\n\\node[textt,text width=19mm](1956)at(axis cs:1958,0.00007){\\textcolor{red}{Summer 1956}\\\\\n\\textbf{Dartmouth Workshop} A formative conference organized by AI pioneer John McCarthy.};\n\\draw[Line] (axis cs:1956,0) -- (1956.255);\n%\n\\node[textt](1957)at(axis cs:1969,0.00022){\\textcolor{red}{1957}\\\\\n\\textbf{Cornell psychologist Frank Rosenblatt invents the perceptron}, a system that paves the way for\nmodern neural networks\n(see “The Turbulent Past and Uncertain Future of Artificial Intelligence,” p. 26).};\n\\draw[Line] (axis cs:1957,0) -- ++(0mm,17mm)-|(1957.248);\n%\n\\node[textt,text width=21mm](1966)at(axis cs:1972,0.00012){\\textcolor{red}{1966}\\\\\n\\textbf{ELIZA chatbot} An early example of natural-language programming created by\nMIT professor Joseph Weizenbaum.};\n\\draw[Line] (axis cs:1966,0) -- ++(0mm,17mm)-|(1966);\n%\n\\node[textt,text width=20mm](1979)at(axis cs:1985,0.00012){\\textcolor{red}{1979}\\\\\nHans Moravec builds the \\textbf{Stanford Cart}, one of the first autonomous vehicles.};\n\\draw[Line] (axis cs:1979,0) -- ++(0mm,17mm)-|(1979.245);\n%\n\\node[textt,text width=21mm](1981)at(axis cs:1990,0.00006){\\textcolor{red}{1981}\\\\\nJapanese \\textbf{Fifth-Generation Computer Systems} project begins. The infusion of\nresearch funding helps end first “AI winter.”};\n\\draw[Line] (axis cs:1981,0) -- ++(0mm,10mm)-|(1981);\n%\n\\node[textt,text width=15mm](1997)at(axis cs:2001,0.00007){\\textcolor{red}{1997}\\\\\n\\textbf{IBM’s Deep Blue} beats world chess champion Garry Kasparov};\n\\draw[Line] (axis cs:1997,0) -- ++(0mm,10mm)-|(1997);\n%\n\\node[textt,text width=15mm](2011)at(axis cs:2014,0.00003){\\textcolor{red}{2011}\\\\\n\\textbf{IBM’s Watson} wins at Jeopardy!};\n\\draw[Line] (axis cs:2011,0) -- (2011);\n%\n\\node[textt,text width=19mm](2005)at(axis cs:2012,0.00009){\\textcolor{red}{2005}\\\\\n\\textbf{DARPA Grand Challenge} Stanford wins the agency’s second driverless-car\ncompetition by driving 211 kilometers on an unhearsed trail};\n\\draw[Line] (axis cs:2005,0) -- (2005);\n%\n\\node[textt,text width=30mm](2020)at(axis cs:2010,0.00017){\\textcolor{red}{2020}\\\\\n\\textbf{OpenAI introduces GPT-3}. The enormously powerful natural-language model\nlater causes an outcry when it begins spouting bigoted remarks};\n\\draw[Line] (axis cs:2020,0) |- (2020);\n%\n\\draw[Line,solid,-] (axis cs:1991,0.0002) --++(50:35mm)\nnode[bluegraph,above,align=center,text width=30mm]{Percent of U.S.-published books\nin Google’s database that mention artificial intelligence};\n\\end{axis}\n\\end{tikzpicture}\n\n\nFigure 1.1: Milestones in AI from 1950 to 2020. Source: IEEE Spectrum\n\n\n\nLet’s explore how we got here.\n\n1.3.1 Symbolic AI Era\nThe story of machine learning begins at the historic Dartmouth Conference in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term “artificial intelligence.” Their approach was based on a compelling idea: intelligence could be reduced to symbol manipulation. Consider Daniel Bobrow’s STUDENT system from 1964, one of the first AI programs that could solve algebra word problems. It was one of the first AI programs to demonstrate natural language understanding by converting English text into algebraic equations, marking an important milestone in symbolic AI.\n\n\n\n\n\n\nExample: STUDENT (1964)\n\n\n\nProblem: \"If the number of customers Tom gets is twice the\nsquare of 20% of the number of advertisements he runs, and\nthe number of advertisements is 45, what is the number of\ncustomers Tom gets?\"\n\nSTUDENT would:\n\n1. Parse the English text\n2. Convert it to algebraic equations\n3. Solve the equation: n = 2(0.2 × 45)²\n4. Provide the answer: 162 customers\n\n\nEarly AI like STUDENT suffered from a fundamental limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. Imagine a language translator that only works when sentences follow perfect grammatical structure; even slight variations, such as changing word order, using synonyms, or natural speech patterns, would cause the STUDENT to fail. This “brittleness” meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation wasn’t just a technical inconvenience—it revealed a deeper problem with rule-based approaches to AI: they couldn’t genuinely understand or generalize from their programming, they could only match and manipulate patterns exactly as specified.\n\n\n1.3.2 Expert Systems Era\nBy the mid-1970s, researchers realized that general AI was too ambitious. Instead, they focused on capturing human expert knowledge in specific domains. MYCIN, developed at Stanford, was one of the first large-scale expert systems designed to diagnose blood infections.\n\n\n\n\n\n\nExample: MYCIN (1976)\n\n\n\nRule Example from MYCIN:\nIF\n  The infection is primary-bacteremia\n  The site of the culture is one of the sterile sites\n  The suspected portal of entry is the gastrointestinal tract\nTHEN\n  Found suggestive evidence (0.7) that infection is bacteroid\n\n\nWhile MYCIN represented a major advance in medical AI with its 600 expert rules for diagnosing blood infections, it revealed fundamental challenges that still plague ML today. Getting domain knowledge from human experts and converting it into precise rules proved incredibly time-consuming and difficult—doctors often couldn’t explain exactly how they made decisions. MYCIN struggled with uncertain or incomplete information, unlike human doctors who could make educated guesses. Perhaps most importantly, maintaining and updating the rule base became exponentially more complex as MYCIN grew, as adding new rules frequently conflicted with existing ones, while medical knowledge itself continued to evolve. These same challenges of knowledge capture, uncertainty handling, and maintenance remain central concerns in modern machine learning, even though we now use different technical approaches to address them.\n\n\n1.3.3 Statistical Learning Era\nThe 1990s marked a radical transformation in artificial intelligence as the field moved away from hand-coded rules toward statistical learning approaches. This wasn’t a simple choice—it was driven by three converging factors that made statistical methods both possible and powerful. The digital revolution meant massive amounts of data were suddenly available to train the algorithms. Moore’s Law delivered the computational power needed to process this data effectively. And researchers developed new algorithms like Support Vector Machines and improved neural networks that could actually learn patterns from this data rather than following pre-programmed rules. This combination fundamentally changed how we built AI: instead of trying to encode human knowledge directly, we could now let machines discover patterns automatically from examples, leading to more robust and adaptable AI.\nConsider how email spam filtering evolved:\n\n\n\n\n\n\nExample: Early Spam Detection Systems\n\n\n\nRule-based (1980s):\nIF contains(\"viagra\") OR contains(\"winner\") THEN spam\n\nStatistical (1990s):\nP(spam|word) = (frequency in spam emails) / (total frequency)\n\nCombined using Naive Bayes:\nP(spam|email) ∝ P(spam) × ∏ P(word|spam)\n\n\nThe move to statistical approaches fundamentally changed how we think about building AI by introducing three core concepts that remain important today. First, the quality and quantity of training data became as important as the algorithms themselves. AI could only learn patterns that were present in its training examples. Second, we needed rigorous ways to evaluate how well AI actually performed, leading to metrics that could measure success and compare different approaches. Third, we discovered an inherent tension between precision (being right when we make a prediction) and recall (catching all the cases we should find), forcing designers to make explicit trade-offs based on their application’s needs. For example, a spam filter might tolerate some spam to avoid blocking important emails, while medical diagnosis might need to catch every potential case even if it means more false alarms.\nTable 1.1 encapsulates the evolutionary journey of AI approaches we have discussed so far, highlighting the key strengths and capabilities that emerged with each new paradigm. As we move from left to right across the table, we can observe several important trends. We will talk about shallow and deep learning next, but it is useful to understand the trade-offs between the approaches we have covered so far.\n\n\n\nTable 1.1: Evolution of AI, Key Positive Aspects\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nSymbolic AI\nExpert Systems\nStatistical Learning\nShallow / Deep Learning\n\n\n\n\nKey Strength\nLogical reasoning\nDomain expertise\nVersatility\nPattern recognition\n\n\nBest Use Case\nWell-defined, rule-based problems\nSpecific domain problems\nVarious structured data problems\nComplex, unstructured data problems\n\n\nData Handling\nMinimal data needed\nDomain knowledge-based\nModerate data required\nLarge-scale data processing\n\n\nAdaptability\nFixed rules\nDomain-specific adaptability\nAdaptable to various domains\nHighly adaptable to diverse tasks\n\n\nProblem Complexity\nSimple, logic-based\nComplicated, domain- specific\nComplex, structured\nHighly complex, unstructured\n\n\n\n\n\n\nThe table serves as a bridge between the early approaches we’ve discussed and the more recent developments in shallow and deep learning that we’ll explore next. It sets the stage for understanding why certain approaches gained prominence in different eras and how each new paradigm built upon and addressed the limitations of its predecessors. Moreover, it illustrates how the strengths of earlier approaches continue to influence and enhance modern AI techniques, particularly in the era of foundation models.\n\n\n1.3.4 Shallow Learning Era\nThe 2000s marked a fascinating period in machine learning history that we now call the ``shallow learning’’ era. To understand why it’s “shallow,” imagine building a house: deep learning (which came later) is like having multiple construction crews working at different levels simultaneously, each crew learning from the work of crews below them. In contrast, shallow learning typically had just one or two levels of processing, similar to having just a foundation crew and a framing crew.\nDuring this time, several powerful algorithms dominated the machine learning landscape. Each brought unique strengths to different problems: Decision trees provided interpretable results by making choices much like a flowchart. K-nearest neighbors made predictions by finding similar examples in past data, like asking your most experienced neighbors for advice. Linear and logistic regression offered straightforward, interpretable models that worked well for many real-world problems. Support Vector Machines (SVMs) excelled at finding complex boundaries between categories using the “kernel trick”—imagine being able to untangle a bowl of spaghetti into straight lines by lifting it into a higher dimension. These algorithms formed the foundation of practical machine learning.\nConsider a typical computer vision solution from 2005:\n\n\n\n\n\n\nExample: Traditional Computer Vision Pipeline\n\n\n\n1. Manual Feature Extraction\n  - SIFT (Scale-Invariant Feature Transform)\n  - HOG (Histogram of Oriented Gradients)\n  - Gabor filters\n2. Feature Selection/Engineering\n3. \"Shallow\" Learning Model (e.g., SVM)\n4. Post-processing\n\n\nWhat made this era distinct was its hybrid approach: human-engineered features combined with statistical learning. They had strong mathematical foundations (researchers could prove why they worked). They performed well even with limited data. They were computationally efficient. They produced reliable, reproducible results.\nTake the example of face detection, where the Viola-Jones algorithm (2001) achieved real-time performance using simple rectangular features and a cascade of classifiers. This algorithm powered digital camera face detection for nearly a decade.\n\n\n1.3.5 Deep Learning Era\nWhile Support Vector Machines excelled at finding complex boundaries between categories using mathematical transformations, deep learning took a radically different approach inspired by the human brain’s architecture. Deep learning is built from layers of artificial neurons, where each layer learns to transform its input data into increasingly abstract representations. Imagine processing an image of a cat: the first layer might learn to detect simple edges and contrasts, the next layer combines these into basic shapes and textures, another layer might recognize whiskers and pointy ears, and the final layers assemble these features into the concept of “cat.”\nUnlike shallow learning methods that required humans to carefully engineer features, deep learning networks can automatically discover useful features directly from raw data. This ability to learn hierarchical representations, ranging from simple to complex and concrete to abstract, is what makes deep learning “deep,” and it turned out to be a remarkably powerful approach for handling complex, real-world data like images, speech, and text.\nIn 2012, a deep neural network called AlexNet, shown in Figure 1.2, achieved a breakthrough in the ImageNet competition that would transform the field of machine learning. The challenge was formidable: correctly classify 1.2 million high-resolution images into 1,000 different categories. While previous approaches struggled with error rates above 25%, AlexNet achieved a 15.3% error rate, dramatically outperforming all existing methods.\nThe success of AlexNet wasn’t just a technical achievement; it was a watershed moment that demonstrated the practical viability of deep learning. It showed that with sufficient data, computational power, and architectural innovations, neural networks could outperform hand-engineered features and shallow learning methods that had dominated the field for decades. This single result triggered an explosion of research and applications in deep learning that continues to this day.\n\n\n\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\clip (-11.2,-2) rectangle (15.5,5.45);\n%\\draw[red](-11.2,-1.7) rectangle (15.5,5.45);\n\\tikzset{%\n LineD/.style={line width=0.7pt,black!50,dashed,dash pattern=on 3pt off 2pt},\n  LineG/.style={line width=0.75pt,GreenLine},\n  LineR/.style={line width=0.75pt,RedLine},\n  LineA/.style={line width=0.75pt,BrownLine,-latex,text=black}\n}\n\\newcommand\\FillCube[4]{\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\def\\nc{#1}\n% Lower front left corner\n\\coordinate (A\\nc) at (0, 0);\n% Donji prednji desni\n\\coordinate (B\\nc) at (\\width, 0);\n% Upper front right\n\\coordinate (C\\nc) at (\\width, \\height);\n% Upper front left\n\\coordinate (D\\nc) at (0, \\height);\n% Pomak u \"dubinu\"\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n% Last points (moved)\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n% Front side\n\\draw[GreenLine,fill=green!08,line width=0.5pt] (A\\nc) -- (B\\nc) -- (C\\nc) --(D\\nc) -- cycle;\n% Top side\n\\draw[GreenLine,fill=green!20,line width=0.5pt] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n% Left\n\\draw[GreenLine,fill=green!15] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n\\draw[GreenLine,line width=0.75pt](A\\nc)--(B\\nc)--(C\\nc)--(D\\nc)--(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--(H\\nc);\n}\n%%%\n\\newcommand\\SmallCube[4]{\n\\def\\nc{#1}\n\\def\\depth{#2}\n\\def\\width{#3}\n\\def\\height{#4}\n\\coordinate (A\\nc) at (0, 0);\n\\coordinate (B\\nc) at (\\width, 0);\n\\coordinate (C\\nc) at (\\width, \\height);\n\\coordinate (D\\nc) at (0, \\height);\n\\coordinate (shift) at (-0.7*\\depth, \\depth);\n\\coordinate (E\\nc) at ($(A\\nc) + (shift)$);\n\\coordinate (F\\nc) at ($(B\\nc) + (shift)$);\n\\coordinate (G\\nc) at ($(C\\nc) + (shift)$);\n\\coordinate (H\\nc) at ($(D\\nc) + (shift)$);\n\\draw[RedLine,fill=red!08,line width=0.5pt,fill opacity=0.7] (A\\nc) -- (B\\nc) -- (C\\nc) -- (D\\nc) -- cycle;\n\\draw[RedLine,fill=red!20,line width=0.5pt,fill opacity=0.7] (D\\nc) -- (H\\nc) -- (G\\nc) -- (C\\nc);\n\\draw[RedLine,fill=red!15,fill opacity=0.7] (A\\nc) -- (E\\nc) -- (H\\nc)--(D\\nc)--cycle;\n\\draw[] (E\\nc) -- (H\\nc);\n}\n%%%%%%%%%%%%%%%%%%%%%\n%%4 column\n%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}\n%big cube\n\\begin{scope}\n\\FillCube{4VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,0.4)},line width=0.5pt]\n\\SmallCube{4MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{4VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(0,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{4VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.18,0.55)}]\n\\SmallCube{4MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n\\def\\nc{4VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%\n%%5 column\n%%%%\n%%small cube\n\\begin{scope}[shift={(4.15,0)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VD}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.10,1.25)}]\n\\SmallCube{5MD}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{13} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{13} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(4.15,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{5VG}{0.8}{3}{2}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.08,0.28)}]\n\\SmallCube{5MG}{0.4}{3}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{5VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{192} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%3 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-3.75,-0.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VD}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.10,0.45)}]\n\\SmallCube{3MDI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\end{scope}\n%%small cube - up\n\\begin{scope}[shift={(-0.12,2.23)}]\n\\SmallCube{3MDII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1,pos=0.4]{27} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{27} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-3.75,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{3VG}{1.5}{2.33}{3}\n\\end{scope}\n%%small cube-down\n\\begin{scope}[shift={(-0.42,0.75)}]\n\\SmallCube{3MGI}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[GreenLine,line width=0.75pt](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n%%small cube-up\n\\begin{scope}[shift={(-0.06,0.18)}]\n\\SmallCube{3MGII}{0.4}{2.33}{0.6}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{3}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{3}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{3VG}\n\\draw[LineG](A\\nc)--node[below,text=black]{128} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%2 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-6.8,-1)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VD}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.2,2.5)}]\n\\SmallCube{2MD}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[pos=0.6,right,text=black,text opacity=1]{55} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[pos=0.26,right,text=black,text opacity=1]{55} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%Above\n\\begin{scope}[shift={(-6.8,3.5)}]\n%big cube\n\\begin{scope}\n\\FillCube{2VG}{2}{1.3}{3.8}\n\\end{scope}\n%%small cube\n\\begin{scope}[shift={(-0.1,0.5)}]\n\\SmallCube{2MG}{0.4}{1.3}{1}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--node[left,text=black]{5}\n(C\\nc)--(D\\nc)-- (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left,text=black]{5}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{2VG}\n\\draw[LineG](A\\nc)--node[above,text=black]{48} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)--node[right,text=black,text opacity=1]{} (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%%%%%%%%%%%%%%%%%%%%\n%%1 column\n%%%%%%%%%%%%%%%%%%%%%%%\n\\begin{scope}[shift={(-9.0,-1.2)}]\n%big cube\n\\begin{scope}\n\\FillCube{1VD}{2}{0.2}{4.55}\n\\end{scope}\n%%small cube=down\n\\begin{scope}[shift={(-0.25,0.5)}]\n\\SmallCube{1MDI}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n%%small cube=up\n\\begin{scope}[shift={(-0.75,3.4)}]\n\\SmallCube{1MDII}{0.8}{0.15}{1.7}\n%%\n\\draw[LineR](A\\nc)-- (B\\nc)--\n(C\\nc)--(D\\nc)-- node[left=-2pt,text=black,pos=0.4]{11}(A\\nc)\n(A\\nc)--(E\\nc)--(H\\nc)--(G\\nc)--node[left=3pt,text=black,pos=0.9]{11}(C\\nc)\n(D\\nc)-- (H\\nc);\n%\n\\def\\nc{1VD}\n\\draw[LineG](A\\nc)--node[below,text=black]{3} (B\\nc)--\n(C\\nc)--(D\\nc)--node[right,text=black,text opacity=1]{} (A\\nc)\n(A\\nc)--node[below left,text=black]{224}(E\\nc)--\nnode[left,text=black,text opacity=1]{224}(H\\nc)--(G\\nc)--(C\\nc)\n(D\\nc)-- (H\\nc);\n\\end{scope}\n\\end{scope}\n%%%%\n\\begin{scope}[shift={(8.15,0)}]\n\\begin{scope}\n\\FillCube{6VD}{0.8}{2.0}{2}\n\\path(A6VD)--node[below]{128}(B6VD);\n\\path(A6VD)--node[right]{13}(D6VD);\n\\path(D6VD)--node[right]{13}(H6VD);\n\\end{scope}\n%up\n\\begin{scope}[shift={(0,3.5)}]\n\\FillCube{6VG}{0.8}{2.0}{2}\n\\path(A6VG)--node[below]{128}(B6VG);\n\\end{scope}\n\\end{scope}\n\n\\newcommand\\Boxx[3]{\n\\node[draw,LineG,fill=green!10,rectangle,minimum width=7mm,minimum height=#2](#1){};\n\\node[below=2pt of #1]{#3};\n}\n\\begin{scope}[shift={(11.7,1.0)}]\n \\Boxx{B1D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(11.7,5.25)}]\n \\Boxx{B1G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,1.0)}]\n \\Boxx{B2D}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(13.5,5.25)}]\n \\Boxx{B2G}{35mm}{2048}\n\\end{scope}\n\\begin{scope}[shift={(15.0,1.0)}]\n \\Boxx{B3}{19mm}{1000}\n\\end{scope}\n%%%\n\\node[right=3pt of B1VD,align=center]{Stride\\\\ of 4};\n\\node[right=3pt of B2VD,align=center]{Max\\\\ pooling};\n\\node[right=3pt of B3VD,align=center]{Max\\\\ pooling};\n\\node[below=3pt of B6VD,align=center]{Max\\\\ pooling};\n%\n\\coordinate(1C2)at($(A2VD)!0.4!(D2VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDI)--(1C2);\n}\n\\coordinate(2C2)at($(E2VG)!0.2!(H2VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 1MDII)--(2C2);\n}\n%3\n\\coordinate(1C3)at($(A3VD)!0.55!(H3VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MD)--(1C3);\n}\n\\coordinate(2C3)at($(A3MGI)!0.35!(D3MGI)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 2MG)--(2C3);\n}\n%4\n\\coordinate(1C4)at($(A4VG)!0.15!(D4VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGI)--(1C4);\n}\n\\coordinate(2C4)at($(G4MD)!0.15!(H4MD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MGII)--(2C4);\n}\n\\coordinate(3C4)at($(A4MG)!0.5!(C4MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDII)--(3C4);\n}\n\\coordinate(3C4)at($(A4VD)!0.12!(D4VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 3MDI)--(3C4);\n}\n%5\n\\coordinate(1C5)at($(A5MG)!0.82!(H5MG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MG)--(1C5);\n}\n\\coordinate(2C5)at($(A5VD)!0.52!(C5VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 4MD)--(2C5);\n}\n%6\n\\coordinate(1C6)at($(A6VG)!0.52!(C6VG)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MG)--(1C6);\n}\n\\coordinate(1C6)at($(D6VD)!0.3!(B6VD)$);\n\\foreach\\i in{B,C,G}{\n\\draw[LineD](\\i 5MD)--(1C6);\n}\n%\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)coordinate(X1)--\nnode[below]{dense}(X1-|B1D.north west);\n\\draw[LineA](B1D)--node[below]{dense}(B2D);\n\\draw[LineA](B2D)--(B3);\n%\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)coordinate(X1)--(X1-|B1G.north west);\n\\draw[LineA]($(B6VG)!0.52!(C6VG)$)--(B1D);\n\\draw[LineA]($(B6VD)!0.52!(C6VD)$)--(B1G);\n\\draw[LineA](B1D)--(B2G);\n\\draw[LineA](B1G)--(B2D);\n\\draw[LineA](B2G)--node[right]{dense}(B3);\n\\draw[LineA]($(B1G.north east)!0.7!(B1G.south east)$)--($(B2G.north west)!0.7!(B2G.south west)$);\n\\end{tikzpicture}\n\n\nFigure 1.2: Deep neural network architecture for Alexnet. Source: Krizhevsky, Sutskever, and Hinton (2017)\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “ImageNet Classification with Deep Convolutional Neural Networks.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.\n\n\nFrom this foundation, deep learning entered an era of unprecedented scale. By the late 2010s, companies like Google, Facebook, and OpenAI were training neural networks thousands of times larger than AlexNet. These massive models, often called “foundation models,” took deep learning to new heights. GPT-3, released in 2020, contained 175 billion parameters—imagine a student that could read through all of Wikipedia multiple times and learn patterns from every article. These models showed remarkable abilities: writing human-like text, engaging in conversation, generating images from descriptions, and even writing computer code. The key insight was simple but powerful: as we made neural networks bigger and fed them more data, they became capable of solving increasingly complex tasks. However, this scale brought unprecedented systems challenges: how do you efficiently train models that require thousands of GPUs working in parallel? How do you store and serve models that are hundreds of gigabytes in size? How do you handle the massive datasets needed for training?\nThe deep learning revolution of 2012 didn’t emerge from nowhere, as it was founded on neural network research dating back to the 1950s. The story begins with Frank Rosenblatt’s Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. While it could only handle linearly separable problems, a limitation that was dramatically highlighted by Minsky and Papert’s 1969 book, “Perceptrons,” it introduced the fundamental concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using convolutional neural networks (CNNs).\nYet these networks largely languished through the 1990s and 2000s, not because the ideas were wrong, but because they were ahead of their time. The field lacked three important ingredients: sufficient data to train complex networks, enough computational power to process this data, and the technical innovations needed to train very deep networks effectively.\n\n\n\n\n\n\nConvolutional Network Demo from 1989\n\n\n\n\n\n\nThe field had to wait for the convergence of big data, better computing hardware, and algorithmic breakthroughs before deep learning’s potential could be unlocked. This long gestation period helps explain why the 2012 ImageNet moment was less a sudden revolution and more the culmination of decades of accumulated research finally finding its moment. As we’ll explore in the following sections, this evolution has led to two significant developments in the field. First, it has given rise to define the field of machine learning systems engineering, a discipline that teaches how to bridge the gap between theoretical advancements and practical implementation. Second, it has necessitated a more comprehensive definition of machine learning systems, one that encompasses not just algorithms, but also data and computing infrastructure. Today’s challenges of scale echo many of the same fundamental questions about computation, data, and learning methods that researchers have grappled with since the field’s inception, but now within a more complex and interconnected framework.\nAs AI progressed from symbolic reasoning to statistical learning and deep learning, its applications became increasingly ambitious and complex. This growth introduced challenges that extended beyond algorithms, necessitating a new focus: engineering entire systems capable of deploying and sustaining AI at scale. This gave rise to the discipline of Machine Learning Systems Engineering.\n\nSelf-Check: Question 1.2\n\nWhich AI era introduced the concept of using statistical methods to learn patterns from data rather than following pre-programmed rules?\n\nSymbolic AI Era\nExpert Systems Era\nStatistical Learning Era\nDeep Learning Era\n\nExplain why the transition from rule-based AI to statistical learning was significant for the development of modern machine learning systems.\nTrue or False: The Deep Learning Era solved all the challenges faced by previous AI paradigms.\nOrder the following AI eras chronologically: Expert Systems Era, Symbolic AI Era, Statistical Learning Era, Deep Learning Era.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-engineering-c9fb",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-engineering-c9fb",
    "title": "1  Introduction",
    "section": "1.4 ML Systems Engineering",
    "text": "1.4 ML Systems Engineering\nThe story we’ve traced, from the early days of the Perceptron through the deep learning revolution, has largely been one of algorithmic breakthroughs. Each era brought new mathematical insights and modeling approaches that pushed the boundaries of what AI could achieve. But something important changed over the past decade: the success of AI systems became increasingly dependent not just on algorithmic innovations, but on sophisticated engineering.\nThis shift mirrors the evolution of computer science and engineering in the late 1960s and early 1970s. During that period, as computing systems grew more complex, a new discipline emerged: Computer Engineering. This field bridged the gap between Electrical Engineering’s hardware expertise and Computer Science’s focus on algorithms and software. Computer Engineering arose because the challenges of designing and building complex computing systems required an integrated approach that neither discipline could fully address on its own.\nToday, we’re witnessing a similar transition in the field of AI. While Computer Science continues to push the boundaries of ML algorithms and Electrical Engineering advances specialized AI hardware, neither discipline fully addresses the engineering principles needed to deploy, optimize, and sustain ML systems at scale. This gap highlights the need for a new discipline: Machine Learning Systems Engineering.\nThere is no explicit definition of what this field is as such today, but it can be broadly defined as such:\n\n\n\n\n\n\nDefinition of Machine Learning Systems Engineering\n\n\n\nMachine Learning Systems Engineering (MLSysEng) is the engineering discipline focused on building reliable, efficient, and scalable AI systems across computational platforms, ranging from embedded devices to data centers. It spans the entire AI lifecycle, including data acquisition, model development, system integration, deployment, and operations, with an emphasis on resource-awareness and system-level optimization.\n\n\nLet’s consider space exploration. While astronauts venture into new frontiers and explore the vast unknowns of the universe, their discoveries are only possible because of the complex engineering systems supporting them, such as the rockets that lift them into space, the life support systems that keep them alive, and the communication networks that keep them connected to Earth. Similarly, while AI researchers push the boundaries of what’s possible with learning algorithms, their breakthroughs only become practical reality through careful systems engineering. Modern AI systems need robust infrastructure to collect and manage data, powerful computing systems to train models, and reliable deployment platforms to serve millions of users.\nThis emergence of machine learning systems engineering as a important discipline reflects a broader reality: turning AI algorithms into real-world systems requires bridging the gap between theoretical possibilities and practical implementation. It’s not enough to have a brilliant algorithm if you can’t efficiently collect and process the data it needs, distribute its computation across hundreds of machines, serve it reliably to millions of users, or monitor its performance in production.\nUnderstanding this interplay between algorithms and engineering has become fundamental for modern AI practitioners. While researchers continue to push the boundaries of what’s algorithmically possible, engineers are tackling the complex challenge of making these algorithms work reliably and efficiently in the real world. This brings us to a fundamental question: what exactly is a machine learning system, and what makes it different from traditional software systems?\n\nSelf-Check: Question 1.3\n\nMachine Learning Systems Engineering focuses on building AI systems that are reliable, efficient, and ____ across computational platforms.\nWhich of the following best describes the role of Machine Learning Systems Engineering in AI development?\n\nDeveloping new AI algorithms\nOptimizing and deploying AI systems at scale\nCreating specialized AI hardware\nFocusing solely on data acquisition\n\nExplain why Machine Learning Systems Engineering is necessary for the practical implementation of AI systems.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-defining-ml-systems-8f42",
    "href": "contents/core/introduction/introduction.html#sec-introduction-defining-ml-systems-8f42",
    "title": "1  Introduction",
    "section": "1.5 Defining ML Systems",
    "text": "1.5 Defining ML Systems\nThere’s no universally accepted, clear-cut textbook definition of a machine learning system. This ambiguity stems from the fact that different practitioners, researchers, and industries often refer to machine learning systems in varying contexts and with different scopes. Some might focus solely on the algorithmic aspects, while others might include the entire pipeline from data collection to model deployment. This loose usage of the term reflects the rapidly evolving and multidisciplinary nature of the field.\nGiven this diversity of perspectives, it is important to establish a clear and comprehensive definition that encompasses all these aspects. In this textbook, we take a holistic approach to machine learning systems, considering not just the algorithms but also the entire ecosystem in which they operate. Therefore, we define a machine learning system as follows:\n\n\n\n\n\n\nDefinition of a Machine Learning System\n\n\n\nA machine learning system is an integrated computing system comprising three core components: (1) data that guides algorithmic behavior, (2) learning algorithms that extract patterns from this data, and (3) computing infrastructure that enables both the learning process (i.e., training) and the application of learned knowledge (i.e., inference/serving). Together, these components create a computing system capable of making predictions, generating content, or taking actions based on learned patterns.\n\n\nThe core of any machine learning system consists of three interrelated components, as illustrated in Figure 1.3: Models/Algorithms, Data, and Computing Infrastructure. These components form a triangular dependency where each element fundamentally shapes the possibilities of the others. The model architecture dictates both the computational demands for training and inference, as well as the volume and structure of data required for effective learning. The data’s scale and complexity influence what infrastructure is needed for storage and processing, while simultaneously determining which model architectures are feasible. The infrastructure capabilities establish practical limits on both model scale and data processing capacity, creating a framework within which the other components must operate.\n\n\n\n\\scalebox{0.9}{%\n\\begin{tikzpicture}[line join=round,font=\\usefont{T1}{phv}{m}{n}\\small]\n\\tikzset{\n Line/.style={line width=0.35pt,black!50,text=black},\n ALineA/.style={violet!80!black!50,line width=3pt,shorten &lt;=2pt,shorten &gt;=2pt,\n{Triangle[width=1.1*6pt,length=0.8*6pt]}-{Triangle[width=1.1*6pt,length=0.8*6pt]}},\nLineD/.style={line width=0.75pt,black!50,text=black,dashed,dash pattern=on 5pt off 3pt},\nCircle/.style={inner xsep=2pt,\n  % node distance=1.15,\n  circle,\n    draw=BrownLine,\n    line width=0.75pt,\n    fill=BrownL!40,\n    minimum size=16mm\n  },\n circles/.pic={\n\\pgfkeys{/channel/.cd, #1}\n\\node[circle,draw=\\channelcolor,line width=\\Linewidth,fill=\\channelcolor!10,\nminimum size=2.5mm](\\picname){};\n        }\n}\n\\tikzset {\npics/cloud/.style = {\n        code = {\n\\colorlet{red}{RedLine}\n\\begin{scope}[local bounding box=CLO,scale=0.5, every node/.append style={transform shape}]\n\\draw[red,fill=white,line width=0.9pt](0.67,1.21)to[out=55,in=90,distance=13](1.5,0.96)\nto[out=360,in=30,distance=9](1.68,0.42);\n\\draw[red,fill=white,line width=0.9pt](0,0)to[out=170,in=180,distance=11](0.1,0.61)\nto[out=90,in=105,distance=17](1.07,0.71)\nto[out=20,in=75,distance=7](1.48,0.36)\nto[out=350,in=0,distance=7](1.48,0)--(0,0);\n\\draw[red,fill=white,line width=0.9pt](0.27,0.71)to[bend left=25](0.49,0.96);\n\n\\end{scope}\n}\n}\n}\n%streaming\n\\tikzset{%\n LineST/.style={-{Circle[\\channelcolor,fill=RedLine,length=4pt]},draw=\\channelcolor,line width=\\Linewidth,rounded corners},\n ellipseST/.style={fill=\\channelcolor,ellipse,minimum width = 2.5mm, inner sep=2pt, minimum height =1.5mm},\n BoxST/.style={line width=\\Linewidth,fill=white,draw=\\channelcolor,rectangle,minimum width=56,\n minimum height=16,rounded corners=1.2pt},\n pics/streaming/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[BoxST,minimum width=44,minimum height=48](\\picname-RE1){};\n\\foreach \\i/\\j in{1/north,2/center,3/south}{\n\\node[BoxST](\\picname-GR\\i)at(\\picname-RE1.\\j){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.2!(\\picname-GR\\i.east)$){};\n\\node[ellipseST]at($(\\picname-GR\\i.west)!0.4!(\\picname-GR\\i.east)$){};\n}\n%\\draw[LineST](\\picname-GR1.40)--++(0,0.5)--++(1,0)coordinate(\\picname-C1);\n%\\draw[LineST](\\picname-GR1)--++(2,0)coordinate(\\picname-C2);\n%\\draw[LineST](\\picname-GR2)--++(2,0)coordinate(\\picname-C3);\n\\draw[LineST](\\picname-GR3)--++(2,0)coordinate(\\picname-C4);\n\\draw[LineST](\\picname-GR3.320)--++(0,-0.7)--++(0.8,0)coordinate(\\picname-C5);\n\\draw[LineST](\\picname-GR3.220)--++(0,-0.7)--++(-0.8,0)coordinate(\\picname-C6);\n\\draw[LineST](\\picname-GR3)--++(-2,0)coordinate(\\picname-C7);\n%\\draw[LineST](\\picname-GR2)--++(-2,0)coordinate(\\picname-C8);\n%\\draw[LineST](\\picname-GR1)--++(-2,0)coordinate(\\picname-C9);\n%\\draw[LineST](\\picname-GR1.140)--++(0,0.5)--++(-1,0)coordinate(\\picname-C10);\n \\end{scope}\n     }\n  }\n}\n%data\n\\tikzset{mycylinder/.style={cylinder, shape border rotate=90, aspect=1.3, draw, fill=white,\nminimum width=25mm,minimum height=11mm,line width=\\Linewidth,node distance=-0.15},\npics/data/.style = {\n        code = {\n        \\pgfkeys{/channel/.cd, #1}\n\\begin{scope}[local bounding box=STREAMING,scale=\\scalefac, every node/.append style={transform shape}]\n\\node[mycylinder,fill=\\channelcolor!50] (A) {};\n\\node[mycylinder, above=of A,fill=\\channelcolor!30] (B) {};\n\\node[mycylinder, above=of B,fill=\\channelcolor!10] (C) {};\n \\end{scope}\n     }\n  }\n}\n\\pgfkeys{\n  /channel/.cd,\n  channelcolor/.store in=\\channelcolor,\n  drawchannelcolor/.store in=\\drawchannelcolor,\n  scalefac/.store in=\\scalefac,\n  Linewidth/.store in=\\Linewidth,\n  picname/.store in=\\picname,\n  channelcolor=BrownLine,\n  drawchannelcolor=BrownLine,\n  scalefac=1,\n  Linewidth=0.5pt,\n  picname=C\n}\n\\node[Circle](MO){};\n\\node[Circle,below left=1.5 and 2 of MO,draw=GreenLine,fill=GreenL!40,](IN){};\n\\node[Circle,below right=1.5 and 2 of MO,draw=OrangeLine,fill=OrangeL!40,](DA){};\n\\draw[ALineA](MO)--(IN);\n\\draw[ALineA](MO)--(DA);\n\\draw[ALineA](DA)--(IN);\n\\node[below=2pt of MO]{Model};\n\\node[below=2pt of IN]{Infra};\n\\node[below=2pt of DA]{Data};\n%%\n\\begin{scope}[local bounding box=CIRCLE1,shift={($(MO)+(0.04,-0.24)$)},\nscale=0.55, every node/.append style={transform shape}]\n%1 column\n\\foreach \\j in {1,2,3} {\n  \\pgfmathsetmacro{\\y}{(1.5-\\j)*0.43 + 0.7}\n  \\pic at (-0.8,\\y) {circles={channelcolor=RedLine,picname=1CD\\j}};\n}\n%2 column\n\\foreach \\i in {1,...,4} {\n  \\pgfmathsetmacro{\\y}{(2-\\i)*0.43+0.7}\n  \\pic at (0,\\y) {circles={channelcolor=RedLine, picname=2CD\\i}};\n}\n%3 column\n\\foreach \\j in {1,2} {\n  \\pgfmathsetmacro{\\y}{(1-\\j)*0.43 + 0.7}\n  \\pic at (0.8,\\y) {circles={channelcolor=RedLine,picname=3CD\\j}};\n}\n\\foreach \\i in {1,2,3}{\n  \\foreach \\j in {1,2,3,4}{\n\\draw[Line](1CD\\i)--(2CD\\j);\n}}\n\\foreach \\i in {1,2,3,4}{\n  \\foreach \\j in {1,2}{\n\\draw[Line](2CD\\i)--(3CD\\j);\n}}\n\\end{scope}\n%\n\\pic[shift={(-0.4,-0.08)}] at (IN) {cloud};\n%\n\\pic[shift={(-0.05,-0.13)}] at  (IN){streaming={scalefac=0.25,picname=2,channelcolor=RedLine, Linewidth=0.65pt}};\n%\n\\pic[shift={(0,-0.3)}] at  (DA){data={scalefac=0.3,picname=1,channelcolor=green!70!black, Linewidth=0.4pt}};\n\\end{tikzpicture}}\n\n\nFigure 1.3: Machine learning systems involve algorithms, data, and computation, all intertwined together.\n\n\n\nEach of these components serves a distinct but interconnected purpose:\n\nAlgorithms: Mathematical models and methods that learn patterns from data to make predictions or decisions\nData: Processes and infrastructure for collecting, storing, processing, managing, and serving data for both training and inference.\nComputing: Hardware and software infrastructure that enables efficient training, serving, and operation of models at scale.\n\nThe interdependency of these components means no single element can function in isolation. The most sophisticated algorithm cannot learn without data or computing resources to run on. The largest datasets are useless without algorithms to extract patterns or infrastructure to process them. And the most powerful computing infrastructure serves no purpose without algorithms to execute or data to process.\nTo illustrate these relationships, we can draw an analogy to space exploration. Algorithm developers are like astronauts, who explore new frontiers and make discoveries. Data science teams function like mission control specialists, who ensure the constant flow of critical information and resources necessary to maintain the mission’s operation. Computing infrastructure engineers are like rocket engineers—designing and building the systems that make the mission possible. Just as a space mission requires the seamless integration of astronauts, mission control, and rocket systems, a machine learning system demands the careful orchestration of algorithms, data, and computing infrastructure.\n\nSelf-Check: Question 1.4\n\nWhich of the following best describes the core components of a machine learning system as defined in this textbook?\n\nAlgorithms, Data, and Computing Infrastructure\nModels, Sensors, and Networking\nData, User Interfaces, and Algorithms\nHardware, Software, and User Experience\n\nExplain how the interdependency of algorithms, data, and computing infrastructure shapes the design and capabilities of a machine learning system.\nIn the analogy to space exploration, algorithm developers are likened to ____, who explore new frontiers and make discoveries.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-lifecycle-ml-systems-e40e",
    "href": "contents/core/introduction/introduction.html#sec-introduction-lifecycle-ml-systems-e40e",
    "title": "1  Introduction",
    "section": "1.6 Lifecycle of ML Systems",
    "text": "1.6 Lifecycle of ML Systems\nTraditional software systems follow a predictable lifecycle where developers write explicit instructions for computers to execute. These systems are built on decades of established software engineering practices. Version control systems maintain precise histories of code changes. Continuous integration and deployment pipelines automate testing and release processes. Static analysis tools measure code quality and identify potential issues. This infrastructure enables reliable development, testing, and deployment of software systems, following well-defined principles of software engineering.\nMachine learning systems represent a fundamental departure from this traditional paradigm. While traditional systems execute explicit programming logic, machine learning systems derive their behavior from patterns in data. This shift from code to data as the primary driver of system behavior introduces new complexities.\nAs illustrated in Figure 1.4, the ML lifecycle consists of interconnected stages from data collection through model monitoring, with feedback loops for continuous improvement when performance degrades or models need enhancement.\n\n\n\n\\begin{tikzpicture}[font=\\small\\usefont{T1}{phv}{m}{n}]\n\\tikzset{\n  Box/.style={inner xsep=2pt,\n  draw=GreenLine,\n    line width=0.75pt,\n    fill=GreenL,\n    anchor=west,\n    text width=20mm,align=flush center,\n    minimum width=20mm, minimum height=8mm\n  },\n Line/.style={line width=1.0pt,black!50,text=black,-{Triangle[width=0.8*6pt,length=0.98*6pt]}},\n  Text/.style={inner sep=4pt,\n    draw=none, line width=0.75pt,\n    fill=TextColor!70,\n    font=\\fontsize{8pt}{9}\\selectfont\\usefont{T1}{phv}{m}{n},\n    align=flush center,\n    minimum width=7mm, minimum height=5mm\n  },\n}\n\n\\node[Box](B1){ Data\\\\ Preparation};\n\\node[Box,node distance=15mm,right=of B1,fill=RedL,draw=RedLine](B2){Model\\\\ Evaluation};\n\\node[Box,node distance=32mm,right=of B2,fill=VioletL,draw=VioletLine](B3){Model \\\\ Deployment};\n\\node[Box,node distance=9mm,above=of $(B1)!0.5!(B2)$,\nfill=BackColor!60!yellow!90,draw=BackLine](GB){Model\\\\ Training};\n\\node[Box,node distance=9mm,below left=1.1 and 0 of B1.south west,\nfill=BlueL,draw=BlueLine](DB1){Data\\\\ Collection};\n\\node[Box,node distance=9mm,below right=1.1 and 0 of B3.south east,\nfill=OrangeL,draw=OrangeLine](DB2){Model \\\\Monitoring};\n\\draw[Line](B2)--node[Text,pos=0.5]{Meets\\\\ Requirements}(B3);\n\\draw[Line](B2)--++(270:1.2)-|node[Text,pos=0.25]{Needs\\\\ Improvement}(B1);\n\\draw[Line](DB2)--node[Text,pos=0.25]{Performance\\\\Degrades}(DB1);\n\\draw[Line](DB1)|-(B1);\n\\draw[Line](B1)|-(GB);\n\\draw[Line](GB)-|(B2);\n\\draw[Line](B3)-|(DB2);\n\\end{tikzpicture}\n\n\nFigure 1.4: The typical lifecycle of a machine learning system.\n\n\n\nUnlike source code, which changes only when developers modify it, data reflects the dynamic nature of the real world. Changes in data distributions can silently alter system behavior. Traditional software engineering tools, designed for deterministic code-based systems, prove insufficient for managing these data-dependent systems. For example, version control systems that excel at tracking discrete code changes struggle to manage large, evolving datasets. Testing frameworks designed for deterministic outputs must be adapted for probabilistic predictions. This data-dependent nature creates a more dynamic lifecycle, requiring continuous monitoring and adaptation to maintain system relevance as real-world data patterns evolve.\nUnderstanding the machine learning system lifecycle requires examining its distinct stages. Each stage presents unique requirements from both learning and infrastructure perspectives. This dual consideration, of learning needs and systems support, is wildly important for building effective machine learning systems.\nHowever, the various stages of the ML lifecycle in production are not isolated; they are, in fact, deeply interconnected. This interconnectedness can create either virtuous or vicious cycles. In a virtuous cycle, high-quality data enables effective learning, robust infrastructure supports efficient processing, and well-engineered systems facilitate the collection of even better data. However, in a vicious cycle, poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations prevent the improvement of data collection—each problem compounds the others.\n\nSelf-Check: Question 1.5\n\nOrder the following stages of the machine learning system lifecycle: Model Deployment, Data Collection, Model Training, Model Evaluation, Data Preparation, Model Monitoring.\nWhich of the following best describes a challenge unique to machine learning systems compared to traditional software systems?\n\nVersion control of source code\nTesting deterministic outputs\nManaging evolving datasets\nAutomating deployment processes\n\nExplain how the interconnected stages of the ML lifecycle can lead to either virtuous or vicious cycles.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-wild-e270",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-wild-e270",
    "title": "1  Introduction",
    "section": "1.7 ML Systems in the Wild",
    "text": "1.7 ML Systems in the Wild\nThe complexity of managing machine learning systems becomes even more apparent when we consider the broad spectrum across which ML is deployed today. ML systems exist at vastly different scales and in diverse environments, each presenting unique challenges and constraints.\nAt one end of the spectrum, we have cloud-based ML systems running in massive data centers. These systems, like large language models or recommendation engines, process petabytes of data and serve millions of users simultaneously. They can leverage virtually unlimited computing resources but must manage enormous operational complexity and costs.\nAt the other end, we find TinyML systems running on microcontrollers and embedded devices. These systems must perform ML tasks with severe constraints on memory, computing power, and energy consumption. Imagine a smart home device, such as Alexa or Google Assistant, that must recognize voice commands using less power than a LED bulb, or a sensor that must detect anomalies while running on a battery for months or even years.\nBetween these extremes, we find a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with battery life and processor limitations on smartphones and tablets. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements.\n\nSelf-Check: Question 1.6\n\nWhich of the following best describes a tradeoff faced by cloud-based ML systems?\n\nLimited computing resources\nHigh operational complexity and costs\nSevere constraints on memory and power\nInability to integrate with existing infrastructure\n\nTrue or False: TinyML systems are designed to operate with the same resource availability as cloud-based ML systems.\nExplain how edge ML systems can reduce latency and bandwidth requirements.\nMobile ML systems must balance sophisticated capabilities with ____ life and processor limitations.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-impact-lifecycle-f5f9",
    "href": "contents/core/introduction/introduction.html#sec-introduction-ml-systems-impact-lifecycle-f5f9",
    "title": "1  Introduction",
    "section": "1.8 ML Systems Impact on Lifecycle",
    "text": "1.8 ML Systems Impact on Lifecycle\nThe diversity of ML systems across the spectrum represents a complex interplay of requirements, constraints, and trade-offs. These decisions fundamentally impact every stage of the ML lifecycle we discussed earlier, from data collection to continuous operation.\nPerformance requirements often drive initial architectural decisions. Latency-sensitive applications, like autonomous vehicles or real-time fraud detection, might require edge or embedded architectures despite their resource constraints. Conversely, applications requiring massive computational power for training, such as large language models, naturally gravitate toward centralized cloud architectures. However, raw performance is just one consideration in a complex decision space.\nResource management varies dramatically across architectures. Cloud systems must optimize for cost efficiency at scale—balancing expensive GPU clusters, storage systems, and network bandwidth. Edge systems face fixed resource limits and must carefully manage local compute and storage. Mobile and embedded systems operate under the strictest constraints, where every byte of memory and milliwatt of power matters. These resource considerations directly influence both model design and system architecture.\nOperational complexity increases with system distribution. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle the complexity of distributed system management. This complexity manifests throughout the ML lifecycle—from data collection and version control to model deployment and monitoring. This operational complexity can compound over time if not carefully managed.\nData considerations often introduce competing pressures. Privacy requirements or data sovereignty regulations might push toward edge or embedded architectures, while the need for large-scale training data might favor cloud approaches. The velocity and volume of data also influence architectural choices—real-time sensor data might require edge processing to manage bandwidth, while batch analytics might be better suited to cloud processing.\nEvolution and maintenance requirements must be considered from the start. Cloud architectures offer flexibility for system evolution but can incur significant ongoing costs. Edge and embedded systems might be harder to update but could offer lower operational overhead. The continuous cycle of ML systems we discussed earlier becomes particularly challenging in distributed architectures, where updating models and maintaining system health requires careful orchestration across multiple tiers.\nThese trade-offs are rarely simple binary choices. Modern ML systems often adopt hybrid approaches, carefully balancing these considerations based on specific use cases and constraints. The key is understanding how these decisions will impact the system throughout its lifecycle, from initial development through continuous operation and evolution.\n\n1.8.1 Emerging Trends\nThe landscape of machine learning systems is evolving rapidly, with innovations happening from user-facing applications down to core infrastructure. These changes are reshaping how we design and deploy ML systems.\n\nApplication-Level Innovation\nThe rise of agentic systems marks a profound shift from traditional reactive ML systems that simply made predictions based on input data. Modern applications can now take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tasks, introducing new requirements for decision-making frameworks and safety constraints.\nThis increased sophistication extends to operational intelligence. Applications will likely incorporate sophisticated self-monitoring, automated resource management, and adaptive deployment strategies. They can automatically handle data distribution shifts, model updates, and system optimization, marking a significant advance in autonomous operation.\n\n\nSystem Architecture Evolution\nSupporting these advanced applications requires fundamental changes in the underlying system architecture. Integration frameworks are evolving to handle increasingly complex interactions between ML systems and broader technology ecosystems. Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design.\nResource efficiency has become a central architectural concern as ML systems scale. Innovation in model compression and efficient training techniques is being driven by both environmental and economic factors. Future architectures must carefully balance the pursuit of more powerful models against growing sustainability concerns.\nAt the infrastructure level, new hardware is reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum—from powerful data center chips to efficient edge processors to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables dynamic model distribution across tiers based on computing capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems.\nThese trends are creating ML systems that are more capable and efficient while managing increasing complexity. Success in this evolving landscape requires understanding how application requirements flow down to infrastructure decisions, ensuring systems can grow sustainably while delivering increasingly sophisticated capabilities.\n\nSelf-Check: Question 1.7\n\nWhich architectural choice is most likely to be driven by latency-sensitive applications such as autonomous vehicles?\n\nCentralized cloud architecture\nEdge or embedded architecture\nHybrid architecture\nMobile architecture\n\nExplain how resource management differs between cloud and edge ML systems and the implications for system design.\nIn a distributed ML system, ____ complexity increases with the number of system components and their interactions.\nTrue or False: The evolution and maintenance of ML systems are easier in edge architectures compared to cloud architectures.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-practical-applications-cd2d",
    "href": "contents/core/introduction/introduction.html#sec-introduction-practical-applications-cd2d",
    "title": "1  Introduction",
    "section": "1.9 Practical Applications",
    "text": "1.9 Practical Applications\nThe diverse architectures and scales of ML systems demonstrate their potential to revolutionize industries. By examining real-world applications, we can see how these systems address practical challenges and drive innovation. Their ability to operate effectively across varying scales and environments has already led to significant changes in numerous sectors. This section highlights examples where theoretical concepts and practical considerations converge to produce tangible, impactful results.\n\n1.9.1 FarmBeats: ML in Agriculture\nFarmBeats, a project developed by Microsoft Research, shown in Figure 1.5 is a significant advancement in the application of machine learning to agriculture. This system aims to increase farm productivity and reduce costs by leveraging AI and IoT technologies. FarmBeats exemplifies how edge and embedded ML systems can be deployed in challenging, real-world environments to solve practical problems. By bringing ML capabilities directly to the farm, FarmBeats demonstrates the potential of distributed AI systems in transforming traditional industries.\n\n\n\n\n\n\nFigure 1.5: Microsoft FarmBeats: AI, Edge & IoT for Agriculture.\n\n\n\n\nData Considerations\nThe data ecosystem in FarmBeats is diverse and distributed. Sensors deployed across fields collect real-time data on soil moisture, temperature, and nutrient levels. Drones equipped with multispectral cameras capture high-resolution imagery of crops, providing insights into plant health and growth patterns. Weather stations contribute local climate data, while historical farming records offer context for long-term trends. The challenge lies not just in collecting this heterogeneous data, but in managing its flow from dispersed, often remote locations with limited connectivity. FarmBeats employs innovative data transmission techniques, such as using TV white spaces (unused broadcasting frequencies) to extend internet connectivity to far-flung sensors. This approach to data collection and transmission embodies the principles of edge computing we discussed earlier, where data processing begins at the source to reduce bandwidth requirements and enable real-time decision making.\n\n\nAlgorithmic Considerations\nFarmBeats uses a variety of ML algorithms tailored to agricultural applications. For soil moisture prediction, it uses temporal neural networks that can capture the complex dynamics of water movement in soil. Computer vision algorithms process drone imagery to detect crop stress, pest infestations, and yield estimates. These models must be robust to noisy data and capable of operating with limited computational resources. Machine learning methods such as “transfer learning” allow models to learn on data-rich farms to be adapted for use in areas with limited historical data. The system also incorporates a mixture of methods that combine outputs from multiple algorithms to improve prediction accuracy and reliability. A key challenge FarmBeats addresses is model personalization, adapting general models to the specific conditions of individual farms. These conditions may include unique soil compositions, microclimates, and farming practices.\n\n\nInfrastructure Considerations\nFarmBeats exemplifies the edge computing paradigm we explored in our discussion of the ML system spectrum. At the lowest level, embedded ML models run directly on IoT devices and sensors, performing basic data filtering and anomaly detection. Edge devices, such as ruggedized field gateways, aggregate data from multiple sensors and run more complex models for local decision-making. These edge devices operate in challenging conditions, requiring robust hardware designs and efficient power management to function reliably in remote agricultural settings. The system employs a hierarchical architecture, with more computationally intensive tasks offloaded to on-premises servers or the cloud. This tiered approach allows FarmBeats to balance the need for real-time processing with the benefits of centralized data analysis and model training. The infrastructure also includes mechanisms for over-the-air model updates, ensuring that edge devices can receive improved models as more data becomes available and algorithms are refined.\n\n\nFuture Implications\nFarmBeats shows how ML systems can be deployed in resource-constrained, real-world environments to drive significant improvements in traditional industries. By providing farmers with AI-driven insights, the system has shown potential to increase crop yields, reduce water usage, and optimize resource allocation. Looking forward, the FarmBeats approach could be extended to address global challenges in food security and sustainable agriculture. The success of this system also highlights the growing importance of edge and embedded ML in IoT applications, where bringing intelligence closer to the data source can lead to more responsive, efficient, and scalable solutions. As edge computing capabilities continue to advance, we can expect to see similar distributed ML architectures applied to other domains, from smart cities to environmental monitoring.\n\n\n\n1.9.2 AlphaFold: Scientific ML\nAlphaFold, developed by DeepMind, is a landmark achievement in the application of machine learning to complex scientific problems. This AI system is designed to predict the three-dimensional structure of proteins, as shown in Figure 1.6, from their amino acid sequences, a challenge known as the “protein folding problem” that has puzzled scientists for decades. AlphaFold’s success demonstrates how large-scale ML systems can accelerate scientific discovery and potentially revolutionize fields like structural biology and drug design. This case study exemplifies the use of advanced ML techniques and massive computational resources to tackle problems at the frontiers of science.\n\n\n\n\n\n\nFigure 1.6: Examples of protein targets within the free modeling category. Source: Google DeepMind\n\n\n\n\nData Considerations\nThe data underpinning AlphaFold’s success is vast and multifaceted. The primary dataset is the Protein Data Bank (PDB), which contains the experimentally determined structures of over 180,000 proteins. This is complemented by databases of protein sequences, which number in the hundreds of millions. AlphaFold also utilizes evolutionary data in the form of multiple sequence alignments (MSAs), which provide insights into the conservation patterns of amino acids across related proteins. The challenge lies not just in the volume of data, but in its quality and representation. Experimental protein structures can contain errors or be incomplete, requiring sophisticated data cleaning and validation processes. Moreover, the representation of protein structures and sequences in a form amenable to machine learning is a significant challenge in itself. AlphaFold’s data pipeline involves complex preprocessing steps to convert raw sequence and structural data into meaningful features that capture the physical and chemical properties relevant to protein folding.\n\n\nAlgorithmic Considerations\nAlphaFold’s algorithmic approach represents a tour de force in the application of deep learning to scientific problems. At its core, AlphaFold uses a novel neural network architecture that combines with techniques from computational biology. The model learns to predict inter-residue distances and torsion angles, which are then used to construct a full 3D protein structure. A key innovation is the use of “equivariant attention” layers that respect the symmetries inherent in protein structures. The learning process involves multiple stages, including initial “pretraining” on a large corpus of protein sequences, followed by fine-tuning on known structures. AlphaFold also incorporates domain knowledge in the form of physics-based constraints and scoring functions, creating a hybrid system that leverages both data-driven learning and scientific prior knowledge. The model’s ability to generate accurate confidence estimates for its predictions is crucial, allowing researchers to assess the reliability of the predicted structures.\n\n\nInfrastructure Considerations\nThe computational demands of AlphaFold epitomize the challenges of large-scale scientific ML systems. Training the model requires massive parallel computing resources, leveraging clusters of GPUs or TPUs (Tensor Processing Units) in a distributed computing environment. DeepMind utilized Google’s cloud infrastructure, with the final version of AlphaFold trained on 128 TPUv3 cores for several weeks. The inference process, while less computationally intensive than training, still requires significant resources, especially when predicting structures for large proteins or processing many proteins in parallel. To make AlphaFold more accessible to the scientific community, DeepMind has collaborated with the European Bioinformatics Institute to create a public database of predicted protein structures, which itself represents a substantial computing and data management challenge. This infrastructure allows researchers worldwide to access AlphaFold’s predictions without needing to run the model themselves, demonstrating how centralized, high-performance computing resources can be leveraged to democratize access to advanced ML capabilities.\n\n\nFuture Implications\nAlphaFold’s impact on structural biology has been profound, with the potential to accelerate research in areas ranging from fundamental biology to drug discovery. By providing accurate structural predictions for proteins that have resisted experimental methods, AlphaFold opens new avenues for understanding disease mechanisms and designing targeted therapies. The success of AlphaFold also serves as a powerful demonstration of how ML can be applied to other complex scientific problems, potentially leading to breakthroughs in fields like materials science or climate modeling. However, it also raises important questions about the role of AI in scientific discovery and the changing nature of scientific inquiry in the age of large-scale ML systems. As we look to the future, the AlphaFold approach suggests a new paradigm for scientific ML, where massive computational resources are combined with domain-specific knowledge to push the boundaries of human understanding.\n\n\n\n1.9.3 Autonomous Vehicles and ML\nWaymo, a subsidiary of Alphabet Inc., stands at the forefront of autonomous vehicle technology, representing one of the most ambitious applications of machine learning systems to date. Evolving from the Google Self-Driving Car Project initiated in 2009, Waymo’s approach to autonomous driving exemplifies how ML systems can span the entire spectrum from embedded systems to cloud infrastructure. This case study demonstrates the practical implementation of complex ML systems in a safety-critical, real-world environment, integrating real-time decision-making with long-term learning and adaptation.\n\nData Considerations\nThe data ecosystem underpinning Waymo’s technology is vast and dynamic. Each vehicle serves as a roving data center, its sensor suite, which comprises LiDAR, radar, and high-resolution cameras, generating approximately one terabyte of data per hour of driving. This real-world data is complemented by an even more extensive simulated dataset, with Waymo’s vehicles having traversed over 20 billion miles in simulation and more than 20 million miles on public roads. The challenge lies not just in the volume of data, but in its heterogeneity and the need for real-time processing. Waymo must handle both structured (e.g., GPS coordinates) and unstructured data (e.g., camera images) simultaneously. The data pipeline spans from edge processing on the vehicle itself to massive cloud-based storage and processing systems. Sophisticated data cleaning and validation processes are necessary, given the safety-critical nature of the application. Moreover, the representation of the vehicle’s environment in a form amenable to machine learning presents significant challenges, requiring complex preprocessing to convert raw sensor data into meaningful features that capture the dynamics of traffic scenarios.\n\n\nAlgorithmic Considerations\nWaymo’s ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs deep learning techniques, including convolutional neural networks, to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, leverage recurrent neural networks (RNNs) to understand temporal sequences. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate reinforcement learning or imitation learning techniques to navigate complex traffic scenarios. A key innovation in Waymo’s approach is the integration of these diverse models into a coherent system capable of real-time operation. The ML models must also be interpretable to some degree, as understanding the reasoning behind a vehicle’s decisions is vital for safety and regulatory compliance. Waymo’s learning process involves continuous refinement based on real-world driving experiences and extensive simulation, creating a feedback loop that constantly improves the system’s performance.\n\n\nInfrastructure Considerations\nThe computing infrastructure supporting Waymo’s autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs). This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google’s data centers for training models, running large-scale simulations, and performing fleet-wide learning. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo’s infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo’s operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles.\n\n\nFuture Implications\nWaymo’s impact extends beyond technological advancement, potentially revolutionizing transportation, urban planning, and numerous aspects of daily life. The launch of Waymo One, a commercial ride-hailing service using autonomous vehicles in Phoenix, Arizona, represents a significant milestone in the practical deployment of AI systems in safety-critical applications. Waymo’s progress has broader implications for the development of robust, real-world AI systems, driving innovations in sensor technology, edge computing, and AI safety that have applications far beyond the automotive industry. However, it also raises important questions about liability, ethics, and the interaction between AI systems and human society. As Waymo continues to expand its operations and explore applications in trucking and last-mile delivery, it serves as an important test bed for advanced ML systems, driving progress in areas such as continual learning, robust perception, and human-AI interaction. The Waymo case study underscores both the tremendous potential of ML systems to transform industries and the complex challenges involved in deploying AI in the real world.\n\nSelf-Check: Question 1.8\n\nWhich of the following best describes a key challenge faced by FarmBeats in deploying ML systems in agricultural environments?\n\nHigh computational power requirements\nLimited internet connectivity and data transmission\nLack of available data\nExcessive power consumption by IoT devices\n\nTrue or False: The infrastructure for Waymo’s autonomous vehicles relies solely on edge computing for real-time decision-making.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-challenges-ml-systems-f08f",
    "href": "contents/core/introduction/introduction.html#sec-introduction-challenges-ml-systems-f08f",
    "title": "1  Introduction",
    "section": "1.10 Challenges in ML Systems",
    "text": "1.10 Challenges in ML Systems\nBuilding and deploying machine learning systems presents unique challenges that go beyond traditional software development. These challenges help explain why creating effective ML systems is about more than just choosing the right algorithm or collecting enough data. Let’s explore the key areas where ML practitioners face significant hurdles.\n\n1.10.1 Data-Related Challenges\nThe foundation of any ML system is its data, and managing this data introduces several fundamental challenges. First, there’s the basic question of data quality, as real-world data is often messy and inconsistent. Imagine a healthcare application that needs to process patient records from different hospitals. Each hospital might record information differently, use different units of measurement, or have different standards for what data to collect. Some records might have missing information, while others might contain errors or inconsistencies that need to be cleaned up before the data can be useful.\nAs ML systems grow, they often need to handle increasingly large amounts of data. A video streaming service like Netflix, for example, needs to process billions of viewer interactions to power its recommendation system. This scale introduces new challenges in how to store, process, and manage such large datasets efficiently.\nAnother critical challenge is how data changes over time. This phenomenon, known as “data drift”, occurs when the patterns in new data begin to differ from the patterns the system originally learned from. For example, many predictive models struggled during the COVID-19 pandemic because consumer behavior changed so dramatically that historical patterns became less relevant. ML systems need ways to detect when this happens and adapt accordingly.\n\n\n1.10.2 Model-Related Challenges\nCreating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be extremely complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through backpropagation. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices.\nTraining these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples through techniques like transfer learning. This learning process involves many choices: How should we structure the model? How long should we train it? How can we tell if it’s learning the right things? Making these decisions often requires both technical expertise and considerable trial and error.\nA particularly important challenge is ensuring that models work well in real-world conditions. A model might perform excellently on its training data but fail when faced with slightly different situations in the real world. This gap between training performance and real-world performance is a central challenge in machine learning, especially for critical applications like autonomous vehicles or medical diagnosis systems.\n\n\n1.10.3 System-Related Challenges\nGetting ML systems to work reliably in the real world introduces its own set of challenges. Unlike traditional software that follows fixed rules, ML systems need to handle uncertainty and variability in their inputs and outputs. They also typically need both training systems (for learning from data) and serving systems (for making predictions), each with different requirements and constraints.\nConsider a company building a speech recognition system. They need infrastructure to collect and store audio data, systems to train models on this data, and then separate systems to actually process users’ speech in real-time. Each part of this pipeline needs to work reliably and efficiently, and all the parts need to work together seamlessly.\nThese systems also need constant monitoring and updating. How do we know if the system is working correctly? How do we update models without interrupting service? How do we handle errors or unexpected inputs? These operational challenges become particularly complex when ML systems are serving millions of users.\n\n\n1.10.4 Ethical Considerations\nAs ML systems become more prevalent in our daily lives, their broader impacts on society become increasingly important to consider. One major concern is fairness, as ML systems can sometimes learn to make decisions that discriminate against certain groups of people. This often happens unintentionally, as the systems pick up biases present in their training data. For example, a job application screening system might inadvertently learn to favor certain demographics if those groups were historically more likely to be hired.\nAnother important consideration is transparency. Many modern ML models, particularly deep learning models, work as “black boxes”—while they can make predictions, it’s often difficult to understand how they arrived at their decisions. This becomes particularly problematic when ML systems are making important decisions about people’s lives, such as in healthcare or financial services.\nPrivacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models don’t inadvertently memorize and reveal private information through inference attacks? These challenges aren’t merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment.\nThese challenges aren’t merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. Throughout this book, we’ll explore these challenges in detail and examine strategies for addressing them effectively.\n\nSelf-Check: Question 1.9\n\nExplain why ensuring that ML models work well in real-world conditions is a significant challenge.\nTrue or False: Ethical considerations in ML systems only concern the technical performance of the models.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-looking-ahead-532e",
    "href": "contents/core/introduction/introduction.html#sec-introduction-looking-ahead-532e",
    "title": "1  Introduction",
    "section": "1.11 Looking Ahead",
    "text": "1.11 Looking Ahead\nAs we look to the future of machine learning systems, several exciting trends are shaping the field. These developments promise to both solve existing challenges and open new possibilities for what ML systems can achieve.\nOne of the most significant trends is the democratization of AI technology. Just as personal computers transformed computing from specialized mainframes to everyday tools, ML systems are becoming more accessible to developers and organizations of all sizes. Cloud providers now offer pre-trained models and automated ML platforms that reduce the expertise needed to deploy AI solutions. This democratization is enabling new applications across industries, from small businesses using AI for customer service to researchers applying ML to previously intractable problems.\nAs concerns about computational costs and environmental impact grow, there’s an increasing focus on making ML systems more efficient. Researchers are developing new techniques for training models with less data and computing power. Innovation in specialized hardware, from improved GPUs to custom AI chips, is making ML systems faster and more energy-efficient. These advances could make sophisticated AI capabilities available on more devices, from smartphones to IoT sensors.\nPerhaps the most transformative trend is the development of more autonomous ML systems that can adapt and improve themselves. These systems are beginning to handle their own maintenance tasks, such as detecting when they need retraining, automatically finding and correcting errors, and optimizing their own performance. This automation could dramatically reduce the operational overhead of running ML systems while improving their reliability.\nWhile these trends are promising, it’s important to recognize the field’s limitations. Creating truly artificial general intelligence remains a distant goal. Current ML systems excel at specific tasks but lack the flexibility and understanding that humans take for granted. Challenges around bias, transparency, and privacy continue to require careful consideration. As ML systems become more prevalent, addressing these limitations while leveraging new capabilities will be crucial.\n\nSelf-Check: Question 1.10\n\nWhich of the following best describes the impact of AI democratization on small businesses?\n\nIncreased computational costs\nLimited access to ML technologies\nEnhanced ability to deploy AI solutions\nDecreased need for customer service\n\nTrue or False: The development of more efficient ML systems is primarily driven by the need to reduce computational costs and environmental impact.\nExplain how autonomous ML systems could reduce the operational overhead of running machine learning systems.\nThe trend towards more autonomous ML systems involves developing models that can ____ and improve themselves.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#sec-introduction-book-structure-learning-path-411f",
    "href": "contents/core/introduction/introduction.html#sec-introduction-book-structure-learning-path-411f",
    "title": "1  Introduction",
    "section": "1.12 Book Structure and Learning Path",
    "text": "1.12 Book Structure and Learning Path\nThis book is designed to guide you from understanding the fundamentals of ML systems to effectively designing and implementing them. To address the complexities and challenges of Machine Learning Systems engineering, we’ve organized the content around five fundamental pillars that encompass the lifecycle of ML systems. These pillars provide a framework for understanding, developing, and maintaining robust ML systems.\n\n\n\n\n\n\nFigure 1.7: Overview of the five fundamental system pillars of Machine Learning Systems engineering.\n\n\n\nAs illustrated in Figure 1.7, the five pillars central to the framework are:\n\nData: Emphasizing data engineering and foundational principles critical to how AI operates in relation to data.\nTraining: Exploring the methodologies for AI training, focusing on efficiency, optimization, and acceleration techniques to enhance model performance.\nDeployment: Encompassing benchmarks, on-device learning strategies, and machine learning operations to ensure effective model application.\nOperations: Highlighting the maintenance challenges unique to machine learning systems, which require specialized approaches distinct from traditional engineering systems.\nEthics & Governance: Addressing concerns such as security, privacy, responsible AI practices, and the broader societal implications of AI technologies.\n\nEach pillar represents a critical phase in the lifecycle of ML systems and is composed of foundational elements that build upon each other. This structure ensures a comprehensive understanding of MLSE, from basic principles to advanced applications and ethical considerations.\nFor more detailed information about the book’s overview, contents, learning outcomes, target audience, prerequisites, and navigation guide, please refer to the About the Book section. There, you’ll also find valuable details about our learning community and how to maximize your experience with this resource.\n\n\n\nSelf-Check: Question 1.11\n\nWhich of the following is NOT one of the five fundamental pillars of Machine Learning Systems Engineering?\n\nData\nTraining\nDeployment\nHardware Design\n\nOrder the following ML system lifecycle phases according to the book’s framework: Operations, Training, Data, Deployment, Ethics & Governance.\nExplain why the book’s structure emphasizes the interconnected nature of ML system components.\nThe ____ pillar addresses concerns such as security, privacy, responsible AI practices, and broader societal implications of AI technologies.\n\nSee Answers →",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/core/introduction/introduction.html#self-check-answers",
    "href": "contents/core/introduction/introduction.html#self-check-answers",
    "title": "1  Introduction",
    "section": "1.13 Self-Check Answers",
    "text": "1.13 Self-Check Answers\n\nSelf-Check: Answer 1.1\n\nExplain how the relationship between AI and ML is similar to the relationship between physics and mechanical engineering.\nAnswer: AI provides the theoretical frameworks that inform ML’s practical development of intelligent systems, similar to how physics provides the theoretical foundation for mechanical engineering’s practical applications in structural design and machinery.\nLearning Objective: Analyze the relationship between AI and ML in the context of theoretical and practical applications.\nTrue or False: Machine Learning systems implement intelligence through predetermined rules.\nAnswer: False. Machine Learning systems do not implement intelligence through predetermined rules; instead, they use optimization techniques like gradient descent to learn from data.\nLearning Objective: Correct misconceptions about how ML systems implement intelligence.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.2\n\nWhich AI era introduced the concept of using statistical methods to learn patterns from data rather than following pre-programmed rules?\n\nSymbolic AI Era\nExpert Systems Era\nStatistical Learning Era\nDeep Learning Era\n\nAnswer: The correct answer is C. The Statistical Learning Era introduced the use of statistical methods to learn patterns from data, marking a shift from rule-based AI to data-driven approaches.\nLearning Objective: Understand the transition to statistical learning and its significance in AI evolution.\nExplain why the transition from rule-based AI to statistical learning was significant for the development of modern machine learning systems.\nAnswer: The transition was significant because it allowed AI systems to learn from data rather than relying on hand-coded rules. This shift enabled more adaptable and robust AI, capable of handling real-world complexity and variability, laying the groundwork for modern machine learning systems.\nLearning Objective: Analyze the impact of transitioning from rule-based to statistical learning on modern ML systems.\nTrue or False: The Deep Learning Era solved all the challenges faced by previous AI paradigms.\nAnswer: False. While deep learning addressed many challenges, such as feature extraction, it introduced new systems challenges like scaling, data requirements, and computational demands.\nLearning Objective: Understand the ongoing challenges in AI despite advancements in deep learning.\nOrder the following AI eras chronologically: Expert Systems Era, Symbolic AI Era, Statistical Learning Era, Deep Learning Era.\nAnswer: Symbolic AI Era, Expert Systems Era, Statistical Learning Era, Deep Learning Era. This sequence reflects the chronological development of AI paradigms.\nLearning Objective: Recall the chronological order of AI eras to understand the historical progression of AI development.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.3\n\nMachine Learning Systems Engineering focuses on building AI systems that are reliable, efficient, and ____ across computational platforms.\nAnswer: scalable. Machine Learning Systems Engineering aims to ensure AI systems can handle increasing workloads and user demands efficiently.\nLearning Objective: Understand the core focus areas of Machine Learning Systems Engineering.\nWhich of the following best describes the role of Machine Learning Systems Engineering in AI development?\n\nDeveloping new AI algorithms\nOptimizing and deploying AI systems at scale\nCreating specialized AI hardware\nFocusing solely on data acquisition\n\nAnswer: The correct answer is B. Machine Learning Systems Engineering focuses on optimizing and deploying AI systems at scale, bridging the gap between theoretical algorithms and practical implementation.\nLearning Objective: Identify the primary role of Machine Learning Systems Engineering in the AI lifecycle.\nExplain why Machine Learning Systems Engineering is necessary for the practical implementation of AI systems.\nAnswer: Machine Learning Systems Engineering is necessary because it addresses the challenges of deploying AI systems efficiently and reliably. It ensures that AI algorithms can be integrated into real-world applications, handling data acquisition, system optimization, and scalability across various platforms.\nLearning Objective: Analyze the necessity of Machine Learning Systems Engineering for deploying AI systems in real-world scenarios.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.4\n\nWhich of the following best describes the core components of a machine learning system as defined in this textbook?\n\nAlgorithms, Data, and Computing Infrastructure\nModels, Sensors, and Networking\nData, User Interfaces, and Algorithms\nHardware, Software, and User Experience\n\nAnswer: The correct answer is A. The textbook defines a machine learning system as comprising Algorithms, Data, and Computing Infrastructure, emphasizing the interdependency of these components.\nLearning Objective: Understand the core components of a machine learning system as defined in the textbook.\nExplain how the interdependency of algorithms, data, and computing infrastructure shapes the design and capabilities of a machine learning system.\nAnswer: The interdependency means that each component influences and limits the others. Algorithms dictate computational and data requirements, data scale affects infrastructure needs and model feasibility, and infrastructure sets practical limits on model and data processing. This creates a framework where all components must align for effective system operation.\nLearning Objective: Analyze the interdependent relationships among the components of a machine learning system.\nIn the analogy to space exploration, algorithm developers are likened to ____, who explore new frontiers and make discoveries.\nAnswer: astronauts. This analogy highlights the role of algorithm developers in exploring new possibilities and making discoveries within the ML system.\nLearning Objective: Apply the space exploration analogy to understand the roles within a machine learning system.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.5\n\nOrder the following stages of the machine learning system lifecycle: Model Deployment, Data Collection, Model Training, Model Evaluation, Data Preparation, Model Monitoring.\nAnswer: Data Collection, Data Preparation, Model Training, Model Evaluation, Model Deployment, Model Monitoring. This sequence reflects the typical progression of tasks required to develop, evaluate, and deploy a machine learning model, followed by monitoring its performance.\nLearning Objective: Understand the sequence and purpose of lifecycle stages in ML systems.\nWhich of the following best describes a challenge unique to machine learning systems compared to traditional software systems?\n\nVersion control of source code\nTesting deterministic outputs\nManaging evolving datasets\nAutomating deployment processes\n\nAnswer: The correct answer is C. Managing evolving datasets. Unlike traditional software, ML systems rely on data that can change over time, affecting system behavior and requiring continuous adaptation.\nLearning Objective: Identify challenges faced by ML systems due to their data-dependent nature.\nExplain how the interconnected stages of the ML lifecycle can lead to either virtuous or vicious cycles.\nAnswer: In a virtuous cycle, high-quality data leads to effective learning, robust infrastructure supports processing, and well-engineered systems improve data collection, enhancing the entire lifecycle. Conversely, in a vicious cycle, poor data quality undermines learning, inadequate infrastructure hampers processing, and system limitations degrade data collection, compounding issues.\nLearning Objective: Analyze the impact of interconnected lifecycle stages on ML system performance.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.6\n\nWhich of the following best describes a tradeoff faced by cloud-based ML systems?\n\nLimited computing resources\nHigh operational complexity and costs\nSevere constraints on memory and power\nInability to integrate with existing infrastructure\n\nAnswer: The correct answer is B. Cloud-based ML systems, while having virtually unlimited computing resources, must manage enormous operational complexity and costs due to their scale and the volume of data processed.\nLearning Objective: Understand the tradeoffs and challenges faced by cloud-based ML systems.\nTrue or False: TinyML systems are designed to operate with the same resource availability as cloud-based ML systems.\nAnswer: False. TinyML systems operate with severe constraints on memory, computing power, and energy consumption, unlike cloud-based systems that have access to vast resources.\nLearning Objective: Recognize the resource constraints specific to TinyML systems compared to cloud-based systems.\nExplain how edge ML systems can reduce latency and bandwidth requirements.\nAnswer: Edge ML systems bring computation closer to the data sources, which reduces the need to send data to centralized cloud servers. This proximity minimizes latency and decreases bandwidth usage, as data processing occurs locally, improving response times and reducing network load.\nLearning Objective: Analyze how edge ML systems optimize latency and bandwidth by processing data closer to its source.\nMobile ML systems must balance sophisticated capabilities with ____ life and processor limitations.\nAnswer: battery. Mobile ML systems need to provide advanced functionalities while managing the limited battery life and processing power available on mobile devices.\nLearning Objective: Identify the specific constraints that mobile ML systems must manage.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.7\n\nWhich architectural choice is most likely to be driven by latency-sensitive applications such as autonomous vehicles?\n\nCentralized cloud architecture\nEdge or embedded architecture\nHybrid architecture\nMobile architecture\n\nAnswer: The correct answer is B. Edge or embedded architecture is often chosen for latency-sensitive applications like autonomous vehicles because it allows for real-time processing close to the data source, reducing latency.\nLearning Objective: Understand the impact of latency requirements on architectural decisions in ML systems.\nExplain how resource management differs between cloud and edge ML systems and the implications for system design.\nAnswer: Cloud systems focus on cost efficiency and scalability, managing large-scale resources like GPU clusters and storage. Edge systems operate under fixed resource limits, requiring careful management of local compute and storage. This influences model design, as cloud systems can afford larger models, while edge systems prioritize efficiency and compactness.\nLearning Objective: Analyze how different resource management strategies affect ML system design and operation.\nIn a distributed ML system, ____ complexity increases with the number of system components and their interactions.\nAnswer: operational. Operational complexity increases as more components and interactions are introduced, requiring careful management throughout the ML lifecycle.\nLearning Objective: Identify the factors contributing to operational complexity in distributed ML systems.\nTrue or False: The evolution and maintenance of ML systems are easier in edge architectures compared to cloud architectures.\nAnswer: False. Cloud architectures offer more flexibility for system evolution and maintenance due to their centralized nature and access to mature deployment tools, whereas edge architectures may face challenges in updating and maintaining distributed components.\nLearning Objective: Evaluate the challenges of maintaining and evolving ML systems across different architectures.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.8\n\nWhich of the following best describes a key challenge faced by FarmBeats in deploying ML systems in agricultural environments?\n\nHigh computational power requirements\nLimited internet connectivity and data transmission\nLack of available data\nExcessive power consumption by IoT devices\n\nAnswer: The correct answer is B. Limited internet connectivity and data transmission. FarmBeats addresses the challenge of limited connectivity by using innovative data transmission techniques like TV white spaces to extend internet connectivity to remote sensors.\nLearning Objective: Understand the data transmission challenges and solutions in deploying ML systems in remote environments.\nTrue or False: The infrastructure for Waymo’s autonomous vehicles relies solely on edge computing for real-time decision-making.\nAnswer: False. While Waymo’s vehicles use edge computing for real-time decision-making, they also rely on cloud infrastructure for model training, large-scale simulations, and fleet-wide learning. This hybrid approach ensures robust and scalable operations.\nLearning Objective: Understand the hybrid infrastructure model combining edge and cloud computing in autonomous vehicle systems.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.9\n\nExplain why ensuring that ML models work well in real-world conditions is a significant challenge.\nAnswer: Ensuring ML models work well in real-world conditions is challenging because models may perform well on training data but fail in different real-world scenarios. This performance gap is critical in applications like autonomous vehicles or medical diagnosis, where model errors can have serious consequences. Adapting models to handle real-world variability and uncertainty is essential for reliable deployment.\nLearning Objective: Analyze the challenges of deploying ML models in real-world scenarios and their implications.\nTrue or False: Ethical considerations in ML systems only concern the technical performance of the models.\nAnswer: False. Ethical considerations in ML systems extend beyond technical performance to include issues like fairness, transparency, and privacy. These considerations shape how ML systems are designed and deployed, ensuring they do not inadvertently discriminate or violate privacy.\nLearning Objective: Recognize the scope of ethical considerations in ML systems beyond technical performance.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.10\n\nWhich of the following best describes the impact of AI democratization on small businesses?\n\nIncreased computational costs\nLimited access to ML technologies\nEnhanced ability to deploy AI solutions\nDecreased need for customer service\n\nAnswer: The correct answer is C. Enhanced ability to deploy AI solutions. AI democratization enables small businesses to access pre-trained models and automated ML platforms, allowing them to implement AI solutions without requiring extensive expertise.\nLearning Objective: Understand the impact of AI democratization on various industries, particularly small businesses.\nTrue or False: The development of more efficient ML systems is primarily driven by the need to reduce computational costs and environmental impact.\nAnswer: True. The push for more efficient ML systems is largely motivated by the desire to minimize computational expenses and environmental consequences, making AI technologies more sustainable and accessible.\nLearning Objective: Recognize the motivations behind efforts to improve the efficiency of ML systems.\nExplain how autonomous ML systems could reduce the operational overhead of running machine learning systems.\nAnswer: Autonomous ML systems can self-manage tasks such as retraining, error correction, and performance optimization, reducing the need for human intervention. This automation decreases the operational burden and enhances system reliability, allowing organizations to focus resources on other strategic areas.\nLearning Objective: Analyze the potential operational benefits of autonomous ML systems.\nThe trend towards more autonomous ML systems involves developing models that can ____ and improve themselves.\nAnswer: adapt. Autonomous ML systems are designed to adapt and improve themselves by handling maintenance tasks and optimizing performance, which reduces the need for manual intervention.\nLearning Objective: Understand the concept of autonomous ML systems and their self-improvement capabilities.\n\n← Back to Questions\n\n\n\nSelf-Check: Answer 1.11\n\nWhich of the following is NOT one of the five fundamental pillars of Machine Learning Systems Engineering?\n\nData\nTraining\nDeployment\nHardware Design\n\nAnswer: The correct answer is D. Hardware Design. The five pillars are Data, Training, Deployment, Operations, and Ethics & Governance. Hardware Design is not a separate pillar but is covered within other pillars.\nLearning Objective: Recall the five fundamental pillars of ML Systems Engineering as outlined in the book.\nOrder the following ML system lifecycle phases according to the book’s framework: Operations, Training, Data, Deployment, Ethics & Governance.\nAnswer: Data, Training, Deployment, Operations, Ethics & Governance. This sequence reflects the typical progression of ML system development and the ongoing considerations throughout the lifecycle.\nLearning Objective: Understand the logical progression of the five pillars in the ML system lifecycle.\nExplain why the book’s structure emphasizes the interconnected nature of ML system components.\nAnswer: The book emphasizes interconnectedness because ML systems require all components (data, algorithms, infrastructure) to work together effectively. Changes in one component affect others, and successful ML systems require understanding these interdependencies to make informed design decisions.\nLearning Objective: Analyze the importance of understanding component interdependencies in ML systems.\nThe ____ pillar addresses concerns such as security, privacy, responsible AI practices, and broader societal implications of AI technologies.\nAnswer: Ethics & Governance. This pillar ensures that ML systems are developed and deployed responsibly, considering their impact on society and individuals.\nLearning Objective: Identify the pillar responsible for ethical considerations in ML systems.\n\n← Back to Questions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]